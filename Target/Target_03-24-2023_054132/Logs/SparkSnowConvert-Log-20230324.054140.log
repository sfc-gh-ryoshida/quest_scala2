[03/24/2023 05:41:40] Debug: Session id: 589b8506-ce73-40ee-9bc0-d938649a9163
[03/24/2023 05:41:40] Debug: Execution id: 1def2910-44ff-48e6-879f-88d17005e59a
[03/24/2023 05:41:40] Info: Starting to analyze the input path '/tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources'...
[03/24/2023 05:41:40] Info: Found 186 SCALA files with a total size of 0.73MB
[03/24/2023 05:41:40] Info: Found 0 SCALA files
[03/24/2023 05:41:40] Info: Found 0 GRADLE files
[03/24/2023 05:41:40] Info: Found 0 SBT files
[03/24/2023 05:41:40] Info: Found 0 MAVEN files
[03/24/2023 05:41:40] Info: Found 71 TOTAL files with a total size of 0.21MB
[03/24/2023 05:41:40] Info: Starting process of 257 files with 0.94MB of code.
[03/24/2023 05:41:42] Info: EFProcess 638152333022131823 started
[03/24/2023 05:41:42] Debug: TaskParam customMapDirectoryPath = 
[03/24/2023 05:41:42] Debug: TaskParam mappingDictionaries = Mobilize.SparkCommon.TransformationCore.MappingDictionaries
[03/24/2023 05:41:42] Debug: TaskParam LoadMappingsTask.Enabled = True
[03/24/2023 05:41:42] Debug: TaskParam Repository = Value is not created.
[03/24/2023 05:41:42] Debug: TaskParam RepositoryDirectory = /tmp/Output-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/.mobilize/CommonEF
[03/24/2023 05:41:42] Debug: TaskParam codeModelWriter = Mobilize.Common.AssessmentModel.Writer.CodeModelWriter
[03/24/2023 05:41:42] Debug: TaskParam dependenciesToAdd = libs/snowparkextensions-0.0.9.jar, com.snowflake:snowpark:1.6.0
[03/24/2023 05:41:42] Debug: TaskParam dependenciesToRemove = org.apache.spark
[03/24/2023 05:41:42] Debug: TaskParam excludedProjectFiles = *.xml, *.png, glacial.mobilize, *.scala, .wspark
[03/24/2023 05:41:42] Debug: TaskParam ExecutionModeValue = Conversion
[03/24/2023 05:41:42] Debug: TaskParam inputPath = /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources
[03/24/2023 05:41:42] Debug: TaskParam outputPath = /tmp/Output-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Output/SparkSnowConvert
[03/24/2023 05:41:42] Debug: TaskParam projectId = 
[03/24/2023 05:41:42] Debug: TaskParam ProjectGenerationTask.Enabled = True
[03/24/2023 05:41:42] Debug: TaskParam Repository = Value is not created.
[03/24/2023 05:41:42] Debug: TaskParam RepositoryDirectory = /tmp/Output-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/.mobilize/CommonEF
[03/24/2023 05:41:42] Info: Step 2/9 - Generating project: STARTED
[03/24/2023 05:41:42] Info: Step 2/9 - Generating project: COMPLETED
[03/24/2023 05:41:42] Debug: TaskParam parsedFilesCount = 0
[03/24/2023 05:41:42] Debug: TaskParam parsingProgressDescriptor = Mobilize.Common.Utils.Progress.SingleProgressDescriptor
[03/24/2023 05:41:42] Debug: TaskParam processingProgressDescriptor = Mobilize.Common.Utils.Progress.SingleProgressDescriptor
[03/24/2023 05:41:42] Debug: TaskParam projectId = 
[03/24/2023 05:41:42] Debug: TaskParam inputPath = /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources
[03/24/2023 05:41:42] Debug: TaskParam filesToProcess = 
[03/24/2023 05:41:42] Debug: TaskParam transformationVisitors = Value is not created.
[03/24/2023 05:41:42] Debug: TaskParam ConversionBeginTask.Input = /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/input.wsp
[03/24/2023 05:41:42] Debug: TaskParam workingSet = Value is not created.
[03/24/2023 05:41:42] Debug: TaskParam ConversionBeginTask.ItemMedatada = Artinsoft.Common.Store.ItemMetadata
[03/24/2023 05:41:42] Debug: TaskParam ConversionBeginTask.Enabled = True
[03/24/2023 05:41:42] Debug: TaskParam Repository = Value is not created.
[03/24/2023 05:41:42] Debug: TaskParam RepositoryDirectory = /tmp/Output-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/.mobilize/CommonEF
[03/24/2023 05:41:42] Info: Step 3/9 - Loading Code: STARTED
[03/24/2023 05:41:42] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/KanaConverter.scala
[03/24/2023 05:41:42] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/ResourceInfo.scala
[03/24/2023 05:41:42] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/ResourceDates.scala
[03/24/2023 05:41:42] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/SparkApp.scala
[03/24/2023 05:41:42] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/PostCodeNormalizer.scala
[03/24/2023 05:41:42] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/MD5Utils.scala
[03/24/2023 05:41:42] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/Logging.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/DateConverter.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/InputArgs.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/ResourceRunningEnvs.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/JefConverter.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/MakeDate.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/Udfs.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/spark/common/FileCtl.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/spark/common/SparkContexts.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/spark/common/PqCtl.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/spark/common/Logging.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/spark/common/DfCtl.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/InputArgsTest.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/DateConverterTest.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/MakeResourceTest.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/SparkAppTest.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/spark/common/DbCtl.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/PostCodeNormalizerTest.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/JefConverterTest.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/TestArgs.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/KanaConverterTest.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/HiRDBTest.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/spark/common/PqCtlTest.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/UdfsTest.scala
[03/24/2023 05:41:43] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/spark/common/DfCtlTest.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/spark/common/FileCtlTest.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/spark/common/DbCtlWithHintTest.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/spark/common/LargeInsertTest.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/spark/common/DbInfoTest.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/parser/ComponentDefParser.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/MakeResource.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/parser/ItemDefParser.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/parser/ComponentFlowParser.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/parser/AppDefParser.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/parser/D2kParser.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/ResourceItemRouteFinder.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/ResourceRelationFinder.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/ItemRenameRouteFinder.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/GeneratingApplicationRouteFinder.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/ItemReferenceFinder.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/appErrorDetect.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/WritePq.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/CommonServices.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/SingleReadDb.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/SingleReadPq.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/ReadPq.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/InputInfo.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/MultiReadPq.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/WriteFile.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/MultiReadDb.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/Commons.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/ReadDb.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/Executor.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/ReadFile.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/WriteDb.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/fileConv/ConfParser.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/fileConv/TextConverter.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/fileConv/FileConv.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/fileConv/DomainProcessor.scala
[03/24/2023 05:41:44] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/fileConv/FixedConverter.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/appdefdoc/parser/ComponentFlowParserTest.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/WriteFilePartitionTest.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/fileConv/Converter.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/spark/common/DbCtlTest.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/HiRDB_readTest.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/DbConnectionInfoTest.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/WriteFileTest.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/WritePqTest.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/ReadPqTest.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/ReadDbTest.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/fileConv/FileConvPartition1Test.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/fileConv/BinaryRecordTest.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/fileConv/DomainProcessorTest.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/fileConv/DomainProcessorJefTest.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/fileConv/FileConvPartition2Test.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/fileConv/ConfParserTest.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/fileConv/FileConvTest.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/test/TestGenerator.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/test/GenerateTestCase.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/WriteDbTest.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/appdef/ApplicationDefGenerator.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/tmpl/TemplateCatalogGenerator.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/sql/SqlDefParser.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/sql/SqlGenerator.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/sql/SqlLogicParser.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/dic/DictionaryGenerator.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/src/ItemReplace.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/dic/GenerateDictionary.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/src/FlowLogicGenerator.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/rc/RoughConceptGenerator.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/jsonbase/ResourceItemRouteFinder.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/src/SourceGenerator.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/jsonbase/ResourceRelationFinder.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/jsonbase/ItemRenameRouteFinder.scala
[03/24/2023 05:41:45] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/jsonbase/GeneratingApplicationRouteFinder.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/jsonbase/ItemReferenceFinder.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/app/test/common/MakeDf.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/app/test/common/MarkdownTester.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/app/test/common/TestArgs.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/app/test/common/ExcelConverter.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/file/output/VariableFile.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/file/output/FixedFileWithConfFile.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/app/test/common/D2kTest_.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/mixIn/OraLoaderHdfs.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/mixIn/OraLoader.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/executor/ConvNa.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/executor/BinaryRecordConverter.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/executor/Nothing_.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/executor/PqCommonColumnRemover.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/executor/DbOutputCommonFunctions.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/executor/RowErrorRemover.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/DfJoinVariableToDf.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/PqToXxx.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/DfJoinToDf.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/FileToXxx.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/MultiDbToMapDf.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/MultiPqToMapDf.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/DfToXxx.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/DfUnionToDf.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/PqJoinToPq.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/DbToXxx.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/DfJoinPqToDf.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/flow/OneInToMapOutForDf.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/flow/OneInToOneOutForDf.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/flow/TwoInToOneOutForDf.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/org/apache/spark/sql/JdbcCtl.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/appdefdoc/gen/test/GenerateTestCaseTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/appdefdoc/gen/dic/DictionaryGeneratorTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/appdefdoc/gen/src/SourceGeneratorTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/app/test/common/TestToolsTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/file/output/FixedFileTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/file/output/FixedFile.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/mixIn/OraLoaderHdfsTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/mixIn/OraLoaderTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/executor/PqCommonColumnRemoverTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/executor/NothingTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/executor/ConvNaTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/template/DfUnionToDfTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/template/FileToXxxTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/template/FileToPq_DbTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/file/output/VariableFileTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/template/DfToFileTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/template/FileToDf_UTF8Test.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/template/DfJoinPqToDfTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/org/apache/spark/sql/JdbcCtlTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/component/cmn/PostCodeConverter.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/component/sh/CommissionBaseChannelSelector.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/executor/face/DomainConverter.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/sh/CommissionBaseChannelSelectorTmpl.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/FileToAny.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/AnyToDb.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/MultiAnyToMapDf.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/DfJoinMultiPqToAny.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/template/DfJoinVariableToDfTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/AnyToPq_Db.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/MultiPqToMultiAny.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/AnyToPq.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/TwoAnyToDf.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/AnyToVal.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/DfJoinVariableToAny.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/TwoDfUnionToAny.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/MultiDbToMultiAny.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/TwoAnyToPq.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/AnyToDf.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/PqToAny.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/DbToAny.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/DfToAny.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/TwoAnyToDb.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/AnyToFile.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/TwoPqJoinToAny.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/flow/base/TwoInToOneOut.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/flow/base/OneInToMapOut.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/flow/base/OneInToOneOut.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/component/cmn/PostCodeConverterTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/component/sh/CommissionBaseChannelSelectorTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/executor/face/DomainConvereterTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/template/base/DfUnionToDfTest.scala
[03/24/2023 05:41:46] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/TwoDfJoinToAny.scala
[03/24/2023 05:41:54] Debug: Processing File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/src/ConvertTemplateDefine.scala
[03/24/2023 05:41:54] Info: Step 3/9 - Loading Code: COMPLETED
[03/24/2023 05:41:54] Debug: TaskParam codeModelWriter = Mobilize.Common.AssessmentModel.Writer.CodeModelWriter
[03/24/2023 05:41:54] Debug: TaskParam inputPath = /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources
[03/24/2023 05:41:54] Debug: TaskParam outputPath = /tmp/Output-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Output/SparkSnowConvert
[03/24/2023 05:41:54] Debug: TaskParam projectId = Sources
[03/24/2023 05:41:54] Debug: TaskParam ExecutionModeValue = Conversion
[03/24/2023 05:41:54] Debug: TaskParam ParsingErrorsTask.Input = /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/input.wsp
[03/24/2023 05:41:54] Debug: TaskParam workingSet = Artinsoft.Common.Store.RepositoryWorkingSet`2[System.String,Artinsoft.Common.Store.IItemContainer]
[03/24/2023 05:41:54] Debug: TaskParam ParsingErrorsTask.ItemMedatada = Artinsoft.Common.Store.ItemMetadata
[03/24/2023 05:41:54] Debug: TaskParam ParsingErrorsTask.Enabled = True
[03/24/2023 05:41:54] Debug: TaskParam Repository = Artinsoft.Common.Store.Repository
[03/24/2023 05:41:54] Debug: TaskParam RepositoryDirectory = /tmp/Output-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/.mobilize/CommonEF
[03/24/2023 05:41:54] Info: Step 4/9 - Parsing Errors: STARTED
[03/24/2023 05:41:54] Info: Register EWI - IssueCode: SPRKSCL1001 IssueDescription: This code section has parsing errors, so it was commented out
[03/24/2023 05:41:54] Error: File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/src/ConvertTemplateDefine.scala has parsing errors
[03/24/2023 05:41:54] Info: Register EWI - IssueCode: SPRKSCL1001 IssueDescription: This code section has parsing errors, so it was commented out
[03/24/2023 05:41:54] Error: File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/src/ItemReplace.scala has parsing errors
[03/24/2023 05:41:54] Info: Register EWI - IssueCode: SPRKSCL1001 IssueDescription: This code section has parsing errors, so it was commented out
[03/24/2023 05:41:54] Error: File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/parser/ComponentDefParser.scala has parsing errors
[03/24/2023 05:41:54] Info: Register EWI - IssueCode: SPRKSCL1001 IssueDescription: This code section has parsing errors, so it was commented out
[03/24/2023 05:41:54] Error: File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/file/output/VariableFile.scala has parsing errors
[03/24/2023 05:41:54] Info: Register EWI - IssueCode: SPRKSCL1001 IssueDescription: This code section has parsing errors, so it was commented out
[03/24/2023 05:41:54] Error: File: /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/flow/OneInToMapOutForDf.scala has parsing errors
[03/24/2023 05:41:54] Info: Step 4/9 - Parsing Errors: COMPLETED
[03/24/2023 05:41:54] Debug: TaskParam filesToProcess = /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/KanaConverter.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/ResourceInfo.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/ResourceDates.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/SparkApp.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/PostCodeNormalizer.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/MD5Utils.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/Logging.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/DateConverter.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/InputArgs.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/ResourceRunningEnvs.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/JefConverter.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/MakeDate.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/Udfs.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/spark/common/FileCtl.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/spark/common/SparkContexts.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/spark/common/PqCtl.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/spark/common/Logging.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/spark/common/DfCtl.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/InputArgsTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/DateConverterTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/MakeResourceTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/SparkAppTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/spark/common/DbCtl.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/PostCodeNormalizerTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/JefConverterTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/TestArgs.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/KanaConverterTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/HiRDBTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/spark/common/PqCtlTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/UdfsTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/spark/common/DfCtlTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/spark/common/FileCtlTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/spark/common/DbCtlWithHintTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/spark/common/LargeInsertTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/spark/common/DbInfoTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/parser/ComponentDefParser.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/MakeResource.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/parser/ItemDefParser.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/parser/ComponentFlowParser.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/parser/AppDefParser.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/parser/D2kParser.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/ResourceItemRouteFinder.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/ResourceRelationFinder.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/ItemRenameRouteFinder.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/GeneratingApplicationRouteFinder.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/ItemReferenceFinder.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/appErrorDetect.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/WritePq.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/CommonServices.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/SingleReadDb.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/SingleReadPq.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/ReadPq.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/InputInfo.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/MultiReadPq.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/WriteFile.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/MultiReadDb.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/Commons.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/ReadDb.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/Executor.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/ReadFile.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/WriteDb.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/fileConv/ConfParser.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/fileConv/TextConverter.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/fileConv/FileConv.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/fileConv/DomainProcessor.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/fileConv/FixedConverter.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/appdefdoc/parser/ComponentFlowParserTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/WriteFilePartitionTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/fileConv/Converter.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/spark/common/DbCtlTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/HiRDB_readTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/DbConnectionInfoTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/WriteFileTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/WritePqTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/ReadPqTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/ReadDbTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/fileConv/FileConvPartition1Test.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/fileConv/BinaryRecordTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/fileConv/DomainProcessorTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/fileConv/DomainProcessorJefTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/fileConv/FileConvPartition2Test.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/fileConv/ConfParserTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/fileConv/FileConvTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/test/TestGenerator.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/test/GenerateTestCase.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/WriteDbTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/appdef/ApplicationDefGenerator.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/tmpl/TemplateCatalogGenerator.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/sql/SqlDefParser.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/sql/SqlGenerator.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/sql/SqlLogicParser.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/dic/DictionaryGenerator.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/src/ItemReplace.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/dic/GenerateDictionary.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/src/FlowLogicGenerator.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/rc/RoughConceptGenerator.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/jsonbase/ResourceItemRouteFinder.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/src/SourceGenerator.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/jsonbase/ResourceRelationFinder.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/jsonbase/ItemRenameRouteFinder.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/jsonbase/GeneratingApplicationRouteFinder.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/jsonbase/ItemReferenceFinder.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/app/test/common/MakeDf.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/app/test/common/MarkdownTester.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/app/test/common/TestArgs.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/app/test/common/ExcelConverter.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/file/output/VariableFile.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/file/output/FixedFileWithConfFile.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/app/test/common/D2kTest_.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/mixIn/OraLoaderHdfs.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/mixIn/OraLoader.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/executor/ConvNa.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/executor/BinaryRecordConverter.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/executor/Nothing_.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/executor/PqCommonColumnRemover.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/executor/DbOutputCommonFunctions.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/executor/RowErrorRemover.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/DfJoinVariableToDf.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/PqToXxx.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/DfJoinToDf.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/FileToXxx.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/MultiDbToMapDf.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/MultiPqToMapDf.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/DfToXxx.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/DfUnionToDf.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/PqJoinToPq.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/DbToXxx.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/DfJoinPqToDf.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/flow/OneInToMapOutForDf.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/flow/OneInToOneOutForDf.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/flow/TwoInToOneOutForDf.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/org/apache/spark/sql/JdbcCtl.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/appdefdoc/gen/test/GenerateTestCaseTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/appdefdoc/gen/dic/DictionaryGeneratorTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/appdefdoc/gen/src/SourceGeneratorTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/app/test/common/TestToolsTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/file/output/FixedFileTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/file/output/FixedFile.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/mixIn/OraLoaderHdfsTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/mixIn/OraLoaderTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/executor/PqCommonColumnRemoverTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/executor/NothingTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/executor/ConvNaTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/template/DfUnionToDfTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/template/FileToXxxTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/template/FileToPq_DbTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/file/output/VariableFileTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/template/DfToFileTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/template/FileToDf_UTF8Test.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/template/DfJoinPqToDfTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/org/apache/spark/sql/JdbcCtlTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/component/cmn/PostCodeConverter.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/component/sh/CommissionBaseChannelSelector.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/executor/face/DomainConverter.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/sh/CommissionBaseChannelSelectorTmpl.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/FileToAny.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/AnyToDb.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/MultiAnyToMapDf.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/DfJoinMultiPqToAny.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/template/DfJoinVariableToDfTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/AnyToPq_Db.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/MultiPqToMultiAny.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/AnyToPq.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/TwoAnyToDf.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/AnyToVal.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/DfJoinVariableToAny.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/TwoDfUnionToAny.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/MultiDbToMultiAny.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/TwoAnyToPq.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/AnyToDf.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/PqToAny.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/DbToAny.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/DfToAny.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/TwoAnyToDb.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/AnyToFile.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/TwoPqJoinToAny.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/flow/base/TwoInToOneOut.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/flow/base/OneInToMapOut.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/flow/base/OneInToOneOut.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/component/cmn/PostCodeConverterTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/component/sh/CommissionBaseChannelSelectorTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/executor/face/DomainConvereterTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/test/scala/d2k/common/df/template/base/DfUnionToDfTest.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/common/df/template/base/TwoDfJoinToAny.scala, /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/src/main/scala/d2k/appdefdoc/gen/src/ConvertTemplateDefine.scala
[03/24/2023 05:41:54] Debug: TaskParam projectId = Sources
[03/24/2023 05:41:54] Debug: TaskParam inputPath = /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources
[03/24/2023 05:41:54] Debug: TaskParam rootOutputPath = /tmp/Output-77c8fe1a-649e-45e2-9dc0-c8386908d0f3
[03/24/2023 05:41:54] Debug: TaskParam shouldGenerateDump = False
[03/24/2023 05:41:54] Debug: TaskParam codeModelWriter = Mobilize.Common.AssessmentModel.Writer.CodeModelWriter
[03/24/2023 05:41:54] Debug: TaskParam SymbolTableLoaderTask.Enabled = True
[03/24/2023 05:41:54] Debug: TaskParam Repository = Artinsoft.Common.Store.Repository
[03/24/2023 05:41:54] Debug: TaskParam RepositoryDirectory = /tmp/Output-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/.mobilize/CommonEF
[03/24/2023 05:41:54] Info: Step 5/9 - Symbol Table Loading: STARTED
[03/24/2023 05:41:56] Info: Step 5/9 - Loading spark symbols
[03/24/2023 05:41:59] Error: Node of type 'Mobilize.Scala.AST.SclFunDcl' is not supported
[03/24/2023 05:41:59] Error: Node of type 'Mobilize.Scala.AST.SclFunDcl' is not supported
[03/24/2023 05:42:01] Info: Step 5/9 - Done loading spark symbols
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.app.test.common

import scala.reflect.io.Path
import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import spark.common.DfCtl._
import spark.common.DfCtl.implicits._
import spark.common.SparkContexts
import SparkContexts.context.implicits._

import scala.io.Source
import org.apache.spark.sql.DataFrame
import org.scalatest.BeforeAndAfter
import d2k.common.InputArgs
import d2k.common.SparkApp
import d2k.common.df.flow.OneInToOneOutForDf
import d2k.common.df.flow.OneInToMapOutForDf
import d2k.common.df.flow.TwoInToOneOutForDf
import java.io.FileNotFoundException

trait D2kJoinTest extends D2kTest {
  val app = D2kTest.dummyApp
  val apps: Seq[SparkApp]

  case class JT(setup: JT => Unit)(check: JT => Unit)(implicit inArgs: InputArgs) {
    val appName = app.toString.split('$').apply(0).split('.').toSeq.last
    val mdPath = s"${appName}/JT"

    def readMdTable(name: String) = makeRes.readMdTable(name)

    "JT:" + appName should {
      "be success" in {
        setup(this)
        apps.foreach(_.exec)
        check(this)
      }
    }
  }
}

object D2kTest {
  case class DummyDf(x: String)

  val emptyDf = SparkContexts.context.emptyDataFrame
  val dummyApp = new SparkApp { def exec(implicit inArgs: InputArgs) = emptyDf }
}

trait D2kTest extends WordSpec with MustMatchers with BeforeAndAfter {
  val app: SparkApp
  val outputPath: String = sys.env.getOrElse("FILE_INPUT_PATH_DEFAULT", "test/dev/data/output")
  val readMdPath: String = "test/markdown"

  before { clean }

  def clean = {
    val path = Path(outputPath)
    path.createDirectory(true, false)
  }

  import D2kTest._
  val makeRes = d2k.common.MakeResource(outputPath, readMdPath)

  case class FileInfo(path: Path, inMdData: makeRes.MdInfo, outMdData: makeRes.MdInfo) {
    val fileName = path.name.drop(4)
    val splitted = fileName.split('.')
    def no = path.name.take(3)
    def io = splitted(1)
    def name = splitted(0)
  }

  case class AT(setup: AT => Unit)(check: AT => Unit)(implicit inArgs: InputArgs) {
    val appName = app.toString.split('$').apply(0).split('.').toSeq.last
    val mdPath = s"${appName}/AT"

    def readMdTable(name: String) = makeRes.readMdTable(s"$mdPath/$name")

    "AT:" + appName should {
      "be success" in {
        setup(this)
        app.exec
        check(this)
      }
    }
  }

  case class CT(testCase: String)(implicit inArgs: InputArgs) {
    val appName = app.toString.split('$').apply(0).split('.').toSeq.last
    val mdPath = s"${appName}/CT/${testCase}"

    private[this] def readMdTableBase(prefixName: String)(name: String) = makeRes.readMdTable(s"$mdPath/${prefixName}/${name}.md")

    case class CTPre() {
      def readMdTable = readMdTableBase("pre") _
    }
    val ctpre = CTPre()

    def pre[IN](component: OneInToOneOutForDf[IN, _])(setup: CTPre => IN)(check: String) = {
      "CT:" + testCase should {
        "be success" when {
          "pre" in {
            ctpre.readMdTable(s"${check}.md").checkDf(component.preExec(setup(ctpre)))
          }
        }
      }
    }

    def pre[IN](component: OneInToMapOutForDf[IN, _])(setup: CTPre => IN)(check: Map[String, String]) = {
      "CT:" + testCase should {
        "be success" when {
          "pre" in {
            val resultMap = component.preExec(setup(ctpre))
            resultMap.keys.foreach(key => ctpre.readMdTable(s"${check(key)}.md").checkDf(resultMap(key)))
          }
        }
      }
    }

    def pre[IN](component: TwoInToOneOutForDf[IN, IN, _])(setup: CTPre => (IN, IN))(check: String) = {
      "CT:" + testCase should {
        "be success" when {
          "pre" in {
            val resultDfs = setup(ctpre)
            ctpre.readMdTable(check).checkDf(component.preExec(resultDfs._1, resultDfs._2))
          }
        }
      }
    }

    case class CTPost[OUT](resultDf: OUT) {
      def readMdTable = readMdTableBase("post") _
    }

    def postMdToDf(name: String) = readMdTableBase("post")(name).toDf
    def post[OUT](component: OneInToOneOutForDf[_, OUT])(setup: String)(check: CTPost[OUT] => Unit) = {
      "CT:" + testCase should {
        "be success" when {
          "post" in {
            check(CTPost(component.postExec(postMdToDf(setup))))
          }
        }
      }
    }

    def post[OUT](component: OneInToMapOutForDf[_, OUT])(setup: Map[String, String])(check: CTPost[OUT] => Unit) = {
      val mapDf = setup.mapValues { name => postMdToDf(name) }
      "CT:" + testCase should {
        "be success" when {
          "post" in {
            check(CTPost(component.postExec(mapDf)))
          }
        }
      }
    }

    def post[OUT](component: TwoInToOneOutForDf[_, _, OUT])(setup: String)(check: CTPost[OUT] => Unit) = {
      "CT:" + testCase should {
        "be success" when {
          "post" in {
            check(CTPost(component.postExec(postMdToDf(setup))))
          }
        }
      }
    }
  }

  case class FT(componentName: String, testCase: String) {
    val appName = app.toString.split('$').apply(0).split('.').toSeq.last
    val mdPath = Path(s"${readMdPath}/${appName}/FT/${componentName}")

    def apply(target: Seq[Editors]) = {
      val mapTarget = target.map(t => (t.colName, t)).toMap
      val targetPath = (mdPath / testCase)
      if (!targetPath.isDirectory) throw new FileNotFoundException(targetPath.toString)
      val fileInfos = targetPath.walk.map { path =>
        val mdStr = Source.fromFile(path.toString).mkString
        val splitted = mdStr.split("# expect")
        FileInfo(path, makeRes.MdInfo(splitted(0).split("# input")(1)), makeRes.MdInfo(splitted(1)))
      }

      s"FT:${componentName}:${testCase}" should {
        "be success" when {
          fileInfos.foreach { fi =>
            val targetColumn = Option(mapTarget(fi.name)).flatMap {
              case e: Edit => Option(e.editor)
              case _       => None
            }.get
            val inputDf = if (fi.inMdData.data.replaceAll("\n", "").trim.isEmpty) {
              Seq(DummyDf("")).toDF
            } else {
              fi.inMdData.toDf
            }
            val result = inputDf.select(targetColumn as fi.name)
            s"${fi.no}:${fi.name}" in {
              withClue(fi.no) {
                val outputPos = fi.inMdData.data.count(_ == '\n')
                fi.outMdData.checkDf(result, outputPos)
              }
            }
          }
        }
      }
    }

    def apply(target: DataFrame => DataFrame) = {
      val targetPath = (mdPath / testCase)
      if (!targetPath.isDirectory) throw new FileNotFoundException(targetPath.toString)
      val fileInfos = targetPath.walk.map { path =>
        val mdStr = Source.fromFile(path.toString).mkString
        val splitted = mdStr.split("# expect")
        FileInfo(path, makeRes.MdInfo(splitted(0).split("# input")(1)), makeRes.MdInfo(splitted(1)))
      }

      s"FT:${componentName}:${testCase}" should {
        "be success" when {
          fileInfos.foreach { fi =>
            val inputDf = if (fi.inMdData.data.replaceAll("\n", "").trim.isEmpty) {
              Seq(DummyDf("")).toDF
            } else {
              fi.inMdData.toDf
            }
            val result = target(inputDf)
            s"${fi.no}:${fi.name}" in {
              withClue(fi.no) {
                val outputPos = fi.inMdData.data.count(_ == '\n')
                fi.outMdData.checkDf(result, outputPos)
              }
            }
          }
        }
      }
    }
  }
}
', Error message: The current scope wasn't closed at Phase2 of Loader when loading node of type 'Mobilize.Scala.AST.SclTypeFunctionArgTypes'.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.app.test.common

import spark.common.FileCtl
import java.io.File
import org.apache.poi.ss.usermodel.WorkbookFactory
import org.apache.poi.ss.usermodel.Cell

object ExcelConverter {
  def toMarkdown(excelPath: String, sheet: String, mdFile: String) = {
    val data = to2DArray(excelPath, sheet)
    val colSize = data(0).filter(!_.isEmpty).size
    val x: Seq[List[String]] = Seq(
      data(0).toList,
      data(0).map(_ => "----").toList,
      data(0).toList,
      data(1).map {
        case x if x.endsWith("_ZD") => "ZD"
        case x if x.endsWith("_PD") => "PD"
        case "全角文字列"                => "全角文字列"
        case _                      => "半角文字列"
      }.toList.take(colSize),
      data(2).toList.take(colSize))
    val x2 = x ++ data.toList.drop(3).map(_.toList)
    val x3 = x2.map(a => a.take(colSize).mkString("|", "|", "|"))

    System.setProperty("line.separator", "\n")
    FileCtl.writeToFile(mdFile, charEnc = "UTF-8") { pw =>
      pw.println(s"# ${sheet}")
      x3.foreach(pw.println)
    }
  }

  def to2DArray(path: String, sheetName: String) = {
    val sheet = getTargetSheet(path, sheetName)
    val rowCnt = sheet.getLastRowNum() + 1


    (0 to rowCnt).flatMap { i =>
      Option(sheet.getRow(i)).map { row =>
        (0 to row.getLastCellNum).flatMap { cellCnt =>
          Option(row.getCell(cellCnt)).map(cell => getStrVal(cell))
        }
      }
    }
  }

  def getStrVal(cell: Cell) = {
    cell.getCellType match {
      case Cell.CELL_TYPE_STRING  => cell.getStringCellValue
      case Cell.CELL_TYPE_NUMERIC => cell.getNumericCellValue.toString
      case Cell.CELL_TYPE_FORMULA => try { cell.getStringCellValue } catch {
        case ex: IllegalStateException => cell.getNumericCellValue.toString
      }
      case Cell.CELL_TYPE_BLANK   => ""
      case Cell.CELL_TYPE_BOOLEAN => cell.getBooleanCellValue.toString
      case Cell.CELL_TYPE_ERROR   => cell.getStringCellValue
    }
  }

  def getTargetSheet(path: String, sheetName: String) = {
    val input = new File(path)
    val book = WorkbookFactory.create(input)
    book.getSheet(sheetName)
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.app.test.common

import java.sql.Timestamp
import java.time.LocalDateTime
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.Row
import spark.common.SparkContexts
import d2k.common.fileConv.Converter
import d2k.common.fileConv.ConfParser
import d2k.common.fileConv.ItemConf
import spark.common.FileCtl

object MakeDf {
  trait MakeDf {
    def allSpace: DataFrame
    def allEmpty: DataFrame
  }

  def apply(confPath: String) = new Plane(confPath)
  def dc(confPath: String) = new DomainConverter(confPath)

  def makeInputDf(rows: Seq[Row], names: Seq[String], domains: Seq[String]) = {
    val rdd = SparkContexts.sc.makeRDD(rows)
    val ziped = names.zip(domains)
    val (nameList, domainList) = ziped.filter { case (names, domain) => !(domain.startsWith(Converter.NOT_USE_PREFIX) || domain.startsWith(Converter.REC_DIV_PREFIX)) }.unzip
    SparkContexts.context.createDataFrame(rdd, Converter.makeSchema(nameList))
  }

  def parseItemConf(confPath: String) = {
    val conf = ConfParser.readConf(confPath) { items =>
      if (items.size == 5) {
        ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true")
      } else {
        ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true", items(5))
      }
    }.toSeq
    (conf, conf.map(_.itemId), conf.map(_.cnvType))
  }

  class Plane(confPath: String) extends MakeDf {
    val (itemConfs, names, domains) = parseItemConf(confPath)

    def allSpace = exec { (ic: ItemConf, cnt: Int) => " " * ic.length.toInt }
    def allEmpty = exec { (ic: ItemConf, cnt: Int) => "" }

    private[this] def exec(func: (ItemConf, Int) => String) = {
      val orgData = itemConfs.zipWithIndex.map { case (ic, cnt) => func(ic, cnt) }
      makeInputDf(Seq(Row(orgData: _*)), names, domains)
    }
  }

  class DomainConverter(confPath: String) extends MakeDf {
    val (itemConfs, names, domains) = parseItemConf(confPath)

    def allSpace = exec { (ic: ItemConf, cnt: Int) => " " * ic.length.toInt }
    def allEmpty = exec { (ic: ItemConf, cnt: Int) => "" }

    private[this] def exec(func: (ItemConf, Int) => String) = {
      val orgData = itemConfs.zipWithIndex.map { case (ic, cnt) => func(ic, cnt) }

      val zipped = orgData.zip(domains).zip(names)
      val convedData = Converter.domainConvert(zipped)
      makeInputDf(Seq(Row(convedData: _*)), names, domains)
    }
  }

  implicit class DataFrameConverter(df: DataFrame) {
    def writeFixedFile(writePath: String, append: Boolean = false,
                       header: Boolean = false, footer: Boolean = false,
                       newLine: Boolean = true, lineSeparator: String = "\n") = {
      System.setProperty("line.separator", lineSeparator)
      FileCtl.writeToFile(writePath, append) { pw =>
        df.collect.map(_.mkString).foreach { x =>
          if (header) pw.println(" " * x.mkString.length)
          if (newLine) pw.println(x.mkString) else pw.print(x.mkString)
          if (footer) pw.println(" " * x.mkString.length)
        }
      }
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.app.test.common

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import org.apache.spark.sql.DataFrame

import scala.util.Try
import d2k.common.MakeResource

trait MarkdownTester extends WordSpec with MustMatchers with BeforeAndAfter {
  val showData = Try(sys.env("MarkdownTesterShowData")).map(d => if (d == "true") true else false).getOrElse(false)

  def execUt(componentInstanceName: String)(targets: (DataFrame => DataFrame)*) = {
    val classNames = targets.head.getClass.getName.split('$')
    val appName = classNames.head.split('.').last
    val makeRes = MakeResource("test/dev/data/output", s"${appName}Test/ut/${componentInstanceName}")

    s"be success ${componentInstanceName}" when {
      targets.foreach { func =>
        val funcName = func.getClass.getName.split('$').dropRight(1).takeRight(1).head
        funcName in {
          val df = makeRes.readMdTable(s"${funcName}_data.md").toDf
          if (showData) println(s"[Input Data:${componentInstanceName}:${funcName}]"); df.show(false)

          val expect = makeRes.readMdTable(s"${funcName}_expect.md")
          if (showData) println(s"[Expect Data:${componentInstanceName}:${funcName}]"); expect.toDf.show(false)

          val result = func(df)

          if (showData) println(s"[Result Data:${componentInstanceName}:${funcName}]"); result.show(false)
          withClue("Record Size Check") {
            result.count mustBe expect.toDf.count
          }
          expect.checkDf(result)
        }
      }
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.app.test.common

import d2k.common.InputArgs

case class TestArgs(confPath: String = "conf", dataPath: String = "data",
                    projectId: String = "projectId", processId: String = "processId", applicationId: String = "appId",
                    runningDateFileFullPath: String = "test/dev/RUNNING_DATE.txt") {
  def toArray = Array("test", "dev", confPath, dataPath, projectId, processId,
    applicationId, runningDateFileFullPath)
  def toInputArgs = InputArgs("test", "dev", confPath, dataPath, projectId, processId,
    applicationId, runningDateFileFullPath)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.finder

import Commons._

import scala.util.Try
import scala.reflect.io.Directory
import java.io.FileWriter
import org.apache.commons.io.output.FileWriterWithEncoding
import d2k.appdefdoc.parser._
import java.io.File
import d2k.appdefdoc.parser.D2kParser

object AppErrorDetector extends App with D2kParser {
  val isLocalMode = args.size >= 4
  val (baseUrl, branch, targetName) = (args(0), args(1), args(2))
  val basePath = if (isLocalMode) args(3) else "C:/d2k_docs"
  val writeBase = s"data/irFinder/${targetName}"
  val writePath = Directory(writeBase)

  println(s"[Start App Relation Finder${if (isLocalMode) " on Local Mode" else ""}] ${args.mkString(" ")}")

  def recList(file: File): Array[File] = {
    val files = file.listFiles
    files ++ files.filter(_.isDirectory).flatMap(recList)
  }

  //  val itemBasePath = s"${basePath}/apps/common"
  //  val itemNames = recList(new File(itemBasePath)).filter(_.getName.contains(".md"))
  //  val targetResouces = itemNames.filter { path =>
  //    Try { readItemDefMd(path.toString).contains(targetName) }.getOrElse({ println(s"itemDef parse error: ${path}"); false })
  //  }.toList
  //  println(targetResouces)
  //  val itemDefList = targetResouces.flatMap(x => Try((x.toString, ItemDefParser(x.toString).get)).toOption).toList
  //  println(itemDefList)
  //
  val appBasePath = s"${basePath}/apps"
  var errList = List.empty[String]
  val appDefList = recList(new File(appBasePath)).filter(_.getName.contains("README.md"))
    .flatMap(x => Try((x.toString, AppDefParser(x.toString).get)).toOption
      .orElse({ errList = x.toString :: errList; None }))
      println(errList.size)
      errList.reverse.foreach(println)
  //
  //  println(appDefList.head._1)
  //  println(appDefList.head._2.appInfo)
  //  println(appDefList.head._2.componentList)

  /*

  val result = appDefList.map { x =>
    val (path, appdef) = x
    val in = appdef.inputList.map(_.id).contains(targetName)
    val out = appdef.outputList.map(_.id).contains(targetName)
    val containType = (in, out) match {
      case (false, false) => "none"
      case (true, false)  => "in"
      case (false, true)  => "out"
      case (true, true)   => "io"
    }
    val ioData = appdef.inputList.filter(_.id == targetName).headOption.orElse(appdef.outputList.filter(_.id == targetName).headOption)
    RrfData(path, appdef.appInfo, ioData, containType)
  }.filter(_.containType != "none")

  result.headOption.map { r =>
    writePath.createDirectory(true, false)
    val targetObj = r.ioData.get
    val targetObjPath = s"${basePath}/apps/common/${targetObj.path.split("/common/")(1)}"
    val targetObjTitle = s"[${result.head.ioData.get.id}](${targetObjPath})[${targetObj.name}]"

    val targetObjUml = s"""artifact "${targetObj.id}\\n${targetObj.name}" as ${targetObj.id}"""
    val appUml = result.map { d =>
      s"[${d.appInfo.id}\\n${d.appInfo.name}] as ${d.appInfo.id}"
    }
    val chainUml = result.map { d =>
      d.containType match {
        case "in"  => s"${targetObj.id} --> ${d.appInfo.id} :Input"
        case "out" => s"${d.appInfo.id} --> ${targetObj.id} :Output"
        case "io"  => s"${targetObj.id} --> ${d.appInfo.id} :Input\\n${d.appInfo.id} --> ${targetObj.id} :Output"
        case _     => ""
      }
    }
    val umls = targetObjUml :: appUml ++ chainUml

    def dataToTable(arf: RrfData) = s"| [${arf.appInfo.id}](${arf.path}) | ${arf.appInfo.name} |"
    val outputTables = result.filter(_.containType == "out").map(dataToTable)
    val inputTables = result.filter(_.containType == "in").map(dataToTable)

    val tmpl = fileToStr("arfTemplates/searchResult.tmpl")
    val writeFilePath = s"${writeBase}/${targetObj.id}.md"
    val writer = new FileWriter(writeFilePath)
    val conved = tmpl.replaceAllLiterally("%%SearchTarget%%", targetObjTitle)
      .replaceAllLiterally("%%ResultPlantuml%%", umls.mkString("\n"))
      .replaceAllLiterally("%%ResultOutput%%", outputTables.mkString("\n"))
      .replaceAllLiterally("%%ResultInput%%", inputTables.mkString("\n"))
    writer.write(conved)
    writer.close

    val csvData = result.map { arf =>
      Seq(arf.appInfo.id, arf.appInfo.name, arf.containType).mkString(",")
    }.mkString("\n")
    val writeCsvFilePath = s"${writeBase}/${targetObj.id}.csv"
    val csvWriter = new FileWriter(writeCsvFilePath)
    csvWriter.write(csvData)
    csvWriter.close

    println(s"[Finish App Relation Finder] ${writeFilePath}")
  }.getOrElse(println(s"[Finish App Relation Finder] Not Found Application"))
  *
  */
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.finder

import scala.io.Source
import scala.reflect.io.Directory
import java.io.FileWriter
import d2k.appdefdoc.parser.AppDef
import org.apache.commons.io.output.FileWriterWithEncoding
import java.io.File
import scala.util.Try
import d2k.appdefdoc.parser._
import scala.annotation.tailrec

case class RirfFlow(kind: String, id: String, name: String = "")
case class IrrfFlow(rirfFlow: RirfFlow, beforeName: String, afterName: String)
object Commons {
  def fileToStr(fileName: String) =
    Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString

  def mkTable(data: String*) = data.mkString("| ", " | ", " |")

  def pathOutputString(path: String) = path.replaceAllLiterally("\\", "/")

  def localPath2Url(baseUrl: String, basePath: String, localPath: String) =
    s"${baseUrl}/${
      localPath.replaceAllLiterally("\\", "/")
        .replaceAllLiterally(basePath.replaceAllLiterally("\\", "/"), "tree/master")
    }"

  def recList(file: File): Array[File] = {
    val files = file.listFiles
    files ++ files.filter(_.isDirectory).flatMap(recList)
  }

  val appDefRegx = """.*\\apps\\\w{3}\d*\\\w+\\README.md""".r
  def appDefList(appBasePath: String) = {
    recList(new File(appBasePath)).filter(_.getName.contains("README.md"))
      .filter(x => appDefRegx.findFirstMatchIn(x.toString).isDefined)
      .flatMap(x => Try((x.toString, AppDefParser(x.toString).get))
        .toOption.orElse { println(s"  appDef parse error: ${x.toString}"); None }).toList
  }

  def createRrfData(targetName: String, appDefList: Seq[(String, AppDef)]) = {
    appDefList.map { x =>
      val (path, appdef) = x
      val in = appdef.inputList.map(_.id).contains(targetName)
      val out = appdef.outputList.map(_.id).contains(targetName)
      val containType = (in, out) match {
        case (false, false) => "none"
        case (true, false)  => "in"
        case (false, true)  => "out"
        case (true, true)   => "io"
      }
      val ioData = appdef.inputList.filter(_.id == targetName).headOption.orElse(appdef.outputList.filter(_.id == targetName).headOption)
      RrfData(path, appdef.appInfo, ioData, containType)
    }.filter(_.containType != "none")
  }

  def writeRrfData(targetName: String, baseUrl: String, basePath: String, writePath: Directory, finishMessage: Option[String], rrfData: Seq[RrfData]) = {
    rrfData.headOption.map { r =>
      writePath.createDirectory(true, false)
      val targetObj = r.ioData.get
      val targetObjTitle = if (targetObj.path.isEmpty) {
        val appInfo = s"${rrfData.head.ioData.get.id}[${targetObj.name}]"
        println(s"  appDef not found: ${appInfo}")
        appInfo
      } else {
        val targetObjPath = s"${basePath}/apps/common/${targetObj.path.split("/common/")(1)}"
        s"[${rrfData.head.ioData.get.id}](${targetObjPath})[${targetObj.name}]"
      }

      val targetObjUml = s"""artifact "${targetObj.id}\\n${targetObj.name}" as ${targetObj.id}_res"""
      val appUml = rrfData.map { d =>
        s"[${d.appInfo.id}\\n${d.appInfo.name}] as ${d.appInfo.id}"
      }
      val chainUml = rrfData.map { d =>
        d.containType match {
          case "in"  => s"${targetObj.id}_res --> ${d.appInfo.id} :Input"
          case "out" => s"${d.appInfo.id} --> ${targetObj.id}_res :Output"
          case "io"  => s"${targetObj.id}_res --> ${d.appInfo.id} :Input\\n${d.appInfo.id} --> ${targetObj.id}_res :Output"
          case _     => ""
        }
      }
      val umls = targetObjUml :: (appUml ++ chainUml).toList

      def dataToTable(rrf: RrfData) = if (rrf.path.isEmpty) {
        println(s"  appDef not found: ${rrf.appInfo.id}[${rrf.appInfo.name}]")
        s"| ${rrf.appInfo.id} | ${rrf.appInfo.name} |"
      } else {
        s"| [${rrf.appInfo.id}](${rrf.path}) | ${rrf.appInfo.name} |"
      }
      val outputTables = rrfData.filter(_.containType == "out").map(dataToTable)
      val inputTables = rrfData.filter(_.containType == "in").map(dataToTable)

      val tmpl = fileToStr("finderTemplates/rrResult.tmpl")
      val writeFilePath = s"${writePath.toString}/${targetObj.id}.md"
      val writer = new FileWriter(writeFilePath)
      val conved = tmpl.replaceAllLiterally("%%SearchTarget%%", targetObjTitle)
        .replaceAllLiterally("%%ResultPlantuml%%", umls.mkString("\n"))
        .replaceAllLiterally("%%ResultOutput%%", outputTables.mkString("\n"))
        .replaceAllLiterally("%%ResultInput%%", inputTables.mkString("\n"))
      writer.write(conved)
      writer.close

      val csvTitle = Seq("Target Name", "App Id", "App Name", "Io Type", "Url").mkString("", " , ", "\n")

      val csvData = rrfData.map { rrf =>
        val path = if (rrf.path.isEmpty) { "" } else { localPath2Url(baseUrl, basePath, rrf.path) }
        Seq(targetName, rrf.appInfo.id, rrf.appInfo.name, rrf.containType, path).mkString(" , ")
      }.mkString("\n")
      val writeCsvFilePath = s"${writePath.toString}/${targetObj.id}.csv"
      val csvWriter = new FileWriterWithEncoding(writeCsvFilePath, "MS932")
      csvWriter.write(csvTitle)
      csvWriter.write(csvData)
      csvWriter.write("\n")
      csvWriter.close
      finishMessage.foreach(mes => println(s"${mes} ${pathOutputString(writeFilePath)}"))
      Some(targetObj, r)
    }.getOrElse({ finishMessage.foreach(mes => println(s"${mes} Not Found Application. target[${targetName}]")); None })
  }

  def implementList(targetItemId: String, appdefList: List[(String, AppDef)]) = appdefList.flatMap {
    case (appdefpath, appdef) =>
      val compolist = appdef.componentList

      val result = compolist.flatMap { x =>
        val path = appdefpath.dropRight(9) + x.mdName
        val str = Source.fromFile(path).getLines.mkString
        if (str.contains(targetItemId)) Some(x.mdName) else None
      }

      result.map { subId =>
        val subPath = s"${appdefpath.dropRight(9)}/${subId}"
        val outputTable =
          mkTable(s"[${appdef.appInfo.id}](${appdefpath}) / [${subId.dropRight(3)}](${subPath})", appdef.appInfo.name)

        ((appdefpath, appdef), outputTable, subId)
      }
  }

  def createItemDefMap(targetItemId: String, itemNames: Array[File]) = {
    itemNames.flatMap { path =>
      Try {
        val itemdef = ItemDefParser(path.toString).get
        val itemDetail = itemdef.details.find(_.id == targetItemId)
        itemDetail.map { item =>
          (itemdef.id, RirfDetail(itemdef.id, itemdef.name, item.name, path.toString))
        }
      }.getOrElse { println(s"  itemDef parse error: ${path}"); None }
    }.toMap
  }

  val maxDepth = 10
  def recursiveSearch(
    appDefList: List[(String, AppDef)], itemDefMap: Map[String, RirfDetail], appFind: AppDef => Seq[IoData], resourceFind: AppDef => Seq[IoData], depth: Int = 0)(targetResourceId: String, targetAppId: Option[String] = None): Seq[RirfData] = {
    val itemDefMapKeys = itemDefMap.keySet
    val targetApp = appDefList.filter {
      case (path, appdef) => appFind(appdef).exists(_.id == targetResourceId)
    }
    val result = targetApp.flatMap { appdef =>
      val resources = resourceFind(appdef._2).filter(x => itemDefMapKeys.exists(_ == x.id))
      val resResult = resources.foldLeft(Seq.empty[RirfData]) { (l, r) =>
        if (depth > maxDepth) {
          l
        } else {
          l ++ recursiveSearch(appDefList, itemDefMap, appFind, resourceFind, depth + 1)(r.id, Some(appdef._2.appInfo.id))
        }
      }
      RirfData(itemDefMap.getOrElse(targetResourceId, RirfDetail(targetResourceId)), Some(RirfAppDetail(appdef._2, appdef._1)), targetAppId) +: resResult
    }
    if (result.isEmpty)
      Seq(RirfData(itemDefMap.getOrElse(targetResourceId, RirfDetail(targetResourceId)), None, targetAppId))
    else
      result
  }

  @tailrec
  private[this] def renameSearch(data: Map[String, String], targetId: String): String = {
    val v = data.get(targetId)
    if (v.isEmpty)
      targetId
    else
      renameSearch((data - targetId), v.get)
  }

  val renameRegx = """####\s+\d{3}\s+:\s+(\w+)\[.*\]\s+->\s+(\w+)\[.*\].*""".r
  def componentDetailData(app: (String, AppDef), targetItemId: String) = {
    app._2.componentList.scanLeft(RenameData("", targetItemId, targetItemId)) { (l, r) =>
      val path = app._1.dropRight(9) + r.mdName
      val str = Source.fromFile(path).getLines.mkString("\n")
      val regxResult = renameRegx.findAllMatchIn(str).map { x => (x.group(1), x.group(2)) }.toMap
      RenameData(r.id, l.afterName, renameSearch(regxResult, l.afterName))
    }
  }

  def recursiveSearchWithRename(
    appDefList: List[(String, AppDef)], itemDefMap: Map[String, RirfDetail], appFind: AppDef => Seq[IoData], resourceFind: AppDef => Seq[IoData],
    targetItemId: String, itemFileNames: Array[File], depth: Int = 0)(
    targetResourceId: String, targetAppId: Option[String] = None): Seq[IrrfData] = {
    val targetApp = appDefList.filter {
      case (path, appdef) => appFind(appdef).exists(_.id == targetResourceId)
    }

    val result = targetApp.flatMap { appdef =>
      val renamedItemIdList = componentDetailData(appdef, targetItemId)
      val renameComponentList = renamedItemIdList.filter(x => !x.componentId.isEmpty && x.beforeName != x.afterName)
      val filteredItemDefMap = createItemDefMap(renamedItemIdList.last.afterName, itemFileNames)

      val itemDefMapKeys = filteredItemDefMap.keySet
      val resources = resourceFind(appdef._2).filter(x => itemDefMapKeys.exists(_ == x.id))
      val resResult = resources.foldLeft(Seq.empty[IrrfData]) { (l, r) =>
        if (depth > 3) {
          l
        } else {
          l ++ recursiveSearchWithRename(appDefList, itemDefMap, appFind, resourceFind, targetItemId, itemFileNames, depth + 1)(r.id, Some(appdef._2.appInfo.id))
        }
      }
      IrrfData(RirfData(filteredItemDefMap.getOrElse(targetResourceId, RirfDetail(targetResourceId)), Some(RirfAppDetail(appdef._2, appdef._1)), targetAppId), renameComponentList) +: resResult
    }

    if (result.isEmpty)
      Seq(IrrfData(RirfData(itemDefMap.getOrElse(targetResourceId, RirfDetail(targetResourceId)), None, targetAppId), Seq.empty[RenameData]))
    else
      result
  }

  def createFlowRender(
    result:       Seq[RirfData],
    flowLinkFunc: RirfData => Seq[String] = d => Seq(d.appDetail.map(x => s"${d.resDetail.id}_res --> ${x.appDef.appInfo.id}"), d.parentAppId.map(x => s"${x} --> ${d.resDetail.id}_res")).flatten) = {
    val flowResult = result.flatMap { x =>
      Seq(Some(RirfFlow("res", x.resDetail.id, x.resDetail.name)), x.appDetail.map(d => RirfFlow("app", d.appDef.appInfo.id, d.appDef.appInfo.name))).flatten
    }

    val flowObjects = flowResult.map { x =>
      x.kind match {
        case "app" => s"[${x.id}\\n${x.name}] as ${x.id}"
        case "res" => s"""artifact "${x.id}\\n${x.name}" as ${x.id}_res"""
      }
    }
    val flowLinks = result.flatMap {
      case d: RirfData => flowLinkFunc(d)
      case _           => Seq.empty[String]
    }.distinct
    (flowObjects ++ flowLinks).mkString("\n")
  }

  def createFlowRenderWithRename(
    result:       Seq[IrrfData],
    flowLinkFunc: RirfData => Seq[String] = d => Seq(d.appDetail.map(x => s"${d.resDetail.id}_res --> ${x.appDef.appInfo.id}"), d.parentAppId.map(x => s"${x} --> ${d.resDetail.id}_res")).flatten) = {
    val flowResult = result.flatMap { x =>
      val beforeName = x.renameApps.headOption.map(_.beforeName).getOrElse("")
      val afterName = x.renameApps.lastOption.map(_.afterName).getOrElse("")
      Seq(
        Some(IrrfFlow(RirfFlow("res", x.appData.resDetail.id, x.appData.resDetail.name), "", "")),
        x.appData.appDetail.map(d => IrrfFlow(RirfFlow("app", d.appDef.appInfo.id, d.appDef.appInfo.name), beforeName, afterName))).flatten
    }

    val flowObjects = flowResult.map { ir =>
      val x = ir.rirfFlow
      x.kind match {
        case "app" if (ir.beforeName == ir.afterName) => s"[${x.id}\\n${x.name}] as ${x.id}"
        case "app" if (ir.beforeName != ir.afterName) => s"[${x.id}\\n${x.name}] as ${x.id}\nnote right of ${x.id} : ${ir.beforeName} -> ${ir.afterName}"
        case "res"                                    => s"""artifact "${x.id}\\n${x.name}" as ${x.id}_res"""
      }
    }.distinct
    val flowLinks = result.flatMap {
      case d: IrrfData => flowLinkFunc(d.appData)
      case _           => Seq.empty[String]
    }.distinct
    (flowObjects ++ flowLinks).mkString("\n")
  }

  def createReferResult(result: Seq[RirfData]) = {
    result.map { x =>
      val res = if (x.resDetail.path.isEmpty) {
        Seq(x.resDetail.id, x.resDetail.name)
      } else {
        Seq(s"[${x.resDetail.id}](${x.resDetail.path})", x.resDetail.name)
      }
      val app = x.appDetail.map { app =>
        if (app.path.isEmpty) {
          Seq(app.appDef.appInfo.id, app.appDef.appInfo.name)
        } else {
          Seq(s"[${app.appDef.appInfo.id}](${app.path})", app.appDef.appInfo.name)
        }
      }.getOrElse(Seq("-", "-"))
      (res ++ app).mkString("| ", " | ", " |")
    }.distinct.mkString("\n")
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.finder

import d2k.appdefdoc.finder.Commons._
import d2k.appdefdoc.parser._
import d2k.appdefdoc.parser.D2kParser
import scala.reflect.io.Directory
import java.io.File
import scala.collection.Seq
import scala.reflect.io.Path.string2path
import scala.annotation.tailrec
import scala.util.Try
import java.io.FileWriter
import org.apache.commons.io.output.FileWriterWithEncoding

case class GafData(baseAppId: Option[String], baseResourceId: String, result: Seq[(String, AppDef)])
object GeneratingApplicationRouteFinder extends App with D2kParser {
  val isLocalMode = args.size >= 4
  val (baseUrl, branch, targetResourceId) = (args(0), args(1), args(2))
  val basePath = if (isLocalMode) args(3) else "C:/d2k_docs"
  val writeBase = s"data/gaRouteFinder/${targetResourceId}"
  val writePath = Directory(writeBase)

  println(s"[Start Generating Application Route Finder ${if (isLocalMode) " on Local Mode" else ""}] ${args.mkString(" ")}")

  val appBasePath = s"${basePath}/apps"
  val appDefList = Commons.appDefList(appBasePath)
  def searchResource(resourceName: String) = appDefList.filter {
    case (_, appdef) =>
      val existCheck = appdef.outputList.filter(_.id == resourceName)
      !existCheck.isEmpty
  }

  val itemBasePath = s"${basePath}/apps/common"
  val itemNames = recList(new File(itemBasePath)).filter(x => x.getName.contains(".md") && !x.getName.endsWith("README.md"))

  val itemDefMap = itemNames.flatMap { path =>
    Try {
      val itemdef = ItemDefParser(path.toString).get
      itemdef.details.map { item =>
        (itemdef.id, RirfDetail(itemdef.id, itemdef.name, item.name, path.toString))
      }
    }.getOrElse { println(s"  itemDef parse error: ${path}"); Seq.empty[(String, RirfDetail)] }
  }.toMap

  val targetResource = itemDefMap(targetResourceId)
  val searchTargetResult = s"[${targetResource.id}](${targetResource.path})[${targetResource.name}]"

  val result = recursiveSearch(appDefList, itemDefMap,
    (appdef: AppDef) => appdef.outputList, (appdef: AppDef) => appdef.inputList)(targetResourceId).distinct

  val flowRender = createFlowRender(
    result,
    d => Seq(d.appDetail.map(x => s"${x.appDef.appInfo.id} --> ${d.resDetail.id}"), d.parentAppId.map(x => s"${d.resDetail.id} --> ${x}")).flatten)

  val referResult = createReferResult(result)

  val tmpl = fileToStr("finderTemplates/garResult.tmpl")
  val writeFilePath = s"${writePath.toString}/${targetResourceId}.md"
  writePath.createDirectory(true, false)
  val writer = new FileWriter(writeFilePath)
  val conved = tmpl.replaceAllLiterally("%%SearchTarget%%", searchTargetResult)
    .replaceAllLiterally("%%ResultFlow%%", flowRender)
    .replaceAllLiterally("%%ResultApplicationList%%", referResult)
  writer.write(conved)
  writer.close

  val csvReferTitle = Seq("Target Resource Id", "Output Resource Id", "Output Resource Name", "App Id", "App Name", "Resource Url", "App Url").mkString("", " , ", "\n")
  val csvReferData = result.map { data =>
    val resPath = localPath2Url(baseUrl, basePath, data.resDetail.path)
    val csvData = data.appDetail.map { x =>
      val appUrl = localPath2Url(baseUrl, basePath, x.path)
      Seq(targetResourceId, data.resDetail.id, data.resDetail.name, x.appDef.appInfo.id, x.appDef.appInfo.name, resPath, appUrl)
    }.getOrElse(Seq(targetResourceId, data.resDetail.id, data.resDetail.name, "", "", resPath, ""))
    csvData.mkString(" , ")
  }.mkString("\n")
  val csvReferFilePath = s"${writePath.toString}/refer.csv"
  val csvReferWriter = new FileWriterWithEncoding(csvReferFilePath, "MS932")
  csvReferWriter.write(csvReferTitle)
  csvReferWriter.write(csvReferData)
  csvReferWriter.write("\n")
  csvReferWriter.close

  println(s"[Finish Generating Application Route Finder] ${pathOutputString(writeFilePath)}")
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.finder

import Commons._

import scala.util.Try
import scala.reflect.io.Directory
import java.io.FileWriter
import org.apache.commons.io.output.FileWriterWithEncoding
import d2k.appdefdoc.parser._
import java.io.File
import d2k.appdefdoc.parser.D2kParser
import scala.io.Source

object ItemReferenceFinder extends App with D2kParser {
  val isLocalMode = args.size >= 4
  val (baseUrl, branch, targetItemId) = (args(0), args(1), args(2))
  val basePath = if (isLocalMode) args(3) else "C:/d2k_docs"
  val writeBase = s"data/irFinder/${targetItemId}"
  val writePath = Directory(writeBase)

  println(s"[Start Item Reference Finder${if (isLocalMode) " on Local Mode" else ""}] ${args.mkString(" ")}")

  val itemBasePath = s"${basePath}/apps/common"
  val itemNames = recList(new File(itemBasePath)).filter(_.getName.contains(".md"))
  val targetResouces = itemNames.filter { path =>
    Try { readItemDefMd(path.toString).contains(targetItemId) }.getOrElse({ println(s"  itemDef parse error: ${path}"); false })
  }.toList
  val itemDefList = targetResouces.flatMap(x => Try((x.toString, ItemDefParser(x.toString).get)).toOption).toList

  val appBasePath = s"${basePath}/apps"
  val appDefList = Commons.appDefList(appBasePath)

  val rrdList = itemDefList.flatMap { itemDef =>
    val targetName = itemDef._2.id
    val filtered = appDefList.filter(_._2.inputList.map(_.id).exists(_.contains(targetName.trim)))

    val result = createRrfData(targetName, filtered)
    val writeResult = writeRrfData(targetName, baseUrl, basePath, writePath, Some("    [Finish Resource Relation Finder]"), result)
    writeResult.map(x => (x._1, x._2))
  }

  val resultResource = rrdList.map {
    case (io, _) => mkTable(s"[${io.id}](${io.id}.md)", io.srcType, io.name)
  }

  val implList = implementList(targetItemId, appDefList)
  
  val tmpl = fileToStr("finderTemplates/irResult.tmpl")
  val writeFilePath = s"${writePath.toString}/README.md"
  writePath.createDirectory(true, false)
  val writer = new FileWriter(writeFilePath)
  val conved = tmpl.replaceAllLiterally("%%SearchTarget%%", targetItemId)
    .replaceAllLiterally("%%ResultResource%%", resultResource.mkString("\n"))
    .replaceAllLiterally("%%ResultItemReference%%", implList.map(_._2).mkString("\n"))
  writer.write(conved)
  writer.close

  val allAppNames = (rrdList.map(_._2.appInfo.id) ++ implList.map(_._1._2.appInfo.id)).toSet
  val allApps = allAppNames.map { x => appDefList.filter(_._2.appInfo.id == x).head }

  val csvImplTitle = Seq("Target Name", "App Id", "Sub App Id", "App Name", "Url", "Sub Url").mkString("", " , ", "\n")
  val csvImplData = implList.map {
    case (app, _, subImpl) =>
      val appdef = appDefList.filter(_._2.appInfo.id == app._2.appInfo.id).head
      val localPath = localPath2Url(baseUrl, basePath, appdef._1)
      Seq(targetItemId, app._2.appInfo.id, subImpl.dropRight(3), app._2.appInfo.name,
        localPath, s"${localPath.dropRight(9)}${subImpl}").mkString(" , ")
  }.mkString("\n")

  val csvImplFilePath = s"${writePath.toString}/impl.csv"
  val csvImplWriter = new FileWriterWithEncoding(csvImplFilePath, "MS932")
  csvImplWriter.write(csvImplTitle)
  csvImplWriter.write(csvImplData)
  csvImplWriter.write("\n")
  csvImplWriter.close

  val csvAllTitle = Seq("Target Name", "App Id", "App Name", "Url").mkString("", " , ", "\n")
  val csvAllData = allApps.map {
    case (path, appDef) =>
      Seq(targetItemId, appDef.appInfo.id, appDef.appInfo.name, localPath2Url(baseUrl, basePath, path)).mkString(" , ")
  }.mkString("\n")

  val csvAllFilePath = s"${writePath.toString}/all.csv"
  val csvAllWriter = new FileWriterWithEncoding(csvAllFilePath, "MS932")
  csvAllWriter.write(csvAllTitle)
  csvAllWriter.write(csvAllData)
  csvAllWriter.write("\n")
  csvAllWriter.close

  println(s"[Finish Item Reference Finder] ${pathOutputString(writeFilePath)}")
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.finder

import d2k.appdefdoc.finder.Commons._
import d2k.appdefdoc.parser._
import d2k.appdefdoc.parser.D2kParser
import scala.util.Try
import scala.reflect.io.Directory
import java.io.File
import scala.io.Source
import scala.collection.TraversableOnce.flattenTraversableOnce
import scala.reflect.io.Path.string2path
import org.apache.commons.io.output.FileWriterWithEncoding
import java.io.FileWriter
import scala.annotation.tailrec

case class IrrfData(appData: RirfData, renameApps: Seq[RenameData])
case class RenameData(componentId: String, beforeName: String, afterName: String)
object ItemRenameRouteFinder extends App with D2kParser {
  val isLocalMode = args.size >= 5
  val (baseUrl, branch, targetResourceId, targetItemId) = (args(0), args(1), args(2), args(3))
  val basePath = if (isLocalMode) args(4) else "C:/d2k_docs"
  val writeBase = s"data/irRouteFinder/${targetResourceId}_${targetItemId}"
  val writePath = Directory(writeBase)

  println(s"[Start Item Rename RouteFinder${if (isLocalMode) " on Local Mode" else ""}] ${args.mkString(" ")}")

  val itemBasePath = s"${basePath}/apps/common"
  val itemFileNames = recList(new File(itemBasePath)).filter(x => x.getName.contains(".md") && !x.getName.endsWith("README.md"))
  val itemDefMap = createItemDefMap(targetItemId, itemFileNames)

  val appBasePath = s"${basePath}/apps"
  val appDefList = Commons.appDefList(appBasePath)

  itemDefMap.get(targetResourceId).map { targetResource =>
    val searchTargetResult = s"[${targetResource.id}](${targetResource.path})[${targetResource.name}] / ${targetItemId}[${targetResource.itemName}]"
    val result = recursiveSearchWithRename(appDefList, itemDefMap,
      (appdef: AppDef) => appdef.inputList, (appdef: AppDef) => appdef.outputList, targetItemId, itemFileNames)(targetResourceId).distinct

    val flowRender = createFlowRenderWithRename(result)
    val referResult = createReferResult(result.map(_.appData))
    val renameAppList = result.flatMap { result =>
      val rirfData = result.appData
      result.renameApps.flatMap { renameData =>
        rirfData.appDetail.map { rirfData =>
          (rirfData.appDef.appInfo.id, rirfData.path, rirfData.appDef.appInfo.name,
            renameData.componentId, s"${rirfData.path.dropRight(9)}${renameData.componentId}.md", renameData.beforeName, renameData.afterName)
        }
      }
    }

    val renameAppTableList = renameAppList.map { x =>
      mkTable(s"[${x._1}](${x._2})", x._3, s"[${x._4}](${x._5})", x._6, x._7)
    }.mkString("\n")

    val tmpl = fileToStr("finderTemplates/irrResult.tmpl")
    val writeFilePath = s"${writePath.toString}/${targetResourceId}_${targetItemId}.md"
    writePath.createDirectory(true, false)
    val writer = new FileWriter(writeFilePath)
    val conved = tmpl.replaceAllLiterally("%%SearchTarget%%", searchTargetResult)
      .replaceAllLiterally("%%ResultFlow%%", flowRender)
      .replaceAllLiterally("%%ResultApplicationList%%", referResult)
      .replaceAllLiterally("%%ResultItemRenameApplicationList%%", renameAppTableList)
    writer.write(conved)
    writer.close

    val csvReferTitle = Seq("Target Resource Id", "Target Item Id", "Input Resource Id", "Input Resource Name", "App Id", "App Name", "Resource Url", "App Url").mkString("", " , ", "\n")
    val csvReferData = result.map { data =>
      val resPath = localPath2Url(baseUrl, basePath, data.appData.resDetail.path)
      val csvData = data.appData.appDetail.map { x =>
        val appUrl = localPath2Url(baseUrl, basePath, x.path)
        Seq(data.appData.resDetail.id, data.appData.resDetail.name, x.appDef.appInfo.id, x.appDef.appInfo.name, resPath, appUrl)
      }.getOrElse(Seq(data.appData.resDetail.id, data.appData.resDetail.name, "", "", resPath, ""))
      (Seq(targetResourceId, targetItemId) ++ csvData).mkString(" , ")
    }.mkString("\n")
    val csvReferFilePath = s"${writePath.toString}/refer.csv"
    val csvReferWriter = new FileWriterWithEncoding(csvReferFilePath, "MS932")
    csvReferWriter.write(csvReferTitle)
    csvReferWriter.write(csvReferData)
    csvReferWriter.write("\n")
    csvReferWriter.close

    val csvImplTitle = Seq("App Id", "App Name", "Sub App Id", "Url", "Sub Url", "Before Name", "After Name").mkString("", " , ", "\n")
    val csvImplData = renameAppList.map {
      case (appid, path, appName, subAppid, subAppPath, beforeName, afterName) =>
        val localAppPath = localPath2Url(baseUrl, basePath, path)
        val localSubAppPath = localPath2Url(baseUrl, basePath, subAppPath)
        Seq(appid, appName, subAppid, localAppPath, localSubAppPath, beforeName, afterName).mkString(" , ")
    }.mkString("\n")
    val csvImplFilePath = s"${writePath.toString}/rename.csv"
    val csvImplWriter = new FileWriterWithEncoding(csvImplFilePath, "MS932")
    csvImplWriter.write(csvImplTitle)
    csvImplWriter.write(csvImplData)
    csvImplWriter.write("\n")
    csvImplWriter.close

    println(s"[Finish Item Rename RouteFinder] ${pathOutputString(writeFilePath)}")

  }.orElse {
    println(s"[Error Item Rename RouteFinder] Not Found ${targetResourceId} or ${targetItemId}")
    None
  }
}


', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.finder.jsonbase

import scala.reflect.io.Path
import scala.io.Source

import org.json4s.JsonAST.JValue
import org.json4s._
import org.json4s.jackson.JsonMethods._
import org.json4s.DefaultFormats

import d2k.appdefdoc.finder.{ Commons => fcom }
import java.io.File
import scala.util.Try
import d2k.appdefdoc.parser.AppDefParser
import d2k.appdefdoc.parser.IoData
import d2k.appdefdoc.parser.AppDef
import d2k.appdefdoc.parser.AppInfo
import d2k.appdefdoc.parser.ComponentDefInfo

case class LinkData(appId: String, resourceId: String, dataType: String)
case class NodeData(logical_name: String, physical_name: String, dataType: String)
object Commons {
  def createJsonPath(basePath: String) = Path(basePath).parent / "concept_flow" / "json"

  def jsStr(jv: JValue)(name: String) = (jv \ name).values.toString

  implicit val formats = DefaultFormats

  def createLinkReadJson(jsonPath: Path) = {
    val linkReadPath = (jsonPath / "link_read.json").jfile
    Source.fromFile(linkReadPath, "MS932").getLines.map { str =>
      val value = jsStr(parse(str)) _
      LinkData(value("to_node"), value("from_node"), "input")
    }
  }

  def createLinkWriteJson(jsonPath: Path) = {
    val linkWritePath = (jsonPath / "link_write.json").jfile
    Source.fromFile(linkWritePath, "MS932").getLines.map { str =>
      val value = jsStr(parse(str)) _
      LinkData(value("from_node"), value("to_node"), "output")
    }
  }

  def createNodeAppMap(jsonPath: Path) = {
    val nodeAppPath = (jsonPath / "node_application.json").jfile
    Source.fromFile(nodeAppPath, "MS932").getLines.map { str =>
      val value = jsStr(parse(str)) _
      (value("physical_name"), NodeData(value("logical_name"), value("physical_name"), value("language")))
    }.toMap
  }

  def createNodeResourceMap(jsonPath: Path) = {
    val nodeResourcePath = (jsonPath / "node_io.json").jfile
    Source.fromFile(nodeResourcePath, "MS932").getLines.map { str =>
      val value = jsStr(parse(str)) _
      (value("key"), NodeData(value("logical_name"), value("physical_name"), value("data_type")))
    }.toMap
  }

  def appDefList(appBasePath: String) = {
    fcom.recList(new File(appBasePath)).filter(_.getName.contains("README.md"))
      .filter(x => fcom.appDefRegx.findFirstMatchIn(x.toString).isDefined)
      .flatMap(x => Try {
        val appdef = AppDefParser(x.toString).get
        (appdef.appInfo.id, x.toString)
      }.toOption.orElse { println(s"  appDef parse error: ${x.toString}"); None }).toList
  }

  def createIoData(itemNamePath: Array[(String, String)], nodeResourceMap: Map[String, NodeData])(
    data: Option[List[LinkData]]) = {
    data.map {
      _.map { ld =>
        val path = itemNamePath.find { x =>
          ld.resourceId.contains(x._1)
        }.map(_._2).getOrElse("")
        IoData(
          nodeResourceMap.get(ld.resourceId).map(_.physical_name).getOrElse(""),
          path,
          nodeResourceMap.get(ld.resourceId).map(_.dataType).getOrElse(""),
          nodeResourceMap.get(ld.resourceId).map(_.logical_name).getOrElse(""))
      }
    }.getOrElse(List.empty[IoData])
  }

  def createItemNamePathList(basePath: String) = {
    val itemBasePath = s"${basePath}/apps/common"
    val itemNames = fcom.recList(new File(itemBasePath)).filter(_.getName.contains(".md"))
    itemNames.map(x => (Path(x).name.dropRight(3), Path(x).toString.replaceAllLiterally("\\", "/")))
  }

  def removePhyphen(s: String) = s.replaceAllLiterally("-", "")
  def createJsonAppdef(basePath: String, itemNamePath: Array[(String, String)],
                       nodeAppMap: Map[String, NodeData], nodeResourceMap: Map[String, NodeData],
                       linkJson: List[LinkData]) = {
    val appDefResult = appDefList(s"${basePath}/apps").toMap
    val creIoData = createIoData(itemNamePath, nodeResourceMap) _
    linkJson.toList.groupBy(_.appId).mapValues(_.groupBy(_.dataType)).map {
      case (k, v) =>
        AppDef(AppInfo(removePhyphen(k), nodeAppMap.get(k).map(_.logical_name).getOrElse(""), ""), None,
          List.empty[ComponentDefInfo], creIoData(v.get("input")), creIoData(v.get("output")))
    }.map(appdef => (appDefResult.get(appdef.appInfo.id).getOrElse(""), appdef))
  }

  def mergeAppdef(jsonAppDef: Seq[(String, AppDef)], appDef: Seq[(String, AppDef)]) =
    jsonAppDef.map { jAppdef =>
      appDef.find { x => jAppdef._2.appInfo.id.contains(x._2.appInfo.id) }.getOrElse(jAppdef)
    }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.finder.jsonbase

import d2k.appdefdoc.finder.{ Commons => fcom }
import d2k.appdefdoc.parser._
import d2k.appdefdoc.parser.D2kParser
import scala.reflect.io.Directory
import java.io.File
import scala.collection.Seq
import scala.reflect.io.Path.string2path
import scala.annotation.tailrec
import scala.util.Try
import java.io.FileWriter
import org.apache.commons.io.output.FileWriterWithEncoding

import Commons._
import d2k.appdefdoc.finder.RirfDetail

case class GafData(baseAppId: Option[String], baseResourceId: String, result: Seq[(String, AppDef)])
object GeneratingApplicationRouteFinder extends App with D2kParser {
  val isLocalMode = args.size >= 4
  val (baseUrl, branch, targetResourceId) = (args(0), args(1), args(2))
  val basePath = if (isLocalMode) args(3) else "C:/d2k_docs"
  val writeBase = s"data/js/gaRouteFinder/${targetResourceId}"
  val writePath = Directory(writeBase)
  val jsonPath = createJsonPath(basePath)

  println(s"[Start Generating Application Route Finder ${if (isLocalMode) " on Local Mode" else ""}] ${args.mkString(" ")}")

  val itemBasePath = s"${basePath}/apps/common"
  val itemNames = fcom.recList(new File(itemBasePath)).filter(x => x.getName.contains(".md") && !x.getName.endsWith("README.md"))

  val linkReadJson = createLinkReadJson(jsonPath)
  val linkWriteJson = createLinkWriteJson(jsonPath)

  val nodeAppMap = createNodeAppMap(jsonPath)
  val nodeResourceMap = createNodeResourceMap(jsonPath)

  val itemNamePath = createItemNamePathList(basePath)
  val targetResource = nodeResourceMap.get(targetResourceId)

  val searchTargetResult = nodeResourceMap.toList.find {
    case (k, v) => k.contains(targetResourceId)
  }.flatMap { node =>
    itemNamePath.find { p =>
      node._2.physical_name.contains(p._1)
    }.map { p =>
      s"[${node._2.physical_name}](${p._2})[${node._2.logical_name}]"
    }
  }.getOrElse(s"${targetResourceId}")

  val appBasePath = s"${basePath}/apps"

  val jsonAppdef = createJsonAppdef(basePath, itemNamePath, nodeAppMap, nodeResourceMap, (linkReadJson ++ linkWriteJson).toList)
  val itemDefMap = itemNamePath.flatMap {
    case (name, path) =>
      Try {
        val itemdef = ItemDefParser(path.toString).get
        itemdef.details.map { item =>
          (itemdef.id, RirfDetail(itemdef.id, itemdef.name, item.name, path.toString))
        }
      }.getOrElse { println(s"  itemDef parse error: ${path}"); Seq.empty[(String, RirfDetail)] }
  }.toMap

  val result = fcom.recursiveSearch(jsonAppdef.toList, itemDefMap,
    (appdef: AppDef) => appdef.outputList, (appdef: AppDef) => appdef.inputList)(targetResourceId).distinct

  val flowRender = fcom.createFlowRender(
    result,
    d => Seq(d.appDetail.map(x => s"${x.appDef.appInfo.id} --> ${d.resDetail.id}_res"), d.parentAppId.map(x => s"${d.resDetail.id}_res --> ${x}")).flatten)

  val referResult = fcom.createReferResult(result)

  val tmpl = fcom.fileToStr("finderTemplates/garResult.tmpl")
  val writeFilePath = s"${writePath.toString}/${targetResourceId}.md"
  writePath.createDirectory(true, false)
  val writer = new FileWriter(writeFilePath)
  val conved = tmpl.replaceAllLiterally("%%SearchTarget%%", searchTargetResult)
    .replaceAllLiterally("%%ResultFlow%%", flowRender)
    .replaceAllLiterally("%%ResultApplicationList%%", referResult)
  writer.write(conved)
  writer.close

  val csvReferTitle = Seq("Target Resource Id", "Output Resource Id", "Output Resource Name", "App Id", "App Name", "Resource Url", "App Url").mkString("", " , ", "\n")
  val csvReferData = result.map { data =>
    val resPath = fcom.localPath2Url(baseUrl, basePath, data.resDetail.path)
    val csvData = data.appDetail.map { x =>
      val appUrl = fcom.localPath2Url(baseUrl, basePath, x.path)
      Seq(targetResourceId, data.resDetail.id, data.resDetail.name, x.appDef.appInfo.id, x.appDef.appInfo.name, resPath, appUrl)
    }.getOrElse(Seq(targetResourceId, data.resDetail.id, data.resDetail.name, "", "", resPath, ""))
    csvData.mkString(" , ")
  }.mkString("\n")
  val csvReferFilePath = s"${writePath.toString}/refer.csv"
  val csvReferWriter = new FileWriterWithEncoding(csvReferFilePath, "MS932")
  csvReferWriter.write(csvReferTitle)
  csvReferWriter.write(csvReferData)
  csvReferWriter.write("\n")
  csvReferWriter.close

  println(s"[Finish Generating Application Route Finder] ${fcom.pathOutputString(writeFilePath)}")
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.finder.jsonbase

import d2k.appdefdoc.finder._
import d2k.appdefdoc.finder.{ Commons => fcom }
import Commons._

import scala.util.Try
import scala.reflect.io.Directory
import java.io.FileWriter
import org.apache.commons.io.output.FileWriterWithEncoding
import d2k.appdefdoc.parser._
import java.io.File
import d2k.appdefdoc.parser.D2kParser
import scala.io.Source

object ItemReferenceFinder extends App with D2kParser {
  val isLocalMode = args.size >= 4
  val (baseUrl, branch, targetItemId) = (args(0), args(1), args(2))
  val basePath = if (isLocalMode) args(3) else "C:/d2k_docs"
  val writeBase = s"data/js/irFinder/${targetItemId}"
  val writePath = Directory(writeBase)
  val jsonPath = createJsonPath(basePath)

  println(s"[Start Item Reference Finder${if (isLocalMode) " on Local Mode" else ""}] ${args.mkString(" ")}")

  val itemBasePath = s"${basePath}/apps/common"
  val itemNames = fcom.recList(new File(itemBasePath)).filter(_.getName.contains(".md"))
  val targetResouces = itemNames.filter { path =>
    Try { readItemDefMd(path.toString).contains(targetItemId) }.getOrElse({ println(s"  itemDef parse error: ${path}"); false })
  }.toList
  val itemDefList = targetResouces.flatMap(x => Try((x.toString, ItemDefParser(x.toString).get)).toOption).toList

  val appBasePath = s"${basePath}/apps"
  val appDefList = fcom.appDefList(appBasePath)

  val linkReadJson = createLinkReadJson(jsonPath)
  val linkWriteJson = createLinkWriteJson(jsonPath)
  val nodeAppMap = createNodeAppMap(jsonPath)
  val nodeResourceMap = createNodeResourceMap(jsonPath)
  val itemNamePath = createItemNamePathList(basePath)
  val jsonAppdefNoFlow = createJsonAppdef(basePath, itemNamePath, nodeAppMap, nodeResourceMap, (linkReadJson ++ linkWriteJson).toList)
  val jsonAppdef = mergeAppdef(jsonAppdefNoFlow.toList, appDefList)

  val rrdList = itemDefList.flatMap { itemDef =>
    val targetName = itemDef._2.id
    val filtered = jsonAppdef.filter(_._2.inputList.map(_.id).exists(_.contains(targetName.trim)))

    val result = fcom.createRrfData(targetName, filtered)
    val writeResult = fcom.writeRrfData(targetName, baseUrl, basePath, writePath, Some("    [Finish Resource Relation Finder]"), result)
    writeResult.map(x => (x._1, x._2))
  }

  val resultResource = rrdList.map {
    case (io, _) => fcom.mkTable(s"[${io.id}](${io.id}.md)", io.srcType, io.name)
  }

  val implList = fcom.implementList(targetItemId, jsonAppdef.toList)

  val tmpl = fcom.fileToStr("finderTemplates/irResult.tmpl")
  val writeFilePath = s"${writePath.toString}/README.md"
  writePath.createDirectory(true, false)
  val writer = new FileWriter(writeFilePath)
  val conved = tmpl.replaceAllLiterally("%%SearchTarget%%", targetItemId)
    .replaceAllLiterally("%%ResultResource%%", resultResource.mkString("\n"))
    .replaceAllLiterally("%%ResultItemReference%%", implList.map(_._2).mkString("\n"))
  writer.write(conved)
  writer.close

  val allAppNames = (rrdList.map(_._2.appInfo.id) ++ implList.map(_._1._2.appInfo.id)).toSet
  val allApps = allAppNames.map(x => jsonAppdef.filter(_._2.appInfo.id == x).head)

  val csvImplTitle = Seq("Target Name", "App Id", "Sub App Id", "App Name", "Url", "Sub Url").mkString("", " , ", "\n")
  val csvImplData = implList.map {
    case (app, _, subImpl) =>
      val appdef = jsonAppdef.filter(_._2.appInfo.id == app._2.appInfo.id).head
      val localPath = fcom.localPath2Url(baseUrl, basePath, appdef._1)
      Seq(targetItemId, app._2.appInfo.id, subImpl.dropRight(3), app._2.appInfo.name,
        localPath, s"${localPath.dropRight(9)}${subImpl}").mkString(" , ")
  }.mkString("\n")

  val csvImplFilePath = s"${writePath.toString}/impl.csv"
  val csvImplWriter = new FileWriterWithEncoding(csvImplFilePath, "MS932")
  csvImplWriter.write(csvImplTitle)
  csvImplWriter.write(csvImplData)
  csvImplWriter.write("\n")
  csvImplWriter.close

  val csvAllTitle = Seq("Target Name", "App Id", "App Name", "Url").mkString("", " , ", "\n")
  val csvAllData = allApps.map {
    case (path, appDef) =>
      Seq(targetItemId, appDef.appInfo.id, appDef.appInfo.name, fcom.localPath2Url(baseUrl, basePath, path)).mkString(" , ")
  }.mkString("\n")

  val csvAllFilePath = s"${writePath.toString}/all.csv"
  val csvAllWriter = new FileWriterWithEncoding(csvAllFilePath, "MS932")
  csvAllWriter.write(csvAllTitle)
  csvAllWriter.write(csvAllData)
  csvAllWriter.write("\n")
  csvAllWriter.close

  println(s"[Finish Item Reference Finder] ${fcom.pathOutputString(writeFilePath)}")
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.finder.jsonbase

import d2k.appdefdoc.finder._
import d2k.appdefdoc.finder.{ Commons => fcom }
import Commons._

import d2k.appdefdoc.parser._
import d2k.appdefdoc.parser.D2kParser
import scala.util.Try
import scala.reflect.io.Directory
import java.io.File
import scala.io.Source
import scala.collection.TraversableOnce.flattenTraversableOnce
import scala.reflect.io.Path.string2path
import org.apache.commons.io.output.FileWriterWithEncoding
import java.io.FileWriter
import scala.annotation.tailrec

object ItemRenameRouteFinder extends App with D2kParser {
  val isLocalMode = args.size >= 5
  val (baseUrl, branch, targetResourceId, targetItemId) = (args(0), args(1), args(2), args(3))
  val basePath = if (isLocalMode) args(4) else "C:/d2k_docs"
  val jsonPath = createJsonPath(basePath)

  val writeBase = s"data/js/irRouteFinder/${targetResourceId}_${targetItemId}"
  val writePath = Directory(writeBase)

  println(s"[Start Item Rename RouteFinder${if (isLocalMode) " on Local Mode" else ""}] ${args.mkString(" ")}")

  val itemBasePath = s"${basePath}/apps/common"
  val itemFileNames = fcom.recList(new File(itemBasePath)).filter(x => x.getName.contains(".md") && !x.getName.endsWith("README.md"))
  val itemDefMap = fcom.createItemDefMap(targetItemId, itemFileNames)

  val appBasePath = s"${basePath}/apps"
  val appDefList = fcom.appDefList(appBasePath)

  val linkReadJson = createLinkReadJson(jsonPath)
  val linkWriteJson = createLinkWriteJson(jsonPath)
  val nodeAppMap = createNodeAppMap(jsonPath)
  val nodeResourceMap = createNodeResourceMap(jsonPath)
  val itemNamePath = createItemNamePathList(basePath)
  val jsonAppdefNoFlow = createJsonAppdef(basePath, itemNamePath, nodeAppMap, nodeResourceMap, (linkReadJson ++ linkWriteJson).toList)
  val jsonAppdef = mergeAppdef(jsonAppdefNoFlow.toList, appDefList)

  itemDefMap.get(targetResourceId).map { targetResource =>
    val searchTargetResult = s"[${targetResource.id}](${targetResource.path})[${targetResource.name}] / ${targetItemId}[${targetResource.itemName}]"
    val result = fcom.recursiveSearchWithRename(jsonAppdef.toList, itemDefMap,
      (appdef: AppDef) => appdef.inputList, (appdef: AppDef) => appdef.outputList, targetItemId, itemFileNames)(targetResourceId).distinct

    val flowRender = fcom.createFlowRenderWithRename(result)
    val referResult = fcom.createReferResult(result.map(_.appData))
    val renameAppList = result.flatMap { result =>
      val rirfData = result.appData
      result.renameApps.flatMap { renameData =>
        rirfData.appDetail.map { rirfData =>
          (rirfData.appDef.appInfo.id, rirfData.path, rirfData.appDef.appInfo.name,
            renameData.componentId, s"${rirfData.path.dropRight(9)}${renameData.componentId}.md", renameData.beforeName, renameData.afterName)
        }
      }
    }

    val renameAppTableList = renameAppList.map { x =>
      fcom.mkTable(s"[${x._1}](${x._2})", x._3, s"[${x._4}](${x._5})", x._6, x._7)
    }.mkString("\n")

    val tmpl = fcom.fileToStr("finderTemplates/irrResult.tmpl")
    val writeFilePath = s"${writePath.toString}/${targetResourceId}_${targetItemId}.md"
    writePath.createDirectory(true, false)
    val writer = new FileWriter(writeFilePath)
    val conved = tmpl.replaceAllLiterally("%%SearchTarget%%", searchTargetResult)
      .replaceAllLiterally("%%ResultFlow%%", flowRender)
      .replaceAllLiterally("%%ResultApplicationList%%", referResult)
      .replaceAllLiterally("%%ResultItemRenameApplicationList%%", renameAppTableList)
    writer.write(conved)
    writer.close

    val csvReferTitle = Seq("Target Resource Id", "Target Item Id", "Input Resource Id", "Input Resource Name", "App Id", "App Name", "Resource Url", "App Url").mkString("", " , ", "\n")
    val csvReferData = result.map { data =>
      val resPath = fcom.localPath2Url(baseUrl, basePath, data.appData.resDetail.path)
      val csvData = data.appData.appDetail.map { x =>
        val appUrl = fcom.localPath2Url(baseUrl, basePath, x.path)
        Seq(data.appData.resDetail.id, data.appData.resDetail.name, x.appDef.appInfo.id, x.appDef.appInfo.name, resPath, appUrl)
      }.getOrElse(Seq(data.appData.resDetail.id, data.appData.resDetail.name, "", "", resPath, ""))
      (Seq(targetResourceId, targetItemId) ++ csvData).mkString(" , ")
    }.mkString("\n")
    val csvReferFilePath = s"${writePath.toString}/refer.csv"
    val csvReferWriter = new FileWriterWithEncoding(csvReferFilePath, "MS932")
    csvReferWriter.write(csvReferTitle)
    csvReferWriter.write(csvReferData)
    csvReferWriter.write("\n")
    csvReferWriter.close

    val csvImplTitle = Seq("App Id", "App Name", "Sub App Id", "Url", "Sub Url", "Before Name", "After Name").mkString("", " , ", "\n")
    val csvImplData = renameAppList.map {
      case (appid, path, appName, subAppid, subAppPath, beforeName, afterName) =>
        val localAppPath = fcom.localPath2Url(baseUrl, basePath, path)
        val localSubAppPath = fcom.localPath2Url(baseUrl, basePath, subAppPath)
        Seq(appid, appName, subAppid, localAppPath, localSubAppPath, beforeName, afterName).mkString(" , ")
    }.mkString("\n")
    val csvImplFilePath = s"${writePath.toString}/rename.csv"
    val csvImplWriter = new FileWriterWithEncoding(csvImplFilePath, "MS932")
    csvImplWriter.write(csvImplTitle)
    csvImplWriter.write(csvImplData)
    csvImplWriter.write("\n")
    csvImplWriter.close

    println(s"[Finish Item Rename RouteFinder] ${fcom.pathOutputString(writeFilePath)}")

  }.orElse {
    println(s"[Error Item Rename RouteFinder] Not Found ${targetResourceId} or ${targetItemId}")
    None
  }
}


', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.finder.jsonbase

import d2k.appdefdoc.finder._
import d2k.appdefdoc.finder.{ Commons => fcom }
import Commons._

import scala.util.Try
import scala.reflect.io.Directory
import java.io.FileWriter
import org.apache.commons.io.output.FileWriterWithEncoding
import d2k.appdefdoc.parser._
import java.io.File
import d2k.appdefdoc.parser.D2kParser
import scala.io.Source
import scala.annotation.tailrec

object ResourceItemRouteFinder extends App with D2kParser {
  val isLocalMode = args.size >= 5
  val (baseUrl, branch, targetResourceId, targetItemId) = (args(0), args(1), args(2), args(3))
  val basePath = if (isLocalMode) args(4) else "C:/d2k_docs"
  val writeBase = s"data/js/riRouteFinder/${targetResourceId}_${targetItemId}"
  val writePath = Directory(writeBase)
  val jsonPath = createJsonPath(basePath)

  println(s"[Start Resource Item Finder using json data${if (isLocalMode) " on Local Mode" else ""}] ${args.mkString(" ")}")

  val itemBasePath = s"${basePath}/apps/common"
  val itemNames = fcom.recList(new File(itemBasePath)).filter(x => x.getName.contains(".md") && !x.getName.endsWith("README.md"))
  val itemDefMap = fcom.createItemDefMap(targetItemId, itemNames)

  val linkReadJson = createLinkReadJson(jsonPath)
  val linkWriteJson = createLinkWriteJson(jsonPath)
  val nodeAppMap = createNodeAppMap(jsonPath)
  val nodeResourceMap = createNodeResourceMap(jsonPath)
  val itemNamePath = createItemNamePathList(basePath)
  val jsonAppdef = createJsonAppdef(basePath, itemNamePath, nodeAppMap, nodeResourceMap, (linkReadJson ++ linkWriteJson).toList)

  def searchResource = jsonAppdef.find {
    case (_, appdef) => appdef.inputList.exists(x => itemDefMap.keySet.contains(x.id))
  }

  val targetResource = itemDefMap(targetResourceId)
  val searchTargetResult = s"[${targetResource.id}](${targetResource.path})[${targetResource.name}] / ${targetItemId}[${targetResource.itemName}]"

  val result = fcom.recursiveSearch(jsonAppdef.toList, itemDefMap,
    (appdef: AppDef) => appdef.inputList, (appdef: AppDef) => appdef.outputList)(targetResourceId).distinct

  val flowRender = fcom.createFlowRender(result)
  val referResult = fcom.createReferResult(result)

  val appBasePath = s"${basePath}/apps"
  val appDefList = fcom.appDefList(appBasePath)
  val appIds = result.flatMap(_.appDetail.map(_.appDef.appInfo.id))
  val filteredAppDefList = appDefList.filter(x => appIds.contains(x._2.appInfo.id))
  val implList = fcom.implementList(targetItemId, filteredAppDefList)
  val implTableRender = implList.map(_._2).mkString("\n")

  val tmpl = fcom.fileToStr("finderTemplates/rirResult.tmpl")
  val writeFilePath = s"${writePath.toString}/${targetResourceId}_${targetItemId}.md"
  writePath.createDirectory(true, false)
  val writer = new FileWriter(writeFilePath)
  val conved = tmpl.replaceAllLiterally("%%SearchTarget%%", searchTargetResult)
    .replaceAllLiterally("%%ResultFlow%%", flowRender)
    .replaceAllLiterally("%%ResultApplicationList%%", referResult)
    .replaceAllLiterally("%%ResultItemReference%%", implTableRender)
  writer.write(conved)
  writer.close

  val csvReferTitle = Seq("Target Resource Id", "Target Item Id", "Input Resource Id", "Input Resource Name", "App Id", "App Name", "Resource Url", "App Url").mkString("", " , ", "\n")
  val csvReferData = result.map { data =>
    val resPath = fcom.localPath2Url(baseUrl, basePath, data.resDetail.path)
    val csvData = data.appDetail.map { x =>
      val appUrl = fcom.localPath2Url(baseUrl, basePath, x.path)
      Seq(data.resDetail.id, data.resDetail.name, x.appDef.appInfo.id, x.appDef.appInfo.name, resPath, appUrl)
    }.getOrElse(Seq(data.resDetail.id, data.resDetail.name, "", "", resPath, ""))
    (Seq(targetResourceId, targetItemId) ++ csvData).mkString(" , ")
  }.mkString("\n")
  val csvReferFilePath = s"${writePath.toString}/refer.csv"
  val csvReferWriter = new FileWriterWithEncoding(csvReferFilePath, "MS932")
  csvReferWriter.write(csvReferTitle)
  csvReferWriter.write(csvReferData)
  csvReferWriter.write("\n")
  csvReferWriter.close

  val csvImplTitle = Seq("Target Resource Id", "Target Item Id", "App Id", "Sub App Id", "App Name", "Url", "Sub Url").mkString("", " , ", "\n")
  val csvImplData = implList.map {
    case ((path, appdef), _, subId) =>
      val localPath = fcom.localPath2Url(baseUrl, basePath, path)
      Seq(targetResourceId, targetItemId, appdef.appInfo.id, subId.dropRight(3),
        appdef.appInfo.name, localPath, s"${localPath.dropRight(9)}${subId}").mkString(" , ")
  }.mkString("\n")
  val csvImplFilePath = s"${writePath.toString}/impl.csv"
  val csvImplWriter = new FileWriterWithEncoding(csvImplFilePath, "MS932")
  csvImplWriter.write(csvImplTitle)
  csvImplWriter.write(csvImplData)
  csvImplWriter.write("\n")
  csvImplWriter.close

  println(s"[Finish Resource Item Route Finder] ${fcom.pathOutputString(writeFilePath)}")
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.finder.jsonbase

import d2k.appdefdoc.finder._
import d2k.appdefdoc.finder.{ Commons => fcom }
import Commons._

import scala.io.Source
import scala.util.Try
import scala.reflect.io.Directory
import java.io.FileWriter
import org.apache.commons.io.output.FileWriterWithEncoding
import d2k.appdefdoc.parser._
import java.io.File
import scala.reflect.io.Path

import org.json4s._
import org.json4s.jackson.JsonMethods._
import org.json4s.DefaultFormats

object ResourceRelationFinder extends App {
  val isLocalMode = args.size >= 4
  val (baseUrl, branch, targetName) = (args(0), args(1), args(2))
  val basePath = if (isLocalMode) args(3) else "C:/d2k_docs"
  val jsonPath = createJsonPath(basePath)

  println(s"[Start App Relation Finder using json data${if (isLocalMode) " on Local Mode" else ""}] ${args.mkString(" ")}")

  val linkReadJson = createLinkReadJson(jsonPath)
  val linkWriteJson = createLinkWriteJson(jsonPath)

  val writePath = Directory(s"data/js/rrFinder/${targetName}")

  val nodeAppMap = createNodeAppMap(jsonPath)
  val nodeResourceMap = createNodeResourceMap(jsonPath)

  val itemNamePath = createItemNamePathList(basePath)

  val jsonAppdef = createJsonAppdef(basePath, itemNamePath, nodeAppMap, nodeResourceMap, (linkReadJson ++ linkWriteJson).toList)
  val appBasePath = s"${basePath}/apps"
  val result = fcom.createRrfData(targetName, jsonAppdef.toSeq)
  fcom.writeRrfData(targetName, baseUrl, basePath, writePath, Some("[Finish App Relation Finder using json data]"), result)
}


', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.finder

import Commons._

import scala.util.Try
import scala.reflect.io.Directory
import java.io.FileWriter
import org.apache.commons.io.output.FileWriterWithEncoding
import d2k.appdefdoc.parser._
import java.io.File
import d2k.appdefdoc.parser.D2kParser
import scala.io.Source
import scala.annotation.tailrec

case class RirfDetail(id: String, name: String = "", itemName: String = "", path: String = "")
case class RirfAppDetail(appDef: AppDef, path: String)
case class RirfData(resDetail: RirfDetail, appDetail: Option[RirfAppDetail], parentAppId: Option[String] = None)
object ResourceItemRouteFinder extends App with D2kParser {
  val isLocalMode = args.size >= 5
  val (baseUrl, branch, targetResourceId, targetItemId) = (args(0), args(1), args(2), args(3))
  val basePath = if (isLocalMode) args(4) else "C:/d2k_docs"
  val writeBase = s"data/riRouteFinder/${targetResourceId}_${targetItemId}"
  val writePath = Directory(writeBase)

  println(s"[Start Resource Item Finder${if (isLocalMode) " on Local Mode" else ""}] ${args.mkString(" ")}")

  val itemBasePath = s"${basePath}/apps/common"
  val itemNames = recList(new File(itemBasePath)).filter(x => x.getName.contains(".md") && !x.getName.endsWith("README.md"))
  val itemDefMap = createItemDefMap(targetItemId, itemNames)

  val appBasePath = s"${basePath}/apps"
  val appDefList = Commons.appDefList(appBasePath)

  def searchResource = appDefList.find {
    case (_, appdef) => appdef.inputList.exists(x => itemDefMap.keySet.contains(x.id))
  }

  val targetResource = itemDefMap(targetResourceId)
  val searchTargetResult = s"[${targetResource.id}](${targetResource.path})[${targetResource.name}] / ${targetItemId}[${targetResource.itemName}]"

  val result = recursiveSearch(appDefList, itemDefMap,
    (appdef: AppDef) => appdef.inputList, (appdef: AppDef) => appdef.outputList)(targetResourceId).distinct

  val flowRender = createFlowRender(result)
  val referResult = createReferResult(result)

  val appIds = result.flatMap(_.appDetail.map(_.appDef.appInfo.id))
  val filteredAppDefList = appDefList.filter(x => appIds.contains(x._2.appInfo.id))

  val implList = implementList(targetItemId, filteredAppDefList)
  val implTableRender = implList.map(_._2).mkString("\n")

  val tmpl = fileToStr("finderTemplates/rirResult.tmpl")
  val writeFilePath = s"${writePath.toString}/${targetResourceId}_${targetItemId}.md"
  writePath.createDirectory(true, false)
  val writer = new FileWriter(writeFilePath)
  val conved = tmpl.replaceAllLiterally("%%SearchTarget%%", searchTargetResult)
    .replaceAllLiterally("%%ResultFlow%%", flowRender)
    .replaceAllLiterally("%%ResultApplicationList%%", referResult)
    .replaceAllLiterally("%%ResultItemReference%%", implTableRender)
  writer.write(conved)
  writer.close

  val csvReferTitle = Seq("Target Resource Id", "Target Item Id", "Input Resource Id", "Input Resource Name", "App Id", "App Name", "Resource Url", "App Url").mkString("", " , ", "\n")
  val csvReferData = result.map { data =>
    val resPath = localPath2Url(baseUrl, basePath, data.resDetail.path)
    val csvData = data.appDetail.map { x =>
      val appUrl = localPath2Url(baseUrl, basePath, x.path)
      Seq(data.resDetail.id, data.resDetail.name, x.appDef.appInfo.id, x.appDef.appInfo.name, resPath, appUrl)
    }.getOrElse(Seq(data.resDetail.id, data.resDetail.name, "", "", resPath, ""))
    (Seq(targetResourceId, targetItemId) ++ csvData).mkString(" , ")
  }.mkString("\n")
  val csvReferFilePath = s"${writePath.toString}/refer.csv"
  val csvReferWriter = new FileWriterWithEncoding(csvReferFilePath, "MS932")
  csvReferWriter.write(csvReferTitle)
  csvReferWriter.write(csvReferData)
  csvReferWriter.write("\n")
  csvReferWriter.close

  val csvImplTitle = Seq("Target Resource Id", "Target Item Id", "App Id", "Sub App Id", "App Name", "Url", "Sub Url").mkString("", " , ", "\n")
  val csvImplData = implList.map {
    case ((path, appdef), _, subId) =>
      val localPath = localPath2Url(baseUrl, basePath, path)
      Seq(targetResourceId, targetItemId, appdef.appInfo.id, subId.dropRight(3),
        appdef.appInfo.name, localPath, s"${localPath.dropRight(9)}${subId}").mkString(" , ")
  }.mkString("\n")
  val csvImplFilePath = s"${writePath.toString}/impl.csv"
  val csvImplWriter = new FileWriterWithEncoding(csvImplFilePath, "MS932")
  csvImplWriter.write(csvImplTitle)
  csvImplWriter.write(csvImplData)
  csvImplWriter.write("\n")
  csvImplWriter.close

  println(s"[Finish Resource Item Route Finder] ${pathOutputString(writeFilePath)}")
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.finder

import Commons._

import scala.io.Source
import scala.util.Try
import scala.reflect.io.Directory
import java.io.FileWriter
import org.apache.commons.io.output.FileWriterWithEncoding
import d2k.appdefdoc.parser._
import java.io.File

case class RrfData(path: String, appInfo: AppInfo, ioData: Option[IoData], containType: String)
object ResourceRelationFinder extends App {
  val isLocalMode = args.size >= 4
  val (baseUrl, branch, targetName) = (args(0), args(1), args(2))
  val basePath = if (isLocalMode) args(3) else "C:/d2k_docs"
  val writePath = Directory(s"data/rrFinder/${targetName}")

  println(s"[Start App Relation Finder${if (isLocalMode) " on Local Mode" else ""}] ${args.mkString(" ")}")

  val appBasePath = s"${basePath}/apps"
  val result = createRrfData(targetName, appDefList(appBasePath))
  writeRrfData(targetName, baseUrl, basePath, writePath, Some("[Finish Resource Relation Finder]"), result)
}


', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.appdef

import scala.io.Source
import scala.util.Try
import scala.reflect.io.Directory
import java.io.FileWriter
import org.apache.commons.io.output.FileWriterWithEncoding
import d2k.appdefdoc.parser._

case class InputData(id: String, cType: String, desc: String)
object ApplicationDefGenerator extends App {
  val (grpId, appId, inputCsv) = (args(0), args(1), if (args.size >= 3) args(2) else "./data/input.csv")
  println(s"[Start ApplicationDef Generate] ${Seq(grpId, appId, inputCsv).mkString(" ")}")
  val adg = new ApplicationDefGenerator(grpId, appId, inputCsv)
  val result = adg.makeAppDef
  adg.makeComponentDef
  println(s"[Finish ApplicationDef Generate] ${result}")

  def searchAndReplace(target: Seq[String], searchElem: String, replaceElem: String) = {
    val idx = target.indexWhere(_.contains(searchElem))
    target.updated(idx, replaceElem)
  }

  def fileToStr(fileName: String) = Source.fromFile(fileName)

  def resToStr(fileName: String) =
    Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString
}

class ApplicationDefGenerator(grpId: String, appId: String, inputCsv: String) {
  import ApplicationDefGenerator._

  val writeBase = s"data/appGen/${grpId}/${appId}"
  val writePath = Directory(writeBase)
  writePath.createDirectory(true, false)
  val indata = fileToStr(inputCsv).getLines.zipWithIndex.map {
    case (x, idx) =>
      val splitted = x.split(',')
      InputData(f"${idx + 1}%02d_${splitted(0)}", splitted(0), splitted(1))
  }.toSeq

  def makeAppDef = {
    val appPath = "./catalogs/01_application/README.md"
    val appMd = fileToStr(appPath).getLines.toSeq
    val componentList = indata.map { x =>
      s"| [${x.id}](${x.id}.md) | ${x.desc} |"
    }
    val componentFlow = indata.sliding(2).map { x =>
      if (x.size == 1) {
        (x(0), x(0))
      } else {
        (x(0), x(1))
      }
    }.toSeq
    val componentFlowStr = if (componentFlow.size == 1) {
      componentFlow.take(1).map { x =>
        s"""${x._2.id} --> (*)"""
      }
    } else {
      componentFlow.take(1).map { x =>
        s""""${x._1.id}\\n${x._1.desc}" as ${x._1.id} --> "${x._2.id}\\n${x._2.desc}" as ${x._2.id}"""
      } ++
        componentFlow.drop(1).map { x =>
          s"""${x._1.id} --> "${x._2.id}\\n${x._2.desc}" as ${x._2.id}"""
        } ++
        componentFlow.takeRight(1).map { x =>
          s"""${x._2.id} --> (*)"""
        }
    }

    val appdefStr = Seq(
      ("## %%AppId%%", s"## ${appId}"),
      ("%%[01_XxToXx](01_XxToXx.md)%%", componentList.mkString("\n")),
      ("%%01_XxToXx%%", componentFlowStr.mkString("\n"))).foldLeft(appMd) { (l, r) =>
        searchAndReplace(l, r._1, r._2)
      }.mkString("\n")

    val writeFilePath = s"${writeBase}/README.md"
    val writer = new FileWriter(writeFilePath)
    writer.write(appdefStr)
    writer.close
    writeFilePath
  }

  def makeComponentDef = {
    val catalogPath = "./catalogs/02_templates"

    indata.foreach { x =>
      val cMd = fileToStr(s"${catalogPath}/_${x.cType}.md").getLines.toSeq

      val cdefStr = Seq(
        (s"## _${x.cType}", s"## ${x.id}"),
        ("%%コンポーネント名%%", x.desc),
        ("%%コンポーネントID%%", s"    ${appId}${x.id.take(2)}")).foldLeft(cMd) { (l, r) =>
          searchAndReplace(l, r._1, r._2)
        }.mkString("\n")

      val writeFilePath = s"${writeBase}/${x.id}.md"
      val writer = new FileWriter(writeFilePath)
      writer.write(cdefStr)
      writer.close
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.dic

object DictionaryGenerator extends App {
  val (baseUrl, branch) = (args(0), args(1))
  println(s"[Start Dictionary Generate] ${args.mkString(" ")}")
  GenerateDictionary(baseUrl).generate(branch)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.dic

import d2k.common.MakeResource
import scala.io.Source
import scala.reflect.io.Path
import scala.reflect.io.Path.string2path
import d2k.appdefdoc.parser.ItemDefParser
import java.io.FileWriter
import spark.common.FileCtl
import scala.reflect.io.Directory

object GenerateDictionary {
  def apply(baseUrl: String) = new GenerateDictionary(baseUrl)
}

case class DicData(id: String, name: String, dic: String) {
  override def toString = Seq(id, dic, "固有名詞", name).mkString("\t")
}
class GenerateDictionary(baseUrl: String) {
  def generate(branch: String) = {
    val dbResult = generateDictionary(baseUrl, branch, "db")
    val pqResult = generateDictionary(baseUrl, branch, "pq")
    val result = (dbResult ++ pqResult).groupBy(d => (d.id, d.dic)).map { case (k, d) => d.head }.toList

    val outputPath = "data/dicGen"
    val outputFile = s"${outputPath}/d2k_appdef.txt"
    Directory(outputPath).createDirectory(true, false)
    FileCtl.writeToFile(outputFile, false, "MS932") { w =>
      result.sortBy(_.id).foreach(w.println)
    }
    println(s"[Finish Dictionary Generate] ${outputFile}")
  }

  def generateDictionary(baseUrl: String, branch: String, category: String) = {
    val itemsUrl = s"${baseUrl}/raw/${branch}/apps/common/items/${category}"
    val url = s"${itemsUrl}/README.md"

    val dicConv = dicConvertPattern(baseUrl, "master")

    val md = Source.fromURL(s"${url}?private_token=${sys.env("GITLAB_TOKEN")}").getLines.toList
    val grpList = md.filter(!_.isEmpty).dropWhile(line => !line.contains("## 業務グループ一覧"))
      .drop(3).map(_.split('|')(1).trim.split('(')(1).dropRight(1))
    val files = fileList(itemsUrl, grpList.head)
    (for {
      g <- grpList
      f <- fileList(itemsUrl, g)
    } yield {
      makeDic(dicConv, baseUrl, branch, category, g.split('/')(0), f)
    }).flatten
  }

  def fileList(baseUrl: String, grpReadme: String) = {
    val grpUrl = s"${baseUrl}/${grpReadme}"
    val md = Source.fromURL(s"${grpUrl}?private_token=${sys.env("GITLAB_TOKEN")}").getLines.toList
    md.filter(!_.isEmpty).dropWhile(line => !line.contains("## "))
      .drop(3).map(_.split('|')(1).trim.split('(')(1).dropRight(1))
  }

  def makeDic(dicConv: Map[String, String], baseUrl: String, branch: String, category: String, grpName: String, fileName: String) = {
    val dicKeys = dicConv.keys.toList
    val itemdef = ItemDefParser(baseUrl, branch, s"${category}/${grpName}/${fileName}").get
    itemdef.details.map { items =>
      val target = dicConv.keys.filter(k => itemdef.id.toLowerCase.startsWith(k)).head
      val conved = itemdef.id.toLowerCase.replaceAllLiterally(target, dicConv(target))
      DicData(conved, itemdef.name, s"${items.id}[${items.name}]")
    }
  }

  def dicConvertPattern(baseUrl: String, branch: String) = {
    val dicConvUrl = s"${baseUrl}/raw/${branch}/guide/dicgen/dicPattern.md"
    val md = Source.fromURL(s"${dicConvUrl}?private_token=${sys.env("GITLAB_TOKEN")}").getLines.toList
    md.filter(!_.isEmpty).dropWhile(line => !line.contains("## 項目変換パターン"))
      .drop(3).map { x =>
        val splitted = x.split('|')
        (splitted(1).trim -> splitted(2).trim)
      }.toMap
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.rc

import scala.io.Source
import scala.util.Try
import scala.reflect.io.Directory
import java.io.FileWriter
import org.apache.commons.io.output.FileWriterWithEncoding
import d2k.appdefdoc.parser._
import java.io.File

object RCData {
  private val strToPlantUml = (obj: String) => {
    obj.split("_")(1).split("To")
  }

  private def splitIdAndName(s: String) = {
    val splitted = s.split('[')
    (splitted(0).trim, splitted(1).trim.dropRight(1))
  }
  private val inputTypeToPlantUml =
    (appId: String, indata: (Cf, ComponentDetail)) => {
      val (cf, cd) = indata
      strToPlantUml(cf.componentId)(0) match {
        case "Pq" => {
          val (id, name) = splitIdAndName(cd.findById("01").value)
          (s"""artifact "${id}\\n${name}" as ${id}""", s"""[抽出] as ${cf.componentId}""", s"${id} --> ${cf.componentId}")
        }
        case "File" => {
          val (id, name) = splitIdAndName(cd.findById("02", "02.02").value.mkString)
          (s"""artifact "${id}\\n${name}" as ${id}""", s"""[抽出] as ${cf.componentId}""", s"${id} --> ${cf.componentId}")
        }
        case "Db" => {
          val dbInfo = cd.processes.map(_.detail.filter(_.param.id == "02").head.values.head.asInstanceOf[ProcessParamValue].value).head
          val (id, name) = splitIdAndName(cd.findById("02").value)
          (s"""database "${id}\\n${name}" as ${id}""", s"""[抽出] as ${cf.componentId}""", s"${id} --> ${cf.componentId}")
        }
        case "Df"     => ("", s"""[編集] as ${cf.componentId}""", "")
        case "DfJoin" => ("", s"""[結合] as ${cf.componentId}""", "")
        case _        => ("", "", "")
      }
    }

  private val outputTypeToPlantUml =
    (appId: String, indata: (Cf, ComponentDetail)) => {
      val (cf, cd) = indata
      strToPlantUml(cf.componentId)(1) match {
        case "Pq" => {
          val pqInfo = cd.processes.map(_.detail.filter(_.param.id == "01").head.values.head.asInstanceOf[ProcessParamValue].value).head
          val (id, name) = splitIdAndName(pqInfo)
          (s"""artifact "${id}\\n${name}" as ${id}""", s"${cf.componentId} --> ${id}")
        }
        case "File" => {
          val fileInfo = cd.processes.map(_.detail.filter(_.param.id == "02").head.values.flatMap {
            case x: ProcessParamSubDetail => Some(x)
            case _                        => None
          }.filter(_.id == "02.02.").head.value).head
          val (id, name) = splitIdAndName(fileInfo.mkString)
          (s"""artifact "${id}\\n${name}" as ${id}""", s"${cf.componentId} --> ${id}")
        }
        case "Db" => {
          val dbInfo = cd.processes.map(_.detail.filter(_.param.id == "02").head.values.head.asInstanceOf[ProcessParamValue].value).head
          val (id, name) = splitIdAndName(dbInfo)
          (s"""database "${id}\\n${name}" as ${id}""", s"${cf.componentId} --> ${id}")
        }
        case _ => ("", "")
      }
    }

  def apply(appdef: AppDef)(indata: (Cf, ComponentDetail)) = {
    val in = inputTypeToPlantUml(appdef.appInfo.id, indata)
    val out = outputTypeToPlantUml(appdef.appInfo.id, indata)
    Seq(new RCData(in._1, in._2, in._3), new RCData(out._1, "", out._2))
  }
}

case class RCData(obj: String, frame: String, link: String)

case class RCDataStore(objs: List[String] = List.empty[String], frames: List[String] = List.empty[String], links: List[String] = List.empty[String]) {
  def +=(rcd: RCData) = this.copy(
    if (rcd.obj.isEmpty) objs else rcd.obj :: objs,
    if (rcd.frame.isEmpty) frames else rcd.frame :: frames,
    if (rcd.link.isEmpty) links else rcd.link :: links)
  def result = RCDataStore(objs.reverse, frames.reverse, links.reverse)
}

object RoughConceptGenerator extends App {
  val isLocalMode = args.size >= 5
  val (baseUrl, branch, appGroup, appId) = (args(0), args(1), args(2), args(3))
  val basePath = if (isLocalMode) args(4) else ""
  val writeBase = s"data/rcGen/${appGroup}/${appId}"
  val writePath = Directory(writeBase)
  writePath.createDirectory(true, false)

  println(s"[Start Rough Concept Generate${if (isLocalMode) " on Local Mode" else ""}] ${args.mkString(" ")}")
  val appdef = if (isLocalMode) {
    AppDefParser(basePath, appGroup, appId).get
  } else {
    AppDefParser(baseUrl, branch, appGroup, appId).get
  }
  val appdefStr = generate(appdef)

  val writeFilePath = s"${writeBase}/README.md"
  val writer = new FileWriter(writeFilePath)
  writer.write(appdefStr)
  writer.close

  val psFilePath = s"${writeBase}/ps.puml"
  val fileCheck = new File(psFilePath).exists
  if (!fileCheck) new FileWriter(psFilePath).close

  println(s"[Finish Rough Concept Generate] ${writeFilePath}")

  def fileToStr(fileName: String) =
    Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString

  def ioToTableStr(iodata: IoData) = Seq(
    s"[${iodata.id}](${iodata.path})",
    iodata.srcType,
    iodata.name).mkString("| ", " | ", " |")

  def generate(appdef: AppDef) = {
    val componentdefs = appdef.componentList.map { comp =>
      println(s"Parsing ${comp.id}[${comp.name}]")
      if (isLocalMode) {
        ComponentDefParser(basePath, appGroup, appId, comp.mdName).get
      } else {
        ComponentDefParser(baseUrl, branch, appGroup, appId, comp.mdName).get
      }
    }

    val componentFlow = appdef.componentFlow
    val rcd = RCData(appdef) _
    val objects =
      appdef.componentFlow.get.pairs.flatMap(x => Seq(x.cf1, x.cf2))
        .flatMap {
          case cf: Cf => {
            val targetComponent = componentdefs.seq.filter(_.componentInfo.id == cf.componentId)
            Some((cf, targetComponent.head.componentDetail))
          }
          case _ => None
        }.sortBy(_._1.componentId).flatMap(rcd)

    val links = appdef.componentFlow.get.pairs.flatMap { pair =>
      pair.cf2 match {
        case CfEnd => None
        case _ =>
          Some(s"${pair.cf1.componentId} --> ${pair.cf2.componentId}")
      }
    }
    val rcds = objects.foldLeft(RCDataStore()) { (l, r) => l += r }.result
    def objToMd(rcds: RCDataStore) = {
      s"""${rcds.objs.mkString("\n")}
      
frame ${appdef.appInfo.id} {
${rcds.frames.mkString("\n")}
}

${(rcds.links ++ links).mkString("\n")}

!include ps.puml"""
    }
    val rcMdTmpl = fileToStr("genTemplates/rc/AppMd.tmpl")
    Seq(
      ("%%AppId%%", appdef.appInfo.id),
      ("%%AppName%%", appdef.appInfo.name),
      ("%%AppDesc%%", appdef.appInfo.desc),
      ("%%PlantUml%%", objToMd(rcds)),
      ("%%IoInput%%", appdef.inputList.map(ioToTableStr).mkString("\n")),
      ("%%IoOutput%%", appdef.outputList.map(ioToTableStr).mkString("\n")))
      .foldLeft(rcMdTmpl) { (l, r) => l.replaceAllLiterally(r._1, r._2) }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.sql

import scala.util.parsing.combinator.JavaTokenParsers
import d2k.appdefdoc.parser.D2kParser
import d2k.appdefdoc.parser.IoData

case class SqlDef(
  appInfo: SqlInfo, sqlLogic: Seq[String], inputList: Seq[IoData], outputList: Seq[IoData])
case class SqlInfo(id: String, name: String, desc: String)
object SqlDefParser extends JavaTokenParsers with D2kParser {

  def apply(baseUrl: String, branch: String, appGroup: String, appId: String) = {
    val parsed = parseAll(sqlDef, readAppDefMd(baseUrl, branch, appGroup, appId, "README.md"))
    println(parsed)
    parsed
  }

  def apply(baseUrl: String, appGroup: String, appId: String) = {
    val parsed = parseAll(sqlDef, readAppDefMd(baseUrl, appGroup, appId, "README.md"))
    println(parsed)
    parsed
  }

  def eol = '\n'
  def separator = "----" ~ eol
  val tableSep2 = repN(2, "\\|[\\s\\:\\-]\\-+".r) ~ "|"
  val tableSep3 = repN(3, "\\|[\\s\\:\\-]\\-+".r) ~ "|"
  val anyWords = "^(?!(#{5}|#{4}|#{3}|#{2}|-{4})).*".r

  def sqlDef = SQL情報 ~ (SQLロジック <~ リレーション図) ~ 入力データ情報 ~ 出力データ情報 ^^ {
    case a ~ b ~ c ~ d => SqlDef(a, b, c, d)
  }

  def SQL定義タイトル = "#" ~> "SQL定義" ~ eol
  def SQLId = "##" ~> ident <~ eol
  def SQL名 = ".*".r <~ eol
  def 概要 = "##" ~> "概要" ~ eol ~> repsep(anyWords,eol) <~ eol
  def SQL情報 = (SQL定義タイトル ~> SQLId) ~ SQL名 ~ (概要 <~ separator) ^^ {
    case a ~ b ~ c => SqlInfo(a, b, c.mkString)
  }

  def SQLロジック = {
    "##" ~> "01. SQL" ~> ("```sql" ~> repsep("^(?!(`{3})).*".r, eol) <~ "```")
  }

  def リレーション図 = separator ~ "##" ~> "02. リレーション図" ~> リレーション図定義
  def リレーション図定義 = "```plantuml" ~ repsep("[^`]*".r, eol) ~ "```" ~ eol ~ separator

  def IOデータ = IOデータ1 ~ IOデータ2 ~ IOデータ3 ^^ {
    case a ~ b ~ c => IoData(a._1, a._2, b, c.trim)
  }
  def IOデータ1 = "|" ~> IOデータ定義ID ~ IOデータPath
  def IOデータ2 = "|" ~> "[\\w\\(\\)]*".r
  def IOデータ3 = "|" ~> "[^|]*".r <~ "|"
  def IOデータ定義ID = "[" ~> "[\\w\\.]*".r <~ "]"
  def IOデータPath = "(" ~> "[\\w\\./]*".r <~ ")"

  def 入力データタイトル =
    "##" ~ "03. 入出力データ一覧" ~ "###" ~ "03.01. 入力" ~ "|" ~ "定義ID" ~ "|" ~ "ソースタイプ" ~ "|" ~ "定義名" ~ "|" ~ eol ~ tableSep3
  def 入力データ情報 = 入力データタイトル ~> rep(IOデータ)

  def 出力データタイトル =
    "###" ~ "03.02. 出力" ~ "|" ~ "定義ID" ~ "|" ~ "ソースタイプ" ~ "|" ~ "定義名" ~ "|" ~ eol ~ tableSep3
  def 出力データ情報 = 出力データタイトル ~> rep(IOデータ)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.sql

import scala.io.Source
import scala.reflect.io.Directory
import java.io.FileWriter
import scala.collection.Seq
import scala.ref

import d2k.appdefdoc.parser.D2kParser
import d2k.appdefdoc.parser.IoData

object SqlGenerator extends App {
  val isLocalMode = args.size >= 5
  val (baseUrl, branch, appGroup, appId) = (args(0), args(1), args(2), args(3))
  val basePath = if (isLocalMode) args(4) else ""
  val writeBase = s"data/sqlGen/${appGroup}"
  val writePath = Directory(writeBase)
  writePath.createDirectory(true, false)

  println(s"[Start Sql Generate${if (isLocalMode) " on Local Mode" else ""}] ${args.mkString(" ")}")
  val (appdef, sqlLogic) = if (isLocalMode) {
    (SqlDefParser(basePath, appGroup, appId).get, SqlLogicParser(basePath, appGroup, appId))
  } else {
    (SqlDefParser(baseUrl, branch, appGroup, appId).get, SqlLogicParser(baseUrl, branch, appGroup, appId))
  }
  val appdefStr = generate(appdef, sqlLogic)

  val writeFilePath = s"${writeBase}/${appId}.sql"
  val writer = new FileWriter(writeFilePath)
  writer.write(appdefStr)
  writer.close

  println(s"[Finish Sql Generate] ${writeFilePath}")

  def fileToStr(fileName: String) =
    Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString

  def generate(appdef: SqlDef, sqlLogic: String) = {
    val mainTmpl = fileToStr("genTemplates/sqlMain.tmpl")
    val mainRepList = Seq(
      ("%%appId%%", appdef.appInfo.id),
      ("%%appDesc%%", appdef.appInfo.desc),
      ("%%sql%%", sqlLogic))
    mainRepList.foldLeft(mainTmpl) { (l, r) =>
      l.replaceAllLiterally(r._1, r._2)
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.sql

import scala.util.parsing.combinator.JavaTokenParsers
import d2k.appdefdoc.parser.D2kParser
import d2k.appdefdoc.parser.IoData

object SqlLogicParser extends JavaTokenParsers with D2kParser {

  def apply(baseUrl: String, branch: String, appGroup: String, appId: String) = {
    (parser _ andThen replaceComment _)(readAppDefMd(baseUrl, branch, appGroup, appId, "README.md"))
  }

  def apply(baseUrl: String, appGroup: String, appId: String) = {
    (parser _ andThen replaceComment _)(readAppDefMd(baseUrl, appGroup, appId, "README.md"))
  }

  def parser(s: String) = s.split("```sql")(1).split("```")(0)

  val regxItem = """(\S+)(\[.*?\])""".r
  val regxChar = """([\'].*?[\'])\((.*?)\)""".r
  val regxDecimal = """(\d+)(\(.*?\))""".r
  val regxParent = """(\S+)\[[^\[\]]*\]\.""".r
  def replaceComment(inStr: String) = {
    Seq(
      regxChar.findAllMatchIn(inStr)
        .map(x => (x.toString, x.group(1))),
      regxItem.findAllMatchIn(inStr)
        .map(x => (x.toString, x.group(1))),
      regxDecimal.findAllMatchIn(inStr)
        .map(x => (x.toString, x.group(1))),
      regxParent.findAllMatchIn(inStr)
        .map(x => (x.toString, s"""${x.group(1)}.""")))
      .reduce(_ ++ _).toList.sortBy(x => x._1.size).reverse.foldLeft(inStr) { (l, r) => l.replaceAllLiterally(r._1, r._2) }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.src

import d2k.appdefdoc.parser._
import org.apache.spark.sql.catalyst.expressions.StartsWith

object ConvertTemplateDefine {
  val fileInfoClass = Map(
    ("csv" -> "CsvInfo"),
    ("tsv" -> "TsvInfo"),
    ("vsv" -> "VsvInfo"),
    ("ssv" -> "SsvInfo"),
    ("fixed" -> "FixedInfo"))

  val fileInputNameConverter = Map(
    ("読込ファイル形式" -> "fileInputInfo"),
    ("読込ファイル名" -> "inputFiles"),
    ("読込ファイルパス設定名" -> "envName"),
    ("ダブルクォーテーション削除(only TsvInfo)" -> "dropDoubleQuoteMode"),
    ("ヘッダ有無" -> "header"),
    ("フッタ有無" -> "footer"),
    ("改行有無" -> "newLine"),
    ("連番指定有無(only FixedInfo)" -> "withIndex"),
    ("レコード長チェック有無(only FixedInfo)" -> "recordLengthCheck"),
    ("文字コード" -> "charSet"),
    ("改行コード" -> "newLineCode"),
    ("ドメインコンバート事前Filter(only FixedInfo)" -> "preFilter"))

  def createFileInputInfo(d: ProcessDetail) = {
    val fileInfo = d.values.flatMap {
      case ProcessParamSubDetail(id, name, defcode, value) if !value.startsWith("実装不要") =>
        Some((fileInputNameConverter(name.trim), value.mkString.trim))
      case _ => None
    }.toMap
    val fileInfoStr = (fileInfo - "fileInputInfo").map { case (n, v) => s"${n} = ${v}" }.mkString(", ")
    s"${fileInfoClass(fileInfo("fileInputInfo").toLowerCase)}(${fileInfoStr})"
  }

  def createFileOutputMode(baseUrl: String, branch: String)(cd: ComponentDetail) = {
    val fileOutput = cd.processes.find(_.desc.value.startsWith("File出力"))
    val fileKind = fileOutput.map(_.detail.filter(_.param.name.startsWith("出力ファイル種別")))
    val fileKindDef = fileKind.map { x =>
      val a = x.head
      (a.defaultCode.get.value.split('=').head.trim, a.values.map {
        case ProcessParamValue(v) => v
        case _                    => ""
      }.head)
    }
    val fileParam = fileOutput.map(_.detail.filter(_.param.name.startsWith("出力ファイルパラメータ")))
    val fileParamDetails = fileParam.map { x =>
      x.flatMap(_.values.flatMap {
        case ProcessParamSubDetail(_, name, default, value) => Some((name, value))
        case _ => None
      })
    }

    def paramDetailValue(name: String) = fileParamDetails.flatMap(_.find(_._1.startsWith(name)).map(_._2.mkString).flatMap {
      case "実装不要" => None
      case x      => Some(x)
    })

    val fileParamOutputLen = paramDetailValue("出力項目長").map { v =>
      if (v.contains("項目定義物理名")) {
        val tableData = v.split('|').toList.takeRight(2)
        val item = tableData.head.split("\\]\\(")
        val resId = item(0).tail
        val filePath = item(1).trim.dropRight(1).split("/").toList.takeRight(3).mkString("/")
        println(s"  Parsing ${resId.drop(1)}[${tableData.last.trim}]")
        val itemPs = ItemDefParser(baseUrl, branch, filePath).get
        val lastItem = itemPs.details.last
        itemPs.details.dropRight(1).map { d =>
          s"        ${d.size}, //${d.name}"
        }.mkString("\n", "\n", "\n") + s"        ${lastItem.size} //${lastItem.name}\n      "
      } else {
        v
      }
    }
    val fileParamDqCol = paramDetailValue("ダブルクォート対象カラム名")
    fileKindDef.flatMap { x =>
      x._2.trim.toLowerCase match {
        case "csv" => Some(fileParamDqCol.map { c =>
          val splitted = c.mkString.split(',')
          s"WriteFileMode.Csv(${splitted.mkString("\"", s"""", """", "\"")})"
        }.getOrElse("WriteFileMode.Csv"))
        case "tsv"            => Some("WriteFileMode.Tsv")
        case "fixed" | "実装不要" => Some(fileParamOutputLen.map { c => s"WriteFileMode.Fixed(${c})" }.getOrElse("WriteFileMode.Fixed"))
        case _                => None
      }
    }.getOrElse("実装不要")
  }

  def convertScalaSource(baseUrl: String, branch: String)(cd: ComponentDetail, d: ProcessDetail) = {
    d.defaultCode.flatMap { dc =>
      val seqData = d.values.flatMap {
        case ProcessParamValue(value) if d.param.name == "空DataFrame用Schema" => {
          val regx = """.*\((.*\.md)\)""".r
          Some(regx.findFirstMatchIn(value).map { r =>
            val path = r.group(1).split("/").toList.takeRight(3).mkString("/")
            val itemPs = ItemDefParser(baseUrl, branch, path)
            itemPs.get.details.map { x =>
              s"""("${x.id}", "${x.dataType}")"""
            }.mkString("Seq(", ",", ")")
          }.getOrElse(value))
        }
        case ProcessParamValue(value) if d.param.name == "結合条件" =>
          Some(ItemReplace.column.replaceLitStr(ItemReplace.column.replaceJoinStr(value)))
        case ProcessParamValue(value) if d.param.name.startsWith("DB読込時条件") =>
          Some((ItemReplace.scala.replaceLitStr _ andThen ItemReplace.db.replaceLitStringMod)(value))
        case ProcessParamValue(value) =>
          Some((ItemReplace.scala.replaceLitStr _ andThen ItemReplace.scala.replaceLitStringMod)(value))
        case x => None
      }
      val data = (d.param.name match {
        case "空DataFrame用Schema" => seqData.filterNot(_.startsWith("|"))
        case "読込ファイル情報"          => Seq(createFileInputInfo(d))
        case "出力ファイル種別"          => Seq(createFileOutputMode(baseUrl, branch)(cd))
        case _                   => seqData
      }).mkString("      ", "\n      ", "")

      if (data.contains("実装不要")) {
        None
      } else {
        val overrideVal = if (dc.value.contains(" = ")) "override" else ""
        Some(s"\n    ${overrideVal} ${dc.value.split(" = ").head} =\n${data}")
      }
    }
  }

  def replaceJoinSelect(targetData: String, defaultCode: String)(
    splitPattern: String, dropSize: Int,
    replaceLeft: String => String, replaceRight: String => String) = {
    val splitted = targetData.split(splitPattern)
    for {
      left <- splitted.headOption.map(_.drop(dropSize))
      right <- splitted.tail.headOption
    } yield {
      val replacedLeft = replaceLeft(left)
      val replacedRight = replaceRight(right)
      s"\n    ${defaultCode} = Seq(\n    ${replacedLeft}\n    ${replacedRight}\n    )\n"
    }
  }

  def replacePrefix(defaultCode: String, prefixName: String) =
    s"""\n    ${defaultCode} = \n      mergeWithPrefix(left, right, "${prefixName}")\n"""

  def replaceDropDuplicate(defaultCode: String) =
    s"\n    ${defaultCode} = \n      mergeDropDuplicate(left, right)\n"

  def replaceLeftRight(baseUrl: String, branch: String, targetData: String)(cd: ComponentDetail, d: ProcessDetail) = {
    val hasLeftDrop = targetData.contains("left.drop")
    val hasRightDrop = targetData.contains("right.drop")
    val replacer = replaceJoinSelect(targetData, d.defaultCode.get.value) _
    val replaced = (hasLeftDrop, hasRightDrop) match {
      case (true, true) =>
        replacer("right.drop:", "left.drop:".size,
          ItemReplace.join.genJoinSelectorDrop(_, false).mkString("  left", "\n          ", ""),
          ItemReplace.join.genJoinSelectorDrop(_, true).mkString("  right", "\n           ", ""))
      case (true, false) =>
        replacer("right:", "left.drop:".size,
          ItemReplace.join.genJoinSelectorDrop(_, false).mkString("  left", "\n          ", ""),
          ItemReplace.join.genJoinSelectorAppend(_, true).mkString("  right", "\n      right", ""))
      case (false, true) =>
        replacer("right.drop:", "left.drop:".size,
          ItemReplace.join.genJoinSelectorAppend(_, false).mkString("  left", "\n      left", ""),
          ItemReplace.join.genJoinSelectorDrop(_, true).mkString("  right", "\n           ", ""))
      case _ =>
        replacer("right:", "left:".size,
          ItemReplace.join.genJoinSelectorAppend(_, false).mkString("  left", "\n      left", ""),
          ItemReplace.join.genJoinSelectorAppend(_, true).mkString("  right", "\n      right", ""))
    }
    replaced.orElse(convertScalaSource(baseUrl, branch)(cd, d))
  }

  def modDetail(baseUrl: String, branch: String)(cd: ComponentDetail, d: ProcessDetail) = {
    val targetData = d.values.map {
      case v: ProcessParamValue => v.value.replaceAllLiterally(" ", "")
      case v                    => v
    }.mkString

    (d.param.id, d.param.name) match {
      case ("04", "項目選択") if targetData.startsWith("left") =>
        replaceLeftRight(baseUrl, branch, targetData)(cd, d)
      case ("04", "項目選択") if targetData.startsWith("接頭辞付与") =>
        targetData.split(':').lastOption.map(replacePrefix(d.defaultCode.get.value, _))
      case ("04", "項目選択") if targetData.startsWith("重複項目削除") =>
        Some(replaceDropDuplicate(d.defaultCode.get.value))
      case _ => convertScalaSource(baseUrl, branch)(cd, d)
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.src

object FlowLogicGenerator {
  sealed trait Tree {
    def toFlow: String
  }

  case class Node(id: String, t: Tree) extends Tree {
    def toFlow = {
      s"${t.toFlow} ~>\n    c${id}.run"
    }
  }

  case class Leaf(id: String) extends Tree {
    def toFlow = s"c${id}.run"
  }

  case class Top(id: String) extends Tree {
    def toFlow = s"c${id}.run(Unit)"
  }

  case class Join(id: String, l: Tree, r: Tree) extends Tree {
    def toFlow = s"(${l.toFlow}, ${r.toFlow}) ~>\n    c${id}.run"
  }

  def apply(target: Seq[(String, String)]) = {
    val flowMap = target.foldLeft(Map.empty[String, Seq[String]]) { (l, r) =>
      val (a, b) = r
      l.updated(b, l.get(b).getOrElse(Seq.empty[String]) :+ a)
    }

    def conv(s: String): Tree = {
      val flowId = flowMap.get(s).getOrElse(Seq.empty[String])
      flowId.size match {
        case 0 => Top(s)
        case 1 => Node(s, conv(flowId.head))
        case 2 => Join(s, conv(flowId(0)), conv(flowId(1)))
      }
    }
    flowMap("CfEnd").map(e => conv(e).toFlow).mkString("\n\n")
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.src

import scala.util.matching.Regex.Match
import scala.util.matching.Regex

object ItemReplace {
  def addDq(s: String) = s""""$s""""
  val regxWithParent = """(\S+)\[[^\[\]]*\]\s(_\S+)\[[^\[\]]*\]""".r
  val regx = """[\"\'](.*?)[\"\']\((.*?)\)""".r
  val regxNull = """[Nn][Uu][Ll][Ll]\((.*?)\)""".r
  val regxDecimal = """\d+\((.*?)\)""".r
  val regxItem = """\S+\[(.*?)\]""".r
  val regxJoinLeft = """left\.\S+\[(.*?)\]""".r
  val regxJoinRight = """right\.\S+\[(.*?)\]""".r

  val runningDates = Seq(
    ("[運用日]", "inArgs.runningDate.MANG_DT"),
    ("[運用日(年)]", "inArgs.runningDate.MANG_DT.take(4)"),
    ("[運用日(年月)]", "inArgs.runningDate.MANG_DT.take(6)"),
    ("[運用日(日)]", "inArgs.runningDate.MANG_DT.takeRight(2)"),
    ("[前日]", "inArgs.runningDate.YST_DY"),
    ("[翌日]", "inArgs.runningDate.NXT_DT"),
    ("[前月]", "inArgs.runningDate.BEF_MO"),
    ("[前月_月初日]", "inArgs.runningDate.BEF_MO_FRST_MTH_DT"),
    ("[前月_月末日]", "inArgs.runningDate.BEF_MO_MTH_DT"),
    ("[当月]", "inArgs.runningDate.CURR_MO"),
    ("[当月_月初日]", "inArgs.runningDate.CURR_MO_FRST_MTH_DT"),
    ("[当月_月末日]", "inArgs.runningDate.CURR_MO_MTH_DT"),
    ("[翌月]", "inArgs.runningDate.NXT_MO"),
    ("[翌月_月初日]", "inArgs.runningDate.NXT_MO_FRST_MTH_DT"),
    ("[翌月_月末日]", "inArgs.runningDate.NXT_MO_MTH_DT"))

  def dropDescription(divKey: Char) =
    (_: Regex.Match).toString.split(divKey).dropRight(1).mkString(divKey.toString)

  object scala {
    def replaceLitStr(inStr1: String) = {
      val inStr = inStr1.split(",").map(_.trim).mkString("\"", "\", \"", "\"")
      val replaceBasic = Seq(
        regxWithParent.findAllMatchIn(inStr)
          .map(x => (x.group(0).toString, s"${x.group(1) + x.group(2)}")),
        regx.findAllMatchIn(inStr)
          .map(x => (x.toString, s"${dropDescription('(')(x)}")),
        regxNull.findAllMatchIn(inStr)
          .map(x => (x.toString, "null")),
        regxDecimal.findAllMatchIn(inStr)
          .map(x => (x.toString, s"${dropDescription('(')(x)}")),
        regxItem.findAllMatchIn(inStr)
          .map(x => (x.toString, s"${dropDescription('[')(x)}")))
        .reduce(_ ++ _).toList.sortBy(x => x._1.size).reverse.foldLeft(inStr) { (l, r) => l.replaceAllLiterally(r._1, r._2) }
      runningDates.foldLeft(replaceBasic) { (l, r) => l.replaceAllLiterally(r._1, r._2) }
    }

    def replaceLitStringMod(org: String) =
      Seq(toSubstr, toPadding).foldLeft(org) { (l, r) => r(l) }

    val toSubstr = (target: String) => {
      """(\w*)/(\d*):(\d*)""".r.findAllMatchIn(target).map { r =>
        val name = r.group(1)
        (r.toString, (r.group(2), r.group(3)) match {
          case (pos, "")                   => s"${name}.drop(${pos.toInt - 1})"
          case ("", len) if len.toInt <= 0 => name
          case ("", len)                   => s"${name}.take(${len})"
          case (pos, len)                  => s"${name}.slice(${pos.toInt - 1}, ${pos.toInt + len.toInt - 1})"
        })
      }.foldLeft(target) { (l, r) => l.replaceAllLiterally(r._1, r._2) }
    }

    val toPadding = (target: String) => {
      """(\w*)/pad(\d*)""".r.findAllMatchIn(target).map { r =>
        val name = r.group(1)
        (r.toString, r.group(2) match {
          case len => s"""${name}.padTo(${len}, ' ')"""
        })
      }.foldLeft(target) { (l, r) => l.replaceAllLiterally(r._1, r._2) }
    }
  }

  object column {
    val replaceLitStr = (s: String) => {
      val rslt = Seq(
        regxWithParent.findAllMatchIn(s)
          .map(x => (x.group(0).toString, s"""$$"${x.group(1) + x.group(2)}"""")),
        regx.findAllMatchIn(s)
          .map(x => (x.toString, s"lit(${dropDescription('(')(x)})")),
        regxNull.findAllMatchIn(s)
          .map(x => (x.toString, "lit(null)")),
        regxDecimal.findAllMatchIn(s)
          .map(x => (x.toString, s"lit(${dropDescription('(')(x)})")),
        regxItem.findAllMatchIn(s)
          .map(x => (x.toString, s"""$$"${dropDescription('[')(x)}"""")))
        .reduce(_ ++ _).toList.sortBy(x => x._1.size).reverse.foldLeft(s) { (l, r) => l.replaceAllLiterally(r._1, r._2) }
      val rslt2 = runningDates.foldLeft(rslt) { (l, r) => l.replaceAllLiterally(r._1, s"lit(${r._2})") }
      Seq(replaceSignStr, toSubstr, toPadding).foldLeft(rslt2) { (l, r) => r(l) }
    }

    val toSubstr = (target: String) => {
      """(\$"\w*)/(\d*):(\d*)"""".r.findAllMatchIn(target).map { r =>
        val name = r.group(1) + "\""
        (r.toString, (r.group(2), r.group(3)) match {
          case (pos, "")  => s"substring(${name}, ${pos}, 1024)"
          case ("", len)  => s"substring(${name}, 1, ${len})"
          case (pos, len) => s"substring(${name}, ${pos}, ${len})"
        })
      }.foldLeft(target) { (l, r) => l.replaceAllLiterally(r._1, r._2) }
    }

    val toPadding = (target: String) => {
      """(\$"\w*)/pad(\d*)"""".r.findAllMatchIn(target).map { r =>
        val name = r.group(1) + "\""
        (r.toString, r.group(2) match {
          case len => s"""rpad(${name}, ${len}, " ")"""
        })
      }.foldLeft(target) { (l, r) => l.replaceAllLiterally(r._1, r._2) }
    }

    val replaceSignStr = (s: String) =>
      Seq(
        (" = ", " === "),
        (" != ", " !== "))
        .foldLeft(s) { (l, r) => l.replaceAllLiterally(r._1, r._2) }

    def replaceJoinStr(inStr: String) = {
      def toColumn(tag: String) = (s: Match) => {
        val item = dropDescription('[')(s).split('.')(1)
        (s.toString, s"""${tag}("${item}")""")
      }

      Seq(
        regxJoinLeft.findAllMatchIn(inStr).map(toColumn("left")),
        regxJoinRight.findAllMatchIn(inStr).map(toColumn("right")))
        .reduce(_ ++ _).toList.foldLeft(inStr) { (l, r) => l.replaceAllLiterally(r._1, r._2) }
    }

    def replaceLit(inStr: Seq[String]) = inStr.map(replaceLitStr)

    def replaceLitUdfStr(s: String) = {
      Seq(
        regxWithParent.findAllMatchIn(s)
          .map(x => (x.group(0).toString, x.group(1) + x.group(2))),
        regx.findAllMatchIn(s)
          .map(x => (x.toString, s"${dropDescription('(')(x)}")),
        regxNull.findAllMatchIn(s)
          .map(x => (x.toString, "null")),
        regxDecimal.findAllMatchIn(s)
          .map(x => (x.toString, s"${dropDescription('(')(x)}")),
        regxItem.findAllMatchIn(s)
          .map(x => (x.toString, s"""${dropDescription('[')(x)}""")))
        .reduce(_ ++ _).toList.sortBy(x => x._1.size).reverse.foldLeft(s) { (l, r) => l.replaceAllLiterally(r._1, r._2) }
    }

    def replaceLitUdf(inStrs: Seq[String]) = {
      val x = inStrs.map(replaceLitUdfStr)
      Seq(scala.toSubstr, scala.toPadding).foldLeft(x) { (l, r) => l.map(x => r(x)) }
    }
  }

  object db {
    def replaceLitStringMod(org: String) =
      Seq(toSubstr, toPadding).foldLeft(org) { (l, r) => r(l) }

    val toSubstr = (target: String) => {
      """(\w*)/(\d*):(\d*)""".r.findAllMatchIn(target).map { r =>
        val name = r.group(1)
        (r.toString, (r.group(2), r.group(3)) match {
          case (pos, "")                   => s"SUBSTR(${name},${pos})"
          case ("", len) if len.toInt <= 0 => name
          case ("", len)                   => s"SUBSTR(${name}, 1, ${len})"
          case (pos, len)                  => s"SUBSTR(${name}, ${pos}, ${len})"
        })
      }.foldLeft(target) { (l, r) => l.replaceAllLiterally(r._1, r._2) }
    }

    val toPadding = (target: String) => {
      """(\w*)/pad(\d*)""".r.findAllMatchIn(target).map { r =>
        val name = r.group(1)
        (r.toString, r.group(2) match {
          case len => s"""RPAD(${name}, ${len})"""
        })
      }.foldLeft(target) { (l, r) => l.replaceAllLiterally(r._1, r._2) }
    }
  }

  object join {
    def genJoinSelectorAppend(str: String, isAllLast: Boolean) = genJoinSelector(str) {
      case ((logic, comment), isLast) =>
        val comma = (isLast, isAllLast) match {
          case (true, true) => ""
          case _            => ","
        }
        s"""("${logic}")${comma} //${comment}"""
    }

    def genJoinSelectorDrop(str: String, isAllLast: Boolean) = genJoinSelector(str) {
      case ((logic, comment), isLast) =>
        val aster = (isLast, isAllLast) match {
          case (true, true)  => """("*")"""
          case (true, false) => """("*"),"""
          case _             => ""
        }
        s""".drop("${logic}")${aster} //${comment}"""
    }

    val regx = """(\S+)\[(.*?)\]""".r
    def genJoinSelector(str: String)(func: ((String, String), Boolean) => String) = {
      val parsed = str.split(',').flatMap {
        case "全て" => Seq(("*", "全て"))
        case s    => regx.findAllMatchIn(s).map(reg => (reg.group(1), reg.group(2)))
      }
      parsed.dropRight(1).map(x => func(x, false)) ++ parsed.takeRight(1).map(x => func(x, true))
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.src

import scala.io.Source
import scala.util.Try
import scala.reflect.io.Directory
import java.io.FileWriter
import ItemReplace._
import ItemReplace.scala._
import org.apache.commons.io.output.FileWriterWithEncoding
import d2k.appdefdoc.parser._

object SourceGenerator extends App {
  val isLocalMode = args.size >= 5
  val (baseUrl, branch, appGroup, appId) = (args(0), args(1), args(2), args(3))
  val basePath = if (isLocalMode) args(4) else ""
  val writeBase = s"data/srcGen/${appGroup}"
  val writePath = Directory(writeBase)
  writePath.createDirectory(true, false)

  println(s"[Start Source Generate${if (isLocalMode) " on Local Mode" else ""}] ${args.mkString(" ")}")
  val appdef = if (isLocalMode) {
    AppDefParser(basePath, appGroup, appId).get
  } else {
    AppDefParser(baseUrl, branch, appGroup, appId).get
  }
  val appdefStr = generate(appdef)

  val writeFilePath = s"${writeBase}/${appId}.scala"
  val writer = new FileWriter(writeFilePath)
  writer.write(appdefStr)
  writer.close

  val inputFiles = appdef.inputList.filter(_.srcType.toLowerCase.startsWith("file"))
  inputFiles.headOption.foreach { _ =>
    generateItemConf(baseUrl, branch, appGroup, appId, writePath)(inputFiles)
  }

  println(s"[Finish Source Generate] ${writeFilePath}")

  def fileToStr(fileName: String) =
    Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString

  def generate(appdef: AppDef) = {
    val componentdefs = appdef.componentList.map { comp =>
      println(s"Parsing ${comp.id}[${comp.name}]")
      if (isLocalMode) {
        ComponentDefParser(basePath, appGroup, appId, comp.mdName).get
      } else {
        ComponentDefParser(baseUrl, branch, appGroup, appId, comp.mdName).get
      }
    }

    val componentTmpl = fileToStr("genTemplates/component.tmpl")
    val componentStr = componentdefs.map { c =>
      println(s"Generating ${c.componentInfo.id}[${c.componentInfo.name}]")
      val conved = c.componentDetail.processes.flatMap { proc =>
        proc.detail.flatMap { x =>
          x.values.flatMap {
            case ProcessParamSubDetail(id, name, defaultCode, value) =>
              Some((defaultCode, value))
            case _ => None
          }.groupBy(_._1).map { b =>
            ProcessDetail(x.param, Some(b._1), b._2.map(x => ProcessParamValue(x._2.mkString)))
          }
        }
      }

      val componentDefs = c.componentDetail.processes.flatMap(
        _.detail.flatMap(p => ConvertTemplateDefine.modDetail(baseUrl, branch)(c.componentDetail, p)))
      val implimentTmpl = fileToStr("genTemplates/impliment.tmpl")
      val funcIds = c.implimentLogic.impls.flatMap {
        case Impliment関数定義(id, desc, args, impls)    => None
        case ImplimentUDF関数定義(id, desc, args, impls) => None
        case x: Impliment関数呼出                        => Some(x.genSrc(""))
        case x                                       => Some(x.id)
      }.mkString(" ~> ")

      val implStr = c.implimentLogic.impls.map {
        case x: Impliment全体編集     => x.genSrc(fileToStr("genTemplates/impl全体編集.tmpl"))
        case x: Impliment部分編集     => x.genSrc(fileToStr("genTemplates/impl部分編集.tmpl"))
        case x: Implimentキャスト     => x.genSrc(fileToStr("genTemplates/implキャスト.tmpl"))
        case x: Impliment選択       => x.genSrc(fileToStr("genTemplates/impl選択.tmpl"))
        case x: Impliment抽出       => x.genSrc(fileToStr("genTemplates/impl抽出.tmpl"))
        case x: Impliment出力項目並び替え => x.copy(baseUrl = baseUrl, branch = branch).genSrc(fileToStr("genTemplates/impl出力項目並び替え.tmpl"))
        case x: Implimentグループ抽出   => x.genSrc(fileToStr("genTemplates/implグループ抽出.tmpl"))
        case x: Impliment集計       => x.genSrc(fileToStr("genTemplates/impl集計.tmpl"))
        case x: Impliment再分割      => x.genSrc(fileToStr("genTemplates/impl再分割.tmpl"))
        case x: Implimentキャッシュ    => x.genSrc(fileToStr("genTemplates/implキャッシュ.tmpl"))
        case x: Implimentキャッシュ解放  => x.genSrc(fileToStr("genTemplates/implキャッシュ解放.tmpl"))
        case x: Impliment関数定義     => x.genSrc(fileToStr("genTemplates/impl関数定義.tmpl"))
        case x: Impliment関数呼出     => ""
        case x: ImplimentUDF関数定義  => x.genSrc(fileToStr("genTemplates/implUDF関数定義.tmpl"))
        case x: ImplimentUDF関数適用  => x.genSrc(fileToStr("genTemplates/implUDF関数適用.tmpl"))
      }.filter(!_.isEmpty).mkString("\n\n")

      val impliments = implimentTmpl
        .replaceAllLiterally("%%functions%%", funcIds)
        .replaceAllLiterally("%%impliments%%", implStr)

      val componentRepList = Seq(
        ("%%componentDesc%%", s"${c.componentInfo.id} ${c.componentInfo.name}"),
        ("%%componentName%%", s"c${c.componentInfo.id}"),
        ("%%templateId%%", c.componentInfo.id.split("_").toList.last),
        ("%%executor%%", if (c.implimentLogic.impls.isEmpty) "Nothing" else "Executor"),
        ("%%componentId%%", addDq(c.componentDetail.componentId)),
        ("%%procImpliments%%", componentDefs.mkString("\n")),
        ("%%impliments%%", if (c.implimentLogic.impls.isEmpty) "" else impliments))
      componentRepList.foldLeft(componentTmpl) { (l, r) =>
        l.replaceAllLiterally(r._1, r._2)
      }
    }.mkString("\n\n")

    def defaultAppFlow = appdef.componentList.map {
      case x if !x.id.contains("_Df") => s"c${x.id}.run(Unit)"
      case x                          => s"c${x.id}.run"
    }.mkString(" ~>\n    ")

    val flowLogic = appdef.componentFlow.map { cf =>
      val flowPair = cf.pairs.map(p => (p.cf1.componentId, p.cf2.componentId))
      FlowLogicGenerator(flowPair)
    }.getOrElse(defaultAppFlow)

    val mainTmpl = fileToStr("genTemplates/main.tmpl")
    val mainRepList = Seq(
      ("%%appDesc%%", appdef.appInfo.desc),
      ("%%appId%%", appdef.appInfo.id),
      ("%%appFlow%%", flowLogic),
      ("%%components%%", componentStr))
    mainRepList.foldLeft(mainTmpl) { (l, r) =>
      l.replaceAllLiterally(r._1, r._2)
    }
  }

  def generateItemConf(baseUrl: String, branch: String, appGroup: String, appId: String, writePath: Directory)(inputFiles: Seq[IoData]) = {
    val fileHeader = Seq("itemId", "itemName", "length", "cnvType", "extractTarget", "comment").mkString("\t")
    val writeConfPath = Directory(s"${writePath}/itemConf")
    writeConfPath.createDirectory(true, false)

    inputFiles.map { iodata =>
      println(s"  Parsing ${iodata.id}[${iodata.name}](${iodata.path})")

      val filePath = iodata.path.split('/').takeRight(2).mkString("/")
      val itemdef = ItemDefParser(baseUrl, branch, filePath).get
      val itemDetails = itemdef.details.map { d =>
        Seq(d.id, d.name, d.size, d.dataType, "false", "").mkString("\t")
      }
      val outputList = fileHeader :: itemDetails

      val writeFilePath = s"${writeConfPath}/${appGroup}_items_${appId}_${iodata.id}.conf"
      val writer = new FileWriterWithEncoding(writeFilePath, "MS932")
      writer.write(outputList.mkString("\r\n"))
      writer.write("\r\n")
      writer.close
      println(s"  [generate itemConf] ${writeFilePath.replaceAllLiterally("\\", "/")}")
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.test

import d2k.common.MakeResource
import scala.io.Source
import scala.reflect.io.Path
import scala.reflect.io.Path.string2path

case class IoMdInfo(ioType: String, id: String, name: String, path: String)
case class OutputData(
  groupId: String, appId: String, testCase: String, inputMdData: Seq[(IoMdInfo, String)], outputMdData: Seq[(IoMdInfo, String)]) {
  def write(outputBasePath: String = s"data/testGen") = {
    writeTestCase(outputBasePath)
    writeInputMd(outputBasePath)
    writeOutputMd(outputBasePath)
  }
  def writeTestCase(outputBasePath: String) = {
    val writePath = Path(s"${outputBasePath}/${groupId}")
    writePath.createDirectory(failIfExists = false)
    val outPath = writePath / s"${appId}Test.scala"
    outPath.toFile.writeAll(testCase)
  }

  def writeInputMd(outputBasePath: String) = {
    val writePath = Path(s"${outputBasePath}/markdown/${appId}/AT")
    writePath.createDirectory(failIfExists = false)
    inputMdData.foreach {
      case (ioMd, tableData) =>
        val outPath = writePath / s"${ioMd.id}.md"
        val outputData = s"# ${ioMd.name}\n${tableData}"
        outPath.toFile.writeAll(tableData)
    }
  }

  def writeOutputMd(outputBasePath: String) = {
    val writePath = Path(s"${outputBasePath}/markdown/${appId}/AT")
    writePath.createDirectory(failIfExists = false)
    outputMdData.foreach {
      case (ioMd, tableData) =>
        val outPath = writePath / s"${ioMd.id}.md"
        val outputData = s"# ${ioMd.name}\n${tableData}"
        outPath.toFile.writeAll(tableData)
    }
  }
}

object GenerateTestCase {
  def apply(baseUrl: String) = new GenerateTestCase(baseUrl)
}

class GenerateTestCase(baseUrl: String) {
  def generate(branch: String, appGroup: String, appId: String) = {
    val appBaseUrl = s"${baseUrl}/raw/${branch}/apps/${appGroup}/${appId}"
    val itemsBaseUrl = s"${baseUrl}/raw/${branch}/apps/common/items"
    val url = s"${appBaseUrl}/README.md"

    val md = Source.fromURL(s"${url}?private_token=${sys.env("GITLAB_TOKEN")}").getLines.toList
    val ioList = md.filter(!_.isEmpty).dropWhile(line => !line.contains("## 03. 入出力データ一覧"))
    val inputList = ioList.drop(2).takeWhile(!_.contains("### 03.02. 出力")).drop(2)
    val outputList = ioList.dropWhile(!_.contains("### 03.02. 出力")).drop(1)

    def strToIoMdInfo(str: String) = {
      val ioInfoRegx = "\\|\\s*\\[(.*)]\\((.*)\\)\\s*\\|(.*)\\|(.*)\\|".r
      ioInfoRegx.findFirstMatchIn(str)
        .map(g => IoMdInfo(g.group(3).trim, g.group(1).trim, g.group(4).trim, g.group(2).trim))
    }

    val ioTypeToCnvMethodName = (ioType: String, appId: String) => ioType.toLowerCase match {
      case "pq"          => s"""toPq("${appId}")"""
      case "db"          => s"""toDb("${appId}")"""
      case "jef"         => s"""toJef("${appId}")"""
      case "file(fixed)" => "toFixed(\"writePath\")"
      case "file(csv)"   => "toCsv(\"writePath\")"
      case "file(tsv)"   => "toTsv(\"writePath\")"
    }

    val ioTypeToCheckMethodName = (ioType: String, appId: String) => ioType.toLowerCase match {
      case "pq"          => s"""checkPq("${appId}.pq")"""
      case "db"          => s"""checkDb("${appId}")"""
      case "file(fixed)" => "checkFixed(\"writePath\")"
      case "file(csv)"   => "checkCsv(\"writePath\")"
      case "file(tsv)"   => "checkTsv(\"writePath\")"
    }

    val tableTemplate = "        //%%inputDataName%%\n        %%inputData%%.%%inputConvMethod%%"
    def imiToTemplate(imi: IoMdInfo, ioTypeCnv: (String, String) => String) = {
      val itemName = imi.path.split("/").takeRight(2).mkString("/")
      tableTemplate
        .replaceAllLiterally("%%inputDataName%%", imi.name)
        .replaceAllLiterally("%%inputData%%", s"""x.readMdTable("${imi.id}.md")""")
        .replaceAllLiterally("%%inputConvMethod%%", ioTypeCnv(imi.ioType, imi.id))
    }

    def imiToMdData(imi: IoMdInfo) = {
      println(s"read:${imi.id}:${imi.name}")
      val itemName = imi.path.split("/").takeRight(3).mkString("/")
      s"# ${imi.name}\n${MakeResource.itemsMdToTable(s"${itemsBaseUrl}/${itemName}").getOrElse("")}\n"
    }

    val template = Option(getClass.getClassLoader.getResourceAsStream("genTemplates/testcaseAt.tmpl"))
      .map(is => Source.fromInputStream(is)).get.mkString
    val inputInfos = inputList.flatMap(strToIoMdInfo).map(d => imiToTemplate(d, ioTypeToCnvMethodName))
    val inputMdData = inputList.flatMap(strToIoMdInfo).map(d => (d, imiToMdData(d)))
    val outputInfos = outputList.flatMap(strToIoMdInfo).map(d => imiToTemplate(d, ioTypeToCheckMethodName))
    val outputMdData = outputList.flatMap(strToIoMdInfo).map(d => (d, imiToMdData(d)))
    val testCaseStr = template.replaceAllLiterally("%%APP_NAME%%", appId)
      .replaceAllLiterally("%%PROJECT_ID%%", appGroup)
      .replaceAllLiterally("%%READ_DATA%%", inputInfos.mkString("\n\n"))
      .replaceAllLiterally("%%CHECK_DATA%%", outputInfos.mkString("\n\n"))

    OutputData(appGroup, appId, testCaseStr, inputMdData, outputMdData)
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.test

object TestGenerator extends App {
  val (baseUrl, branch, appGroup, appId) = (args(0), args(1), args(2), args(3))
  println(s"[Start Test Case Generate] ${args.mkString(" ")}")
  GenerateTestCase(baseUrl).generate(branch, appGroup, appId).write()
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.tmpl

import scala.io.Source
import scala.reflect.io.Directory
import java.io.FileWriter
import scala.reflect.io.Path.string2path

case class CatalogInfo(
  name: String, data: String)

object CatalogMaker extends App {
  println("create templates start")
  def fileToStr(fileName: String) =
    Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString

  val base = fileToStr("templates/base.md")

  val readDb = fileToStr("templates/readDb.md")
  val singleReadDb = readDb.replaceAll("%%name%%", fileToStr("templates/singleReadDb.md"))
  val multiReadDb = readDb.replaceAll("%%name%%", fileToStr("templates/multiReadDb.md"))
  val readFile = fileToStr("templates/readFile.md")
  val readPq = fileToStr("templates/readPq.md")
  val singleReadPq = readPq.replaceAll("%%name%%", fileToStr("templates/singleReadPq.md"))
  val multiReadPq = readPq.replaceAll("%%name%%", fileToStr("templates/multiReadPq.md"))

  val joinDf = fileToStr("templates/joinDf.md")
  val joinPq = fileToStr("templates/joinPq.md")
  val unionDf = fileToStr("templates/unionDf.md")

  val writeDb = fileToStr("templates/writeDb.md")
  val writeFile = fileToStr("templates/writeFile.md")
  val writePq = fileToStr("templates/writePq.md")
  val toVal = fileToStr("templates/toVal.md")

  val ins = List(
    CatalogInfo("Db", singleReadDb),
    CatalogInfo("File", readFile),
    CatalogInfo("Pq", singleReadPq),
    CatalogInfo("Df", ""))

  val outs = List(
    CatalogInfo("Db", writeDb),
    CatalogInfo("File", writeFile),
    CatalogInfo("Pq", writePq),
    CatalogInfo("Val", toVal),
    CatalogInfo("Df", ""))

  val writePath = Directory("catalogs/02_templates")
  writePath.deleteRecursively
  writePath.createDirectory(true, false)

  def makeTemplate(i: CatalogInfo, o: CatalogInfo) = {
    val templateName = s"${i.name}To${o.name}"
    val fileName = s"_${templateName}.md"
    val repStr = i.data + o.data

    val writer = new FileWriter((writePath / fileName).toString)
    writer.write(
      base.replaceAll("%%templatePattern%%", templateName).replaceAll("%%insert%%", repStr))
    writer.close
  }

  for {
    i <- ins
    o <- outs
  } makeTemplate(i, o)

  makeTemplate(CatalogInfo("MultiDb", multiReadDb), CatalogInfo("MapDf", ""))
  makeTemplate(CatalogInfo("MultiPq", multiReadPq), CatalogInfo("MapDf", ""))

  makeTemplate(CatalogInfo("DfJoin", joinDf), CatalogInfo("Df", ""))
  makeTemplate(CatalogInfo("PqJoin", joinPq), CatalogInfo("Pq", writePq))
  makeTemplate(CatalogInfo("DfUnion", unionDf), CatalogInfo("Df", ""))

  println("create templates finish")
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.parser

import scala.util.parsing.combinator.JavaTokenParsers

case class AppDef(
  appInfo: AppInfo, componentFlow: Option[CfData], componentList: Seq[ComponentDefInfo], inputList: Seq[IoData], outputList: Seq[IoData])
case class AppInfo(id: String, name: String, desc: String)
case class ComponentDefInfo(id: String, mdName: String, name: String)
case class IoData(id: String, path: String, srcType: String, name: String)
object AppDefParser extends JavaTokenParsers with D2kParser {

  def apply(baseUrl: String, branch: String, appGroup: String, appId: String) = {
    val parsed = parseAll(appDef, readAppDefMd(baseUrl, branch, appGroup, appId, "README.md"))
    println(parsed)
    parsed
  }

  def apply(baseUrl: String, appGroup: String, appId: String) = {
    val parsed = parseAll(appDef, readAppDefMd(baseUrl, appGroup, appId, "README.md"))
    println(parsed)
    parsed
  }

  def apply(path: String) = {
    parseAll(appDef, readAppDefMd(path))
  }

  def eol = '\n'
  def separator = "----" ~ eol
  val tableSep2 = repN(2, "\\|[\\s\\:\\-]\\-+".r) ~ "|"
  val tableSep3 = repN(3, "\\|[\\s\\:\\-]\\-+".r) ~ "|"
  val anyWords = "^(?!(#{5}|#{4}|#{3}|#{2}|-{4})).*".r

  def appDef = アプリ情報 ~ コンポーネント情報 ~ 入力データ情報 ~ 出力データ情報 ^^ {
    case a ~ b ~ c ~ d => AppDef(a, Option(ComponentFlowParser(c._1.mkString("\n")).getOrElse(null)), b, c._2, d)
  }

  def アプリケーション定義 = "#" ~> "アプリケーション定義" ~ eol
  def アプリId = "##" ~> ident <~ eol
  def アプリ名 = ".*".r <~ eol
  def 概要 = "##" ~> "概要" ~ eol ~> anyWords <~ eol
  def アプリ情報 = アプリケーション定義 ~> アプリId ~ アプリ名 ~ 概要 <~ separator ^^ {
    case a ~ b ~ c => AppInfo(a, b, c)
  }

  def コンポーネント情報 = コンポーネントタイトル ~> repsep(コンポーネントリスト, eol)

  def コンポーネントタイトル = コンポーネントタイトル1 ~> コンポーネントタイトル2 ~> tableSep2
  def コンポーネントタイトル1 = "##" ~> "01. コンポーネント一覧" ~ eol
  def コンポーネントタイトル2 = "|" ~ "コンポーネントID" ~ "|" ~ "コンポーネント名" ~ "|" ~ eol

  def コンポーネントリスト = コンポーネントリストLeft ~ コンポーネントリストRight ^^ {
    case a ~ b => ComponentDefInfo(a._1, a._2, b.trim)
  }
  def コンポーネントリストLeft = "|" ~> コンポーネントId <~ "|"
  def コンポーネントリストRight = "[^|]+".r <~ "|"

  def コンポーネントId = コンポーネントName ~ コンポーネントmdPath
  def コンポーネントName = "[" ~> "\\w*".r <~ "]"
  def コンポーネントmdPath = "(" ~> "[\\w\\.]*".r <~ ")"

  def コンポーネントフロー = (separator ~ "##" ~ "02. コンポーネントフロー図") ~> コンポーネントフロー定義
  def コンポーネントフロー定義 = "```plantuml" ~> repsep("[^`]*".r, eol) <~ ("```" ~ eol ~ separator)

  def IOデータ = IOデータ1 ~ IOデータ2 ~ IOデータ3 ^^ {
    case a ~ b ~ c => IoData(a._1, a._2, b, c.trim)
  }
  def IOデータ1 = "|" ~> IOデータ定義ID ~ IOデータPath
  def IOデータ2 = "|" ~> "[\\w\\(\\)]*".r
  def IOデータ3 = "|" ~> "[^|]*".r <~ "|"
  def IOデータ定義ID = "[" ~> "[\\w\\.]*".r <~ "]"
  def IOデータPath = "(" ~> "[\\w\\./]*".r <~ ")"

  def 入力データタイトル =
    コンポーネントフロー <~ ("##" ~ "03. 入出力データ一覧" ~ "###" ~ "03.01. 入力" ~ "|" ~ "定義ID" ~ "|" ~ "ソースタイプ" ~ "|" ~ "定義名" ~ "|" ~ eol ~ tableSep3)
  def 入力データ情報 = 入力データタイトル ~ rep(IOデータ) ^^ { case a ~ b => (a, b) }

  def 出力データタイトル =
    "###" ~ "03.02. 出力" ~ "|" ~ "定義ID" ~ "|" ~ "ソースタイプ" ~ "|" ~ "定義名" ~ "|" ~ eol ~ tableSep3
  def 出力データ情報 = 出力データタイトル ~> rep(IOデータ)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.parser

import scala.util.parsing.combinator.JavaTokenParsers
import d2k.appdefdoc.gen.src.ItemReplace
import ItemReplace.column._
import scala.util.matching.Regex

case class ComponentDef(
  componentInfo: ComponentInfo, componentDetail: ComponentDetail, implimentLogic: ImplimentLogic)
case class ComponentInfo(id: String, name: String, desc: String)
case class ComponentDetail(componentId: String, processes: List[Process]) {
  def findById(id: String) = processes.map(_.findById(id)).head
  def findById(id: String, subId: String) = processes.map(_.findById(id, subId)).head
}

case class Process(desc: ProcessDesc, detail: List[ProcessDetail]) {
  def findById(id: String) = detail.find(_.param.id == id).get.values.head.asInstanceOf[ProcessParamValue]
  def findById(id: String, subId: String) = detail.find(_.param.id == id).flatMap(_.findById(subId)).get.asInstanceOf[ProcessParamSubDetail]
}
case class ProcessDesc(id: String, value: String)
case class ProcessDetail(param: ProcessParam, defaultCode: Option[ProcessDefaultCode], values: List[ProcessParamBase]) {
  def findById(id: String) = values.find(_.id == id)
}

sealed trait ProcessParamBase {
  val id: String
}
case class ProcessParam(id: String, name: String)
case class ProcessParamValue(value: String) extends ProcessParamBase { val id = "" }
case class ProcessParamSubDetail(id: String, name: String, defaultCode: ProcessDefaultCode, value: Seq[String]) extends ProcessParamBase
case class ProcessDefaultCode(value: String)

case class ImplimentLogic(defaultCode: ImplimentDefaultCode, impls: List[ImplimentLogicBase])
case class ImplimentDefaultCode(value: String)
case class ImplimentValue(value: String)

object TableItem2 {
  def apply(s: String) = {
    val splitted = s.split('|')
    new TableItem2(splitted(1), splitted(2))
  }
}
case class TableItem2(item1: String, item2: String)

object TableItem3 {
  def apply(s: String) = {
    val splitted = s.split('|')
    new TableItem3(splitted(1), splitted(2), splitted(3))
  }
}
case class TableItem3(item1: String, item2: String, item3: String)

object CastTableItem {
  def apply(s: String) = {
    val splitted = s.drop(1).dropRight(1).split("\\s+\\|\\s+")
    new CastTableItem(splitted(0).trim, splitted(1).trim)
  }
}
case class CastTableItem(item1: String, item2: String)

sealed trait ImplimentLogicBase {
  val id: String
  def genSrc(tmpl: String): String
  def addDq(s: String) = {
    val trimed = s.trim
    if (trimed.startsWith("\"")) trimed else s""""${trimed}""""
  }

  def removeComments(s: String) = replaceLit(Seq(s)).head

  def tbl2ToStr(tbls: Seq[TableItem2], doAddDq: Boolean = true) = {
    val bottomData = tbls.last
    (tbls.dropRight(1).map { s =>
      s"      ${if (doAddDq) addDq(s.item1) else s.item1}, //${s.item2}"
    } :+ {
      s"      ${if (doAddDq) addDq(bottomData.item1) else bottomData.item1}  //${bottomData.item2}"
    }).mkString(" \n")
  }
}

case class Impliment全体編集(id: String, desc: String, editors: Seq[ImplimentEditor]) extends ImplimentLogicBase {
  def genSrc(tmpl: String) = {
    tmpl
      .replaceAllLiterally("%%id%%", id)
      .replaceAllLiterally("%%desc%%", desc)
      .replaceAllLiterally("%%funcs%%", editors.map(x => x.genStr).mkString(",\n"))
  }
}

case class Impliment部分編集(id: String, desc: String, editors: Seq[ImplimentEditor]) extends ImplimentLogicBase {
  def genSrc(tmpl: String) = {
    tmpl
      .replaceAllLiterally("%%id%%", id)
      .replaceAllLiterally("%%desc%%", desc)
      .replaceAllLiterally("%%funcs%%", editors.map(x => x.genStr).mkString(",\n"))
  }
}

sealed trait ImplimentEditor {
  def genStr: String
  def addDq(s: String) = {
    val trimed = s.trim
    if (trimed.startsWith("\"")) trimed else s""""${trimed}""""
  }

  val cnvMap = Seq(
    (" =\\s*", " == "),
    //    (" _\\s*", "_"),
    ("and\\s*", "&& "),
    ("or\\s*", "|| "),
    ("if\\s*", "if( "),
    ("else\\s*$", "} else { "),
    ("elseif\\s*", "} else if "),
    ("elsif\\s*", "} else if "),
    ("elif\\s*", "} else if "),
    ("then\\s*", "){ "),
    ("end\\s*", "} "))
  def cnvToScala(logic: Seq[String]): Seq[String] =
    logic.map { cnvMap.foldLeft(_) { case (l, (org, dest)) => l.replaceAll(org, dest) } }

  val reg = "(\\w+_)*\\w{2}_\\w+".r
  def varList(target: Seq[String]) = reg.findAllMatchIn(target.mkString).map(_.matched).toSet.toSeq

  def mkCallArgs(target: Seq[String]) = {
    val arg = target.map(x => s"""$$"${x}"""").mkString(", ")
    if (target.size <= 10) {
      arg
    } else {
      s"array(${arg})"
    }
  }
  def varType(s: String) = {
    val splitted = s.split('_')
    val prefixType = if (splitted.size >= 2) {
      splitted.dropRight(1).takeRight(1)
    } else {
      splitted
    }

    prefixType.headOption.map {
      case "NM" | "AM" => "jBigDecimal"
      case _           => "String"
    }.getOrElse("String")
  }
  def mkArgs(target: Seq[String]) =
    if (target.size <= 10) {
      target.map { x =>
        s"${x}: ${varType(x)}"
      }.mkString(", ")
    } else {
      "arr:Seq[Any]"
    }
  def argMoveList(target: Seq[String]) =
    if (target.size <= 10) {
      ""
    } else {
      target.zipWithIndex.map {
        case (item, idx) =>
          s"val ${item} = arr(${idx}).asInstanceOf[${varType(item)}]"
      }.mkString("\n          ", "\n          ", "")
    }
}

case class ImplimentEdit(no: String, id: String, name: String, logic: Seq[String]) extends ImplimentEditor {
  def genStr = {
    val convedLogic = cnvToScala(replaceLitUdf(logic))
    val vl = varList(convedLogic)
    if (logic.size == 1) {
      val repLogic = if (logic(0).trim == "編集無し") Seq(s"""$$"${id}"""") else logic
      s"""\n      //${no} ${name.replaceAllLiterally("[\\[\\]]", "")}\n      (${addDq(id)}, ${replaceLit(repLogic).mkString("\n")}).e"""
    } else {
      s"""
      //${no} ${name.replaceAllLiterally("[\\[\\]]", "")}
        \\      {
        \\        val func = udf{(${mkArgs(vl)}) =>${argMoveList(vl)}
        \\        ${ItemReplace.runningDates.foldLeft(convedLogic.mkString("\n          ")) { (l, r) => l.replaceAllLiterally(r._1, r._2) }}
        \\        }
        \\        (${addDq(id)}, func(${mkCallArgs(vl)}))
        \\      }.e""".stripMargin('\\')
    }
  }
}

case class ImplimentRename(no: String, srcId: String, srcName: String, destId: String, destName: String, logic: Seq[String]) extends ImplimentEditor {
  def genStr = {
    val convedLogic = cnvToScala(replaceLitUdf(logic))
    val vl = varList(convedLogic)
    if (logic.isEmpty)
      s"\n      //${no} ${srcName} -> ${destName}\n      (${addDq(srcId)} -> ${addDq(destId)}).r"
    else {
      if (logic.size == 1) {
        s"\n      //${no} ${srcName} -> ${destName}\n      (${addDq(srcId)} -> ${addDq(destId)}, ${replaceLit(logic).mkString("\n")}).r"
      } else {
        s"""
        \\      //${no} ${srcName} -> ${destName}      
        \\      {
        \\        val func = udf{(${mkArgs(vl)}) =>${argMoveList(vl)}
        \\        ${convedLogic.mkString("\n          ")}
        \\        }
        \\        (${addDq(srcId)} -> ${addDq(destId)}, func(${mkCallArgs(vl)}))
        \\      }.r""".stripMargin('\\')
      }

    }
  }
}

case class ImplimentDelete(no: String, id: String, name: String) extends ImplimentEditor {
  def genStr = s"\n      //${no} ${name} -> delete\n      ${addDq(id)}.d"
}

case class Impliment選択(id: String, desc: String, data: Seq[TableItem2]) extends ImplimentLogicBase {
  def genSrc(tmpl: String) = {
    tmpl
      .replaceAllLiterally("%%id%%", id)
      .replaceAllLiterally("%%desc%%", desc)
      .replaceAllLiterally("%%funcs%%", tbl2ToStr(data))
  }
}

case class Impliment抽出(id: String, desc: String, cond: Seq[String]) extends ImplimentLogicBase {
  def genSrc(tmpl: String) = {
    tmpl
      .replaceAllLiterally("%%id%%", id)
      .replaceAllLiterally("%%desc%%", desc)
      .replaceAllLiterally("%%funcs%%", ItemReplace.column.replaceLit(cond).mkString("\n"))
  }
}

case class Implimentキャスト(id: String, desc: String, castType: String, data: Seq[TableItem2]) extends ImplimentLogicBase {
  def postValue(s: String) = if (s.trim.startsWith("[正規表現]")) "cr" else "c"
  def replacePipe(s: String) = s.replaceAllLiterally("&#124;", "|")
  def tbl2ToStr(tbls: Seq[TableItem2]) = {
    val bottomTi = tbls.last
    (tbls.dropRight(1).map { ti =>
      s"      (${addDq(replacePipe(ti.item1))}, ${addDq(castType)}).${postValue(ti.item2)}, //${ti.item2}"
    } :+ {
      s"      (${addDq(replacePipe(bottomTi.item1))}, ${addDq(castType)}).${postValue(bottomTi.item2)} //${bottomTi.item2}"
    }).mkString(" \n")
  }

  def genSrc(tmpl: String) = {
    tmpl
      .replaceAllLiterally("%%id%%", id)
      .replaceAllLiterally("%%desc%%", desc)
      .replaceAllLiterally("%%funcs%%", tbl2ToStr(data))
  }
}

case class Impliment出力項目並び替え(id: String, tbl: TableItem2, baseUrl: String = "", branch: String = "") extends ImplimentLogicBase {
  def genSrc(tmpl: String) = {
    val item = tbl.item1.trim.split("\\]\\(")
    val resId = item(0).tail
    val filePath = item(1).dropRight(1).split("/").toList.takeRight(3).mkString("/")
    println(s"  Parsing ${resId}[${tbl.item2.trim}]")
    val itemPs = ItemDefParser(baseUrl, branch, filePath)
    val items = tbl2ToStr(itemPs.get.details.map(x => TableItem2(x.id, x.name)))
    tmpl
      .replaceAllLiterally("%%id%%", id)
      .replaceAllLiterally("%%desc%%", s"出力項目並び替え ${resId}[${tbl.item2.trim}]")
      .replaceAllLiterally("%%items%%", items)
  }
}

case class Implimentグループ抽出(id: String, desc: String, grpKeys: Seq[TableItem2], sortKeys: Seq[TableItem3]) extends ImplimentLogicBase {
  def genSrc(tmpl: String) = {
    val sortKeyItem2 = sortKeys.map { t =>
      val sortParam = if (t.item3.trim.startsWith("降順")) "desc" else "asc"
      TableItem2(s"$$${addDq(t.item1)}.${sortParam}", s"${t.item2.trim} ${t.item3.trim}")
    }
    tmpl
      .replaceAllLiterally("%%id%%", id)
      .replaceAllLiterally("%%desc%%", desc)
      .replaceAllLiterally("%%grpKeys%%", tbl2ToStr(grpKeys))
      .replaceAllLiterally("%%sortKeys%%", tbl2ToStr(sortKeyItem2, false))
  }
}

case class Impliment集計(id: String, desc: String, grpKeys: Seq[TableItem2], aggKeys: Seq[TableItem3]) extends ImplimentLogicBase {
  def genSrc(tmpl: String) = {
    val aggkeyItem2 = aggKeys.map { t =>
      val sortParam = if (t.item3.trim.startsWith("降順")) "desc" else "asc"
      val item1Dq = addDq(t.item1)
      TableItem2(s"${t.item3.trim.toLowerCase}(${item1Dq}) as ${item1Dq}", s"${t.item2.trim}")
    }
    tmpl
      .replaceAllLiterally("%%id%%", id)
      .replaceAllLiterally("%%desc%%", desc)
      .replaceAllLiterally("%%grpKeys%%", tbl2ToStr(grpKeys))
      .replaceAllLiterally("%%aggKeys%%", tbl2ToStr(aggkeyItem2, false))
  }
}

case class Impliment再分割(id: String, desc: String, size: Int) extends ImplimentLogicBase {
  def genSrc(tmpl: String) = {
    tmpl
      .replaceAllLiterally("%%id%%", id)
      .replaceAllLiterally("%%desc%%", desc)
      .replaceAllLiterally("%%size%%", size.toString)
  }
}

case class Implimentキャッシュ(id: String, desc: String) extends ImplimentLogicBase {
  def genSrc(tmpl: String) = {
    tmpl
      .replaceAllLiterally("%%id%%", id)
      .replaceAllLiterally("%%desc%%", desc)
  }
}

case class Implimentキャッシュ解放(id: String, desc: String) extends ImplimentLogicBase {
  def genSrc(tmpl: String) = {
    tmpl
      .replaceAllLiterally("%%id%%", id)
      .replaceAllLiterally("%%desc%%", desc)
  }
}

case class Impliment関数定義(id: String, desc: String, args: Seq[TableItem3], impls: Seq[String]) extends ImplimentLogicBase {
  def genSrc(tmpl: String) = {
    val argsItem2 = args.map { t =>
      TableItem2(s"${t.item1.trim}: ${t.item3.trim}", s"${t.item2.trim}")
    }
    tmpl
      .replaceAllLiterally("%%id%%", id)
      .replaceAllLiterally("%%desc%%", desc)
      .replaceAllLiterally("%%args%%", tbl2ToStr(argsItem2, false))
      .replaceAllLiterally("%%impls%%", impls.map(x => s"        $x").mkString("\n"))
  }
}

case class Impliment関数呼出(id: String, desc: String, funcName: String, args: Seq[TableItem3]) extends ImplimentLogicBase {
  def genSrc(tmpl: String) = {
    val funcItem2 = args.map { t =>
      TableItem2(s"    ${t.item1.trim}", s"${t.item2.trim}")
    }
    s"\n        ${funcName}(\n${tbl2ToStr(funcItem2, false)})"
  }
}

case class ImplimentUDF関数定義(id: String, desc: String, args: Seq[TableItem3], impls: Seq[String]) extends ImplimentLogicBase {
  def genSrc(tmpl: String) = {
    val argsItem2 = args.map { t =>
      TableItem2(s"${t.item1.trim}: ${t.item3.trim}", s"${t.item2.trim}")
    }
    tmpl
      .replaceAllLiterally("%%id%%", id)
      .replaceAllLiterally("%%desc%%", desc)
      .replaceAllLiterally("%%args%%", tbl2ToStr(argsItem2, false))
      .replaceAllLiterally("%%impls%%", impls.map(x => s"        $x").mkString("\n"))
  }
}

case class ImplimentUDF関数適用(id: String, desc: String, applyLogic: String, data: Seq[TableItem2]) extends ImplimentLogicBase {
  def postValue(s: String) = if (s.trim.startsWith("[正規表現]")) "ar" else "a"
  def replacePipe(s: String) = s.replaceAllLiterally("&#124;", "|")
  def tbl2ToStr(tbls: Seq[TableItem2]) = {
    val bottomTi = tbls.last
    (tbls.dropRight(1).map { ti =>
      s"      (${addDq(replacePipe(ti.item1))}, (col: Column) => ${applyLogic}).${postValue(ti.item2)}, //${ti.item2}"
    } :+ {
      s"      (${addDq(replacePipe(bottomTi.item1))}, (col: Column) => ${applyLogic}).${postValue(bottomTi.item2)} //${bottomTi.item2}"
    }).mkString(" \n")
  }

  def genSrc(tmpl: String) = {
    tmpl
      .replaceAllLiterally("%%id%%", id)
      .replaceAllLiterally("%%desc%%", desc)
      .replaceAllLiterally("%%funcs%%", tbl2ToStr(data))
  }
}

object ComponentDefParser extends JavaTokenParsers with D2kParser {
  def apply(baseUrl: String, branch: String, appGroup: String, appId: String, fileName: String) = {
    val parsed = parseAll(componentDef, readAppDefMd(baseUrl, branch, appGroup, appId, fileName))
    println(parsed)
    parsed
  }

  def apply(baseUrl: String, appGroup: String, appId: String, fileName: String) = {
    val parsed = parseAll(componentDef, readAppDefMd(baseUrl, appGroup, appId, fileName))
    println(parsed)
    parsed
  }

  val eol = '\n'
  val num2 = "[0-9][0-9]".r
  val anyWords = "^(?!(#{5}|#{4}|#{3}|#{2}|-{4})).*".r
  val tableValue2 = anyWords <~ eol ^^ { case s => TableItem2(s) }
  val tableValue3 = anyWords <~ eol ^^ { case s => TableItem3(s) }

  def componentDef = title ~> componentInfo ~ componentDetail ~ implimentLogic ~ ".*".r ^^ {
    case a ~ b ~ c ~ _ => ComponentDef(a, b, c)
  }

  def title = "#" ~> "コンポーネント定義"
  def componentInfo = "##" ~> componentIdAndName ~ ".*".r ~ componentDesc ^^ {
    case a ~ b ~ c => ComponentInfo(s"${a._1}_${a._2}", b, c)
  }
  def componentIdAndName = componentDefId ~ componentName ^^ { case a ~ b => (a, b) }
  def componentDefId = num2 <~ "_"
  def componentName = "\\w*".r
  def componentDesc = "###" ~> "処理概要" ~> ".*".r

  def componentDetail = componentId ~ (opt("----") ~> rep(process)) ^^ { case a ~ b => ComponentDetail(a, b) }
  def componentId = "----" ~> "###" ~> "コンポーネントID" ~> "#####" ~> ".*".r ~> ident

  def process = processDesc ~ rep(processDetail1) ^^ { case a ~ b => Process(a, b) }
  def processDesc = ("##" ~> "[" ~> ident) ~ ("]" ~> ".*".r) ^^ { case a ~ b => ProcessDesc(a, b) }
  def processDetail1 = processParam ~ opt(processDefaultCode) ~ rep(processParamSubDetail | processValue) ^^ {
    case a ~ b ~ c => ProcessDetail(a, b, c)
  }
  def processParam = "###" ~> (num2 <~ ".") ~ anyWords ^^ { case a ~ b => ProcessParam(a, b) }
  def processDefaultCode = "#####" ~> anyWords ^^ { case a => ProcessDefaultCode(a) }
  def processValue = anyWords ^^ { case a => ProcessParamValue(a) }
  def processParamSubDetail = ("####" ~> "[0-9][0-9]\\.[0-9][0-9]".r <~ ".") ~ anyWords ~ processDefaultCode ~ rep(anyWords) ^^ {
    case a ~ b ~ c ~ d => ProcessParamSubDetail(a, b, c, d)
  }

  def implimentLogic = (implimentTitle ~> implimentDefaultCode) ~ (opt(implimentFlowDiagram) ~> impliment) ^^ { case a ~ b => ImplimentLogic(a, b) }
  def implimentTitle = "----" ~ "##" ~ "実装ロジック"
  def implimentDefaultCode = "#####" ~> ".*".r ^^ { case a => ImplimentDefaultCode(a) }
  def implimentFlowDiagram = "###" ~ "実装ロジックフロー図" ~ "```plantuml" ~ rep(implimentFlowDiagramDetail) ~ "```"
  def implimentFlowDiagramDetail = "[\\w\\(\\)\\*]*".r ~> "->" ~> ".*".r

  def impliment = rep(
    impliment全体編集 | impliment部分編集 | impliment選択 | impliment抽出 | implimentキャスト |
      impliment出力項目並び替え | implimentグループ抽出 | impliment集計 |
      impliment再分割 | implimentキャッシュ解放 | implimentキャッシュ |
      impliment関数定義 | impliment関数呼出 | implimentUDF関数定義 | implimentUDF関数適用)

  val implimentDetailId = "###" ~> "\\w{3}".r <~ "."
  val implimentDesc = anyWords
  val implimentValue = anyWords <~ eol

  def tableHeader2 = tableHeader2_1 ~ tableHeader2_2
  val tableHeader2_1 = "|" ~ "物理名" ~ "|" ~ "論理名" ~ "|"
  val tableHeader2_2 = repN(2, "\\|[\\s\\:\\-]\\-+".r) ~ "|"

  def tableHeader3(target: String) = tableHeader3_1(target) ~ tableHeader3_2
  def tableHeader3_1(target: String) = "|" ~ "物理名" ~ "|" ~ "論理名" ~ "|" ~ target ~ "|"
  val tableHeader3_2 = repN(3, "\\|[\\s\\:\\-]\\-+".r) ~ "|"

  def impliment全体編集 = implimentDetailId ~ ("全体編集" ~> anyWords) ~ rep(implimentDelete | implimentRename | implimentEdit) ^^ {
    case a ~ b ~ c => Impliment全体編集(a, b, c)
  }

  def impliment部分編集 = implimentDetailId ~ ("部分編集" ~> anyWords) ~ rep(implimentDelete | implimentRename | implimentEdit) ^^ {
    case a ~ b ~ c => Impliment部分編集(a, b, c)
  }

  def implimentEdit = ("####" ~> "\\d{3}".r) ~ (":" ~> ident) ~ "\\[.*\\]".r ~ rep(implimentValue) ^^ {
    case a ~ b ~ c ~ d => ImplimentEdit(a, b, c, d)
  }
  def implimentRename = ("####" ~> "\\d{3}".r) ~ (":" ~> ident) ~ "\\[(.*?)\\]".r ~ ("->" ~> ident) ~ "\\[(.*?)\\]".r ~ rep(implimentValue) ^^ {
    case a ~ b ~ c ~ d ~ e ~ f => ImplimentRename(a, b, c, d, e, f)
  }
  def implimentDelete = ("####" ~> "\\d{3}".r) ~ (":" ~> ident) ~ ("\\[.*\\]".r <~ "->" <~ "delete") ^^ {
    case a ~ b ~ c => ImplimentDelete(a, b, c)
  }

  def implimentキャスト = (implimentDetailId <~ "キャスト") ~ implimentDesc ~ implimentCastDetail1 ~ implimentCastDetail2 ^^ {
    case a ~ b ~ c ~ d => Implimentキャスト(a, b, c, d)
  }
  def implimentCastDetail1 = ("####" ~ "01." ~ "キャスト型") ~> anyWords
  def implimentCastDetail2 = ("####" ~ "02." ~ "キャスト対象項目" ~ tableHeader2) ~> rep(tableValue2)

  def impliment選択 = implimentDetailId ~ ("選択" ~> implimentDesc) ~ implimentSelect ^^ {
    case a ~ b ~ c => Impliment選択(a, b, c)
  }
  def implimentSelect = ("####" ~ "01." ~ "選択項目") ~> tableHeader2 ~> rep(tableValue2)

  def impliment抽出 = implimentDetailId ~ ("抽出" ~> implimentDesc) ~ implimentAbstruction ^^ {
    case a ~ b ~ c => Impliment抽出(a, b, c)
  }

  def implimentAbstruction = ("####" ~ "01." ~ "抽出条件") ~> rep(implimentDesc <~ eol)

  def impliment出力項目並び替え = (implimentDetailId <~ "出力項目並び替え") ~
    (implimentOutItemTable2_1 ~> implimentOutItemTable2_2 ~> implimentOutItemTable2) ^^ {
      case a ~ b => Impliment出力項目並び替え(a, b)
    }

  val implimentOutItemTable2_1 = "|" ~ "項目定義物理名" ~ "|" ~ "項目定義論理名" ~ "|"
  val implimentOutItemTable2_2 = "|" ~ "\\-*".r ~ "|" ~ "\\-*".r ~ "|"
  val implimentOutItemTable2 = anyWords ^^ { case s => TableItem2(s) }

  def implimentグループ抽出 = (implimentDetailId <~ "グループ抽出") ~ implimentDesc ~ implimentGrpAbstDetail1 ~ implimentGrpAbstDetail2 ^^ {
    case a ~ b ~ c ~ d => Implimentグループ抽出(a, b, c, d)
  }
  def implimentGrpAbstDetail1 = "####" ~ "01." ~ "グルーピングキー" ~> tableHeader2 ~> rep(tableValue2)
  def implimentGrpAbstDetail2 = "####" ~ "02." ~ "ソートキー" ~> tableHeader3("ソート順") ~> rep(tableValue3)

  def impliment集計 = (implimentDetailId <~ "集計") ~ implimentDesc ~ implimentSumDetail1 ~ implimentSumDetail2 ^^ {
    case a ~ b ~ c ~ d => Impliment集計(a, b, c, d)
  }
  def implimentSumDetail1 = ("####" ~ "01." ~ "グルーピングキー" ~ tableHeader2) ~> rep(tableValue2)
  def implimentSumDetail2 = ("####" ~ "02." ~ "集計キー" ~ tableHeader3("集計パターン")) ~> rep(tableValue3)

  def impliment再分割 = (implimentDetailId <~ "再分割") ~ implimentDesc ~ implimentRep ^^ {
    case a ~ b ~ c => Impliment再分割(a, b, c.toInt)
  }
  def implimentRep = "####" ~> "01." ~> "再分割数" ~> decimalNumber

  def implimentキャッシュ = (implimentDetailId <~ "キャッシュ") ~ implimentDesc ^^ {
    case a ~ b => Implimentキャッシュ(a, b)
  }

  def implimentキャッシュ解放 = (implimentDetailId <~ "キャッシュ解放") ~ implimentDesc ^^ {
    case a ~ b => Implimentキャッシュ解放(a, b)
  }

  def impliment関数定義 = ("###" ~> "\\w*".r) ~ ("." ~> "関数定義" ~> implimentDesc) ~ implimentDefFuncDetail1 ~ implimentDefFuncDetail2 ^^ {
    case a ~ b ~ c ~ d => Impliment関数定義(a, b, c, d)
  }
  def implimentDefFuncDetail1 = "####" ~ "01." ~ "入力シグネチャ" ~> tableHeader3("型") ~> rep(tableValue3)
  val implimentDefFuncDetail2 = "####" ~ "02." ~ "実装内容" ~> rep(implimentValue)

  def impliment関数呼出 = ("###" ~> "\\w*".r) ~ ("." ~> "関数呼出" ~> implimentDesc) ~ implimentCallFuncDetail1 ~ implimentCallFuncDetail2 ^^ {
    case a ~ b ~ c ~ d => Impliment関数呼出(a, b, c, d)
  }
  val implimentCallFuncDetail1 = "####" ~ "01." ~ "関数名" ~> implimentDesc
  def implimentCallFuncDetail2 = "####" ~ "02." ~ "呼出パラメータ" ~> tableHeader3("型") ~> rep(tableValue3)

  def implimentUDF関数定義 = ("###" ~> "\\w*".r) ~ ("." ~> "UDF関数定義" ~> implimentDesc) ~ implimentDefUdfFuncDetail1 ~ implimentDefUdfFuncDetail2 ^^ {
    case a ~ b ~ c ~ d => ImplimentUDF関数定義(a, b, c, d)
  }
  def implimentDefUdfFuncDetail1 = "####" ~ "01." ~ "入力シグネチャ" ~> tableHeader3("型") ~> rep(tableValue3)
  val implimentDefUdfFuncDetail2 = "####" ~ "02." ~ "実装内容" ~> rep(implimentValue)

  def implimentUDF関数適用 = (implimentDetailId <~ "UDF関数適用") ~ implimentDesc ~ implimentApplyDetail1 ~ implimentApplyDetail2 ^^ {
    case a ~ b ~ c ~ d => ImplimentUDF関数適用(a, b, c, d)
  }
  def implimentApplyDetail1 = ("####" ~ "01." ~ "適用関数") ~> anyWords
  def implimentApplyDetail2 = ("####" ~ "02." ~ "適用対象項目" ~ tableHeader2) ~> rep(tableValue2)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.parser

import scala.util.parsing.combinator.JavaTokenParsers

trait ComponentFlow {
  val componentId: String
}
case class CfId(componentId: String) extends ComponentFlow
case class Cf(componentId: String, desc: String) extends ComponentFlow
case object CfEnd extends ComponentFlow { val componentId = "CfEnd" }

case class CfPair(cf1: ComponentFlow, cf2: ComponentFlow)
case class CfData(pairs: Seq[CfPair])

object ComponentFlowParser extends JavaTokenParsers with D2kParser {
  def apply(target: String) = {
    parseAll(componentFlow, target)
  }

  val eol = '\n'
  val num2 = "[0-9][0-9]".r
  val anyWords = ".*".r
  val anyWords2 = "^(?!\\|).*".r

  def componentFlow = rep(withLink) ^^ { case x => CfData(x) }

  def componentId = "[0-9\\w_]*".r
  def component = componentId ^^ { case a => CfId(a) }
  def componentWithAs = ("""".*?"""".r <~ "as") ~ componentId ^^ {
    case a ~ b => Cf(b, a.replaceAllLiterally("\"", "").split("\\\\n")(1))
  }
  def componentTail = "(*)" ^^ { case _ => CfEnd }

  def componentPattern = componentTail | componentWithAs | component
  def withLink = ((componentPattern <~ "-->") ~ componentPattern) <~ eol ^^ {
    case a ~ b => CfPair(a, b)
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.parser

import scala.io.Source

trait D2kParser {
  def readAppDefMd(baseUrl: String, branch: String, appGroup: String, appId: String, fileName: String) = {
    val appBaseUrl = s"${baseUrl}/raw/${branch}/apps/${appGroup}/${appId}"
    val url = s"${appBaseUrl}/${fileName}"
    Source.fromURL(s"${url}?private_token=${sys.env("GITLAB_TOKEN")}").getLines.mkString("\n")
  }

  def readItemDefMd(baseUrl: String, branch: String, filePath: String) = {
    val itemsBaseUrl = s"${baseUrl}/raw/${branch}/apps/common/items"
    val url = s"${itemsBaseUrl}/${filePath}"
    Source.fromURL(s"${url}?private_token=${sys.env("GITLAB_TOKEN")}").getLines.mkString("\n")
  }

  def readAppDefMd(basePath: String, appGroup: String, appId: String, fileName: String) = {
    val appBasePath = s"${basePath}/apps/${appGroup}/${appId}"
    val path = s"${appBasePath}/${fileName}"
    Source.fromFile(path).getLines.mkString("\n")
  }

  def readAppDefMd(basePath: String) = {
    Source.fromFile(basePath).getLines.mkString("\n")
  }

  def readItemDefMd(basePath: String) = {
    Source.fromFile(basePath).getLines.mkString("\n")
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.parser

import scala.util.parsing.combinator.JavaTokenParsers

case class ItemDef(id: String, dataType: String, name: String, details: List[ItemDetail])
case class ItemDetail(id: String, name: String, dataType: String, size: String, format: String)

object TableItem5 {
  def apply(s: String) = {
    val splitted = s.split('|')
    new TableItem5(splitted(1), splitted(2), splitted(3), splitted(4), splitted(5))
  }
}
case class TableItem5(item1: String, item2: String, item3: String, item4: String, item5: String)

object ItemDefParser extends JavaTokenParsers with D2kParser {
  def apply(baseUrl: String, branch: String, filePath: String) = {
    val parsed = parseAll(itemDef, readItemDefMd(baseUrl, branch, filePath))
    println(parsed)
    parsed
  }

  def apply(basePath: String) = {
    parseAll(itemDef, readItemDefMd(basePath))
  }

  val eol = '\n'
  val num2 = "[0-9][0-9]".r
  val anyWords = ".*".r
  val anyWords2 = "^(?!\\|).*".r
  val tableValue5 = anyWords <~ eol ^^ { case s => TableItem5(s) }

  def itemDef = dataType ~ (itemInfo <~ opt(comment)) ~ itemDetail <~ ".*".r ^^ {
    case a ~ b ~ c =>
      val details = c.map { x =>
        ItemDetail(x.item1.trim, x.item2.trim, x.item3.trim, x.item4.trim, x.item5.trim)
      }
      val splitName = b.split("_")
      ItemDef(splitName(0), a, splitName(1), details)
  }

  def dataType = "#" ~> "[\\w\\(\\)]*".r <~ "項目定義"
  def itemInfo = "##" ~> anyWords
  def comment = rep(anyWords2)

  def itemDetail = tableTitle1 ~> tableTitle2 ~> rep(tableValue5)
  def tableTitle1 = "|" ~ "物理名" ~ "|" ~ "論理名" ~ "|" ~ ("型(ドメイン)" | "型") ~ "|" ~ "桁数" ~ "|" ~ "フォーマット" ~ "|"
  val tableTitle2 = repN(5, "\\|[\\s\\:\\-]\\-+".r) ~ "|"
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import java.sql.Timestamp
import java.time.format.DateTimeFormatter
import java.time.LocalDateTime

object DateConverter {
  object implicits {
    implicit class Dc(tm: Timestamp) {
      def toYmdhmsS =
        tm.toLocalDateTime.format(DateTimeFormatter.ofPattern("yyyyMMddHHmmssSSS"))
      def toYmdhms =
        tm.toLocalDateTime.format(DateTimeFormatter.ofPattern("yyyyMMddHHmmss"))
      def toYmd =
        tm.toLocalDateTime.format(DateTimeFormatter.ofPattern("yyyyMMdd"))
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import d2k.common.ResourceInfo
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._
import d2k.common.InputArgs

object DbCommonColumnAppender {
  def apply(df: DataFrame, componentId: String)(implicit inArgs: InputArgs) = {
    val addCommonItems =
      df.withColumn("DT_D2KMKDTTM", lit(inArgs.sysSQLDate))
        .withColumn("ID_D2KMKUSR", lit(componentId))
        .withColumn("DT_D2KUPDDTTM", lit(inArgs.sysSQLDate))
        .withColumn("ID_D2KUPDUSR", lit(componentId))
        .withColumn("NM_D2KUPDTMS", lit("0"))
        .withColumn("FG_D2KDELFLG", lit("0"))

    val comonColumnNames = Array("DT_D2KMKDTTM", "ID_D2KMKUSR", "DT_D2KUPDDTTM", "ID_D2KUPDUSR", "NM_D2KUPDTMS", "FG_D2KDELFLG")
    val otherColumns = addCommonItems.columns
    val dropCommonColumns = comonColumnNames.foldLeft(otherColumns) { (l, r) => l.filter(_ != r) }
    val moveToFrontColumns = comonColumnNames ++ dropCommonColumns
    addCommonItems.select(moveToFrontColumns.head, moveToFrontColumns.drop(1).toSeq: _*)
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.component.cmn

import org.apache.spark.sql.DataFrame
import d2k.common.df.template.PqToVal
import d2k.common.InputArgs
import org.apache.spark.sql.Row
import d2k.common.df.executor._
import spark.common.DfCtl.implicits._
import spark.common.DfCtl._
import org.apache.spark.sql.functions._
import spark.common._
import SparkContexts.context.implicits._

object PostCodeConverter {
  val postDataMap =
    new PqToVal[Map[String, (String, String)]] with Nothing {
      val componentId = "MBA935.pq"
      def outputValue(rows: Array[Row]): Map[String, (String, String)] = {
        rows.map { row =>
          (row.getAs[String]("CD_ZIP7LEN").trim, (row.getAs[String]("CD_KENCD"), row.getAs[String]("CD_DEMEGRPCD")))
        }.toMap
      }
    }

  def apply()(implicit inArgs: InputArgs) = new PostCodeConverter
}

class PostCodeConverter(implicit inArgs: InputArgs) extends Serializable {
  import PostCodeConverter._
  private[this] val postMap = postDataMap.run(Unit)

  def localGovernmentCode(postCodeName1: String, postCodeName2: String = "")(outName1: String, outName2: String = "") =
    cnvLocalGovernmentCode(postCodeName1, postCodeName2)(outName1, outName2)

  private[this] def cnvLocalGovernmentCode(
    postCodeName1: String, postCodeName2: String)(outName1: String, outName2: String) = (df: DataFrame) => {

    val convertUdf = udf { (inPostCode: String) =>
      val postCode = Option(inPostCode).map(_.replaceAllLiterally("-", "").trim).getOrElse("")

      def code3 = postMap.get(postCode)

      def code5 = postMap.get(postCode).orElse {
        postMap.get(postCode.take(3))
      }

      def code7 = postMap.get(postCode).orElse {
        postMap.get(postCode.take(3) + "0000")
      }.orElse {
        postMap.get(postCode.take(3))
      }

      (postCode.size match {
        case 3 => code3
        case 5 => code5
        case 7 => code7
        case _ => None
      }).getOrElse(("", ""))
    }

    val postCodeCol = if (postCodeName2.isEmpty)
      col(postCodeName1)
    else
      concat(trim(col(postCodeName1)), col(postCodeName2))

    val df2 = df ~> editColumns(Seq(("_POSTCODES_", convertUdf(postCodeCol)).e))

    val outCol = if (outName2.isEmpty)
      Seq((outName1, $"_POSTCODES_._1").e)
    else
      Seq((outName1, $"_POSTCODES_._1").e, (outName2, $"_POSTCODES_._2").e)

    (df2 ~> editColumns(outCol))
      .drop("_POSTCODES_")
      .na.fill("", Seq(outName1, outName2))
      .na.replace(outName1, Map("" -> "99"))
      .na.replace(outName2, Map("" -> "999"))
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.component.sh

import d2k.common.df.template.sh._
import d2k.common.df.executor.Nothing

object CommissionBaseChannelSelector {
  private[CommissionBaseChannelSelector] trait trComm試算 extends CommissionBaseChannelSelectorTmpl {
    val info = CommissionBaseChannelSelectorInfo("01", "1", "1")
  }
  def comm試算 = new trComm試算 {}
  def comm試算(uniqueKeys: String*) = new trComm試算 { override val groupingKeys = uniqueKeys }

  private[CommissionBaseChannelSelector] trait trComm実績_月次手数料 extends CommissionBaseChannelSelectorTmpl {
    val info = CommissionBaseChannelSelectorInfo("02", "1", "1")
  }
  def comm実績_月次手数料 = new trComm実績_月次手数料 {}
  def comm実績_月次手数料(uniqueKeys: String*) = new trComm実績_月次手数料 { override val groupingKeys = uniqueKeys }

  private[CommissionBaseChannelSelector] trait trComm実績_割賦充当 extends CommissionBaseChannelSelectorTmpl {
    val info = CommissionBaseChannelSelectorInfo("03", "1", "1")
  }
  def comm実績_割賦充当 = new trComm実績_割賦充当 {}
  def comm実績_割賦充当(uniqueKeys: String*) = new trComm実績_割賦充当 { override val groupingKeys = uniqueKeys }

  private[CommissionBaseChannelSelector] trait trComm実績_直営店 extends CommissionBaseChannelSelectorTmpl {
    val info = CommissionBaseChannelSelectorInfo("04", "0", "0")
  }
  def comm実績_直営店 = new trComm実績_直営店 {}
  def comm実績_直営店(uniqueKeys: String*) = new trComm実績_直営店 { override val groupingKeys = uniqueKeys }

  private[CommissionBaseChannelSelector] trait trComm毎月割一時金 extends CommissionBaseChannelSelectorTmpl {
    val info = CommissionBaseChannelSelectorInfo("05", "1", "1")
  }
  def comm毎月割一時金 = new trComm毎月割一時金 {}
  def comm毎月割一時金(uniqueKeys: String*) = new trComm毎月割一時金 { override val groupingKeys = uniqueKeys }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import org.apache.spark.sql.DataFrame
import d2k.common.InputArgs

trait Executor {
  def invoke(df: DataFrame)(implicit inArgs: InputArgs): DataFrame
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.executor

import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame
import d2k.common.InputArgs
import d2k.common.fileConv.ConfParser
import scala.reflect.io.Path
import d2k.common.fileConv.Converter
import org.apache.spark.sql.Row
import spark.common.SparkContexts

trait BinaryRecordConverter extends Executor {
  val binaryRecordName: String
  val itemConfId: String
  val charEnc: String
  def invoke(df: DataFrame)(implicit inArgs: InputArgs): DataFrame = BinaryRecordConverter(binaryRecordName, itemConfId, charEnc)(df)
}

object BinaryRecordConverter {
  def apply(binaryRecordName: String, itemConfId: String, charEnc: String)(df: DataFrame)(implicit inArgs: InputArgs): DataFrame = {
    val itemConfs = ConfParser.parseItemConf(Path(inArgs.fileConvInputFile).toAbsolute.parent, inArgs.projectId, itemConfId).toList
    val len = itemConfs.map(_.length.toInt)
    val names = itemConfs.map(_.itemId)
    val domains = itemConfs.map(_.cnvType)

    def makeSliceLen(len: Seq[Int]) = len.foldLeft((0, List.empty[(Int, Int)])) { (l, r) => (l._1 + r, l._2 :+ (l._1, l._1 + r)) }
    val (totalLen_, sliceLen) = makeSliceLen(len)
    val ziped = names.zip(domains)
    val (nameList, domainList) = ziped.filter { case (names, domain) => !(domain.startsWith(Converter.NOT_USE_PREFIX)) }.unzip

    def cnvFromFixed(names: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)])(inData: Array[Byte]) = {
      val dataAndDomainsAndNames = sliceLen.map { case (start, end) => inData.slice(start, end) }.zip(domains).zip(names)
      val result = Converter.domainConvert(dataAndDomainsAndNames, charEnc)
      Row.fromSeq(result)
    }

    val droppedDf = df.drop("ROW_ERR").drop("ROW_ERR_MESSAGE")
    val rdd = droppedDf.rdd.map { orgRow =>
      val row = cnvFromFixed(names, domains, sliceLen)(orgRow.getAs[Array[Byte]](binaryRecordName))
      Row.merge(orgRow, row)
    }

    val schema = Converter.makeSchema(nameList).foldLeft(droppedDf.schema) { (l, r) => l.add(r.name, r.dataType) }
    SparkContexts.context.createDataFrame(rdd, schema).drop(binaryRecordName)
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.executor

import org.apache.spark.sql.DataFrame
import d2k.common.InputArgs
import org.apache.spark.sql.functions._
import d2k.common.df.Executor

trait ConvNa extends Executor {
  val dateColumns: Seq[String]
  val tsColumns: Seq[String]
  def invoke(df: DataFrame)(implicit inArgs: InputArgs): DataFrame =
    ConvNaTs(ConvNaDate(df, dateColumns), tsColumns)
}

trait ConvNaDate extends Executor {
  val dateColumns: Seq[String]
  def invoke(df: DataFrame)(implicit inArgs: InputArgs): DataFrame = ConvNaDate(df, dateColumns)
}

object ConvNaDate {
  val dateInit = "0001-01-01"
  def apply(df: DataFrame, dateColumnNames: Seq[String])(implicit inArgs: InputArgs) =
    df.na.fill(dateInit, dateColumnNames).na.replace(dateColumnNames, Map("" -> dateInit))
}

trait ConvNaTs extends Executor {
  val tsColumns: Seq[String]
  def invoke(df: DataFrame)(implicit inArgs: InputArgs): DataFrame = ConvNaTs(df, tsColumns)
}

object ConvNaTs {
  val tsInit = "0001-01-01 00:00:00"
  def apply(df: DataFrame, tsColumnNames: Seq[String])(implicit inArgs: InputArgs) =
    df.na.fill(tsInit, tsColumnNames).na.replace(tsColumnNames, Map("" -> tsInit))
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.executor

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._
import d2k.common.df.Executor
import d2k.common.InputArgs
import d2k.common.df.executor._

trait DbOutputCommonFunctions extends Executor {
  def invoke(df: DataFrame)(implicit inArgs: InputArgs) = DbOutputCommonFunctions(df)
}

object DbOutputCommonFunctions {
  def apply(df: DataFrame)(implicit inArgs: InputArgs) =
    PqCommonColumnRemover(RowErrorRemover(df))
      .withColumn("VC_DISPOYMD", lit(inArgs.runningDateYMD))
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.executor.face

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

import d2k.common.InputArgs
import d2k.common.df.Executor
import d2k.common.fileConv.DomainProcessor._
import d2k.common.Udfs._

trait DomainConverter extends Executor {
  /**
   *  Set[(カラム名,ドメイン名)]
   */
  val targetColumns: Set[(String, String)]

  def invoke(orgDf: DataFrame)(implicit inArgs: InputArgs): DataFrame =
    targetColumns.foldLeft(orgDf.na.fill("", targetColumns.map(_._1).toSeq)) { (df, t) =>
      val (name, domain) = t
      val convedColumn = domain match {
        case "年月日"      => MakeDate.date_yyyyMMdd(domainConvert(col(name), lit(domain))).cast("date")
        case "年月日時分秒"   => MakeDate.timestamp_yyyyMMddhhmmss(domainConvert(col(name), lit(domain))).cast("timestamp")
        case "年月日時分ミリ秒" => MakeDate.timestamp_yyyyMMddhhmmssSSS(domainConvert(col(name), lit(domain))).cast("timestamp")
        case _          => domainConvert(col(name), lit(domain))
      }
      df.withColumn(name, convedColumn)
    }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.executor

import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame
import d2k.common.InputArgs
import org.apache.spark.sql.functions._
import d2k.common.fileConv.Converter

trait Nothing extends Executor {
  def invoke(df: DataFrame)(implicit inArgs: InputArgs): DataFrame = df
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.executor

import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame
import d2k.common.InputArgs
import org.apache.spark.sql.functions._
import d2k.common.fileConv.Converter

trait PqCommonColumnRemover extends Executor {
  def invoke(df: DataFrame)(implicit inArgs: InputArgs): DataFrame = PqCommonColumnRemover(df)
}

object PqCommonColumnRemover {
  def apply(df: DataFrame)(implicit inArgs: InputArgs) =
    df.drop(Converter.SYSTEM_COLUMN_NAME.ROW_ERROR).drop(Converter.SYSTEM_COLUMN_NAME.ROW_ERROR_MESSAGE)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.executor

import org.apache.spark.sql.DataFrame
import d2k.common.InputArgs
import org.apache.spark.sql.functions._
import d2k.common.df.Executor

trait RowErrorRemover extends Executor {
  def invoke(df: DataFrame)(implicit inArgs: InputArgs): DataFrame = RowErrorRemover(df)
}

object RowErrorRemover {
  def apply(df: DataFrame)(implicit inArgs: InputArgs) = df.filter(col("ROW_ERR") === lit("false"))
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.flow.base

import d2k.common.InputArgs

trait OneInToMapOut[IN, MID, OUT]
    extends OneInToOneOut[IN, Map[String, MID], MID, Map[String, MID], OUT] {
  def preExec(in: IN)(implicit inArgs: InputArgs): Map[String, MID]

  def exec(df: MID)(implicit inArgs: InputArgs): MID

  def postExec(df: Map[String, MID])(implicit inArgs: InputArgs): OUT

  def run(in: IN)(implicit inArgs: InputArgs): OUT
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.flow.base

import d2k.common.InputArgs

trait OneInToOneOut[IN, PREOUT, MID, POSTIN, OUT] {
  def preExec(in: IN)(implicit inArgs: InputArgs): PREOUT

  def exec(df: MID)(implicit inArgs: InputArgs): MID

  def postExec(df: POSTIN)(implicit inArgs: InputArgs): OUT

  def run(in: IN)(implicit inArgs: InputArgs): OUT
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.flow.base

import d2k.common.InputArgs

trait TwoInToOneOut[IN1, IN2, PREOUT, MID, POSTIN, OUT] {
  def preExec(in1: IN1, in2: IN2)(implicit inArgs: InputArgs): PREOUT

  def exec(df: MID)(implicit inArgs: InputArgs): MID

  def postExec(df: POSTIN)(implicit inArgs: InputArgs): OUT

  def run(in1: IN1, in2: IN2)(implicit inArgs: InputArgs): OUT
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.flow

import org.apache.spark.sql.DataFrame
import d2k.common.Logging
import d2k.common.InputArgs
import d2k.common.df.flow.base.OneInToMapOut

trait OneInToMapOutForDf[IN, OUT] extends OneInToMapOut[IN, DataFrame, OUT] with Logging {
  def preExec(in: IN)(implicit inArgs: InputArgs): Map[String, DataFrame]

  def exec(df: DataFrame)(implicit inArgs: InputArgs): DataFrame

  def postExec(df: Map[String, DataFrame])(implicit inArgs: InputArgs): OUT

  final def run(in: IN)(implicit inArgs: InputArgs): OUT = {
    val input = try {
      preExec(in)
    } catch {
      case t: Throwable => platformError(t); throw t
    }

    if (inArgs.isDebug) {
      println(s"${inArgs.applicationId}[input]")
      input.foreach { data => println(data._1); data._2.show(false) }
    }

    val output = try {
      input.mapValues(exec)
    } catch {
      case t: Throwable => appError(t); throw t
    }

    if (inArgs.isDebug) {
      println(s"${inArgs.applicationId}[output]")
      output.foreach { data => println(data._1); data._2.show(false) }
    }

    try {
      postExec(output)
    } catch {
      case t: Throwable => platformError(t); throw t
    }
  }

  final def debug(in: IN)(implicit inArgs: InputArgs): OUT =
    run(in)(inArgs.copy(isDebug = true))
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.flow

import org.apache.spark.sql.DataFrame
import d2k.common.Logging
import d2k.common.InputArgs
import d2k.common.df.flow.base.OneInToOneOut

trait OneInToOneOutForDf[IN, OUT]
    extends OneInToOneOut[IN, DataFrame, DataFrame, DataFrame, OUT] with Logging {
  def preExec(in: IN)(implicit inArgs: InputArgs): DataFrame

  def exec(df: DataFrame)(implicit inArgs: InputArgs): DataFrame

  def postExec(df: DataFrame)(implicit inArgs: InputArgs): OUT

  final def run(in: IN)(implicit inArgs: InputArgs): OUT = {
    val input = try {
      preExec(in)
    } catch {
      case t: Throwable => platformError(t); throw t
    }

    if (inArgs.isDebug) {
      println(s"${inArgs.applicationId}[input]")
      input.show(false)
    }

    val output = try {
      exec(input)
    } catch {
      case t: Throwable => appError(t); throw t
    }

    if (inArgs.isDebug) {
      println(s"${inArgs.applicationId}[output]")
      output.show(false)
    }

    try {
      postExec(output)
    } catch {
      case t: Throwable => platformError(t); throw t
    }
  }

  final def debug(in: IN)(implicit inArgs: InputArgs): OUT =
    run(in)(inArgs.copy(isDebug = true))
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.flow

import org.apache.spark.sql.DataFrame
import d2k.common.Logging
import d2k.common.InputArgs
import d2k.common.df.flow.base.TwoInToOneOut

trait TwoInToOneOutForDf[IN1, IN2, OUT]
    extends TwoInToOneOut[IN1, IN2, DataFrame, DataFrame, DataFrame, OUT] with Logging {
  def preExec(in1: IN1, in2: IN2)(implicit inArgs: InputArgs): DataFrame

  def exec(df: DataFrame)(implicit inArgs: InputArgs): DataFrame

  def postExec(df: DataFrame)(implicit inArgs: InputArgs): OUT

  final def run(in1: IN1, in2: IN2)(implicit inArgs: InputArgs): OUT = {
    val input = try {
      preExec(in1, in2)
    } catch {
      case t: Throwable => platformError(t); throw t
    }

    if (inArgs.isDebug) {
      println(s"${inArgs.applicationId}[input]")
      input.show(false)
    }

    val output = try {
      exec(input)
    } catch {
      case t: Throwable => appError(t); throw t
    }

    if (inArgs.isDebug) {
      println(s"${inArgs.applicationId}[output]")
      output.show(false)
    }

    try {
      postExec(output)
    } catch {
      case t: Throwable => platformError(t); throw t
    }
  }

  final def debug(in1: IN1, in2: IN2)(implicit inArgs: InputArgs): OUT =
    run(in1, in2)(inArgs.copy(isDebug = true))
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import scala.util.Try
import FileInputInfoBase._
import org.apache.spark.sql.Column

sealed trait InputInfo
object FileInputInfoBase {
  val ENV_NAME_DEFAULT = "DEFAULT"

  sealed trait NewLineCode
  case object CR extends NewLineCode
  case object LF extends NewLineCode
  case object CRLF extends NewLineCode

}

case class CsvInfo(
  inputFiles: Set[String], envName: String = FileInputInfoBase.ENV_NAME_DEFAULT, header: Boolean = false, charSet: String = "MS932",
  itemConfId: String = "", dropRowError: Boolean = true) extends VariableInputInfoBase {
  val fileFormat = "csv"
}

case class TsvInfo(
  inputFiles: Set[String], envName: String = FileInputInfoBase.ENV_NAME_DEFAULT, dropDoubleQuoteMode: Boolean = false, header: Boolean = false, charSet: String = "MS932",
  itemConfId: String = "", dropRowError: Boolean = true) extends VariableInputInfoBase {
  val fileFormat = if (dropDoubleQuoteMode) "tsvDropDoubleQuote" else "tsv"
}

case class VsvInfo(
  inputFiles: Set[String], envName: String = FileInputInfoBase.ENV_NAME_DEFAULT, header: Boolean = false, charSet: String = "MS932",
  itemConfId: String = "", dropRowError: Boolean = true) extends VariableInputInfoBase {
  val fileFormat = "vsv"
}

case class SsvInfo(
  inputFiles: Set[String], envName: String = FileInputInfoBase.ENV_NAME_DEFAULT, header: Boolean = false, charSet: String = "MS932",
  itemConfId: String = "", dropRowError: Boolean = true) extends VariableInputInfoBase {
  val fileFormat = "ssv"
}

case class FixedInfo(inputFiles: Set[String], envName: String = FileInputInfoBase.ENV_NAME_DEFAULT,
                     header: Boolean = false, footer: Boolean = false, newLine: Boolean = true,
                     withIndex: Boolean = false, recordLengthCheck: Boolean = false, charSet: String = "MS932", newLineCode: NewLineCode = LF,
                     itemConfId: String = "", dropRowError: Boolean = true, preFilter: (Seq[String], Map[String, String] => Boolean) = null,
                     withBinaryRecord: String = "") extends FileInputInfoBase {
  val fileFormat = "fixed"
}

trait PqInputInfoBase extends InputInfo {
  val pqName: String
  val envName: String
  def inputDir(componentId: String): String = Try { sys.env(s"PQ_INPUT_PATH_${componentId}") }.
    getOrElse(Try { sys.env(s"PQ_INPUT_PATH_${envName}") }.getOrElse(sys.env(s"PQ_INPUT_PATH_${ENV_NAME_DEFAULT}")))
}
case class PqInfo(pqName: String, envName: String = "") extends PqInputInfoBase
case class VariableJoin(inputInfo: InputInfo, joinExprs: Column, prefixName: String = "", dropCols: Set[String] = Set.empty[String])

trait FileInputInfoBase extends InputInfo {

  val fileFormat: String

  val envName: String
  def inputDir(componentId: String): String = Try { sys.env(s"FILE_INPUT_PATH_${componentId}") }.
    getOrElse(Try { sys.env(s"FILE_INPUT_PATH_${envName}") }.getOrElse(sys.env(s"FILE_INPUT_PATH_${ENV_NAME_DEFAULT}")))
  val inputFiles: Set[String]

  val newLine: Boolean
  val withIndex: Boolean
  val recordLengthCheck: Boolean
  val header: Boolean
  val charSet: String
  /** for DfJoinVariableToDf */
  val itemConfId: String
  /** for DfJoinVariableToDf */
  val dropRowError: Boolean
}

trait VariableInputInfoBase extends FileInputInfoBase {
  override val newLine = true
  override val withIndex = false
  override val recordLengthCheck = false
}

', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.mixIn

import d2k.common.InputArgs
import d2k.common.df.WriteFile
import d2k.common.df.WriteFileMode.Csv

trait OraLoader extends WriteFile {
  override val writeFileVariableWrapDoubleQuote = true
  override val writeFileVariableEscapeChar = "\""
  override val writeFileMode = Csv
  override def writeFilePath(implicit inArgs: InputArgs) = sys.env("DB_LOADING_FILE_PATH")
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.mixIn

import d2k.common.InputArgs
import d2k.common.df.WriteFile
import d2k.common.df.WriteFileMode._

trait OraLoaderHdfs extends WriteFile {
  override val writeFileVariableWrapDoubleQuote = true
  override val writeFileVariableEscapeChar = "\""
  override val writeFileMode = hdfs.Csv
  override def writeFilePath(implicit inArgs: InputArgs) = sys.env("DB_LOADING_FILE_PATH")
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import spark.common.DbInfo
import d2k.common.InputArgs
import spark.common.DbCtl
import d2k.common.ResourceInfo

trait MultiReadDb extends ReadDb {
  val readTableNames: Seq[String]

  def readDb(implicit inArgs: InputArgs) = readTableNames.map(tblnm => (tblnm, readDbSingle(tblnm))).toMap
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import spark.common.DbInfo
import spark.common.PqCtl
import d2k.common.InputArgs
import d2k.common.ResourceInfo

trait MultiReadPq extends ReadPq {
  val readPqNames: Seq[String]

  def readParquet(implicit inArgs: InputArgs) =
    readPqNames.map(pqName => (pqName, readParquetSingle(pqName))).toMap
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import spark.common.DbInfo
import d2k.common.InputArgs
import spark.common.DbCtl
import d2k.common.ResourceInfo

object DbConnectionInfo {
  def mkDbInfo(envLabel: String) =
    DbInfo(sys.env(s"DB_URL_$envLabel"), sys.env(s"DB_USER_$envLabel"), sys.env(s"DB_PASSWORD_$envLabel"))
  lazy val dm1 = mkDbInfo("DM1")
  lazy val dwh1 = mkDbInfo("DWH1")
  lazy val dwh2 = mkDbInfo("DWH2")
  lazy val bat1 = mkDbInfo("BAT1")
  lazy val csp1 = mkDbInfo("CSP1")
  lazy val hi1 = mkDbInfo("HI1")
  lazy val mth1 = mkDbInfo("MTH1")
  lazy val fak1 = mkDbInfo("FAK1")
}

trait ReadDb extends ResourceInfo {
  /**
   * 読込Column選択
   */
  val columns: Array[String] = Array.empty[String]

  /**
   * DB読込時条件
   */
  val readDbWhere: Array[String] = Array.empty[String]
  def readDbWhere(inArgs: InputArgs): Array[String] = Array.empty[String]

  /**
   * DB情報の設定
   */
  val readDbInfo: DbInfo = DbConnectionInfo.bat1

  def readDbSingle(tableName: String)(implicit inArgs: InputArgs) = {
    val tblName = inArgs.tableNameMapper.get(componentId).getOrElse(tableName)
    val dbCtl = new DbCtl(readDbInfo)
    val readDbWhereWithArgs = readDbWhere(inArgs)
    (readDbWhere.isEmpty, readDbWhereWithArgs.isEmpty) match {
      case (true, true)   => selectReadTable(dbCtl, tblName)
      case (false, true)  => selectReadTable(dbCtl, tblName, readDbWhere)
      case (true, false)  => selectReadTable(dbCtl, tblName, readDbWhereWithArgs)
      case (false, false) => throw new IllegalArgumentException("Can not defined both readDbWhere and readDbWhere(inArgs)")
    }
  }

  def selectReadTable(dbCtl: DbCtl, tableName: String, readDbWhere: Array[String] = Array.empty[String]) = {
    (columns.isEmpty, readDbWhere.isEmpty) match {
      case (true, true)   => dbCtl.readTable(tableName)
      case (true, false)  => dbCtl.readTable(tableName, readDbWhere)
      case (false, true)  => dbCtl.readTable(tableName, columns, Array("1 = 1"))
      case (false, false) => dbCtl.readTable(tableName, columns, readDbWhere)
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import d2k.common.InputArgs
import d2k.common.ResourceInfo
import d2k.common.fileConv.FileConv
import scala.util.Try

trait ReadFile extends ResourceInfo {
  val fileInputInfo: FileInputInfoBase
  lazy val itemConfId: String = componentId
  def readFile(implicit inArgs: InputArgs) = {
    new FileConv(componentId, fileInputInfo, itemConfId).makeDf
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import spark.common.DbInfo
import spark.common.PqCtl
import d2k.common.InputArgs
import d2k.common.ResourceInfo

object ReadPq {
  def toSchema(names: Seq[String]) = names.map { name =>
    name.split("_").toList.headOption.map {
      case "DT"        => "date"
      case "NM" | "AM" => "decimal"
      case _           => "string"
    }
  }
}

trait ReadPq extends ResourceInfo {
  def readPqPath(implicit inArgs: InputArgs): String = inArgs.baseInputFilePath
  /**
   * ファイルが存在しなかった場合の挙動を決定する
   * true: Exceptionが発生(default)
   * false: ログにWarningが出力され空のDataFrameが返る
   */
  val readPqStrictCheckMode: Boolean = true

  /**
   * readPqStrictCheckModeでfalseが指定された場合で、ParquetがNotFoundだった場合のschemaを指定する
   */
  val readPqEmptySchema: Seq[(String, String)] = Seq.empty[(String, String)]

  def readParquetSingle(pqName: String)(implicit inArgs: InputArgs) = {
    val pqCtl = new PqCtl(readPqPath)
    pqCtl.readParquet(pqName, readPqStrictCheckMode, readPqEmptySchema)
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import spark.common.DbInfo
import d2k.common.InputArgs
import spark.common.DbCtl
import d2k.common.ResourceInfo

trait SingleReadDb extends ReadDb {
  lazy val readTableName: String = componentId

  def readDb(implicit inArgs: InputArgs) = readDbSingle(readTableName)
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import spark.common.DbInfo
import spark.common.PqCtl
import d2k.common.InputArgs
import d2k.common.ResourceInfo

trait SingleReadPq extends ReadPq {
  lazy val readPqName: String = componentId

  def readParquet(implicit inArgs: InputArgs) = readParquetSingle(readPqName)
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import d2k.common.df.flow.OneInToOneOutForDf
import d2k.common.InputArgs
import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame
import d2k.common.df.WriteDb

trait AnyToDb[IN] extends OneInToOneOutForDf[IN, DataFrame] with WriteDb {
  def postExec(df: DataFrame)(implicit inArgs: InputArgs) = writeDb(df)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import d2k.common.df.flow.OneInToOneOutForDf
import d2k.common.InputArgs
import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame

trait AnyToDf[IN] extends OneInToOneOutForDf[IN, DataFrame] {
  def postExec(df: DataFrame)(implicit inArgs: InputArgs) = df
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import d2k.common.df.flow.OneInToOneOutForDf
import d2k.common.InputArgs
import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame
import d2k.common.df.WriteFile

trait AnyToFile[IN] extends OneInToOneOutForDf[IN, DataFrame] with WriteFile {
  def postExec(df: DataFrame)(implicit inArgs: InputArgs) = writeFile(df)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import d2k.common.df.flow.OneInToOneOutForDf
import d2k.common.InputArgs
import d2k.common.df.Executor
import d2k.common.df.WritePq
import d2k.common.df.WriteDb
import org.apache.spark.sql.DataFrame
import spark.common.PqCtl

trait AnyToPq_Db[IN] extends OneInToOneOutForDf[IN, DataFrame] with WritePq with WriteDb {
  override lazy val writePqName = writeTableName

  def postExec(df: DataFrame)(implicit inArgs: InputArgs) = {
    writeParquet(df)
    val pqCtl = new PqCtl(writePqPath)
    writeDb(pqCtl.readParquet(writePqName))
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import d2k.common.df.flow.OneInToOneOutForDf
import d2k.common.InputArgs
import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame
import d2k.common.df.WritePq

trait AnyToPq[IN] extends OneInToOneOutForDf[IN, DataFrame] with WritePq {
  def postExec(df: DataFrame)(implicit inArgs: InputArgs) = writeParquet(df)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import d2k.common.df.flow.OneInToOneOutForDf
import d2k.common.InputArgs
import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.Row

trait AnyToVal[IN, T] extends OneInToOneOutForDf[IN, T] {
  def outputValue(rows: Array[Row]): T
  def postExec(df: DataFrame)(implicit inArgs: InputArgs): T = outputValue(df.collect)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import d2k.common.df.flow.OneInToOneOutForDf
import d2k.common.df.Executor
import d2k.common.InputArgs
import org.apache.spark.sql.DataFrame
import d2k.common.df.SingleReadDb

trait DbToAny[OUT] extends OneInToOneOutForDf[Unit, OUT] with SingleReadDb {
  def preExec(in: Unit)(implicit inArgs: InputArgs): DataFrame = readDb
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.Column

import spark.common.PqCtl
import d2k.common.InputArgs
import d2k.common.df.flow.OneInToOneOutForDf
import scala.util.Try

case class JoinPqInfo(name: String, joinExprs: Column, dropCols: Set[String] = Set.empty[String], prefixName: String = "", joinType: String = "left_outer")

trait DfJoinMultiPqToAny[OUT] extends OneInToOneOutForDf[DataFrame, OUT] {
  val prefixName: String
  val joinPqInfoList: Seq[JoinPqInfo]
  lazy val envName: String = ""
  private[this] val inputFilePath = Try { sys.env(s"PQ_INPUT_PATH_${envName}") }

  def preExec(left: DataFrame)(implicit inArgs: InputArgs): DataFrame = {
    val inFilePath = inputFilePath.getOrElse(inArgs.baseInputFilePath)
    val orgDf = left.columns.foldLeft(left)((df, name) => df.withColumnRenamed(name, s"$prefixName#$name"))
    joinPqInfoList.foldLeft(orgDf) { (odf, pqInfo) =>
      val pqDf = new PqCtl(inFilePath).readParquet(pqInfo.name)
      val pname = if (pqInfo.prefixName.isEmpty) pqInfo.name else pqInfo.prefixName
      val addNameDf = pqDf.columns.foldLeft(pqDf) { (df, name) =>
        df.withColumnRenamed(name, s"$pname#$name")
      }
      val joinedDf = odf.join(addNameDf, pqInfo.joinExprs, pqInfo.joinType)
      pqInfo.dropCols.foldLeft(joinedDf)((l, r) => l.drop(r))
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import scala.util.Try

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.Column

import spark.common.PqCtl
import d2k.common.InputArgs
import d2k.common.df.flow.OneInToOneOutForDf
import d2k.common.df.ReadFile
import d2k.common.fileConv.FileConv
import d2k.common.ResourceInfo
import d2k.common.df.InputInfo
import d2k.common.df.FileInputInfoBase
import d2k.common.df._

trait DfJoinVariableToAny[OUT] extends OneInToOneOutForDf[DataFrame, OUT] with ResourceInfo {
  val prefixName: String
  val joins: Seq[VariableJoin]

  def preExec(left: DataFrame)(implicit inArgs: InputArgs): DataFrame = {
    val orgDf = left.columns.foldLeft(left)((df, name) => df.withColumnRenamed(name, s"$prefixName#$name"))
    joins.foldLeft(orgDf) { (odf, vj) =>
      val (joinDf, uniqId) = vj.inputInfo match {
        case x: PqInputInfoBase => (new PqCtl(x.inputDir(componentId)).readParquet(x.pqName), x.pqName)
        case x: FileInputInfoBase => {
          val fileDf = new FileConv(componentId, x, x.envName, true).makeDf
          val droppedRowErr = if (x.dropRowError) fileDf.drop("ROW_ERR").drop("ROW_ERR_MESSAGE") else fileDf
          (droppedRowErr, x.itemConfId)
        }
      }
      val joinedPrefixName = if (vj.prefixName.isEmpty) uniqId else vj.prefixName
      val addNameDf = joinDf.columns.foldLeft(joinDf) { (df, name) =>
        df.withColumnRenamed(name, s"${joinedPrefixName}#${name}")
      }
      val joinedDf = odf.join(addNameDf, vj.joinExprs, "left_outer")
      vj.dropCols.foldLeft(joinedDf)((l, r) => l.drop(r))
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import d2k.common.df.flow.OneInToOneOutForDf
import d2k.common.df.Executor
import d2k.common.InputArgs
import org.apache.spark.sql.DataFrame

trait DfToAny[OUT] extends OneInToOneOutForDf[DataFrame, OUT] {
  def preExec(in: DataFrame)(implicit inArgs: InputArgs): DataFrame = in
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import d2k.common.df.flow.OneInToOneOutForDf
import d2k.common.df.ReadFile
import d2k.common.df.Executor
import d2k.common.InputArgs
import org.apache.spark.sql.DataFrame

trait FileToAny[OUT] extends OneInToOneOutForDf[Unit, OUT] with ReadFile {
  def preExec(in: Unit)(implicit inArgs: InputArgs): DataFrame = readFile
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import d2k.common.df.flow.OneInToMapOutForDf
import d2k.common.InputArgs
import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.Row

trait MultiAnyToMapDf[IN] extends OneInToMapOutForDf[IN, Map[String, DataFrame]] {
  def postExec(df: Map[String, DataFrame])(implicit inArgs: InputArgs): Map[String, DataFrame] = df
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import d2k.common.df.flow.OneInToMapOutForDf
import d2k.common.df.Executor
import d2k.common.InputArgs
import org.apache.spark.sql.DataFrame
import d2k.common.df.MultiReadDb

trait MultiDbToMultiAny[OUT] extends OneInToMapOutForDf[Unit, OUT] with MultiReadDb {
  def preExec(in: Unit)(implicit inArgs: InputArgs): Map[String, DataFrame] = readDb
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import d2k.common.df.flow.OneInToMapOutForDf
import d2k.common.df.Executor
import d2k.common.InputArgs
import org.apache.spark.sql.DataFrame
import d2k.common.df.MultiReadPq

trait MultiPqToMultiAny[OUT] extends OneInToMapOutForDf[Unit, OUT] with MultiReadPq {
  def preExec(in: Unit)(implicit inArgs: InputArgs): Map[String, DataFrame] = readParquet
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import d2k.common.df.flow.OneInToOneOutForDf
import d2k.common.df.Executor
import d2k.common.InputArgs
import org.apache.spark.sql.DataFrame
import d2k.common.df.SingleReadPq

trait PqToAny[OUT] extends OneInToOneOutForDf[Unit, OUT] with SingleReadPq {
  def preExec(in: Unit)(implicit inArgs: InputArgs): DataFrame = readParquet
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import d2k.common.df.flow.TwoInToOneOutForDf
import d2k.common.InputArgs
import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame
import d2k.common.df.WriteDb

trait TwoAnyToDb[IN1, IN2] extends TwoInToOneOutForDf[IN1, IN2, DataFrame] with WriteDb {
  def postExec(df: DataFrame)(implicit inArgs: InputArgs) = writeDb(df)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import d2k.common.df.flow.TwoInToOneOutForDf
import d2k.common.InputArgs
import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame

trait TwoAnyToDf[IN1, IN2] extends TwoInToOneOutForDf[IN1, IN2, DataFrame] {
  def postExec(df: DataFrame)(implicit inArgs: InputArgs) = df
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import d2k.common.df.flow.TwoInToOneOutForDf
import d2k.common.InputArgs
import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame
import d2k.common.df.WritePq

trait TwoAnyToPq[IN1, IN2] extends TwoInToOneOutForDf[IN1, IN2, DataFrame] with WritePq {
  def postExec(df: DataFrame)(implicit inArgs: InputArgs) = writeParquet(df)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.Column
import org.apache.spark.sql.functions._

import d2k.common.InputArgs
import d2k.common.df.flow.TwoInToOneOutForDf

trait TwoDfJoinToAny[OUT] extends TwoInToOneOutForDf[DataFrame, DataFrame, OUT] {
  val joinType = "left_outer"
  def joinExprs(left: DataFrame, right: DataFrame): Column
  def select(left: DataFrame, right: DataFrame): Seq[Column]

  protected[this] def addColumnPrefix(name: String) = (df: DataFrame) => {
    df.schema.map(x => df(x.name) as s"${name}_${x.name}")
  }

  protected[this] def dropDuplicate(left: DataFrame, right: DataFrame) = {
    val lNames = left.schema.map(_.name).toSet
    val rNames = right.schema.map(_.name).toSet
    val diff = (rNames -- lNames).toSeq.map(col)
    right.select(diff: _*)("*")
  }

  protected[TwoDfJoinToAny] def mergeDropDuplicate(left: DataFrame, right: DataFrame) =
    Seq(left("*"), dropDuplicate(left, right))

  protected[TwoDfJoinToAny] def mergeWithPrefix(left: DataFrame, right: DataFrame, name: String) =
    left("*") +: addColumnPrefix(name)(right)

  def preExec(left: DataFrame, right: DataFrame)(implicit inArgs: InputArgs): DataFrame = {
    val joined = left.join(right, joinExprs(left, right), joinType)
    joined.select(select(left, right).toArray: _*)
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.Column
import d2k.common.InputArgs
import d2k.common.df.flow.TwoInToOneOutForDf

trait TwoDfUnionToAny[OUT] extends TwoInToOneOutForDf[DataFrame, DataFrame, OUT] {
  def preExec(left: DataFrame, right: DataFrame)(implicit inArgs: InputArgs): DataFrame =
    left.union(right)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.Column
import d2k.common.InputArgs
import d2k.common.df.flow.TwoInToOneOutForDf
import d2k.common.df.SingleReadPq

trait TwoPqJoinToAny[OUT] extends TwoInToOneOutForDf[Unit, Unit, OUT] with SingleReadPq {
  val leftPqName: String
  val rightPqName: String
  val joinType = "left_outer"
  def joinExprs(left: DataFrame, right: DataFrame): Column
  def select(left: DataFrame, right: DataFrame): Seq[Column]

  final def preExec(in1: Unit, in2: Unit)(implicit inArgs: InputArgs): DataFrame = {
    val left = readParquetSingle(leftPqName)
    val right = readParquetSingle(rightPqName)
    val joined = left.join(right, joinExprs(left, right), joinType)
    joined.select(select(left, right).toArray: _*)
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame
import d2k.common.df.template.base._
import d2k.common.InputArgs

trait DbToDb extends DbToAny[DataFrame] with AnyToDb[Unit] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait DbToDf extends DbToAny[DataFrame] with AnyToDf[Unit] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait DbToFile extends DbToAny[DataFrame] with AnyToFile[Unit] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait DbToPq extends DbToAny[DataFrame] with AnyToPq[Unit] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait DbToVal[T] extends DbToAny[T] with AnyToVal[Unit, T] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import org.apache.spark.sql.DataFrame
import d2k.common.df.Executor
import d2k.common.InputArgs
import d2k.common.df.template.base.DfJoinMultiPqToAny
import d2k.common.df.template.base.AnyToDf

trait DfJoinPqToDf extends DfJoinMultiPqToAny[DataFrame] with AnyToDf[DataFrame] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import org.apache.spark.sql.DataFrame
import d2k.common.df.Executor
import d2k.common.InputArgs
import d2k.common.df.template.base.TwoDfJoinToAny
import d2k.common.df.template.base.TwoAnyToDf

trait DfJoinToDf extends TwoDfJoinToAny[DataFrame] with TwoAnyToDf[DataFrame, DataFrame] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import org.apache.spark.sql.DataFrame
import d2k.common.df.Executor
import d2k.common.InputArgs
import d2k.common.df.template.base.AnyToDf
import d2k.common.df.template.base.DfJoinVariableToAny

trait DfJoinVariableToDf extends DfJoinVariableToAny[DataFrame] with AnyToDf[DataFrame] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame
import d2k.common.df.template.base._
import d2k.common.InputArgs

trait DfToDb extends DfToAny[DataFrame] with AnyToDb[DataFrame] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait DfToDf extends DfToAny[DataFrame] with AnyToDf[DataFrame] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait DfToFile extends DfToAny[DataFrame] with AnyToFile[DataFrame] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait DfToPq extends DfToAny[DataFrame] with AnyToPq[DataFrame] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait DfToVal[T] extends DfToAny[T] with AnyToVal[DataFrame, T] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import org.apache.spark.sql.DataFrame
import d2k.common.df.Executor
import d2k.common.InputArgs
import d2k.common.df.template.base.TwoAnyToDf
import d2k.common.df.template.base.TwoDfUnionToAny

trait DfUnionToDf extends TwoDfUnionToAny[DataFrame] with TwoAnyToDf[DataFrame, DataFrame] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame
import d2k.common.df.template.base._
import d2k.common.InputArgs

trait FileToDb extends FileToAny[DataFrame] with AnyToDb[Unit] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait FileToDf extends FileToAny[DataFrame] with AnyToDf[Unit] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait FileToFile extends FileToAny[DataFrame] with AnyToFile[Unit] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait FileToPq_Db extends FileToAny[DataFrame] with AnyToPq_Db[Unit] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait FileToPq extends FileToAny[DataFrame] with AnyToPq[Unit] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait FileToVal[T] extends FileToAny[T] with AnyToVal[Unit, T] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import d2k.common.InputArgs
import org.apache.spark.sql.DataFrame
import d2k.common.df.template.base.MultiDbToMultiAny
import d2k.common.df.template.base.MultiAnyToMapDf
import d2k.common.df.Executor

trait MultiDbToMapDf extends MultiDbToMultiAny[Map[String, DataFrame]] with MultiAnyToMapDf[Unit] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import d2k.common.InputArgs
import org.apache.spark.sql.DataFrame
import d2k.common.df.template.base.MultiPqToMultiAny
import d2k.common.df.template.base.MultiAnyToMapDf
import d2k.common.df.Executor

trait MultiPqToMapDf extends MultiPqToMultiAny[Map[String, DataFrame]] with MultiAnyToMapDf[Unit] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import org.apache.spark.sql.DataFrame
import d2k.common.df.Executor
import d2k.common.InputArgs
import d2k.common.df.template.base.TwoPqJoinToAny
import d2k.common.df.template.base.TwoAnyToPq

trait PqJoinToPq extends TwoPqJoinToAny[DataFrame] with TwoAnyToPq[Unit, Unit] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = PqJoinToPq.this.invoke(df)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import org.apache.spark.sql.DataFrame
import d2k.common.df.Executor
import d2k.common.df.template.base._
import d2k.common.InputArgs

trait PqToDb extends PqToAny[DataFrame] with AnyToDb[Unit] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait PqToDf extends PqToAny[DataFrame] with AnyToDf[Unit] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait PqToFile extends PqToAny[DataFrame] with AnyToFile[Unit] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait PqToPq extends PqToAny[DataFrame] with AnyToPq[Unit] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}

trait PqToVal[T] extends PqToAny[T] with AnyToVal[Unit, T] { self: Executor =>
  def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.sh

import d2k.common.InputArgs
import d2k.common.SparkApp
import d2k.common.df._
import d2k.common.df.executor._
import d2k.common.df.template._
import d2k.common.df.flow.OneInToOneOutForDf
import d2k.common.df.template.base.DfToAny
import d2k.common.df.template.base.AnyToDf

import spark.common.SparkContexts
import spark.common.DfCtl._
import spark.common.DfCtl.implicits._

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.LongType

import SparkContexts.context.implicits._

object CommissionBaseChannelSelectorTmpl {
  val UniqueKey = "_uniqKey_"
}
case class CommissionBaseChannelSelectorInfo(DV_DISPODIV: String, DV_OUTOBJDIV: String, DV_TRICALCOBJDIV: String)
trait CommissionBaseChannelSelectorTmpl extends DfToAny[DataFrame] with AnyToDf[DataFrame] {
  import CommissionBaseChannelSelectorTmpl._

  val info: CommissionBaseChannelSelectorInfo
  val groupingKeys: Seq[String] = Seq(UniqueKey)

  def exec(df: DataFrame)(implicit inArgs: InputArgs) =
    if (groupingKeys.contains(UniqueKey)) {
      (df ~> c03_DfToDf.run, broadcast(c01_DbToDf(info).run(Unit))) ~> c02_DfJoinToDf.run ~> c04_DfToDf.run
    } else {
      (df, broadcast(c01_DbToDf(info).run(Unit))) ~> c02_DfJoinToDf.run
    }

  private[this] def c01_DbToDf(info: CommissionBaseChannelSelectorInfo) = new DbToDf with Executor {
    val componentId = "MAA300"
    override val columns = Array("DV_DISCRDIV", "CD_CHNLCD", "DV_OUTOBJDIV", "DV_TRICALCOBJDIV")
    override val readDbWhere = Array(s"DV_DISPODIV = '${info.DV_DISPODIV}'")

    def invoke(df: DataFrame)(implicit inArgs: InputArgs) =
      df ~> f01

    def f01(implicit inArgs: InputArgs) = (_: DataFrame).na.fill(" ")
  }

  private[this] val c02_DfJoinToDf = new DfJoinToDf with Executor {
    val componentId = "MAA300"

    override def joinExprs(left: DataFrame, right: DataFrame) =
      ((right("DV_DISCRDIV") === "01") and (left("CD_CHNLGRPCD") === right("CD_CHNLCD"))) or
        ((right("DV_DISCRDIV") === "02") and (left("CD_CHNLDETAILCD") === right("CD_CHNLCD")))

    override def select(left: DataFrame, right: DataFrame) =
      mergeWithPrefix(left, right, componentId)

    def invoke(df: DataFrame)(implicit inArgs: InputArgs) =
      df ~> f01 ~> f02 ~> f03 ~> f04

    def f01 = editColumns(Seq(
      ("DV_DISCRDIV", coalesce($"MAA300_DV_DISCRDIV", lit("00"))),
      ("DV_OUTOBJDIV", coalesce($"MAA300_DV_OUTOBJDIV", lit(info.DV_OUTOBJDIV))),
      ("DV_TRICALCOBJDIV", coalesce($"MAA300_DV_TRICALCOBJDIV", lit(info.DV_TRICALCOBJDIV)))).e)

    def f02 = selectMaxValue(groupingKeys, Seq($"DV_DISCRDIV".desc))

    def f03 = dropColumnPrefix(componentId)

    def f04 = editColumns(Seq("DV_DISCRDIV").d)
  }

  //add Uniq Key
  private[this] def c03_DfToDf = new DfToDf with Executor {
    def invoke(df: DataFrame)(implicit inArgs: InputArgs) =
      df ~> f01

    def f01 = (df: DataFrame) => {
      val newRdd = df.rdd.zipWithUniqueId.map {
        case (row, idx) => Row.fromSeq(row.toSeq :+ idx)
      }
      SparkContexts.context.createDataFrame(newRdd, df.schema.add(UniqueKey, LongType))
    }
  }

  //drop Uniq Key
  private[this] def c04_DfToDf = new DfToDf with Executor {
    def invoke(df: DataFrame)(implicit inArgs: InputArgs) =
      df ~> f01

    def f01 = editColumns(Seq(UniqueKey).d)
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import org.apache.spark.sql.SaveMode
import spark.common.DbInfo
import org.apache.spark.sql.DataFrame
import d2k.common.InputArgs
import spark.common.DbCtl
import org.apache.spark.sql.functions._
import d2k.common.ResourceInfo

sealed trait WriteDbMode
object WriteDbMode {
  /**
   * Insert用 writeDbSaveMode(Default:SaveMode.Append)とセットで利用する
   */
  case object Insert extends WriteDbMode

  /**
   * Insert(高速版)用 writeDbSaveMode(Default:SaveMode.Append)とセットで利用する
   */
  case object InsertAcc extends WriteDbMode

  /**
   * Insert not exist用 writeDbSaveMode(Default:SaveMode.Append)とセットで利用する
   */
  case class InsertNotExists(keyNames: String*) extends WriteDbMode

  /**
   * Update用 writeDbPrimaryKeysとセットで利用する
   */
  case object Update extends WriteDbMode
  /**
   * Upsert用 writeDbPrimaryKeysとセットで利用する
   */
  case object Upsert extends WriteDbMode
  /**
   * 論理削除用 writeDbPrimaryKeysとセットで利用する<br>
   * FG_D2KDELFLGへ"1"を設定しUpdateを行う
   */
  case object DeleteLogical extends WriteDbMode
  /**
   * 物理削除用 writeDbPrimaryKeysとセットで利用する
   */
  case object DeletePhysical extends WriteDbMode
}

trait WriteDb extends ResourceInfo {
  import WriteDbMode._
  lazy val writeTableName: String = componentId

  /**
   * DB書込みモード<br>
   * Insert, InsertAcc, Update, Upsert, DeletePhisical, DeleteLogical
   */
  val writeDbMode: WriteDbMode = Insert
  val writeDbSaveMode: SaveMode = SaveMode.Append
  val writeDbUpdateKeys: Set[String] = Set.empty[String]
  val writeDbUpdateIgnoreColumns: Set[String] = Set.empty[String]
  val writeDbHint: String = ""

  /**
   * 共通項目を付加するか
   * ture(default):付加する
   * false:付加しない
   */
  val writeDbWithCommonColumn: Boolean = true

  /**
   * DB情報の設定
   */
  val writeDbInfo: DbInfo = DbCtl.dbInfo1

  /**
   *  NA(空文字若しくはnull)の項目を" "space1文字へ置き換える
   *  true:置き換える false:置き換えない　defaultはfalse(置き換えない)
   */
  val writeDbConvNaMode: Boolean = false

  def writeDb(dforg: DataFrame)(implicit inArgs: InputArgs) = {
    val df = convNa(dforg)
    val tblName = inArgs.tableNameMapper.get(componentId).getOrElse(writeTableName)
    val dbCtl = new DbCtl(writeDbInfo)
    def modUpdateColumn(df: DataFrame) =
      df.withColumn("dt_d2kupddttm", lit(inArgs.sysSQLDate))
        .withColumn("id_d2kupdusr", lit(componentId))
    def checkKeys = if (writeDbUpdateKeys.isEmpty) throw new IllegalArgumentException("writeDbUpdateKeys is empty")

    (writeDbMode, writeDbWithCommonColumn) match {
      case (Insert, true) => {
        dbCtl.insertAccelerated(DbCommonColumnAppender(df, componentId), tblName, writeDbSaveMode, writeDbHint)
      }
      case (Insert, false) => {
        dbCtl.insertAccelerated(df, tblName, writeDbSaveMode, writeDbHint)
      }

      case (InsertAcc, true) => {
        dbCtl.insertAccelerated(DbCommonColumnAppender(df, componentId), tblName, writeDbSaveMode, writeDbHint)
      }

      case (InsertAcc, false) => {
        dbCtl.insertAccelerated(df, tblName, writeDbSaveMode, writeDbHint)
      }

      case (InsertNotExists(keys @ _*), true) => {
        dbCtl.insertNotExists(DbCommonColumnAppender(df, componentId), tblName, keys, writeDbSaveMode, writeDbHint)
      }
      case (InsertNotExists(keys @ _*), false) => {
        dbCtl.insertNotExists(df, tblName, keys, writeDbSaveMode, writeDbHint)
      }

      case (Update, true) => {
        checkKeys
        dbCtl.updateRecords(modUpdateColumn(df), tblName, writeDbUpdateKeys, writeDbUpdateIgnoreColumns, writeDbHint)
      }
      case (Update, false) => {
        checkKeys
        dbCtl.updateRecords(df, tblName, writeDbUpdateKeys, writeDbUpdateIgnoreColumns, writeDbHint)
      }
      case (Upsert, true) => {
        checkKeys
        dbCtl.upsertRecords(DbCommonColumnAppender(df, componentId), tblName,
          writeDbUpdateKeys, Set("dt_d2kmkdttm", "id_d2kmkusr", "nm_d2kupdtms", "fg_d2kdelflg") ++ writeDbUpdateIgnoreColumns, writeDbHint)
      }
      case (Upsert, false) => {
        checkKeys
        dbCtl.upsertRecords(df, tblName, writeDbUpdateKeys, writeDbUpdateIgnoreColumns, writeDbHint)
      }
      case (DeleteLogical, true) => {
        checkKeys
        val deleteFlagName = "fg_d2kdelflg"
        val deleteTarget = df.withColumn(deleteFlagName, lit("1")).select(deleteFlagName, writeDbUpdateKeys.toSeq: _*)
        dbCtl.updateRecords(
          modUpdateColumn(deleteTarget),
          tblName, writeDbUpdateKeys, Set("dt_d2kmkdttm", "id_d2kmkusr", "nm_d2kupdtms"), writeDbHint)
      }
      case (DeleteLogical, false) => throw new IllegalArgumentException("DeleteLogical and writeDbWithCommonColumn == false can not used be togather")
      case (DeletePhysical, _) => {
        checkKeys
        dbCtl.deleteRecords(df, tblName, writeDbUpdateKeys, writeDbHint)
      }
    }
    dforg.sqlContext.emptyDataFrame
  }

  def convNa(df: DataFrame) = if (writeDbConvNaMode) {
    df.na.fill(" ").na.replace(df.columns, Map("" -> " "))
  } else {
    df
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import org.apache.spark.sql.DataFrame
import d2k.common.InputArgs
import d2k.common.file.output.VariableFile
import d2k.common.ResourceInfo
import d2k.common.file.output._

sealed trait WriteFileMode

object WriteFileMode {
  case object Csv extends WriteFileMode
  case class Csv(wrapTargetCols: String*) extends WriteFileMode
  case object Tsv extends WriteFileMode

  case object Fixed extends WriteFileMode
  case class Fixed(itemLengths: Int*) extends WriteFileMode

  object partition {
    case object Csv extends WriteFileMode
    case class Csv(wrapTargetCols: String*) extends WriteFileMode
    case object Tsv extends WriteFileMode
    case object Fixed extends WriteFileMode
    case class Fixed(itemLengths: Int*) extends WriteFileMode
  }

  object hdfs {
    case object Csv extends WriteFileMode
    case object Tsv extends WriteFileMode
    case object Fixed extends WriteFileMode
    case class Fixed(itemLengths: Int*) extends WriteFileMode
  }

  object sequence {
    case object Csv extends WriteFileMode
    case class Csv(wrapTargetCols: String*) extends WriteFileMode
    case object Tsv extends WriteFileMode
    case class Fixed(itemLengths: Int*) extends WriteFileMode
  }
}

trait WriteFile extends ResourceInfo {
  import WriteFileMode._

  lazy val writeFileName: String = componentId
  def writeFilePath(implicit inArgs: InputArgs): String = inArgs.baseOutputFilePath

  /**
   * File書込みモード ※暫定<br>
   * Fixed(default), Fixed(itemLengths), CsvSingle, CsvSingle(wrapTargetCols), TsvSingle
   */
  val writeFileMode: WriteFileMode = Fixed

  /**
   * 可変長出力の場合に項目をDouble Quoteで包むか
   * true:包む
   * false:包まない
   */
  val writeFileVariableWrapDoubleQuote: Boolean = true
  /**
   * 可変長出力でDouble Quoteで包む場合のDouble Quote Escape文字列
   */
  val writeFileVariableEscapeChar: String = ""

  val writeFileFunc: (DataFrame, InputArgs, Map[String, String]) => Unit = null

  val writeCharEncoding: String = "MS932"

  /**
   * パーティション出力対象項目リスト
   */
  val writeFilePartitionColumns: Seq[String] = Seq.empty[String]

  /**
   * パーティション出力ファイル拡張子
   */
  val writeFilePartitionExtention: String = ""

  def writeFile(df: DataFrame)(implicit inArgs: InputArgs) = {
    val writeFilePathAndName = s"${writeFilePath}/${writeFileName}"
    val vari = new VariableFile(writeFilePathAndName, writeFileVariableWrapDoubleQuote, writeFileVariableEscapeChar, writeCharEncoding, writeFilePartitionColumns, writeFilePartitionExtention)
    val fixed = new FixedFile(writeFilePathAndName, writeFilePartitionColumns, writeFilePartitionExtention)
    val writer = writeFileMode match {
      case Csv                                => vari.writeSingle(",")
      case Csv(wrapTargetCols @ _*)           => vari.writeSingleCsvWithDoubleQuote(wrapTargetCols.toSet)
      case Tsv                                => vari.writeSingle("\t")
      case Fixed(itemLengths @ _*)            => fixed.writeSingle_MS932(itemLengths)
      case Fixed                              => new FixedFileWithConfFile(writeFilePathAndName).writeFile(writeFileFunc)
      case partition.Csv                      => vari.writePartition(",")
      case partition.Csv(wrapTargetCols @ _*) => vari.writePartitionCsvWithDoubleQuote(wrapTargetCols.toSet)
      case partition.Tsv                      => vari.writePartition("\t")
      case partition.Fixed(itemLengths @ _*)  => fixed.writePartition_MS932(itemLengths)
      case hdfs.Csv                           => vari.writeHdfs(",")
      case hdfs.Tsv                           => vari.writeHdfs("\t")
      case hdfs.Fixed(itemLengths @ _*)       => fixed.writeHdfs_MS932(itemLengths)
      case sequence.Csv                       => vari.writeSequence(",")
      case sequence.Csv(wrapTargetCols @ _*)  => vari.writeSequenceCsvWithDoubleQuote(wrapTargetCols.toSet)
      case sequence.Tsv                       => vari.writeSequence("\t")
      case sequence.Fixed(itemLengths @ _*)   => fixed.writeSequence_MS932(itemLengths)
      case _                                  => throw new IllegalArgumentException(s"${writeFileMode} is unusable")
    }
    writer(df)
    df.sqlContext.emptyDataFrame
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import spark.common.DbInfo
import spark.common.PqCtl
import d2k.common.InputArgs
import d2k.common.ResourceInfo
import org.apache.spark.sql.DataFrame

trait WritePq extends ResourceInfo {
  lazy val writePqName: String = componentId
  def writePqPath(implicit inArgs: InputArgs): String = inArgs.baseOutputFilePath

  val writePqPartitionColumns: Seq[String] = Seq.empty[String]

  def writeParquet(df: DataFrame)(implicit inArgs: InputArgs) = {
    val pqCtl = new PqCtl(writePqPath)
    import pqCtl.implicits._
    if (writePqPartitionColumns.isEmpty) {
      df.writeParquet(writePqName)
    } else {
      df.writeParquetWithPartitionBy(writePqName, writePqPartitionColumns: _*)
    }
    df.sqlContext.emptyDataFrame
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.file.output

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._
import org.apache.spark.sql.Row
import org.apache.hadoop.io.NullWritable

import java.nio.file.Files
import java.nio.file.Path
import java.io.File
import java.io.FileSystem
import java.nio.file.FileSystems
import scala.reflect.io.Directory

import d2k.common.InputArgs
import d2k.common.fileConv.DomainProcessor
import d2k.common.Logging

import spark.common.FileCtl
import spark.common.SparkContexts
import spark.common.DfCtl
import DfCtl._
import DfCtl.implicits._
import org.apache.spark.sql.types._
import scala.collection.mutable.ListBuffer
import scala.collection.mutable.LinkedHashSet

class FixedFile(fileName: String, writeFilePartitionColumns: Seq[String] = Seq.empty[String], writeFilePartitionExtention: String = "") extends Logging {
  val charSet = "MS932"
  val pad = " ".getBytes(charSet).head

  def writeSingle_MS932(itemLengths: Seq[Int]) = (df: DataFrame) => {
    if (writeFilePartitionColumns.isEmpty) {
      Directory(fileName).createDirectory(true, false)
      Files.deleteIfExists(FileSystems.getDefault.getPath(fileName))
      FileCtl.writeToFile(fileName) { pw =>
        val collected = df.collect
        elapse(s"fileWrite:${fileName}") {
          collected.foreach { row => pw.println(mkOutputStr(itemLengths)(row)) }
        }
      }
    } else {
      FileCtl.deleteDirectory(fileName)
      elapse(s"fileWrite:${fileName}") {
        FileCtl.loanPrintWriterCache { cache =>
          df.collect.foldLeft(cache) { (l, r) =>
            FileCtl.writeToFileWithPartitionColumns(
              fileName, partitionColumns = writeFilePartitionColumns, partitionExtention = writeFilePartitionExtention)(
              mkOutputStr(itemLengths))(l)(r)
          }
        }
      }
    }
  }

  def writeHdfs_MS932(itemLengths: Seq[Int]) = (df: DataFrame) => {

    val partCheckeddDf = if (writeFilePartitionColumns.isEmpty) {
      val paddedDf = df.na.fill("") ~> paddingSpace(itemLengths)
      val sch = StructType(Seq(StructField("str", StringType, true)))
      val rows = paddedDf.rdd.map(r => Row(r.toSeq.mkString("")))
      SparkContexts.context.createDataFrame(rows, sch).write
    } else {
      val targetSchemas = writeFilePartitionColumns.map { n =>
        val sc = df.schema(n)
        sc.copy(dataType = StringType)
      }
      val sch = StructType(targetSchemas ++ Seq(StructField("value", StringType, true)))
      val fieldNames = LinkedHashSet(df.schema.map(_.name): _*)
      val rows = df.rdd.map { row =>
        val keyValues = writeFilePartitionColumns.map(n => row.get(row.fieldIndex(n)).toString)
        val fixedValues = (fieldNames -- writeFilePartitionColumns).zip(itemLengths)
          .foldLeft(ListBuffer.empty[String]) { (l, r) =>
            l.append(paddingMS932(row.get(row.fieldIndex(r._1)), r._2))
            l
          }.mkString("")
        Row((keyValues :+ fixedValues): _*)
      }
      SparkContexts.context.createDataFrame(rows, sch)
        .write.partitionBy(writeFilePartitionColumns: _*)
    }

    partCheckeddDf.mode(SaveMode.Overwrite).text(fileName)
  }

  val paddingMS932 = (inTarget: Any, paddingSize: Int) => {
    val strTarget = Option(inTarget).map(_.toString).getOrElse("")
    val targetBin = strTarget.getBytes("MS932")
    val itemSize = targetBin.length
    if (paddingSize - itemSize > 0) {
      strTarget + (" " * (paddingSize - itemSize))
    } else {
      new String(targetBin.take(paddingSize), "MS932")
    }
  }

  val paddingMS932Udf = udf { paddingMS932 }

  def paddingSpace(targetLengths: Seq[Int]) = (df: DataFrame) => {
    val items = df.schema.map(_.name).zip(targetLengths)
    val padSpace = items.map { case (n, l) => (n, paddingMS932Udf(df(n), lit(l))) }
    df ~> editColumnsAndSelect(padSpace.e)
  }

  def writePartition_MS932(itemLengths: Seq[Int]) = (df: DataFrame) => {
    import DfCtl.implicits._
    if (writeFilePartitionColumns.isEmpty) {
      df.partitionWriteFile(
        fileName, true, partitionExtention = writeFilePartitionExtention)(mkOutputStr(itemLengths))
    } else {
      FileCtl.deleteDirectory(fileName)
      df.partitionWriteToFileWithPartitionColumns(
        fileName, writeFilePartitionColumns, true, partitionExtention = writeFilePartitionExtention)(mkOutputStr(itemLengths))
    }
  }

  def writeSequence_MS932(itemLengths: Seq[Int]) = (df: DataFrame) => {
    FileCtl.deleteDirectory(fileName)
    df.rdd.map(row => (NullWritable.get, mkOutputBinary(itemLengths)(row)))
      .saveAsSequenceFile(fileName, Some(classOf[org.apache.hadoop.io.compress.SnappyCodec]))
  }

  private[this] def mkOutputStr(itemLengths: Seq[Int])(row: Row) =
    itemLengths.zipWithIndex.foldLeft(new StringBuffer)(
      (l, r) => l.append(new String(mkArrByte(row, r), charSet))).toString

  private[this] def mkOutputBinary(itemLengths: Seq[Int])(row: Row) =
    itemLengths.zipWithIndex.foldLeft(Array.empty[Byte])((l, r) => l ++ mkArrByte(row, r))

  private[this] def mkArrByte(row: Row, itemInfo: (Int, Int)) = {
    val (len, idx) = itemInfo
    Option(row.get(idx)).map { x =>
      val target = x.toString.getBytes(charSet)
      if (len <= target.size) { target.take(len) } else {
        target ++ Array.fill(len - target.size)(pad)
      }
    }.getOrElse(Array.fill(len)(pad))
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.file.output

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._
import d2k.common.InputArgs
import scala.io.Source
import spark.common.FileCtl
import org.apache.spark.sql._
import d2k.common.Logging

object FixedFileWithConfFile {
  def confs(confPath: String) = {
    Source.fromFile(confPath).getLines.map { line =>
      val data = line.split("\t")
      (data(0), data(1))
    }.toMap
  }
}

class FixedFileWithConfFile(fileName: String)(implicit inArgs: InputArgs) extends Serializable with Logging {
  val targetConf = FixedFileWithConfFile.confs(inArgs.fileConvOutputFile)
  val outputType = targetConf(s"${inArgs.applicationId}.outputType")

  def writeFile(writeFileFunc: (DataFrame, InputArgs, Map[String, String]) => Unit) = (df: DataFrame) => {
    if (writeFileFunc != null) {
      writeFileFunc(df, inArgs, targetConf)
    } else {
      val output = outputType match {
        case "fixed" => writeFixedFile
        case _       => throw new RuntimeException(s"INVALID OUTPUT TYPE:${outputType}")
      }
      output(fileName, df)
    }
  }

  /*
   * 固定長ファイル出力
   */
  val writeFixedFile = (path: String, df: DataFrame) => {
    def rpad(target: String, len: Int, pad: String = " ") = {
      val str = if (target == null) { "" } else { target }
      val strSize = str.getBytes("MS932").size
      val padSize = len - strSize
      s"${str}${pad * padSize}"
    }

    val itemLens = targetConf(s"${inArgs.applicationId}.itemLengths")
    val itemLenList = itemLens.split(',').map { len => len.toInt }

    val itemLenListWithIdx = itemLenList.zipWithIndex
    def rowToFixedStr(row: Row) = {
      val line = itemLenListWithIdx.foldLeft("") { (acum, elem) =>
        {
          val (len, idx) = elem
          val str = row.getString(idx)
          acum + rpad(str, len)
        }
      }
      line + "\n"
    }
    FileCtl.writeToFile(path) { writer =>
      val collected = df.rdd.map(rowToFixedStr).collect
      elapse(s"fileWrite:${path}") {
        collected.foreach(writer.print)
      }
    }
  }

}

', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.file.output

import org.apache.spark.sql.DataFrame
import d2k.common.InputArgs
import spark.common.FileCtl
import java.nio.file.Files
import java.nio.file.Path
import java.io.File
import java.io.FileSystem
import java.nio.file.FileSystems
import org.apache.spark.sql.Row
import scala.reflect.io.Directory
import spark.common.DfCtl
import spark.common.SparkContexts
import DfCtl.implicits._
import SparkContexts.context.implicits._
import d2k.common.Logging
import org.apache.hadoop.io.NullWritable
import org.apache.spark.sql.SaveMode

object VariableFile {
  def addDoubleQuote(target: String, wrapDoubleQuote: Boolean, escapeChar: String) = {
    if (wrapDoubleQuote) {
      val escaped = target.toString.replaceAll("\"", s"""${escapeChar}"""")
      s""""${escaped}""""
    } else { target }
  }

  def mkOutputStr(row: Row, separator: String, wrapDoubleQuote: Boolean, inEscapeChar: String) = {
    val escapeChar = if (inEscapeChar.isEmpty) "\"" else inEscapeChar
    row.toSeq.map { col =>
      addDoubleQuote(Option(col).map(_.toString).getOrElse(""), wrapDoubleQuote, escapeChar)
    }.mkString(separator)
  }

  def mkOutputBinary(row: Row, separator: String, wrapDoubleQuote: Boolean, escapeChar: String, charEnc: String) =
    mkOutputStr(row, separator, wrapDoubleQuote, escapeChar).getBytes(charEnc)

  def mkOutputCsvStr(row: Row, targetColumns: Set[String], wrapDoubleQuote: Boolean, escapeChar: String) = {
    row.toSeq.zip(row.schema.fieldNames).map {
      case (col, name) =>
        if (targetColumns.exists(_ == name)) {
          Option(col).map(str => addDoubleQuote(str.toString, true, escapeChar)).getOrElse(addDoubleQuote("", wrapDoubleQuote, escapeChar))
        } else {
          Option(col).map(_.toString).getOrElse("")
        }
    }.mkString(",")
  }

  def mkOutputCsvBinary(row: Row, targetColumns: Set[String], wrapDoubleQuote: Boolean, escapeChar: String, charEnc: String) =
    mkOutputCsvStr(row, targetColumns, wrapDoubleQuote, escapeChar).getBytes(charEnc)
}

class VariableFile(
    fileName: String, wrapDoubleQuote: Boolean, escapeChar: String, charEnc: String,
    writeFilePartitionColumns: Seq[String] = Seq.empty[String], writeFilePartitionExtention: String = "") extends Logging {
  import VariableFile._

  def write(df: DataFrame, fileName: String, separator: String = ",") = {
    df.map(_.mkString(separator)).rdd.saveAsTextFile(fileName)
  }

  def writeSingle(separator: String) = (df: DataFrame) => {
    if (writeFilePartitionColumns.isEmpty) {
      Directory(fileName).createDirectory(true, false)
      Files.deleteIfExists(FileSystems.getDefault.getPath(fileName))
      FileCtl.writeToFile(fileName, charEnc = charEnc) { pw =>
        val collected = df.collect
        elapse(s"fileWrite:${fileName}") {
          collected.foreach { row => pw.println(mkOutputStr(row, separator, wrapDoubleQuote, escapeChar)) }
        }
      }
    } else {
      FileCtl.deleteDirectory(fileName)
      elapse(s"fileWrite:${fileName}") {
        FileCtl.loanPrintWriterCache { cache =>
          df.collect.foldLeft(cache) { (l, r) =>
            FileCtl.writeToFileWithPartitionColumns(
              fileName, partitionColumns = writeFilePartitionColumns, partitionExtention = writeFilePartitionExtention)(
                row => mkOutputStr(row, separator, wrapDoubleQuote, escapeChar))(l)(r)
          }
        }
      }
    }
  }

  def writeHdfs(separator: String) = (df: DataFrame) => {
    val paraCheckedDf = if (writeFilePartitionColumns.isEmpty) {
      df.write
    } else {
      df.write.partitionBy(writeFilePartitionColumns: _*)
    }

    val ops = Seq(("delimiter", separator), ("encoding", charEnc),
      ("ignoreLeadingWhiteSpace", "false"), ("ignoreTrailingWhiteSpace", "false"))
    val ops2 = if (wrapDoubleQuote) ops :+ ("quoteAll", "true") else ops :+ ("emptyValue", "")
    val ops3 = if (!escapeChar.isEmpty) ops2 :+ ("escape", escapeChar) else ops2
    paraCheckedDf.mode(SaveMode.Overwrite).options(ops3.toMap).csv(fileName)
  }

  def writePartition(separator: String) = (df: DataFrame) => {
    if (writeFilePartitionColumns.isEmpty) {
      df.partitionWriteFile(fileName, true, charEnc, writeFilePartitionExtention)(row => mkOutputStr(row, separator, wrapDoubleQuote, escapeChar))
    } else {
      FileCtl.deleteDirectory(fileName)
      df.partitionWriteToFileWithPartitionColumns(fileName, writeFilePartitionColumns, true, charEnc, writeFilePartitionExtention)(row => mkOutputStr(row, separator, wrapDoubleQuote, escapeChar))
    }
  }

  def writeSequence(separator: String) = (df: DataFrame) => {
    FileCtl.deleteDirectory(fileName)
    df.rdd.map(row => (NullWritable.get, mkOutputBinary(row, separator, wrapDoubleQuote, escapeChar, charEnc)))
      .saveAsSequenceFile(fileName, Some(classOf[org.apache.hadoop.io.compress.SnappyCodec]))
  }

  def writeSingleCsvWithDoubleQuote(targetColumns: Set[String]) = (df: DataFrame) => {
    if (writeFilePartitionColumns.isEmpty) {
      Directory(fileName).createDirectory(true, false)
      Files.deleteIfExists(FileSystems.getDefault.getPath(fileName))
      FileCtl.writeToFile(fileName, charEnc = charEnc) { pw =>
        val collected = df.collect
        elapse(s"fileWrite:${fileName}") {
          collected.foreach(row => pw.println(mkOutputCsvStr(row, targetColumns, wrapDoubleQuote, escapeChar)))
        }
      }
    } else {
      FileCtl.deleteDirectory(fileName)
      elapse(s"fileWrite:${fileName}") {
        FileCtl.loanPrintWriterCache { cache =>
          df.collect.foldLeft(cache) { (l, r) =>
            FileCtl.writeToFileWithPartitionColumns(
              fileName, partitionColumns = writeFilePartitionColumns, partitionExtention = writeFilePartitionExtention)(
                row => mkOutputCsvStr(row, targetColumns, wrapDoubleQuote, escapeChar))(l)(r)
          }
        }
      }
    }
  }

  def writePartitionCsvWithDoubleQuote(targetColumns: Set[String]) = (df: DataFrame) => {
    if (writeFilePartitionColumns.isEmpty) {
      df.partitionWriteFile(fileName, true, charEnc, partitionExtention = writeFilePartitionExtention)(row => mkOutputCsvStr(row, targetColumns, wrapDoubleQuote, escapeChar))
    } else {
      FileCtl.deleteDirectory(fileName)
      df.partitionWriteToFileWithPartitionColumns(fileName, writeFilePartitionColumns, true, charEnc, writeFilePartitionExtention)(row => mkOutputCsvStr(row, targetColumns, wrapDoubleQuote, escapeChar))
    }
  }

  def writeSequenceCsvWithDoubleQuote(targetColumns: Set[String]) = (df: DataFrame) => {
    FileCtl.deleteDirectory(fileName)
    df.rdd.map(row => (NullWritable.get, mkOutputCsvBinary(row, targetColumns, wrapDoubleQuote, escapeChar, charEnc)))
      .saveAsSequenceFile(fileName, Some(classOf[org.apache.hadoop.io.compress.SnappyCodec]))
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.fileConv

import scala.reflect.io.Path
import scala.reflect.io.Directory
import scala.io.Source
import scala.reflect.io.Path.string2path
import scala.reflect.io.File
import scala.io.BufferedSource
import java.io.FileNotFoundException
import scala.util.Try

case class Conf(appConf: AppConf, itemConfs: Iterator[ItemConf])
case class AppConf(AppId: String, AppName: String, destSystem: String, fileFormat: String,
                   newline: Boolean, header: Boolean, footer: Boolean, storeType: String, inputDir: String, inputFiles: String, comment: String = "")
case class ItemConf(itemId: String, itemName: String, length: String, cnvType: String, extractTarget: Boolean, comment: String = "")

object ConfParser {
  val availableBoolean = Seq("true", "false", "TRUE", "FALSE")
  val availableBooleanWithBlank = Seq("true", "false", "TRUE", "FALSE", "")
  val availableFileFormat = Seq("tsv", "tsvDropDoubleQuote", "tsvStrict", "csv", "csvStrict", "vsv", "vsvStrict", "fixed")
  val availableStoreType = Seq("all", "diff", "diffExt")

  def parse(inFilePath: String) = parseAppConf(inFilePath)

  def readConf[A](inFilePath: String)(proc: Array[String] => A) = {
    val fileEnc = "MS932"
    val itemConfPath = s"itemConf/${File(inFilePath).name}"
    Option(getClass.getClassLoader.getResourceAsStream(itemConfPath))
      .map(is => Source.fromInputStream(is, fileEnc))
      .getOrElse {
        Source.fromFile(inFilePath, fileEnc)
      }.getLines.drop(1).map(line => proc(line.split('\t')))
  }

  def checkItem(items: Array[String])(abailables: Seq[String], targetIdx: Int, comment: String) =
    if (!abailables.contains(items(targetIdx))) {
      throw new IllegalArgumentException(
        s"not available item:${items(targetIdx)}(usage: ${abailables.mkString(" or ")}) in $comment")
    }

  def parseAppConf(inFilePath: String) = {
    val appConfs = readConf(inFilePath) { items =>
      appErrorCheck(items)
      if (items.size == 10) {
        AppConf(items(0), items(1), items(2), items(3), items(4) == "true", items(5) == "true", items(6) == "true", items(7), items(8), items(9))
      } else {
        AppConf(items(0), items(1), items(2), items(3), items(4) == "true", items(5) == "true", items(6) == "true", items(7), items(8), items(9), items(10))
      }
    }
    val basePath = Path(inFilePath).toAbsolute.parent
    val namePrefix = Path(inFilePath).name.split('_')(0)
    appConfs.map(appConf => Conf(appConf, parseItemConf(basePath, namePrefix, appConf.AppId)))
  }

  def getAvailableBoolean(items: Array[String]) = {
    if (items(3) == "fixed") { availableBoolean } else { availableBooleanWithBlank }
  }

  def appErrorCheck(items: Array[String]) = {
    val checker = checkItem(items) _
    def comment(name: String) = s"AppConf[appId:${items(0)} name:${name}]"
    checker(availableFileFormat, 3, comment("fileFormat"))
    checker(getAvailableBoolean(items), 4, comment("newline"))
    checker(availableBoolean, 5, comment("header"))
    checker(availableBoolean, 6, comment("footer"))
    checker(availableStoreType, 7, comment("storeType"))
  }

  def parseItemConf(basePath: Directory, prefix: String, appId: String) = {
    val itemConfPath = basePath / s"${prefix}_items_${appId}.conf"
    readConf(itemConfPath.toString) { items =>
      itemErrorCheck(items, appId)
      if (items.size == 5) {
        ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true")
      } else {
        ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true", items(5))
      }
    }
  }

  def itemErrorCheck(items: Array[String], appId: String) = {
    val checker = checkItem(items) _
    def comment(name: String) = s"ItemConf[appId:${appId} item:${items(0)} name:${name}]"
    checkItem(items)(availableBoolean, 4, comment("extractTarget"))
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.fileConv

import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

case class ErrMessage(name: String, domain: String, value: String, message: String) {
  override def toString = s"${name},${domain},[${value}],${message}"
}

object Converter extends Serializable {
  val REC_DIV_PREFIX = "レコード区分_"
  val NOT_USE_PREFIX = "未使用"
  val REC_DIV_EXTRACT = "D"

  object SYSTEM_COLUMN_NAME {
    val RECORD_INDEX = "_recordIndex_"
    val RECORD_LENGTH_ERROR = "_recordLengthError_"
    val ROW_ERROR = "ROW_ERR"
    val ROW_ERROR_MESSAGE = "ROW_ERR_MESSAGE"
  }

  def makeSchema(inNames: Seq[String]) = {
    val names = inNames :+ SYSTEM_COLUMN_NAME.ROW_ERROR :+ SYSTEM_COLUMN_NAME.ROW_ERROR_MESSAGE
    StructType(names.map { case name => StructField(name, StringType, true) })
  }

  def makeSchemaWithRecordError(inNames: Seq[String]) = {
    val names = inNames :+ SYSTEM_COLUMN_NAME.ROW_ERROR :+ SYSTEM_COLUMN_NAME.ROW_ERROR_MESSAGE :+ SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR
    StructType(names.map { case name => StructField(name, StringType, true) })
  }

  def domainConvert(dataAndDomainsAndNames: Seq[((String, String), String)]) = {
    val (rowData, errMessage) = dataAndDomainsAndNames.foldLeft((Seq.empty[String], Seq.empty[ErrMessage])) { (l, r) =>
      val (convCols, errMessages) = l
      val ((data, domain), name) = r
      if (domain == NOT_USE_PREFIX || domain.startsWith(REC_DIV_PREFIX)) {
        l
      } else {
        val target = if (data == null) { "" } else { data }
        DomainProcessor.exec(domain, target) match {
          case Right(d) => (convCols :+ d, errMessages)
          case Left(m)  => (convCols :+ null, errMessages :+ ErrMessage(name, domain, data, m))
        }
      }
    }
    makeErrorMessage(errMessage, rowData)
  }

  def domainConvert(dataAndDomainsAndNames: Seq[((Array[Byte], String), String)], charEnc: String) = {
    val (rowData, errMessage) = dataAndDomainsAndNames.foldLeft((Seq.empty[String], Seq.empty[ErrMessage])) { (l, r) =>
      val (convCols, errMessages) = l
      val ((data, domain), name) = r
      if (domain == NOT_USE_PREFIX) {
        l
      } else {
        DomainProcessor.execArrayByte(domain, data, charEnc) match {
          case Right(d) => (convCols :+ d, errMessages)
          case Left(m) => (convCols :+ null, errMessages :+ ErrMessage(
            name, domain, new String(data, if (charEnc == "JEF") "ISO-8859-1" else charEnc), m))
        }
      }
    }
    makeErrorMessage(errMessage, rowData)
  }

  def makeErrorMessage(errMessage: Seq[ErrMessage], rowData: Seq[String]) =
    if (errMessage.isEmpty) {
      rowData :+ "false" :+ ""
    } else {
      rowData :+ "true" :+ errMessage.mkString("|")
    }

  def removeHeaderAndFooter[A](data: Seq[A], hasHeader: Boolean, hasFooter: Boolean) = ((hasHeader, hasFooter) match {
    case (true, true)  => data.drop(1).dropRight(1)
    case (true, false) => data.drop(1)
    case (false, true) => data.dropRight(1)
    case _             => data
  })

  def removeHeaderAndFooter(df: DataFrame, hasHeader: Boolean, hasFooter: Boolean, colNames: Seq[String], domainNames: Seq[String]) = {
    if (hasHeader || hasFooter) {
      val dataDivIdx = domainNames.indexWhere { elem => elem.startsWith(REC_DIV_PREFIX) }
      val dataDivColName = colNames(dataDivIdx)
      df.filter(col(dataDivColName) === REC_DIV_EXTRACT).drop(dataDivColName)
    } else {
      df
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.fileConv

import org.apache.spark.sql.functions._
import scala.util.Try
import org.joda.time.format.DateTimeFormat
import org.joda.time.format.DateTimeFormatter
import scala.BigInt
import scala.Left
import scala.Right
import scala.annotation.tailrec
import d2k.common.JefConverter

object DomainProcessor extends Serializable {
  val ERR_MSG_INVALID_VALUE = "不正な形式"
  val ERR_MSG_NOT_EXIST_VALUE = "存在しない値"
  val ERR_MSG_INVALID_DOMAIN = "不正な変換型"
  val ERR_MSG_INVALID_DATA_DIV = "不正なデータ区分"

  val FORMAT_YYYYMMDDHHMMSSMS = "yyyyMMddHHmmssSSS"
  val FORMAT_YYYYMMDD = "yyyyMMdd"
  val FORMAT_YYYYMM = "yyyyMM"
  val FORMAT_MMDD = "MMdd"
  val FORMAT_YYYYMMDDHHMMSS = "yyyyMMddHHmmss"
  val FORMAT_YYYYMMDDHH = "yyyyMMddHH"
  val FORMAT_HHMMSS = "HHmmss"
  val FORMAT_HHMMSSMS = "HHmmssSSS"
  val FORMAT_HHMM = "HHmm"
  val FORMAT_MMSS = "mmss"
  val FORMAT_YYYY = "yyyy"
  val FORMAT_MM = "MM"
  val FORMAT_DD = "dd"
  val FORMAT_HH = "HH"
  val FORMAT_MI = "mm"
  val FORMAT_SS = "ss"

  val FORMATTER_YYYYMMDDHHMMSSMS = DateTimeFormat.forPattern(FORMAT_YYYYMMDDHHMMSSMS)
  val FORMATTER_YYYYMMDD = DateTimeFormat.forPattern(FORMAT_YYYYMMDD)
  val FORMATTER_YYYYMM = DateTimeFormat.forPattern(FORMAT_YYYYMM)
  val FORMATTER_MMDD = DateTimeFormat.forPattern(FORMAT_MMDD)
  val FORMATTER_YYYYMMDDHHMMSS = DateTimeFormat.forPattern(FORMAT_YYYYMMDDHHMMSS)
  val FORMATTER_YYYYMMDDHH = DateTimeFormat.forPattern(FORMAT_YYYYMMDDHH)
  val FORMATTER_YYYYMMDDHHMM = DateTimeFormat.forPattern("yyyyMMddHHmm")
  val FORMATTER_HHMMSS = DateTimeFormat.forPattern(FORMAT_HHMMSS)
  val FORMATTER_HHMMSSMS = DateTimeFormat.forPattern(FORMAT_HHMMSSMS)
  val FORMATTER_HHMM = DateTimeFormat.forPattern(FORMAT_HHMM)
  val FORMATTER_MMSS = DateTimeFormat.forPattern(FORMAT_MMSS)

  val PD_SUFFIX = "_PD"
  val ZD_SUFFIX = "_ZD"

  val REC_DIV_NUMBER_HEAD = "1"
  val REC_DIV_NUMBER_DATA = "2"
  val REC_DIV_NUMBER_FOOT = "3"

  val REC_DIV_ALPHABET_HEAD = "H"
  val REC_DIV_ALPHABET_DATA = "D"
  val REC_DIV_ALPHABET_FOOT = "T"

  def exec(domain: String, data: String): Either[String, String] = dp {
    domain match {
      case "年月日"               => convDate(data, "00010101", "99991231", FORMATTER_YYYYMMDD)
      case "年月日_SL"            => convDate(data.replaceAll("/", ""), "00010101", "99991231", FORMATTER_YYYYMMDD)
      case "年月日_HY"            => convDate(data.replaceAll("-", ""), "00010101", "99991231", FORMATTER_YYYYMMDD)
      case "年月"                => convDate(data, "000101", "999912", FORMATTER_YYYYMM)
      case "年月_SL"             => convDate(data.replaceAll("/", ""), "000101", "999912", FORMATTER_YYYYMM)
      case "月日"                => convDate(data, "0101", "1231", FORMATTER_MMDD)
      case "年"                 => convDateParts(data, "0001", "9999")
      case "月"                 => convDateParts(data, "01", "12")
      case "日"                 => convDateParts(data, "01", "31")
      case "年月日時分秒"            => convDate(data, "00010101000000", "99991231235959", FORMATTER_YYYYMMDDHHMMSS)
      case "年月日時分秒_HC"         => convDate(data.replaceAll("[- :]", ""), "00010101000000", "99991231235959", FORMATTER_YYYYMMDDHHMMSS)
      case "年月日時分秒_SC"         => convDate(data.replaceAll("[/ :]", ""), "00010101000000", "99991231235959", FORMATTER_YYYYMMDDHHMMSS)
      case "年月日時分秒_CO"         => convDate(data.replaceAll("[ :]", ""), "00010101000000", "99991231235959", FORMATTER_YYYYMMDDHHMMSS)
      case "年月日時分ミリ秒"          => convTimeStamp(data, "00010101000000000", "99991231235959999", FORMATTER_YYYYMMDDHHMMSSMS, FORMAT_YYYYMMDDHHMMSSMS)
      case "年月日時分ミリ秒/ミリ秒小数点付加" => addPeriod(convTimeStamp(data, "00010101000000000", "99991231235959999", FORMATTER_YYYYMMDDHHMMSSMS, FORMAT_YYYYMMDDHHMMSSMS), 14)
      case "年月日時"              => convDate(data, "0001010100", "9999123123", FORMATTER_YYYYMMDDHH)
      case "年月日時分"             => convDate(data, "000101010000", "999912312359", FORMATTER_YYYYMMDDHHMM)
      case "年月日時分_SC"          => convDate(data.replaceAll("[/ :]", ""), "000101010000", "999912312359", FORMATTER_YYYYMMDDHHMM)
      case "時分秒"               => convDate(data, "000000", "235959", FORMATTER_HHMMSS)
      case "時分秒_CO"            => convDate(data.replaceAll(":", ""), "000000", "235959", FORMATTER_HHMMSS)
      case "時分ミリ秒"             => convTimeStamp(data, "000000000", "235959999", FORMATTER_HHMMSSMS, FORMAT_HHMMSSMS)
      case "時分ミリ秒/ミリ秒小数点付加"    => addPeriod(convTimeStamp(data, "000000000", "235959999", FORMATTER_HHMMSSMS, FORMAT_HHMMSSMS), 6)
      case "時分"                => convDate(data, "0000", "2359", FORMATTER_HHMM)
      case "時"                 => convDateParts(data, "00", "23")
      case "分"                 => convDateParts(data, "00", "59")
      case "秒"                 => convDateParts(data, "00", "59")
      case "時間"                => convTime(data, "000000", "995959")
      case "数字"                => convDigit(data)
      case "数字_SIGNED"         => convSignedDigit(data)
      case "Byte配列"            => Right(data)
      case "文字列"               => Right(data.trim)
      case "文字列_trim_無し"       => Right(data)
      case "文字列_trim_半角"       => Right(data.trim)
      case "文字列_trim_全角"       => Right(trimFull(data))
      case "文字列_trim_全半角"      => Right(trimFullAndHalf(data))
      case "全角文字列"             => Right(trimFull(data))
      case "全角文字列_trim_無し"     => Right(data)
      case "全角文字列_trim_全角"     => Right(trimFull(data))
      case "レコード区分_NUMBER"     => convDataDiv(data)
      case "レコード区分_ALPHABET"   => Right(data)
      case "通信方式"              => convCommMthd(data, "       ")
      case "識別子"               => Right(data.trim)
      case _                   => throw new RuntimeException(s"${ERR_MSG_INVALID_DOMAIN}:${domain}")
    }
  }

  def trimFull(data: String) = data.dropWhile(_ == '　').reverse.dropWhile(_ == '　').reverse
  def trimFullAndHalf(data: String) = data.dropWhile(s => s == '　' || s == ' ').reverse.dropWhile(s => s == '　' || s == ' ').reverse
  def execArrayByte(domain: String, data: Array[Byte], charEnc: String): Either[String, String] =
    data match {
      case target if domain == "Byte配列" => Right(new String(target, "ISO-8859-1"))

      case target if (isPd(domain) || isZd(domain)) && (isNull(target)) => exec(domain.dropRight(3), new String(data, charEnc))
      case target if (isPd(domain) || isZd(domain)) && (isEmpty(target)) => digitErrOrConvDate(domain.dropRight(3), new String(data, charEnc))

      case target if isPd(domain) && !isValidSign(target) => digitErrOrConvDate(domain.dropRight(3), new String(data, charEnc))
      case target if isPd(domain) => convDigitOrDatePd(domain.dropRight(3), target)

      case target if isZd(domain) && (!isValidSignZd(target)) => digitErrOrConvDate(domain.dropRight(3), new String(data, charEnc))
      case target if isZd(domain) => convZd(domain.dropRight(3), target)

      case target if JefConverter.isJefHalf(domain, charEnc) => exec(domain, JefConverter.convJefToUtfHalf(data))
      case target if JefConverter.isJefFull(domain, charEnc) => exec(domain, JefConverter.convJefToUtfFull(data))

      case _ => exec(domain, new String(data, charEnc))
    }

  def dp(proc: => Either[String, String]) =
    Try(proc).recover {
      case t: RuntimeException => throw t
      case t: Exception        => Left(t.toString)
    }.get

  /*
   * 日付系フォーマットあり 
   */
  def convDate(data: String, min: String, max: String, format: DateTimeFormatter) = data match {
    case target if (isNull(target))             => Right(min)
    case target if (isEmpty(target))            => Right(min)
    case target if (!isDigit(target))           => Right(min)
    case allNineRegex(_*)                       => Right(max)
    case target if (target.toLong < min.toLong) => Right(min)
    case target if (target.toLong > max.toLong) => Right(min)
    case target if (!isDate(target, format))    => Right(min)
    case target                                 => Right(target)
  }
  /*
   * TIMESTAMPは桁数不足のケースがある 
   */
  def convTimeStamp(data: String, min: String, max: String, formatter: DateTimeFormatter, format: String) = {
    val targetMinPad = data.padTo(format.size, '0')
    val targetMaxPad = data.padTo(format.size, '9')
    data match {
      case target if (isNull(target))                    => Right(min)
      case target if (isEmpty(target))                   => Right(min)
      case target if (!isDigit(target))                  => Right(min)
      case allNineRegex(_*)                              => Right(max)
      case target if (targetMinPad.toLong <= min.toLong) => Right(min)
      case target if (targetMaxPad.toLong > max.toLong)  => Right(min)
      case target if (!isDate(target, formatter))        => Right(min)
      case target                                        => Right(targetMinPad)
    }
  }

  /*
   * 日付系の一部分 
   */
  def convDateParts(data: String, min: String, max: String) = data match {
    case target if (isNull(target))           => Right(min)
    case target if (isEmpty(target))          => Right(min)
    case target if (!isDigit(target))         => Right(min)
    case allNineRegex(_*)                     => Right(max)
    case target if (target.toInt < min.toInt) => Right(min)
    case target if (target.toInt > max.toInt) => Right(min)
    case target                               => Right(target)
  }

  /*
   * 時間（時刻でない） 
   */
  def convTime(data: String, min: String, max: String) = data match {
    case target if (isNull(target)) => Right(min)
    case target if (isEmpty(target)) => Right(min)
    case target if (!isDigit(target)) => Right(min)
    case allNineRegex(_*) => Right(max)
    case target if (target.toInt < min.toInt) => Right(min)
    case target if (target.toInt > max.toInt) => Right(min)
    case target if (!isDate(target.drop(2), FORMATTER_MMSS)) => Right(min)
    case target => Right(target)
  }

  def padForDate(domain: String, data: String) = domain match {
    case "年月日"      => zeroPadLeft(data, FORMAT_YYYYMMDD.length)
    case "年月日_SL"   => throw new RuntimeException(s"${ERR_MSG_INVALID_DOMAIN}:${domain}")
    case "年月"       => zeroPadLeft(data, FORMAT_YYYYMM.length)
    case "月日"       => zeroPadLeft(data, FORMAT_MMDD.length)
    case "年"        => zeroPadLeft(data, FORMAT_YYYY.length)
    case "月"        => zeroPadLeft(data, FORMAT_MM.length)
    case "日"        => zeroPadLeft(data, FORMAT_DD.length)
    case "年月日時分秒"   => zeroPadLeft(data, FORMAT_YYYYMMDDHHMMSS.length)
    case "年月日時分ミリ秒" => zeroPadLeft(data, FORMAT_YYYYMMDDHHMMSSMS.length)
    case "年月日時"     => zeroPadLeft(data, FORMAT_YYYYMMDDHH.length)
    case "時分秒"      => zeroPadLeft(data, FORMAT_HHMMSS.length)
    case "時分秒_CO"   => throw new RuntimeException(s"${ERR_MSG_INVALID_DOMAIN}:${domain}")
    case "時分ミリ秒"    => zeroPadLeft(data, FORMAT_HHMMSSMS.length)
    case "時分"       => zeroPadLeft(data, FORMAT_HHMM.length)
    case "時"        => zeroPadLeft(data, FORMAT_HH.length)
    case "分"        => zeroPadLeft(data, FORMAT_MI.length)
    case "秒"        => zeroPadLeft(data, FORMAT_SS.length)
    case "時間"       => zeroPadLeft(data, FORMAT_HHMMSS.length)
    case _          => data
  }

  def convPd(domain: String, data: Array[Byte])(unpack: (Array[Byte] => String)) = {
    try {
      exec(domain, unpack(data))
    } catch {
      case t: NumberFormatException => Left(ERR_MSG_INVALID_VALUE)
      case t: Exception             => throw t
    }
  }

  def convDatePd(domain: String, data: Array[Byte]) = {
    try {
      val unpacked = unpackForNum(data)
      exec(domain, padForDate(domain, unpacked))
    } catch {
      case t: NumberFormatException => exec(domain, "")
      case t: Exception             => throw t
    }
  }

  def convDigitOrDatePd(domain: String, data: Array[Byte]) = {
    domain match {
      case "文字列" => convPd(domain, data)(unpackForStr)
      case "識別子" => convPd(domain, data)(unpackForId)
      case "数字"  => convPd(domain, data)(unpackForNum)
      case _     => convDatePd(domain, data)
    }
  }

  def digitErrOrConvDate(orgDomain: String, data: String): Either[String, String] =
    orgDomain match {
      case "数字" => Left(ERR_MSG_INVALID_VALUE)
      case _    => exec(orgDomain, data)
    }

  def convDataDiv(data: String) = data match {
    case REC_DIV_NUMBER_HEAD => Right(REC_DIV_ALPHABET_HEAD)
    case REC_DIV_NUMBER_DATA => Right(REC_DIV_ALPHABET_DATA)
    case REC_DIV_NUMBER_FOOT => Right(REC_DIV_ALPHABET_FOOT)
    case _                   => throw new RuntimeException(s"${ERR_MSG_INVALID_DATA_DIV}:${data}")
  }

  def convCommMthd(data: String, allSpace: String) = data match {
    case target if (isNull(target))  => Right("")
    case target if (isEmpty(target)) => Right("")
    case _                           => Right(data.trim().substring(0, 1))
  }

  def convSignedDigit(data: String) = {
    val default = "0"
    val num = if (data.isEmpty) { default } else { data }
    convDigit(Try { BigDecimal(num).toString }.getOrElse(default))
  }

  def unpackForNum(data: Array[Byte]) = {
    val str = data.foldLeft("") { (l, r) => l + f"$r%02x" }
    val decimal = BigInt(str.dropRight(1))
    val isMinus = str.takeRight(1) == "d"
    (if (isMinus) { -decimal } else { decimal }).toString
  }

  def unpackForStr(data: Array[Byte]) = {
    val str = data.foldLeft("") { (l, r) => l + f"$r%02x" }
    val decimal = str.dropRight(1)
    val isMinus = str.takeRight(1) == "d"
    (if (isMinus) { s"-$decimal" } else { decimal })
  }

  def unpackForId(data: Array[Byte]) =
    data.foldLeft("") { (l, r) => l + f"$r%02x" }.dropRight(1)

  def zeroPadLeft(target: String, fullLen: Int) = s"${"0" * (fullLen - target.length())}${target}"

  def isDate(target: String, format: DateTimeFormatter) =
    Try(format.withZoneUTC.parseDateTime(target)).map(_ => true).getOrElse(false)

  def isEmpty(target: String) = target.trim.isEmpty

  def isEmpty(target: Array[Byte]) = {
    target.foldLeft(true) { (l, r) => l && r == 0x20 }
  }

  def isNull(target: Array[Byte]) = {
    target.foldLeft(true) { (l, r) => l && r == 0x00 }
  }

  def isNull(target: String): Boolean = {
    isNull(target.getBytes("MS932"))
  }

  def isDigit(target: String) = Try(target.toLong).map(_ => true).getOrElse(false)

  def isPd(domain: String) = domain.endsWith(PD_SUFFIX)
  def isZd(domain: String) = domain.endsWith(ZD_SUFFIX)

  def convDigit(data: String) = data match {
    case target if (isNull(target.getBytes("MS932"))) => Right(target)
    case target if (!isDigit(target.trim()))          => Left(ERR_MSG_INVALID_VALUE)
    case target if (isEmpty(target))                  => Left(ERR_MSG_INVALID_VALUE)
    case target                                       => Right(target)
  }

  def isValidSign(target: Array[Byte]) = {
    val sign = target.last & 0x0F
    sign == 0x0d || sign == 0x0c || sign == 0x0f
  }

  def isValidSignZd(target: Array[Byte]) = {
    val sign = target.last & 0xF0
    sign == 0xf0 || sign == 0xc0 || sign == 0xd0
  }

  val allNineRegex = """^9*$""".r

  def addPeriod(target: Either[String, String], pos: Int) = {
    target.right.map(str => str.take(pos) + "." + str.drop(pos))
  }

  def convZd(domain: String, data: Array[Byte]) = {
    try {
      val unpacked = domain match {
        case "文字列" => unzoneForStr(data)
        case "識別子" => unzoneForId(data)
        case _     => unzone(data)
      }
      exec(domain, padForDate(domain, unpacked))
    } catch {
      case t: NumberFormatException => Left(ERR_MSG_INVALID_VALUE)
      case t: Exception             => throw t
    }
  }

  def unzone(data: Array[Byte]) = {
    val str = data.map(x => f"$x%02x".drop(1)).mkString
    val decimal = BigInt(str)
    val isMinus = data.map(x => f"$x%02x").mkString.reverse.apply(1) == 'd'
    (if (isMinus) { -decimal } else { decimal }).toString
  }

  def unzoneForStr(data: Array[Byte]) = {
    val decimal = data.map(x => f"$x%02x".drop(1)).mkString
    val isMinus = data.map(x => f"$x%02x").mkString.reverse.apply(1) == 'd'
    if (isMinus) { s"-$decimal" } else { decimal }
  }

  def unzoneForId(data: Array[Byte]) =
    data.map(x => f"$x%02x".drop(1)).mkString
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.fileConv

import scala.reflect.io.Path
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._
import scala.reflect.io.Path.string2path
import d2k.common.InputArgs
import spark.common.PqCtl
import d2k.common.df._
import d2k.common.df.FileInputInfoBase
import scala.util.Try
import spark.common.SparkContexts.context
import org.apache.spark.sql.Row
import java.io.FileNotFoundException
import org.apache.hadoop.mapreduce.lib.input.InvalidInputException
import org.apache.hadoop.mapred.FileInputFormat
import spark.common.FileCtl

class FileConv(componentId: String, fileInputInfo: FileInputInfoBase, itemConfIdTemplate: String, mkEmptyDfWhenFileNotExists: Boolean = false)(implicit inArgs: InputArgs) extends Serializable {
  val dirPath = fileInputInfo.inputDir(componentId)
  val filePaths = fileInputInfo.inputFiles.map(file => (Path(dirPath) / file).toString)

  val itemConfId = if (fileInputInfo.itemConfId.isEmpty) itemConfIdTemplate else fileInputInfo.itemConfId
  val itemConfs = ConfParser.parseItemConf(Path(inArgs.fileConvInputFile).toAbsolute.parent, inArgs.projectId, itemConfId).toList
  val extractTargetsForSqlFilter = itemConfs
    .filter(_.extractTarget)
    .map(item => s"substring(${item.itemId},1,8) = ${inArgs.runningDateYMD}")
    .mkString(" or ")
  val extractSqlFilter = s"ROW_ERR = 'true' or ( ${extractTargetsForSqlFilter} )"

  def makeDf = {
    val len = itemConfs.map(_.length.toInt)
    val names = itemConfs.map(_.itemId)
    val domains = itemConfs.map(_.cnvType)

    //入力ファイル変換処理 入力ファイル⇒DataFrame
    val textConverter = new TextConverter(fileInputInfo.header, fileInputInfo.charSet)
    def makeDf = fileInputInfo.fileFormat match {
      case "tsv" => textConverter.tsv(names, domains, filePaths)
      case "csv" => textConverter.csv(names, domains, filePaths)
      case "vsv" => textConverter.vsv(names, domains, filePaths)
      case "ssv" => textConverter.ssv(names, domains, filePaths)
      case "fixed" => {
        val fi: FixedInfo = fileInputInfo.asInstanceOf[FixedInfo]
        val files = filePaths.mkString(",")
        val fc = new FixedConverter(fi.header, fi.footer, fi.charSet, fi.newLine, fi.newLineCode,
          fi.withIndex, fi.preFilter, if (fi.withBinaryRecord.isEmpty) None else Some(fi.withBinaryRecord))
        if (fi.recordLengthCheck) {
          fc.recordErrorCheckAndMakeInputDf(len, names, domains, files)
        } else {
          fc.makeInputDf(len, names, domains, files)
        }
      }
    }
    if (mkEmptyDfWhenFileNotExists && !FileCtl.exists(filePaths.mkString(","))) {
      context.createDataFrame(context.emptyDataFrame.rdd, Converter.makeSchema(names))
    } else {
      makeDf
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.fileConv

import org.apache.spark.sql.Row

import java.io.BufferedInputStream

import scala.collection.mutable.ArrayBuffer
import scala.collection.JavaConversions._

import spark.common.SparkContexts._
import d2k.common.df.FileInputInfoBase._
import org.apache.spark.sql.types._

class FixedConverter(hasHeader: Boolean = false, hasFooter: Boolean = false, charEnc: String = "MS932", lineBreak: Boolean = true, newLineCode: NewLineCode = LF,
                     withIndex: Boolean = false, preFilter: (Seq[String], Map[String, String] => Boolean) = null,
                     withBinaryRecord: Option[String] = None) extends Serializable {
  import Converter._

  def makeSliceLen(len: Seq[Int]) = len.foldLeft((0, List.empty[(Int, Int)])) { (l, r) => (l._1 + r, l._2 :+ (l._1, l._1 + r)) }

  def cnvFromFixed(names: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)])(inData: Array[Byte]) = {
    val dataAndDomainsAndNames = sliceLen.map { case (start, end) => inData.slice(start, end) }.zip(domains).zip(names)
    val result = Converter.domainConvert(dataAndDomainsAndNames, charEnc)
    Row(result: _*)
  }

  def cnvFromFixedWithIndex(names: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)])(inData: (Array[Byte], Long)) = {
    val dataAndDomainsAndNames = sliceLen.map {
      case (start, end) if start == -1 && end == -1 => inData._2.toString.getBytes
      case (start, end)                             => inData._1.slice(start, end)
    }.zip(domains).zip(names)
    val result = Converter.domainConvert(dataAndDomainsAndNames, charEnc)
    Row(result: _*)
  }

  def cnvFromFixedAddAllData(names: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)])(inData: Array[Byte]) = {
    val dataAndDomainsAndNames = sliceLen.map { case (start, end) => inData.slice(start, end) }.zip(domains).zip(names)
    val result = Seq(new String(inData, charEnc)) ++ Converter.domainConvert(dataAndDomainsAndNames, charEnc)
    Row(result: _*)
  }

  def addLineBreak(totalLen: Int, lineBreak: Boolean) = (lineBreak, newLineCode) match {
    case (false, _)   => totalLen
    case (true, CR)   => totalLen + 1
    case (true, LF)   => totalLen + 1
    case (true, CRLF) => totalLen + 2
  }

  def addIndexColumn(names: Seq[String], nameList: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)]) =
    (names :+ SYSTEM_COLUMN_NAME.RECORD_INDEX,
      nameList :+ SYSTEM_COLUMN_NAME.RECORD_INDEX,
      domains :+ "文字列",
      sliceLen :+ (-1, -1))

  def arrToMap(targetNames: Seq[String], names: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)])(inData: Array[Byte]): Map[String, String] = {
    val dataAndDomainsAndNames = sliceLen.map { case (start, end) => inData.slice(start, end) }.zip(domains).zip(names)
    val targetData = targetNames.flatMap { target =>
      dataAndDomainsAndNames.filter(_._2 == target)
    }
    val converted = Converter.domainConvert(targetData, charEnc)
    targetData.map(_._2).zip(converted).toMap
  }

  def addArr(row: Row, arr: Array[Byte]) = {
    val rowValue = Row(row.toSeq: _*)
    withBinaryRecord.map(_ => Row.merge(rowValue, Row(arr.clone))).getOrElse(rowValue)
  }

  def addStructType(st: StructType) =
    withBinaryRecord.map(n => st.add(StructField(n, BinaryType))).getOrElse(st)

  def makeInputDf(len: Seq[Int], names: Seq[String], domains: Seq[String], filePath: String) = {
    val (totalLen_, sliceLen) = makeSliceLen(len)
    val totalLen = addLineBreak(totalLen_, lineBreak)
    val ziped = names.zip(domains)
    val (nameList, domainList) = ziped.filter { case (names, domain) => !(domain.startsWith(Converter.NOT_USE_PREFIX)) }.unzip

    val rdd = Option(preFilter).map { pf =>
      sc.binaryRecords(filePath, totalLen).flatMap { arr =>
        if (pf._2(arrToMap(pf._1, names, domains, sliceLen)(arr))) Some(arr) else None
      }
    }.getOrElse(sc.binaryRecords(filePath, totalLen))
    val df = if (withIndex) {
      if (charEnc == "JEF") { throw new IllegalArgumentException("JEF CharEnc is not supportted") }
      val rddWithIdx = rdd.zipWithIndex
      val (namesWithIdx, nameListWithIdx, domainsWithIdx, sliceLenWithIdx) = addIndexColumn(names, nameList, domains, sliceLen)
      context.createDataFrame(rddWithIdx.map {
        case (arr, long) => addArr(cnvFromFixedWithIndex(namesWithIdx, domainsWithIdx, sliceLenWithIdx)(arr, long), arr)
      }, addStructType(Converter.makeSchema(nameListWithIdx)))
    } else {
      context.createDataFrame(rdd.map(arr =>
        addArr(cnvFromFixed(names, domains, sliceLen)(arr), arr)), addStructType(Converter.makeSchema(nameList)))
    }
    Converter.removeHeaderAndFooter(df, hasHeader, hasFooter, names, domains)
  }

  import scala.collection.JavaConversions._

  case class Result(buf: Array[Byte] = Array.empty[Byte], output: Array[Array[Byte]] = Array.empty[Array[Byte]], len: Int = 0)
  def recordErrorCheckAndMakeInputDf(len: Seq[Int], names: Seq[String], domains: Seq[String], filePath: String) = {
    if (charEnc == "JEF") { throw new IllegalArgumentException("JEF CharEnc is not supportted") }
    if (!lineBreak) { throw new IllegalArgumentException("must be lineBreak = true when recordLengthCheck = true") }
    val (totalLen_, sliceLen) = makeSliceLen(len)
    val totalLen = addLineBreak(totalLen_, lineBreak)
    val ziped = names.zip(domains)
    val (nameList, domainList) = ziped.filter { case (names, domain) => !(domain.startsWith(Converter.NOT_USE_PREFIX)) }.unzip

    val splitByLineBreak = {
      sc.binaryFiles(filePath).flatMap {
        case (_, pds) => {
          var buff = new Array[Byte](1)
          var resultArr = ArrayBuffer[Array[Byte]]()
          var tmpArr = ArrayBuffer[Byte]()
          val br = new BufferedInputStream(pds.open())
          while (br.read(buff) != -1) {
            val byte = buff(0)
            if (byte == '\n' || byte == '\r') {
              resultArr += tmpArr.toArray
              tmpArr = ArrayBuffer[Byte]()
            } else {
              tmpArr += byte
            }
          }
          Option(preFilter).map { pf =>
            resultArr.flatMap { arr =>
              if (pf._2(arrToMap(pf._1, names, domains, sliceLen)(arr))) Some(arr) else None
            }
          }.getOrElse(resultArr)
        }
      }
    }

    val df = if (withIndex) {
      val (namesWithIdx, nameListWithIdx, domainsWithIdx, sliceLenWithIdx) = addIndexColumn(names, nameList, domains, sliceLen)
      val r = splitByLineBreak.zipWithIndex.map {
        case (arr, idx) =>
          val row = cnvFromFixedWithIndex(namesWithIdx, domainsWithIdx, sliceLenWithIdx)(arr, idx)
          addArr(Row(row.toSeq :+ (arr.size != totalLen_).toString: _*), arr)
      }
      context.createDataFrame(r, addStructType(Converter.makeSchemaWithRecordError(nameListWithIdx)))
    } else {
      val r = splitByLineBreak.map { arr =>
        val row = cnvFromFixed(names, domains, sliceLen)(arr)
        addArr(Row(row.toSeq :+ (arr.size != totalLen_).toString: _*), arr)
      }
      context.createDataFrame(r, addStructType(Converter.makeSchemaWithRecordError(nameList)))
    }
    Converter.removeHeaderAndFooter(df, hasHeader, hasFooter, names, domains)
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.fileConv

import org.apache.spark.sql.Row
import java.net.URI
import java.io.InputStreamReader
import java.io.FileInputStream
import java.io.File
import com.univocity.parsers.csv.CsvParserSettings
import com.univocity.parsers.csv.CsvParser
import com.univocity.parsers.tsv.TsvParserSettings
import com.univocity.parsers.tsv.TsvParser

import scala.collection.JavaConversions._
import spark.common.SparkContexts._
import spark.common.SparkContexts
import org.apache.spark.SparkContext

class TextConverter(hasHeader: Boolean = false, charEnc: String = "MS932") extends Serializable {
  def makeDf(options: Map[String, String])(names: Seq[String], domains: Seq[String], path: Set[String]) = {
    val rdd = SparkContexts.context.read.options(options).csv(path.toSeq: _*).rdd.map { row =>
      val dataAndDomainsAndNames = row.toSeq.map(s => Option(s).map(_.toString).getOrElse("")).zip(domains).zip(names)
      Row(Converter.domainConvert(dataAndDomainsAndNames): _*)
    }
    val ziped = names.zip(domains)
    val (nameList, domainList) = ziped.filter { case (names, domain) => !(domain.startsWith(Converter.NOT_USE_PREFIX) || domain.startsWith(Converter.REC_DIV_PREFIX)) }.unzip
    context.createDataFrame(rdd, Converter.makeSchema(nameList))
  }

  val defaultOptions = Seq(("encoding", charEnc), ("header", hasHeader.toString))
  def tsv = makeDf((defaultOptions :+ ("delimiter", "\t")).toMap) _
  val csv = makeDf(defaultOptions.toMap) _
  val vsv = makeDf((defaultOptions :+ ("delimiter", "|")).toMap) _
  val ssv = makeDf((defaultOptions :+ ("delimiter", " ")).toMap) _
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import org.joda.time.format.DateTimeFormat
import java.sql.Date
import scala.io.Source
import scala.util.Try
import spark.common.DbInfo
import d2k.common.df.DbConnectionInfo
import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils
import java.sql.Timestamp
import java.util.Properties
import org.apache.spark.sql._
import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions
import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap

case class InputArgs(d2kBasePath: String, productionMode: String,
                     confPath: String, dataPath: String,
                     projectId: String, processId: String, applicationId: String,
                     runningDateFileFullPath: String,
                     confBasePath:            String,

                     fileConvInputFile:    String,
                     fileConvOutputFile:   String,
                     baseInputFilePath:    String,
                     baseOutputFilePath:   String,
                     baseErrProofFilePath: String,
                     inputRunningDates:    Array[String],
                     inRunningSqlDate:     Date,
                     isDebug:              Boolean       = false) extends {
  val runningDates = inputRunningDates
  val runningSQLDate = inRunningSqlDate
} with ResourceDates with ResourceEnvs {
  val tableNameMapper: Map[String, String] = {
    Try {
      val mapperPath = sys.env("TABLE_MAPPER_FILE_PATH")
      if (mapperPath.isEmpty) {
        Map.empty[String, String]
      } else {
        Source.fromFile(mapperPath).getLines.map { line =>
          val splited = line.split("\t")
          (splited(0), splited(1))
        }.toMap
      }
    }.recover { case t => Map.empty[String, String] }.get
  }

  case class LastUpdateTime(from: Timestamp, to: Timestamp)
  def lastUpdateTime(tableName: String, readDbInfo: DbInfo = DbConnectionInfo.bat1) = {
    val options = new JDBCOptions(Map(
      JDBCOptions.JDBC_URL -> readDbInfo.url,
      JDBCOptions.JDBC_TABLE_NAME -> tableName,
      "user" -> readDbInfo.user,
      "password" -> readDbInfo.password,
      "charSet" -> readDbInfo.charSet))

    val ps = JdbcUtils.createConnectionFactory(options)()
      .prepareStatement("select DT_FROMUPDYMDTM, DT_TOUPDYMDTM from MOP012 where ID_TBLID = ?")
    ps.setString(1, tableName)
    val rs = ps.executeQuery

    val result = try {
      Iterator.continually((rs.next, rs)).takeWhile(_._1).map {
        case (_, rec) => LastUpdateTime(rec.getTimestamp("DT_FROMUPDYMDTM"), rec.getTimestamp("DT_TOUPDYMDTM"))
      }.toSeq
    } finally {
      rs.close
    }

    result.headOption.getOrElse(throw new IllegalArgumentException(s"tableName is not defined[$tableName]"))
  }
}

object InputArgs {
  def apply(d2kBasePath: String, productionMode: String, confPath: String, dataPath: String,
            projectId: String, processId: String, applicationId: String,
            runningDateFileFullPath: String) = {
    val confBasePath = s"$d2kBasePath/$productionMode/$confPath"
    val fileConvInputFile = s"$confBasePath/import/${projectId}_app.conf"
    val fileConvOutputFile = s"$confBasePath/export/${projectId}_app.conf"
    val baseInputFilePath = s"$d2kBasePath/$productionMode/$dataPath/output"
    val baseOutputFilePath = s"$d2kBasePath/$productionMode/$dataPath/output"
    val baseErrProofFilePath = s"$d2kBasePath/$productionMode/$dataPath/error"

    val runningDates = Source.fromFile(runningDateFileFullPath).getLines.toList(1).split(" ")
    val dateFormat = DateTimeFormat.forPattern("yyyyMMdd")
    val runningSQLDate = new Date(dateFormat.withZoneUTC.parseDateTime(runningDates(0)).getMillis)

    new InputArgs(d2kBasePath, productionMode, confPath, dataPath,
      projectId, processId, applicationId,
      runningDateFileFullPath, confBasePath, fileConvInputFile, fileConvOutputFile,
      baseInputFilePath, baseOutputFilePath, baseErrProofFilePath,
      runningDates, runningSQLDate)
  }

  def apply(projectId: String, processId: String, applicationId: String, runningDateFileFullPath: String): InputArgs = {
    apply("/D2Khome", "HN", "APL/conf/spark", "sparkWK/Parquet", projectId, processId, applicationId, runningDateFileFullPath)
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import scala.io.Source
import scala.util.Try

object JefConverter {
  object implicits {
    implicit class JefConvert(s: String) {
      def toJefHalf: Array[Byte] = convUtfToJefHalf(s)
      def toJefFull: Array[Byte] = convUtfToJefFull(s)
    }
  }

  object controlCodes {
    val nullCode = "00"
    val tab = "05"
    val kanjiOut = "28"
    val kanjiIn9 = "29"
    val kanjiIn12 = "38"
  }

  def readData(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).getLines
    .map { line =>
      val kv = line.split('\t')
      (kv(0), kv(1))
    }.toMap

  import controlCodes._
  def addControlCode(origin: Map[String, String]) =
    origin ++ Map(tab -> "\t", kanjiOut -> "", kanjiIn9 -> "", kanjiIn12 -> "")
  lazy val jefToUtfHalfData = addControlCode(readData("jefUtf8ConvHalf.tsv"))
  lazy val jefToUtfFullData = readData("jefUtf8ConvFull.tsv")
  lazy val jefToUtfFullKddiData = readData("jefUtf8ConvFull_kddi.tsv")
  lazy val utfToJefHalfData = (jefToUtfHalfData - nullCode - kanjiOut - kanjiIn9 - kanjiIn12)
    .toSeq.map(x => (x._2.toCharArray.headOption.getOrElse(""), x._1)).toMap
  lazy val utfToJefFullData = jefToUtfFullData.toSeq.map(x => (x._2.toCharArray.head, x._1)).toMap

  def isJefHalf(domain: String, charEnc: String) = charEnc == "JEF" && !domain.startsWith("全角文字列")
  def isJefFull(domain: String, charEnc: String) = charEnc == "JEF" && domain.startsWith("全角文字列")

  def convJefToUtfHalf(data: Array[Byte]): String = {
    val conved = data.map { byte => (byte, Try(jefToUtfHalfData(f"$byte%02X"))) }
    if (!conved.map(_._1).forall(_ == 0x00)) printErrorHalf(conved)
    conved.map(_._2.getOrElse("*")).mkString
  }

  private[this] def printErrorHalf(data: Array[(Byte, Try[String])]) =
    data.foreach {
      case (org, conv) => if (conv.isFailure == true) println(f"!!!![JEF CONV ERROR:HALF]$org%02X")
    }

  def convJefToUtfFull(data: Array[Byte]): String = {
    data.grouped(2).map { byteArr =>
      val jefCode = byteArr.map(x => f"$x%02X").mkString
      jefToUtfFullKddiData.get(jefCode).orElse {
        jefToUtfFullData.get(jefCode)
      }.getOrElse {
        println(s"!!!![JEF CONV ERROR:FULL]${byteArr.map(x => f"$x%02X").mkString}")
        "■"
      }
    }.mkString
  }

  def convUtfToJefHalf(data: String): Array[Byte] =
    data.map(x => Try(utfToJefHalfData(x)).getOrElse("5C")).map(x => Integer.parseInt(x.toString, 16)).map(_.toByte).toArray

  def convUtfToJefFull(data: String): Array[Byte] = {
    data.flatMap(x => Try(utfToJefFullData(x)).getOrElse("A2A3"))
      .grouped(2).map(x => Integer.parseInt(x.toString, 16)).map(_.toByte).toArray
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import scala.io.Source

object KanaConverter extends Serializable {
  private[this] val charEnc = "MS932"

  private[this] def readData(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(s"kanaConv/$fileName"), charEnc).getLines

  private[this] val cnvOrg = " " :: readData("kanaConv.org").toList
  private[this] val cnvOrgSelect = " " :: readData("kanaConvSearch.org").toList
  private[this] val converted = " " :: readData("kanaConv.converted").toList
  private[this] val convertedSelect = " " :: readData("kanaConvSearch.converted").toList
  private[this] val zenkakuCnv = (inStr: String) => { if (inStr.getBytes(charEnc).size >= 2) { "*" } else { inStr } }
  private[this] val cnvMap = cnvOrg.zip(converted).toMap
  private[this] val cnvMapSelect = cnvOrgSelect.zip(convertedSelect).toMap

  def apply(inStr: String) = { if (inStr != null) { inStr.map(c => cnvMap.getOrElse(c.toString, zenkakuCnv(c.toString))).mkString } else { inStr } }
  def select(inStr: String) = { if (inStr != null) { inStr.map(c => cnvMapSelect.getOrElse(c.toString, zenkakuCnv(c.toString))).mkString } else { inStr } }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import org.slf4j.LoggerFactory

trait Logging extends Serializable {
  val logger = LoggerFactory.getLogger(this.getClass)

  def platformError(t: Throwable) = logger.error("[PLATFORM]", t)
  def appError(t: Throwable) = logger.error("[APP]", t)

  def isDebugEnabled = logger.isDebugEnabled

  def elapse(message: String)(func: => Unit) = {
    logger.info(s" Start[${message}]")
    val startTime = System.currentTimeMillis
    func
    val endTime = System.currentTimeMillis
    val elapse = BigDecimal(endTime - startTime) / 1000
    logger.info(f"finish[${message}] elapse:${elapse}%,.3fs")
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import java.sql.Timestamp
import java.time.LocalDateTime
import java.sql.Date

object MakeDate {
  def timestamp_yyyyMMdd(str: String) =
    Timestamp.valueOf(LocalDateTime.of(str.take(4).toInt, str.drop(4).take(2).toInt, str.drop(6).take(2).toInt, 0, 0, 0))

  def timestamp_yyyyMMddhhmmss(str: String) =
    Timestamp.valueOf(LocalDateTime.of(
      str.take(4).toInt, str.drop(4).take(2).toInt, str.drop(6).take(2).toInt,
      str.drop(8).take(2).toInt, str.drop(10).take(2).toInt, str.drop(12).take(2).toInt))

  def timestamp_yyyyMMddhhmmssSSS(str: String) =
    Timestamp.valueOf(LocalDateTime.of(
      str.take(4).toInt, str.drop(4).take(2).toInt, str.drop(6).take(2).toInt,
      str.drop(8).take(2).toInt, str.drop(10).take(2).toInt, str.drop(12).take(2).toInt, str.drop(14).toInt * 1000000))

  def date_yyyyMMdd(str: String) = new Date(timestamp_yyyyMMdd(str).getTime)

  object implicits {
    implicit class MakeDate(str: String) {
      def toDt = new Date(str.toTm.getTime)
      def toTm = {
        str.size match {
          case 8 => timestamp_yyyymmdd(str.take(4).toInt, str.drop(4).take(2).toInt, str.drop(6).take(2).toInt)
          case 14 => Timestamp.valueOf(LocalDateTime.of(
            str.take(4).toInt, str.drop(4).take(2).toInt, str.drop(6).take(2).toInt,
            str.drop(8).take(2).toInt, str.drop(10).take(2).toInt, str.drop(12).take(2).toInt))
        }
      }
    }
  }

  def timestamp_yyyymmdd(yyyy: Int, mm: Int, dd: Int) =
    Timestamp.valueOf(LocalDateTime.of(yyyy, mm, dd, 0, 0, 0))

  val date_00010101 = LocalDateTime.of(1, 1, 1, 0, 0, 0)
  val date_99991231 = LocalDateTime.of(9999, 12, 31, 0, 0, 0)
  val timestamp_00010101 = Timestamp.valueOf(LocalDateTime.of(1, 1, 1, 0, 0, 0))
  val timestamp_00010102 = Timestamp.valueOf(LocalDateTime.of(1, 1, 2, 0, 0, 0))
  val timestamp_00010103 = Timestamp.valueOf(LocalDateTime.of(1, 1, 3, 0, 0, 0))
  val timestamp_00010104 = Timestamp.valueOf(LocalDateTime.of(1, 1, 4, 0, 0, 0))
  val timestamp_00010105 = Timestamp.valueOf(LocalDateTime.of(1, 1, 5, 0, 0, 0))
  val timestamp_00010106 = Timestamp.valueOf(LocalDateTime.of(1, 1, 6, 0, 0, 0))
  val timestamp_00010107 = Timestamp.valueOf(LocalDateTime.of(1, 1, 7, 0, 0, 0))
  val timestamp_99991231 = Timestamp.valueOf(date_99991231)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import java.sql.Timestamp
import java.time.LocalDateTime
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.Row
import spark.common.SparkContexts
import d2k.common.fileConv.Converter
import d2k.common.fileConv.ConfParser
import d2k.common.fileConv.ItemConf
import spark.common.FileCtl
import d2k.common.fileConv.DomainProcessor
import scala.io.Source
import SparkContexts.context.implicits._
import org.apache.spark.sql.types._

import scala.collection.JavaConverters._
import spark.common.PqCtl
import spark.common.DbInfo
import d2k.common.df.DbConnectionInfo
import spark.common.DbCtl
import org.apache.spark.sql.SaveMode
import scala.util.Try
import org.scalatest.MustMatchers._
import java.io.FileNotFoundException
import org.apache.spark.sql.Column
import java.sql.Date
import d2k.common.JefConverter.implicits.JefConvert
import java.io.FileOutputStream
import scala.io.BufferedSource
import org.slf4j.LoggerFactory

case class TableInfo(itemName: String, itemId: String, length: String, cnvType: String, data: String, maxLen: Int)

object MakeResource {
  val logger = LoggerFactory.getLogger(this.getClass)
  def apply = new MakeResource("")

  def makeBdField(len: String, name: String) = {
    val (p, s) = if (len.trim.isEmpty) {
      (10, 0)
    } else {
      val ps = len.trim.split(',')
      if (ps.size == 2) {
        (ps(0).toInt, ps(1).toInt)
      } else {
        (len.toInt, 0)
      }
    }
    StructField(name, DecimalType(p, s), true)
  }

  def itemConfToTable(confPath: String) = {
    val confs = parseItemConf(confPath)
    val tableInfos = confs.map { c =>
      val data = DomainProcessor.exec(c.cnvType, "x" * c.length.toInt).right.get.mkString("\"", "", "\"")
      val ml = Seq(toLength(c.itemName), toLength(c.itemId), toLength(c.length), toLength(c.cnvType), toLength(data)).max
      val toMl = toMaxLength(ml) _
      TableInfo(toMl(c.itemName), toMl(c.itemId), toMl(c.length), toMl(c.cnvType), toMl(data), ml)
    }
    tableInfoToTableStr(tableInfos)
  }

  def itemConfMdToTable(url: String) = {
    val md = Source.fromURL(s"${url}?private_token=${sys.env("GITLAB_TOKEN")}").getLines.toList.drop(6)
    val tableInfos = md.map { line =>
      val items = line.split('|').drop(1).map(_.trim)
      val (id, name, domain, length) = (items(0), items(1), items(2), items(3))
      val data = DomainProcessor.exec(domain, "x" * length.toInt).right.get.mkString("\"", "", "\"")
      val ml = Seq(toLength(name), toLength(id), toLength(length), toLength(domain), toLength(data)).max
      val toMl = toMaxLength(ml) _
      TableInfo(toMl(name), toMl(id), toMl(length), toMl(domain), toMl(data), ml)
    }
    tableInfoToTableStr(tableInfos)
  }

  def itemsMdToTable(url: String) = {
    def tableInfos(md: BufferedSource) = md.getLines.toList.drop(7).filter(!_.isEmpty).map { line =>
      val items = line.split('|').drop(1).map(_.trim)
      val (id, name, domain, length) = (items(0), items(1), items(2), items(3))
      val data = (domain.toLowerCase match {
        case "string" | "varchar2" | "char"      => "x" * length.toInt
        case "timestamp" | "日付時刻"                => "00010101000000"
        case "date"                              => "00010101"
        case "bigdecimal" | "number" | "decimal" => "0"
        case _                                   => DomainProcessor.exec(domain, "x" * length.toInt).right.get
      }).mkString("\"", "", "\"")
      val ml = Seq(toLength(name), toLength(id), toLength(length), toLength(domain), toLength(data)).max
      val toMl = toMaxLength(ml) _
      TableInfo(toMl(name), toMl(id), toMl(length), toMl(domain), toMl(data), ml)
    }
    Try { Source.fromURL(s"${url}?private_token=${sys.env("GITLAB_TOKEN")}") }
      .map(md => tableInfoToTableStr(tableInfos(md))).toOption.orElse {
        System.err.println(s"Specific is not found[$url]")
        None
      }
  }

  def makeSchema(colNames: Seq[String], domains: Seq[String], colLengths: Seq[String]) =
    StructType(colNames.zip(colLengths).zip(domains.map(_.toLowerCase)).map {
      case ((name, _), "string")       => StructField(name, StringType, true)
      case ((name, _), "varchar2")     => StructField(name, StringType, true)
      case ((name, _), "char")         => StructField(name, StringType, true)
      case ((name, _), "date")         => StructField(name, DateType, true)
      case ((name, _), "timestamp")    => StructField(name, TimestampType, true)
      case ((name, _), "日付時刻")         => StructField(name, TimestampType, true)
      case ((name, _), "int")          => StructField(name, IntegerType, true)
      case ((name, _), "integer")      => StructField(name, IntegerType, true)
      case ((name, len), "bigdecimal") => makeBdField(len, name)
      case ((name, len), "numeric")    => makeBdField(len, name)
      case ((name, len), "decimal")    => makeBdField(len, name)
      case ((name, len), "number")     => makeBdField(len, name)
      case ((name, _), _)              => StructField(name, StringType, true)
    })

  protected def toLength(str: String) = str.getBytes("MS932").length
  protected def toMaxLength(maxLen: Int)(str: String) = {
    val addLen = maxLen - str.getBytes("MS932").length
    str + (" " * addLen)
  }
  protected def tableColToList(cols: String, defaultValue: String = "") =
    cols.split('|').map { col =>
      val value = col.trim
      if (value.isEmpty) defaultValue else value
    }.toList
  protected def removeDq(target: String) =
    "^\"(.*)\"$".r.findFirstMatchIn(target).map(_.group(1).replace("\"\"", "\"")).getOrElse(target)
  protected def toTableCol(strSeq: Seq[String]) = strSeq.mkString("| ", " | ", " |")
  protected def tableInfoToTableStr = (tableInfos: Seq[d2k.common.TableInfo]) => {
    val names = tableInfos.map(_.itemName)
    val ids = tableInfos.map(_.itemId)
    val lengths = tableInfos.map(_.length)
    val domains = tableInfos.map(_.cnvType)
    val dataList = tableInfos.map(_.data)
    val tableSep = tableInfos.map("-" * _.maxLen)
    Seq(ids, tableSep, names, lengths, domains, dataList).map(toTableCol).mkString("\n")
  }

  protected def parseItemConf(confPath: String) = {
    ConfParser.readConf(confPath) { items =>
      if (items.size == 5) {
        ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true")
      } else {
        ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true", items(5))
      }
    }.toSeq
  }
}

case class MakeResource(outputPath: String, readMdPath: String = "test/markdown") {
  import MakeResource._
  type SortKeys = Option[DataFrame => Seq[Column]]

  def readMdTable(path: String): MdInfo = {
    println(s"read:${readMdPath}/${path}")
    MdInfo(Source.fromFile(s"${readMdPath}/${path}").getLines.mkString("\n"))
  }

  case class MdInfo(data: String) {
    def toCsv(writeName: String, wrapDoubleQuote: Boolean = false,
              hasHeader: Boolean = false, lineSeparator: String = "\n") =
      toVariable(",")(writeName, wrapDoubleQuote, hasHeader, lineSeparator)

    def toTsv(writeName: String, wrapDoubleQuote: Boolean = false,
              hasHeader: Boolean = false, lineSeparator: String = "\n") =
      toVariable("\t")(writeName, wrapDoubleQuote, hasHeader, lineSeparator)

    def toVsv(writeName: String, wrapDoubleQuote: Boolean = false,
              hasHeader: Boolean = false, lineSeparator: String = "\n") =
      toVariable("|")(writeName, wrapDoubleQuote, hasHeader, lineSeparator)

    def toSsv(writeName: String, wrapDoubleQuote: Boolean = false,
              hasHeader: Boolean = false, lineSeparator: String = "\n") =
      toVariable(" ")(writeName, wrapDoubleQuote, hasHeader, lineSeparator)

    private[this] def toVariable(separator: String)(
      writeName: String, wrapDoubleQuote: Boolean, hasHeader: Boolean, lineSeparator: String) = {
      val allData = data.stripMargin.split("\n").drop(1)
      val itemNames = tableColToList(allData(0))
      val writeData = allData.drop(5).map(tableColToList(_).map(
        x => if (wrapDoubleQuote) x else removeDq(x)).mkString(separator))
      System.setProperty("line.separator", lineSeparator)
      FileCtl.writeToFile(s"$outputPath/$writeName", false) { pw =>
        if (hasHeader) pw.println(itemNames.mkString(separator))
        writeData.foreach(pw.println)
      }
    }

    def toFixed(writeName: String, hasHeader: Boolean = false, hasFooter: Boolean = false,
                lineBreak: Boolean = true, lineSeparator: String = "\n", charEnc: String = "MS932") =
      writeBinData(writeName, lineSeparator) { (data: String, len: String, dataType: String) =>
        dataType match {
          case x if x.endsWith("_PD") => pack(BigDecimal(data).toBigInt, BigDecimal(len).toInt)
          case x if x.endsWith("_ZD") => zone(BigDecimal(data).toBigInt, BigDecimal(len).toInt)
          case "数字"                   => s"%0${len}d".format(data.toInt).getBytes(charEnc)
          case "数字_SIGNED"            => s"%+0${len}d".format(data.toInt).getBytes(charEnc)
          case "未使用"                  => (" " * BigDecimal(len).toInt).getBytes(charEnc)
          case _                      => data.padTo(len.toInt, ' ').getBytes(charEnc)
        }
      }

    def toJef(writeName: String, lineSeparator: String = "\n") =
      writeBinData(writeName, lineSeparator) { (data: String, len: String, dataType: String) =>
        dataType match {
          case x if x.endsWith("_PD") => pack(BigDecimal(data).toBigInt, BigDecimal(len).toInt)
          case x if x.endsWith("_ZD") => zone(BigDecimal(data).toBigInt, BigDecimal(len).toInt)
          case "全角文字列"                => JefConvert(data).toJefFull
          case "未使用"                  => JefConvert(" " * BigDecimal(len).toInt).toJefHalf
          case _                      => JefConvert(data).toJefHalf
        }
      }

    private[this] def writeBinData(writeName: String, lineSeparator: String)(func: (String, String, String) => Array[Byte]) = {
      val allData = data.stripMargin.split("\n")
      val itemNames = tableColToList(allData(1))
      val itemTypes = tableColToList(allData(5))
      val itemLengths = tableColToList(allData(4))
      val outData = allData.drop(6).map {
        tableColToList(_).zip(itemLengths).zip(itemTypes).map {
          case ((data, len), types) => func(removeDq(data), len.split(',').head.trim, types)
        }.foldLeft(Array[Byte]())(_ ++: _)
      }
      writeBytes(s"$outputPath/$writeName")(outData)
    }

    private[this] def zone(target: BigInt, zonedByteLen: Int) = {
      val sign = if (target < 0) { 'd' } else { 'f' }
      val targetStr = String.valueOf(target.abs)
      val pad = "0" * (zonedByteLen - targetStr.length())
      val targetBytes = (pad + targetStr).init.map { char => Integer.parseInt(s"f${char}", 16).toByte }
      (targetBytes :+ Integer.parseInt(s"f${sign}${targetStr.last}", 16).toByte).toArray
    }

    private[this] def pack(target: BigInt, packedByteLen: Int) = {
      val sign = if (target < 0) { 'd' } else { 'f' }
      val targetStr = (String.valueOf(target.abs)) + sign
      val pad = "0" * ((packedByteLen * 2) - targetStr.length())
      val targetBytes = (pad + targetStr).grouped(2).map { hexChar => Integer.parseInt(hexChar, 16).toByte }
      targetBytes.toArray
    }

    private[this] def writeBytes(
      path: String, lineBreak: Boolean = false, newLineCode: String = "\n")(contents: Array[Array[Byte]]) = {
      val out = new FileOutputStream(path)
      contents.foreach { arr =>
        out.write(arr)
        if (lineBreak) out.write(newLineCode.toCharArray.map(_.toByte))
      }
      out.close
    }

    def toDf = {
      val dataTable = data.stripMargin.split("\n").drop(1)
      val colNames = tableColToList(dataTable(0))
      val colLengths = tableColToList(dataTable(3), "10")
      val domains = tableColToList(dataTable(4), "String").map(_.toLowerCase)
      val strDataList = dataTable.drop(5).map(tableColToList(_).map(removeDq))
      val schema = MakeResource.makeSchema(colNames, domains, colLengths)
      val rows = strDataList.map { strData =>
        val items = strData.zip(domains).map { x =>
          val (data, domain) = x

          if (data.toLowerCase == "null") null else {
            import MakeDate.implicits._
            domain.toLowerCase match {
              case "string" | "varchar2" | "char" | "文字列" => data
              case "date" => data.toDt
              case "timestamp" | "日付時刻" => data.toTm
              case "int" | "bigdecimal" | "numeric" | "decimal" | "number" => new java.math.BigDecimal(data)
            }
          }
        }
        Row(items: _*)
      }
      val df = SparkContexts.context.createDataFrame(rows.toList.asJava, schema)
      df.show
      df
    }

    def toPq(name: String): Unit = toPq(new PqCtl(outputPath), name)

    def toPq(pqCtl: PqCtl, name: String) = {
      import pqCtl.implicits._
      toDf.writeParquet(name)
    }

    def toDb(tableName: String, dbInfo: DbInfo = DbConnectionInfo.bat1): Unit = toDb(new DbCtl(dbInfo), tableName)

    def toDb(dbCtl: DbCtl, tableName: String) = {
      import dbCtl.implicits._
      Try { dbCtl.dropTable(tableName) }
      toDf.writeTableStandard(tableName, SaveMode.Append)
    }

    def checkCsv(filePath: String, hasHeader: Boolean = false) = checkVariable(",")(filePath, hasHeader)
    def checkTsv(filePath: String, hasHeader: Boolean = false) = checkVariable("\t")(filePath, hasHeader)

    private[this] def checkVariable(separator: String)(filePath: String, hasHeader: Boolean = false) = {
      val fileList = Source.fromFile(filePath).getLines.toList
      val target = (if (hasHeader) fileList.drop(1) else fileList)
      val expect = toDf.collect
      target.zip(expect.zipWithIndex).map {
        case (t, (e, idx)) =>
          val splitted = t.split(separator).map(removeDq)
          val fieldNames = e.schema.fieldNames
          (0 until splitted.length).foreach { pos =>
            withClue((s"LineNo:${idx + 7}", fieldNames(pos))) {
              splitted(pos).toString mustBe e(pos).toString
            }
          }
      }
    }

    def checkFixed(filePath: String, hasHeader: Boolean = false, hasFooter: Boolean = false) = {
      val fileList = Source.fromFile(filePath).getLines.toList
      val headerChecked = if (hasHeader) fileList.drop(1) else fileList
      val target = (hasHeader, hasFooter) match {
        case (true, true)   => fileList.drop(1).dropRight(1)
        case (true, false)  => fileList.drop(1)
        case (false, true)  => fileList.dropRight(1)
        case (false, false) => fileList
      }

      val dataTable = data.stripMargin.split("\n").drop(1)
      val colLengths = tableColToList(dataTable(3)).map(_.toInt)

      val expect = toDf.collect
      target.zip(expect.zipWithIndex).map {
        case (t, (e, idx)) =>
          val splitted = colLengths.foldLeft((t.getBytes("MS932"), Seq.empty[String])) { (l, r) =>
            (l._1.drop(r), l._2 :+ new String(l._1.take(r), "MS932"))
          }._2
          val fieldNames = e.schema.fieldNames
          (0 until splitted.length).foreach { pos =>
            withClue((s"LineNo:${idx + 7}", fieldNames(pos))) {
              splitted(pos).toString mustBe e(pos).toString
            }
          }
      }
    }

    def sortDf(target: DataFrame, expect: DataFrame, sortKeys: Seq[String]) = {
      target.show
      val result = if (!sortKeys.isEmpty) {
        (target.sort(sortKeys.head, sortKeys.tail: _*), expect.sort(sortKeys.head, sortKeys.tail: _*))
      } else { (target, expect) }
      val expectCollect = result._2.collect
      result._1.show(expectCollect.size, false)
      testingRows(result._1.collect, expect.rdd.zipWithIndex.sortBy(x => sortKeys.map(k => x._1.getAs[String](k)).mkString).collect)
    }

    def checkPq(name: String): Unit = checkPq(new PqCtl(outputPath), name, Seq.empty[String])

    def checkPq(name: String, sortKeys: Seq[String]): Unit = checkPq(new PqCtl(outputPath), name, sortKeys)

    def checkPq(pqCtl: PqCtl, name: String, sortKeys: Seq[String] = Seq.empty[String]) = sortDf(pqCtl.readParquet(name), toDf, sortKeys)

    def checkDb(tableName: String, sortKeys: Seq[String] = Seq.empty[String], dbInfo: DbInfo = DbConnectionInfo.bat1): Unit =
      checkDb(new DbCtl(dbInfo), tableName, sortKeys)

    def checkDb(dbCtl: DbCtl, tableName: String, sortKeys: Seq[String]) = sortDf(dbCtl.readTable(tableName), toDf, sortKeys)

    def checkDf(df: DataFrame, lineOffset: Int = 0) = testingRows(df.collect, toDf.rdd.zipWithIndex.collect, lineOffset)

    private[this] def testingRows(target: Array[Row], expect: Array[(Row, Long)], lineOffset: Int = 0) = {
      val fieldNames = expect.headOption.map(_._1.schema.fieldNames).getOrElse(Array.empty[String])
      val expectTypes = expect.headOption.map(_._1.schema.fields.map(_.dataType.toString)).getOrElse(Array.empty[String])
      val targetTypes = target.headOption.map(_.schema.fields.map(x => (x.name, x.dataType.toString)).toMap).getOrElse(Map.empty[String, String])
      val systemItems = Seq("DT_D2KMKDTTM", "ID_D2KMKUSR", "DT_D2KUPDDTTM", "ID_D2KUPDUSR", "NM_D2KUPDTMS", "FG_D2KDELFLG")
      if (target.isEmpty) logger.warn("Target Data is Empty")
      target.zip(expect).map {
        case (t, (e, idx)) =>
          (0 until e.length).foreach { pos =>
            val name = fieldNames(pos)
            if (!systemItems.contains(name)) {
              withClue((s"LineNo:${idx + 7 + lineOffset}", name)) {
                expectTypes(pos) match {
                  case "DateType" =>
                    t.getAs[Any](name).toString.replaceAll("-", "").take(8) mustBe e.getAs[Any](name).toString.replaceAll("-", "").take(8)
                  case "TimestampType" =>
                    t.getAs[Timestamp](name).toString.replaceAll("[-:\\s\\.]", "").take(14) mustBe e.getAs[Timestamp](name).toString.replaceAll("[-:\\s\\.]", "").take(14)
                  case "IntegerType" =>
                    val targetData = if (targetTypes(name).startsWith("Integer")) {
                      t.getAs[Integer](name)
                    } else {
                      t.getAs[java.math.BigDecimal](name)
                    }
                    Option(targetData).map(_.toString).getOrElse("null") mustBe Option(e.getAs[Integer](name)).map(_.toString).getOrElse("null")
                  case typ if typ.startsWith("DecimalType") => {
                    val targetData = if (targetTypes(name).startsWith("Integer")) {
                      t.getAs[Integer](name)
                    } else {
                      t.getAs[java.math.BigDecimal](name)
                    }
                    Option(targetData).map(_.toString).getOrElse("null") mustBe Option(e.getAs[java.math.BigDecimal](name)).map(_.toString).getOrElse("null")
                  }
                  case _ => {
                    val targetVal = t.getAs[String](name)
                    (if (targetVal == null) "" else targetVal) mustBe e.getAs[String](name)
                  }
                }
              }
            }
          }
      }
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import java.security.MessageDigest
import org.apache.commons.codec.binary.Base64

object MD5Utils {
  val charEnc = "utf-8"
  def getMD5Str(targetStr: String): String = {
    val md5 = MessageDigest.getInstance("MD5")
    val md5Data = md5.digest(targetStr.getBytes(charEnc))
    md5Data.foldLeft("") { (l, r) =>
      val i: Int = r.asInstanceOf[Int]
      val result = if (i < 0) {
        i + 256
      } else {
        i
      }
      if (result < 16) {
        s"${l}0${Integer.toHexString(result)}"
      } else {
        s"${l}${Integer.toHexString(result)}"
      }
    }
  }

  def getMD5Str(targetStr: String, md5WordStr: String): String =
    getMD5Str(targetStr.trim + getMD5Str(md5WordStr))

  def getMD5Base64Str(targetStr: String, md5WordStr: String, flag: Boolean = true): String = {
    val bytes = MessageDigest.getInstance("MD5").digest((targetStr.trim + getMD5Str(md5WordStr)).getBytes(charEnc))
    val base64 = new String(Base64.encodeBase64(bytes))
    if (flag && base64.takeRight(2) == "==") {
      base64.dropRight(2)
    } else {
      base64
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

object PostCodeNormalizer {
  val PARENT_CORRECT_SIZE = Seq(3)
  val CHILD_CORRECT_SIZE = Seq(2, 4)
  val NO_HYPHEN = 1
  val EXIST_HYPHEN = 2
  val PARENT_POSITION = 0
  val CHILE_POSITION = 1

  private[this] def checkPostCode(sizeList: Seq[Int])(postCode: String) = Option(postCode).flatMap { p =>
    if (p.forall(_.isDigit) && sizeList.contains(p.size)) Some(p) else None
  }.getOrElse("")

  val parent = checkPostCode(PARENT_CORRECT_SIZE) _
  val child = checkPostCode(CHILD_CORRECT_SIZE) _

  def apply(postCode: String): String = Option(postCode).map { pCode =>
    val splitted = pCode.split("-")
    splitted.size match {
      case EXIST_HYPHEN =>
        val p = parent(splitted(PARENT_POSITION))
        val c = child(splitted(CHILE_POSITION))
        if (c.isEmpty) p else s"${p}-${c}"
      case NO_HYPHEN =>
        val target = splitted.head
        val parentSize = PARENT_CORRECT_SIZE.head
        parent(target.take(parentSize)) + child(target.drop(parentSize))
    }
  }.getOrElse("")

  def single(postCode: String): String = apply(postCode)
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import org.joda.time.format.DateTimeFormat
import spark.common.SparkContexts
import java.sql.Timestamp
import java.sql.Date

trait ResourceDates {
  val runningDates: Array[String]
  val runningSQLDate: Date

  val sysDateTime = SparkContexts.processTime
  val sysSQLDate = new Timestamp(sysDateTime.getTime)
  val runningDateYMD = runningDates(0)
  val runningYesterdayDateYMD = runningDates(1)
  val runningBeforeMonth = runningDates(3)
  val runningCurrentMonth = runningDates(6)

  val runningDateFmt = s"${runningDateYMD.take(4)}-${runningDateYMD.drop(4).take(2)}-${runningDateYMD.drop(6)}"

  val runningDate = RunningDate(runningDates)
  case class RunningDate(runningDates: Array[String]) {
    /** 運用日 */
    val MANG_DT = runningDates(0)
    /** 前日 */
    val YST_DY = runningDates(1)
    /** 翌日 */
    val NXT_DT = runningDates(2)
    /** 前月 */
    val BEF_MO = runningDates(3)
    /** 前月_月初日 */
    val BEF_MO_FRST_MTH_DT = runningDates(4)
    /** 前月_月末日 */
    val BEF_MO_MTH_DT = runningDates(5)
    /** 当月 */
    val CURR_MO = runningDates(6)
    /** 当月_月初日 */
    val CURR_MO_FRST_MTH_DT = runningDates(7)
    /** 当月_月末日 */
    val CURR_MO_MTH_DT = runningDates(8)
    /** 翌月 */
    val NXT_MO = runningDates(9)
    /** 翌月_月初日 */
    val NXT_MO_FRST_MTH_DT = runningDates(10)
    /** 翌月_月末日 */
    val NXT_MO_MTH_DT = runningDates(11)
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

trait ResourceInfo {
  val componentId: String
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import org.joda.time.format.DateTimeFormat
import spark.common.SparkContexts
import java.sql.Timestamp
import java.sql.Date
import scala.util.Try
import scala.reflect.io.Path
import scala.io.Source

trait ResourceEnvs {
  val runningEnv = RunningEnv()
  case class RunningEnv() {
    val envData = Try {
      val fileDir = Path(sys.env("OM_RUNNING_ENV_DIR"))
      val fullPath = fileDir / s"${sys.env("OM_ROOT_NET_ID")}_runningEnv.properties"
      val lines = Source.fromFile(fullPath.toString).getLines
      lines.map { l =>
        val splitted = l.split('=')
        (splitted(0), splitted(1))
      }.toMap
    }.getOrElse(Map.empty[String, String])

    /** 日次実行数 */
    val DAILY_EXECUTE_CNT = envData.get("DAILY_EXECUTE_CNT").getOrElse("")
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import org.joda.time.DateTime
import java.math.{ BigDecimal => jBigDecimal }
import org.apache.spark.sql.DataFrame

object SparkApp {
  object implicits {
    implicit class JbdToSbd(jbd: jBigDecimal) {
      def toBd = BigDecimal(jbd)
    }
  }
}

trait SparkApp {
  val DATE_FORMAT = "yy/MM/dd HH:mm:ss"

  def exec(implicit inArgs: InputArgs): DataFrame

  private [this] def runner(args: Array[String], isDebug: Boolean = false) {
    println(s"${new DateTime toString (DATE_FORMAT)} INFO START")
    val inputArgs = if (args.length == 8) {
      InputArgs(args(0), args(1), args(2), args(3), args(4), args(5), args(6), args(7))
    } else {
      InputArgs(args(0), args(1), args(2), args(3))
    }
    if (isDebug) { println(inputArgs) }
    try exec(inputArgs.copy(isDebug = isDebug)).sparkSession.stop catch {
      case e: Throwable => println(s"${new DateTime toString (DATE_FORMAT)} ERROR ${e.toString()}"); throw e
    }
    println(s"${new DateTime toString (DATE_FORMAT)} INFO FINISHED")
  }

  def main(args: Array[String]) {
    runner(args)
  }

  def debug(args: Array[String]) {
    runner(args, true)
  }

}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import org.apache.spark.sql.Column
import org.apache.spark.sql.functions._
import java.sql.Date
import java.time.temporal.ChronoUnit
import java.time.LocalDate
import java.time.format.DateTimeFormatter
import scala.reflect.runtime.universe
import java.util.Locale
import java.text.SimpleDateFormat
import org.joda.time.DateTime
import org.joda.time.Duration
import org.joda.time.format.DateTimeFormat
import scala.util.Try
import d2k.common.fileConv.DomainProcessor._
import d2k.common.{ PostCodeNormalizer => pcn }
import java.time.chrono._
import java.time.format._

object Udfs {

  val nullToEmpty = udf { (str: String) => if (str == null) { "" } else { str } }

  val strcat = udf { (str1: String, str2: String) => str1 + str2 }

  val udf_takek = udf { (str: String, len: Int) =>
    val byteStr = str.getBytes("MS932")
    new String(byteStr.toList.take(len).toArray, "MS932")
  }

  def takek(inStr: Column, len: Int) = udf_takek(inStr, lit(len))

  val selectNonNull = udf { (t1: String, t2: String) =>
    (t1, t2) match {
      case (null, null) => ""
      case (t1, null)   => t1
      case (null, t2)   => t2
      case (t1, t2)     => t2
    }
  }

  val rtrimk = udf { (_: String).reverse.dropWhile(_ == '　').reverse }

  val udf_rpadk = udf { (inStr: String, len: Int, pad: String) =>
    val str = if (inStr == null) { "" } else { inStr }
    val strSize = str.getBytes("MS932").size
    val padSize = len - strSize
    s"${str}${pad * padSize}"
  }
  def rpadk(str: Column, len: Int, pad: String) = udf_rpadk(str, lit(len), lit(pad))

  val validPattern = (0 to 4).map(_.toString)
  val udf_dateCnvJpToEu = udf { (orgEra: String, jpDate: String, default: String) =>
    {
      //JDK11ではREIWAだけprivateなのでvaluesで代替
      val jpEras = JapaneseEra.values()

      val eraNum = orgEra match {
        case "R" => 4
        case "H" => 3
        case "S" => 2
        case "T" => 1
        case "M" => 0
        case _   => if (validPattern.contains(orgEra)) orgEra.toInt else -1
      }
      val res = if (eraNum >= 0 && !jpDate.isEmpty) {
        Try {
          val yy = jpDate.take(2).toInt
          val mm = jpDate.slice(2, 4).toInt
          val dd = jpDate.takeRight(2).toInt

          //グレゴリオ暦の導入が明治6年なのでJapaneseDateはそれ以降しか対応していないので別途計算
          if (eraNum == 0 && yy < 6) {
            "%d%02d%02d".format(1867 + yy, mm, dd)
          } else {
            val date = JapaneseDate.of(jpEras(eraNum), yy, mm, dd);
            val dateStr = date.format(DateTimeFormatter.BASIC_ISO_DATE)
            dateStr
          }
        }.getOrElse(default)
      } else {
        default
      }
      res
    }
  }

  def dateCnvJpToEu(era: Column, jpDate: Column) = udf_dateCnvJpToEu(era, jpDate, lit("00010101"))

  def dateCnvJpToEuWithDefault(era: Column, jpDate: Column, default: Column) = udf_dateCnvJpToEu(era, jpDate, default)

  val kanaConv = udf { KanaConverter(_: String) }
  val kanaConvSearch = udf { KanaConverter.select(_: String) }

  def dateCalc(inDt: DateTime, month: Int, day: Int) = {
    val calcDay = (dt: DateTime) => if (day != 0) dt.plusDays(day) else dt
    val calcMonth = (dt: DateTime) => if (month != 0) dt.plusMonths(month) else dt

    Option(inDt).map((calcMonth andThen calcDay)(_)).getOrElse(inDt)
  }

  def dateDurationByMonths(from: DateTime, to: DateTime) = {
    val monthDuration = (dt: DateTime) => (dt.getYear * 12) + dt.getMonthOfYear
    monthDuration(to) - monthDuration(from)
  }

  def dateDurationByDays(from: DateTime, to: DateTime) = {
    (new Duration(from, to)).getStandardDays
  }

  val parseDatePattern = DateTimeFormat.forPattern("yyyyMMdd")
  val outputDatePattern = "yyyyMMdd"
  object date {
    object ym {
      object string {
        val calc = udf { (dt: String, months: Int) =>
          val result = Udfs.dateCalc(DateTime.parse(s"${dt}01", parseDatePattern), months, 0)
          result.toString(outputDatePattern).take(6)
        }

        val duration = udf { (inDt1: String, inDt2: String) =>
          val dt1 = DateTime.parse(s"${inDt1}01", parseDatePattern)
          val dt2 = DateTime.parse(s"${inDt2}01", parseDatePattern)
          Udfs.dateDurationByMonths(dt1, dt2)
        }
      }
    }

    object ymd {
      object string {
        val calc = udf { (dt: String, days: Int) =>
          val result = Udfs.dateCalc(DateTime.parse(s"${dt}", parseDatePattern), 0, days)
          result.toString(outputDatePattern)
        }

        val duration = udf { (inDt1: String, inDt2: String) =>
          val dt1 = DateTime.parse(inDt1, parseDatePattern)
          val dt2 = DateTime.parse(inDt2, parseDatePattern)
          Udfs.dateDurationByDays(dt1, dt2)
        }

        val calcAndDiff = udf { (inTarget: String, inBase: String, beginDaysDiff: Int, endDaysDiff: Int) =>
          def exec = for {
            target <- Option(inTarget)
            base <- Option(inBase)
          } yield {
            val targetDate = DateTime.parse(target.take(8), parseDatePattern)
            val beginBaseDate = Udfs.dateCalc(DateTime.parse(base.take(8), parseDatePattern), 0, beginDaysDiff)
            val endBaseDate = Udfs.dateCalc(DateTime.parse(base.take(8), parseDatePattern), 0, endDaysDiff)
            (targetDate.isAfter(beginBaseDate) || targetDate.isEqual(beginBaseDate)) &&
              (targetDate.isBefore(endBaseDate) || targetDate.isEqual(endBaseDate))
          }
          Try(exec.getOrElse(true)).getOrElse(true)
        }

        def calcAndDiff(base: String, beginDaysDiff: Int, endDaysDiff: Int = 0)(targets: Column*): Column = {
          targets.foldLeft(lit(false)) {
            case (result, target) => {
              result || calcAndDiff(target, lit(base), lit(beginDaysDiff), lit(endDaysDiff))
            }
          }
        }
      }
    }
  }

  val hashAuId = udf { (target: String) =>
    target match {
      case null                                     => null
      case ""                                       => ""
      case target if (target.forall(_.equals(' '))) => target
      case _                                        => MD5Utils.getMD5Base64Str(target, "THINK!KRI")
    }
  }

  val addPointToDateString = udf { (target: String) =>
    target match {
      case null => null
      case ""   => ""
      case _    => target.take(14) + "." + target.drop(14)
    }
  }

  val addHyphenToDateString = udf { (target: String) =>
    target match {
      case null => null
      case ""   => ""
      case _    => target.take(4) + "-" + target(4) + target(5) + "-" + target(6) + target(7)
    }
  }

  val blankToZero = udf { (target: String) =>
    target match {
      case null => "0"
      case ""   => "0"
      case _    => target
    }
  }

  val trancateAfterPoint = udf { (target: String) =>
    target match {
      case null => null
      case ""   => ""
      case _    => target.split('.')(0)
    }
  }

  object DateRangeStatus {
    val STATUS_BEFORE = "0"
    val STATUS_ON = "1"
    val STATUS_END = "9"

    val getStatus = udf { (beginDate: String, endDate: String, runningDateYMD: String) =>
      getDateRangeStatus(removeHyphen(beginDate), removeHyphen(endDate), runningDateYMD)
    }

    val getStatusWithDeleteDate = udf { (beginDate: String, endDate: String, deleteDate: String, runningDateYMD: String) =>
      deleteDate match {
        case target if !isBlankDate(removeHyphen(target)) => STATUS_END
        case _ => getDateRangeStatus(removeHyphen(beginDate), removeHyphen(endDate), runningDateYMD)
      }
    }

    val getStatusWithBlankReplace = udf { (beginDate: String, endDate: String, runningDateYMD: String) =>
      getDateRangeStatus(blankToMaxDate(removeHyphen(beginDate)), blankToMaxDate(removeHyphen(endDate)), runningDateYMD)
    }

    val getStatusWithBlankReplaceAndDeleteDate = udf { (beginDate: String, endDate: String, deleteDate: String, runningDateYMD: String) =>
      deleteDate match {
        case target if !isBlankDate(removeHyphen(target)) => STATUS_END
        case _ => getDateRangeStatus(removeHyphen(beginDate), blankToMaxDate(removeHyphen(endDate)), runningDateYMD)
      }
    }

    def getDateRangeStatus(beginDate: String, endDate: String, runningDateYMD: String) = {
      runningDateYMD match {
        case target if target < nullToMinDate(beginDate) => STATUS_BEFORE
        case target if target >= nullToMaxDate(endDate) => STATUS_END
        case _ => STATUS_ON
      }
    }

    def isBlankDate(target: String) = {
      target match {
        case null       => true
        case ""         => true
        case "00010101" => true
        case _          => false
      }
    }

    def removeHyphen(target: String) = {
      target match {
        case null => null
        case ""   => ""
        case _    => target.replaceAll("-", "")
      }
    }

    def blankToMaxDate(target: String) = if (isBlankDate(target)) "99991231" else target
    def nullToMinDate(target: String) = if (target == null) "00010101" else target
    def nullToMaxDate(target: String) = if (target == null) "99991231" else target
  }

  val isNaOrNull = udf { (target: String) => target == null }

  val isNotNaAndNotNull = udf { (target: String) => target != null }

  val cutLimitStr = udf { (target: String, threshold: Int) =>
    val cut = () => {
      val byteSize = (_: String).getBytes("MS932").size
      (1 to byteSize(target) - threshold).map(i => (i, byteSize(target.dropRight(i))))
        .filter(_._2 <= threshold).headOption.map(d => target.dropRight(d._1)).getOrElse(target)
    }

    target match {
      case null => null
      case ""   => ""
      case _    => cut()
    }
  }

  val calcSchoolAge = udf { (targetBirthDate: java.sql.Timestamp, runningDateYMD: String) =>
    val calc = (birthDateYMD: String) => {
      val (runningDateY, runningDateMD) = runningDateYMD.splitAt(4)
      val (birthDateY, birthDateMD) = birthDateYMD.splitAt(4)

      val isRunDateIn0402_1231 = runningDateMD >= "0402" && runningDateMD <= "1231"
      val isBirthDateIn0402_1231 = birthDateMD >= "0402" && birthDateMD <= "1231"

      val schoolAge = (isRunDateIn0402_1231, isBirthDateIn0402_1231) match {
        case (true, true)   => runningDateY.toInt - birthDateY.toInt
        case (true, false)  => (runningDateY.toInt + 1) - birthDateY.toInt
        case (false, true)  => runningDateY.toInt - birthDateY.toInt - 1
        case (false, false) => runningDateY.toInt - birthDateY.toInt
      }
      if (0 <= schoolAge && schoolAge <= 999) { schoolAge } else { 999 }
    }

    val schoolAge = targetBirthDate match {
      case null => 999
      case _    => calc(targetBirthDate.toString.take(10).replace("-", ""))
    }
    new Integer(schoolAge)
  }

  def strToDt(dateStr: String) = LocalDate.parse(dateStr, DateTimeFormatter.ofPattern("yyyyMMdd"))
  val calcAge = udf { (targetBirthDate: java.sql.Timestamp, runningDateYMD: String) =>
    Try {
      ChronoUnit.YEARS.between(targetBirthDate.toLocalDateTime.toLocalDate, strToDt(runningDateYMD)).toInt
    }.map(x => if (x < 0 || x > 999) 999 else x).getOrElse(999)
  }

  val domainConvert = udf { (target: String, domain: String) => exec(domain, target).right.get }

  object MakeDate {
    val date_yyyyMMdd = udf { (target: String) => d2k.common.MakeDate.date_yyyyMMdd(target) }
    val timestamp_yyyyMMdd = udf { (target: String) => d2k.common.MakeDate.timestamp_yyyyMMdd(target) }
    val timestamp_yyyyMMddhhmmss = udf { (target: String) => d2k.common.MakeDate.timestamp_yyyyMMddhhmmss(target) }
    val timestamp_yyyyMMddhhmmssSSS = udf { (target: String) => d2k.common.MakeDate.timestamp_yyyyMMddhhmmssSSS(target) }
  }

  object PostCodeNormalizer {
    val single = udf { (postCode: String) => pcn.single(postCode) }
    val parent = udf { (postCode: String) => pcn.parent(postCode) }
    val child = udf { (postCode: String) => pcn.child(postCode) }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package org.apache.spark.sql

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter
import spark.common.SparkContexts
import org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.types.StringType
import spark.common.DbCtl
import org.apache.spark.sql.execution.datasources.jdbc._
import org.apache.spark.sql.sources.Filter
import java.sql.Timestamp
import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.sql.types.DecimalType
import org.apache.spark.sql.types.TimestampType
import org.apache.spark.Partition

object JdbcCtl {
  def readTable(dbCtl: DbCtl, tableName: String, requiredColumns: Array[String],
                where: Array[String] = Array("1 = 1"), whereFilter: Array[Filter] = Array.empty[Filter]) = {
    val partitions: Array[Partition] = where.zipWithIndex.map {
      case (w, idx) => JDBCPartition(w, idx)
    }
    val structTypes = JDBCRDD.resolveTable(dbCtl.dbInfo.toOptions(tableName))
    val reqStructTypes = pruneSchema(structTypes, requiredColumns)

    val rdd = JDBCRDD.scanTable(
      SparkContexts.sc, structTypes, requiredColumns, whereFilter, partitions, dbCtl.dbInfo.toOptions(tableName)).map { r =>
        val values = internalRowToRow(r, reqStructTypes).zipWithIndex.map {
          case (strType, idx) =>
            strType match {
              case _ if r.isNullAt(idx) == true => null
              case StringType                   => r.getString(idx)
              case TimestampType                => new Timestamp(r.getLong(idx) / 1000)
              case x: DecimalType               => r.getDecimal(idx, x.precision, x.scale).toBigDecimal
            }
        }
        Row(values: _*)
      }
    SparkContexts.context.createDataFrame(rdd, reqStructTypes)
  }

  def pruneSchema(schema: StructType, columns: Array[String]) = {
    val fieldMap = Map(schema.fields.map(x => x.name -> x): _*)
    new StructType(columns.map(c => fieldMap(c)))
  }

  def internalRowToRow(iRow: InternalRow, schema: StructType) = {
    schema.map(_.dataType)
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package spark.common

import SparkContexts._
import SparkContexts._
import scala.util.Try
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.types.DataType
import org.apache.spark.sql.jdbc.JdbcType
import java.sql.Types._
import org.apache.spark.sql.types._
import org.apache.spark.sql.jdbc.JdbcDialects
import org.apache.spark.sql.jdbc.JdbcDialect
import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils
import org.apache.spark.sql.functions._
import java.sql.PreparedStatement
import java.sql.Date
import java.sql.Timestamp
import java.sql.Types
import org.apache.spark.sql.sources.Filter
import java.sql.Connection
import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions
import org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite

case class DbInfo(
                   url: String, user: String, password: String, charSet: String = "JA16SJISTILDE",
                   commitSize: Option[Int] = Try(sys.env(DbCtl.envName.COMMIT_SIZE)).map(_.toInt).toOption,
                   isDirectPathInsertMode: Boolean = {
                     Try(sys.env(DbCtl.envName.DPI_MODE)).map(DbCtl.checkTrueOrOn).getOrElse(false)
                   },
                   fetchSize: Option[Int] = Try(sys.env(DbCtl.envName.FETCH_SIZE)).map(_.toInt).toOption) {
  val baseMap = Map(
    JDBCOptions.JDBC_URL -> url,
    JDBCOptions.JDBC_TABLE_NAME -> "DUAL",
    "user" -> user,
    "password" -> password,
    "charSet" -> charSet,
    "driver" -> DbCtl.ORACLE_DRIVER)

  def toOptions = {
    new JDBCOptions(fetchSize.map(fs => baseMap + (JDBCOptions.JDBC_BATCH_FETCH_SIZE -> fs.toString)).getOrElse(baseMap))
  }

  def toOptions(tableName: String) = {
    val addFetchSize = fetchSize.map(fs => baseMap + (JDBCOptions.JDBC_BATCH_FETCH_SIZE -> fs.toString)).getOrElse(baseMap)
    new JDBCOptions(addFetchSize + (JDBCOptions.JDBC_TABLE_NAME -> tableName))
  }

  def toOptionsInWrite(tableName: String) = {
    val addFetchSize = fetchSize.map(fs => baseMap + (JDBCOptions.JDBC_BATCH_FETCH_SIZE -> fs.toString)).getOrElse(baseMap)
    new JdbcOptionsInWrite(addFetchSize + (JDBCOptions.JDBC_TABLE_NAME -> tableName))
  }
}

object DbCtl extends Serializable {
  object envName {
    val COMMIT_SIZE = "COMMIT_SIZE"
    val DPI_MODE = "DPI_MODE"
    val FETCH_SIZE = "FETCH_SIZE"
  }

  lazy val dbInfo1 = DbInfo(sys.env("DB_URL"), Try(sys.env("DB_USER")).getOrElse(""), Try(sys.env("DB_PASSWORD")).getOrElse(""))
  lazy val dbInfo2 = DbInfo(sys.env("DB_URL2"), Try(sys.env("DB_USER2")).getOrElse(""), Try(sys.env("DB_PASSWORD2")).getOrElse(""))

  val ORACLE_DRIVER = "oracle.jdbc.driver.OracleDriver"

  val checkTrueOrOn = (s: String) => {
    val d = s.toLowerCase
    if (d == "true" || d == "on") true else false
  }

  def makeWhere10(columnName: String) = (
    (0 to 9).map(cnt => f"substr(${columnName},-1,1) = '$cnt'") :+ s" substr(${columnName},-1,1) not in ('0','1','2','3','4','5','6','7','8','9') ").toArray

  def localImport(tableName: String, cnt: Int = 100) = {
    val orgTb = new DbCtl(DbCtl.dbInfo2).readTable(tableName)

    val posgre = new DbCtl()
    import posgre.implicits._
    context.createDataFrame(sc.makeRDD(orgTb.take(cnt)), orgTb.schema).writeTable(tableName, SaveMode.Overwrite)
    posgre.readTable(tableName).show
  }

  val readAllData = Array("1 = 1")

  type UpdateMode = Int
  val UPDATE: UpdateMode = 0
  val UPSERT: UpdateMode = 1
}

case object OracleDialect extends JdbcDialect {
  override def canHandle(url: String): Boolean = url.startsWith("jdbc:oracle") || url.contains("oracle")

  override def getCatalystType(
                                sqlType: Int, typeName: String, size: Int, md: MetadataBuilder): Option[DataType] = {
    if (sqlType == Types.NUMERIC && size == 0) {
      Option(DecimalType(DecimalType.MAX_PRECISION, 10))
    } else {
      None
    }
  }

  override def getJDBCType(dt: DataType): Option[JdbcType] = dt match {
    case StringType => Some(JdbcType("VARCHAR2(255)", VARCHAR))
    case LongType   => Some(JdbcType("NUMBER", INTEGER))
    case _          => None
  }
}

case object DerbyDialect extends JdbcDialect {
  override def canHandle(url: String): Boolean = url.startsWith("jdbc:derby") || url.contains("derby")
  override def getJDBCType(dt: DataType): Option[JdbcType] = dt match {
    case StringType => Some(JdbcType("CLOB", CLOB))
    case _          => None
  }
}

class DbCtl(val dbInfo: DbInfo = DbCtl dbInfo1) extends Logging with Serializable {
  import java.util.Properties
  import org.apache.spark.sql._

  val props = new Properties
  props.put("user", dbInfo.user)
  props.put("password", dbInfo.password)
  props.put("charSet", dbInfo.charSet)
  props.put("driver", DbCtl.ORACLE_DRIVER)
  sys.props.get(DbCtl.envName.FETCH_SIZE).orElse(dbInfo.fetchSize)
    .foreach { fs => props.put("fetchsize", fs.toString) }

  JdbcDialects.registerDialect(OracleDialect)
  JdbcDialects.registerDialect(DerbyDialect)

  def getTableFullName(tableName: String) = {
    val sql = "SELECT TABLE_OWNER FROM ALL_SYNONYMS WHERE TABLE_NAME = ? AND (OWNER ='PUBLIC' OR OWNER = ?) ORDER BY DECODE(OWNER,'PUBLIC',1,0)"
    val rs = prepExecSql(tableName, sql) { prs =>
      prs.setString(1, tableName.toUpperCase())
      prs.setString(2, dbInfo.user.toUpperCase())
    }
    if (rs.next()) s"${rs.getString(1)}.${tableName}" else tableName
  }

  def clearTable(tableName: String) = {
    elapse("clearTable") {
      val targetTable = if (tableName.contains(".")) tableName else Try(getTableFullName(tableName)).getOrElse(tableName)
      Try(truncateTable(targetTable)).recover { case e => errorLog(s"FAILED TO TRUNCATE ${targetTable}", e); deleteTable(tableName) }.get
    }
  }

  def execSql(tableName: String, sql: String) = {
    println(tableName, sql)
    JdbcUtils.createConnectionFactory(dbInfo.toOptions(tableName))().prepareStatement(sql).executeUpdate
  }
  def prepExecSql(tableName: String, sql: String)(prsFunc: PreparedStatement => Unit) = {
    val prep = JdbcUtils.createConnectionFactory(dbInfo.toOptions(tableName))().prepareStatement(sql)
    prsFunc(prep)
    prep.executeQuery
  }
  def truncateTable(tableName: String) = execSql(tableName, s"TRUNCATE TABLE $tableName")
  def dropTable(tableName: String) =
    JdbcUtils.dropTable(JdbcUtils.createConnectionFactory(dbInfo.toOptions)(), tableName, dbInfo.toOptions)
  def deleteTable(tableName: String) = execSql(tableName, s"DELETE FROM $tableName")
  def columnTypes(conn: Connection, tableName: String) = {
    val result = conn.getMetaData.getColumns(null, null, tableName.toUpperCase, "%")
    new Iterator[(String, Int)] {
      def hasNext = result.next
      def next() = (result.getString("COLUMN_NAME"), result.getString("DATA_TYPE").toInt)
    }.toMap
  }

  def changeColToDfTypes(colType: Int, dfTypes: DataType) = dfTypes match {
    case DateType      => DATE
    case TimestampType => TIMESTAMP
    case IntegerType   => INTEGER
    case LongType      => DOUBLE
    case DecimalType() => DECIMAL
    case _             => colType
  }

  def readTable(tableName: String) = {
    context.read.jdbc(dbInfo.url, tableName, props)
  }

  def readTable(tableName: String, where: Array[String]) = {
    context.read.jdbc(dbInfo.url, tableName, where, props)
  }

  def readTable(tableName: String, requiredColumns: Array[String], where: Array[String]) = {
    JdbcCtl.readTable(this, tableName, requiredColumns, where)
  }

  def loanConnection(tableName: String, sqlStr: String, executeBatch: Boolean = true)(f: PreparedStatement => Unit) {
    val jdbcConn = JdbcUtils.createConnectionFactory(dbInfo.toOptions(tableName))()
    val prs = jdbcConn.prepareStatement(sqlStr)
    try {
      f(prs)
      if (executeBatch) prs.executeBatch
    } finally {
      Some(prs).foreach(_.close)
      Some(jdbcConn).foreach(_.close)
    }
  }

  val recordProcessor = (tableName: String, sqlStr: String, structType: org.apache.spark.sql.types.StructType, fieldNames: Array[String]) =>
    (iter: Iterator[org.apache.spark.sql.Row]) => {
      loanConnection(tableName, sqlStr, false) { prs =>
        lazy val colType = columnTypes(prs.getConnection, tableName)
        val commitSize = sys.props.get(DbCtl.envName.COMMIT_SIZE).map(_.toInt).orElse(dbInfo.commitSize)
        commitSize.map { cs =>
          iter.sliding(cs.toInt, cs.toInt).foreach { commitBlock =>
            commitBlock.foreach(setValues(colType, structType, fieldNames, prs))
            prs.executeBatch()
          }
        }.getOrElse {
          iter.foreach { row =>
            setValues(colType, structType, fieldNames, prs)(row)
          }
          prs.executeBatch()
        }
      }
    }

  def setValues(columnTypes: Map[String, Int], structType: StructType, cols: Array[String], prs: PreparedStatement)(row: Row) = {
    def setPreparedValue(preps: PreparedStatement, col: String, idx: Int) = {
      structType(col).dataType match {
        case _ if row.isNullAt(row.fieldIndex(col)) => preps.setNull(idx + 1, changeColToDfTypes(columnTypes(col), structType(col).dataType))
        case StringType                             => preps.setString(idx + 1, row.getAs[String](col))
        case DateType                               => preps.setDate(idx + 1, row.getAs[Date](col))
        case TimestampType                          => preps.setTimestamp(idx + 1, row.getAs[Timestamp](col))
        case IntegerType                            => preps.setInt(idx + 1, row.getAs[Int](col))
        case LongType                               => preps.setLong(idx + 1, row.getAs[Long](col))
        case DecimalType()                          => preps.setBigDecimal(idx + 1, row.getAs[java.math.BigDecimal](col))
      }
      preps
    }
    cols.zipWithIndex.foldLeft(prs) { case (preps, (col, idx)) => setPreparedValue(preps, col, idx) }
    prs.addBatch
  }

  def insertAccelerated(df: DataFrame, tableName: String, mode: SaveMode = SaveMode.Append, hint: String = "") = {
    val structType = df.schema
    val fieldNames = structType.fieldNames
    val columnsStr = fieldNames.mkString(",")
    val bindStr = fieldNames.map(_ => "?").mkString(",")
    def nonDirectInsert = {
      val insertStr = s"""
      insert /*+ ${hint} */ into ${tableName}(${columnsStr}) values(${bindStr})
      """

      if (mode == SaveMode.Overwrite) clearTable(tableName)
      df.foreachPartition(recordProcessor(tableName, insertStr, structType, fieldNames))
    }

    def directInsert = {
      val insertStr = s"""
      insert /*+ APPEND_VALUES ${hint} */ into ${tableName}(${columnsStr}) values(${bindStr})
      """

      if (mode == SaveMode.Overwrite) clearTable(tableName)
      df.coalesce(1).foreachPartition(recordProcessor(tableName, insertStr, structType, fieldNames))
    }

    val dpi = sys.props.get(DbCtl.envName.DPI_MODE)
      .map(DbCtl.checkTrueOrOn)
      .getOrElse(dbInfo.isDirectPathInsertMode)
    if (dpi) directInsert else nonDirectInsert
  }

  def insertNotExists(df: DataFrame, tableName: String, inKeys: Seq[String], mode: SaveMode = SaveMode.Append, hint: String = "") = {
    val keysStr = inKeys.mkString(",")
    val structType = df.schema
    val fieldNames = structType.fieldNames
    val columnsStr = fieldNames.mkString(",")
    val bindStr = fieldNames.map(_ => "?").mkString(",")
    val insertStr = s"""
      insert /*+ ignore_row_on_dupkey_index(${tableName}(${keysStr})) ${hint} */ into ${tableName}(${columnsStr}) values(${bindStr})
      """
    if (mode == SaveMode.Overwrite) clearTable(tableName)
    df.foreachPartition(recordProcessor(tableName, insertStr, structType, fieldNames))
  }

  def deleteRecords(df: DataFrame, tableName: String, inKeys: Set[String], hint: String = "") = {
    val keysArray = inKeys.toArray
    val keysStr = keysArray.map(k => s"${k.toLowerCase} = ?").mkString(" and ")
    val deleteStr = s"""
      delete /*+ ${hint} */ from $tableName where $keysStr
      """

    val structType = df.schema
    df.foreachPartition(recordProcessor(tableName, deleteStr, structType, keysArray))
  }

  def updateRecords = updateRecordsBase(DbCtl.UPDATE) _
  def upsertRecords = updateRecordsBase(DbCtl.UPSERT) _

  def dropArrayData(target: Seq[String], dropList: Seq[String]) = dropList.foldLeft(target) { (l, r) =>
    l.filterNot(_ == r)
  }

  private[this] def updateRecordsBase(mode: DbCtl.UpdateMode)(df: DataFrame, tableName: String, inKeys: Set[String], ignoreColumnsForUpdate: Set[String] = Set.empty[String], hint: String = "") = {
    val keysArray = inKeys.toArray
    val cols = df.columns
    val colsWithoutKey = dropArrayData(df.columns, keysArray)
    val colsWithoutKeyAndIgnore = dropArrayData(colsWithoutKey, ignoreColumnsForUpdate.toSeq)
    val colsWithoutKeyStr = colsWithoutKeyAndIgnore.map(k => s"${k.toLowerCase} = ?").mkString(",")
    val keysStr = keysArray.map(k => s"${k.toLowerCase} = ?").mkString(" and ")
    val updateStr = s"""
      update /*+ ${hint} */ ${tableName}  set $colsWithoutKeyStr where $keysStr
      """

    val usingStr = cols.map(x => s"? $x").mkString(",")
    val onStr = keysArray.map(x => s"a.${x} = b.${x}").mkString(" and ")
    val updateSetStr = colsWithoutKeyAndIgnore.map(x => s"a.${x} = b.${x}").mkString(",")
    val insertStrLeft = cols.map(x => s"a.${x}").mkString(",")
    val insertStrRight = cols.map(x => s"b.${x}").mkString(",")
    val mergeStr = s"""
      merge /*+ ${hint} */ into ${tableName} a
        using (select ${usingStr} from dual) b
        on (${onStr})
      when matched then
        update set ${updateSetStr}
      when not matched then
        insert (${insertStrLeft}) values (${insertStrRight})
      """
    val structType = df.schema
    mode match {
      case DbCtl.UPSERT =>
        df.foreachPartition(recordProcessor(tableName, mergeStr, structType, cols))

      case DbCtl.UPDATE =>
        df.foreachPartition(recordProcessor(tableName, updateStr, structType, colsWithoutKeyAndIgnore ++: keysArray))
    }
  }

  object implicits {
    implicit class DbCtlDataFrame(df: DataFrame) {
      def writeTable(tableName: String, mode: SaveMode = SaveMode.Append) = mode match {
        case SaveMode.Overwrite => {
          clearTable(tableName)
          JdbcUtils.saveTable(df, None, true, dbInfo.toOptionsInWrite(tableName))
        }
        case SaveMode.Append => {
          JdbcUtils.saveTable(df, None, true, dbInfo.toOptionsInWrite(tableName))
        }
        case _ => df.write.mode(mode).jdbc(dbInfo.url, tableName, props)
      }

      /**
       * test or debug use only
       */
      def writeTableStandard(tableName: String, mode: SaveMode = SaveMode.Append) = mode match {
        case SaveMode.Overwrite => {
          clearTable(tableName)
          df.write.mode(mode).jdbc(dbInfo.url, tableName, props)
        }
        case _ => df.write.mode(mode).jdbc(dbInfo.url, tableName, props)
      }

      /**
       * test or debug use only
       */
      def autoCreateTable(tableName: String) {
        Try(df.limit(1).write.mode(SaveMode.Overwrite).jdbc(dbInfo.url, tableName, props))
        clearTable(tableName)
      }
    }
  }

  import implicits._
  def readParquetAndWriteDb(readParquetBasePath: String, readParquetPath: String, writeTableName: String, saveMode: SaveMode = SaveMode.Append)(proc: DataFrame => DataFrame = df => df) = {
    proc(new PqCtl(readParquetBasePath).readParquet(readParquetPath)).writeTable(writeTableName, saveMode)
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package spark.common

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._
import org.apache.spark.sql.Row
import org.apache.spark.sql.Column
import org.apache.spark.sql.expressions.Window
import scala.util.matching.Regex

object DfCtl {
  sealed trait Editors {
    val colName: String
    def toCols(colNames: Set[String], cols: Seq[Column]): (Set[String], Seq[Column])
  }

  case class Edit(inColName: String, editor: Column) extends Editors {
    val colName = inColName
    def toCols(colNames: Set[String], cols: Seq[Column]) =
      (colNames - colName, cols :+ (editor as colName))
  }

  case class Rename(colNameFrom: String, colNameTo: String, editor: Column = null) extends Editors {
    val colName = s"${colNameFrom}_${colNameTo}"
    def toCols(colNames: Set[String], cols: Seq[Column]) =
      Option(editor).map(e => (colNames - colNameFrom, cols :+ (editor as colNameTo)))
        .getOrElse((colNames - colNameFrom, cols :+ (col(colNameFrom) as colNameTo)))
  }

  case class Delete(inColName: String) extends Editors {
    val colName = inColName
    def toCols(colNames: Set[String], cols: Seq[Column]) =
      (colNames - colName, cols)
  }

  case class CastByName(inColName: String, castType: String) extends Editors {
    val colName = inColName
    def toCols(colNames: Set[String], cols: Seq[Column]) =
      (colNames - colName, cols :+ (col(inColName).cast(castType)))
  }

  case class CastByRegex(inRegex: String, castType: String) extends Editors {
    val colName = inRegex
    def toCols(colNames: Set[String], cols: Seq[Column]) = {
      val regxMatchNames = colNames.flatMap(inRegex.r.findFirstIn)
      (colNames -- regxMatchNames, cols ++ regxMatchNames.map(name => col(name).cast(castType)))
    }
  }

  case class ApplyByName(inColName: String, func: Column => Column) extends Editors {
    val colName = inColName
    def toCols(colNames: Set[String], cols: Seq[Column]) =
      (colNames - colName, cols :+ func(col(inColName)))
  }

  case class ApplyByRegex(inRegex: String, func: Column => Column) extends Editors {
    val colName = inRegex
    def toCols(colNames: Set[String], cols: Seq[Column]) = {
      val regxMatchNames = colNames.flatMap(inRegex.r.findFirstIn)
      (colNames -- regxMatchNames, cols ++ regxMatchNames.map(name => func(col(name))))
    }
  }

  def editColumns(editors: Seq[Editors]) = (df: DataFrame) => {
    val (colNames, cols) = editors.foldLeft((df.schema.fieldNames.toSet, Seq.empty[Column])) {
      case ((colNames, cols), target) => target.toCols(colNames, cols)
    }
    val schemaNames = colNames.map(d => col(d))
    df.select((schemaNames.toSeq ++ cols): _*)
  }

  def editColumnsAndSelect(editors: Seq[Editors]) = (df: DataFrame) => {
    val (colNames, cols) = editors.foldLeft((df.schema.fieldNames.toSet, Seq.empty[Column])) {
      case ((colNames, cols), target) => target.toCols(colNames, cols)
    }
    df.select(cols: _*)
  }

  def applyAll(colNames: Seq[String], applyCode: Column => Column) = (df: DataFrame) => {
    val schemaNames = colNames.map(d => applyCode(col(d)))
    df.select(schemaNames: _*)
  }

  def selectMaxValue(targetCols: Seq[String], orderCols: Seq[Column]) = (df: DataFrame) => {
    val win = Window.partitionBy(targetCols.map(col): _*).orderBy(orderCols: _*)
    df.withColumn("rank", row_number.over(win)).filter("rank = 1").drop("rank")
  }

  def groupingAgg(groupingColumns: Seq[String], aggColumns: Seq[Column]) = (df: DataFrame) => {
    df.groupBy(groupingColumns.map(col): _*).agg(aggColumns.head, aggColumns.tail: _*)
  }

  def groupingSum(groupingColumns: Seq[String], sumColumns: Seq[String]) = (df: DataFrame) => {
    val cols = sumColumns.map(col => sum(col) as col)
    groupingAgg(groupingColumns, cols)(df)
  }

  def addColumnPrefix(name: String) = (df: DataFrame) => {
    val cols = df.schema.map(x => df(x.name) as s"${name}_${x.name}")
    df.select(cols: _*)
  }

  def dropColumnPrefix(name: String) = (df: DataFrame) => {
    val cols = df.schema.map(_.name).filter(!_.startsWith(s"${name}_")).map(col)
    df.select(cols: _*)
  }

  object implicits {
    implicit class MyCommonDF(df: DataFrame) extends Logging {
      def ~>(f: DataFrame => DataFrame) = f(df)
      def ~|>(f: DataFrame => DataFrame) = { df.show(false); f(df) }

      def pickMaxValueRow(pks: String*)(maxValueTarget: String*) =
        df.sort((pks.map(x => col(x)) ++ maxValueTarget.map(x => col(x).desc)): _*).dropDuplicates(pks)

      def partitionWriteFile(
        filePath: String, overwrite: Boolean = true, charEnc: String = "MS932", partitionExtention: String = "")(
        func: Row => String) {
        if (overwrite) FileCtl.deleteDirectory(filePath)
        FileCtl.createDirectory(filePath)

        df.rdd.mapPartitionsWithIndex { (idx, iterRow) =>
          val fullPath = FileCtl.addExtention(s"${filePath}/${idx}", partitionExtention)
          FileCtl.writeToFile(fullPath, true, charEnc) { pw =>
            elapse(s"fileWrite:${fullPath}") {
              iterRow.foreach(row => pw.println(func(row)))
            }
          }
          Seq.empty[Row].toIterator
        }.foreach(_ => ())
      }

      def partitionWriteToFileWithPartitionColumns(
        filePath: String, partitionColumns: Seq[String],
        overwrite: Boolean = true, charEnc: String = "MS932", partitionExtention: String = "")(
        func: Row => String) {
        if (overwrite) FileCtl.deleteDirectory(filePath)
        FileCtl.createDirectory(filePath)

        df.rdd.mapPartitionsWithIndex { (idx, iterRow) =>
          val fullPath = FileCtl.addExtention(s"${filePath}/${idx}", partitionExtention)
          elapse(s"fileWrite:${fullPath}") {
            FileCtl.loanPrintWriterCache { cache =>
              iterRow.foldLeft(cache) { (l, r) =>
                FileCtl.writeToFileWithPartitionColumns(
                  filePath, idx, charEnc, partitionColumns, partitionExtention)(func)(l)(r)
              }
            }
          }
          Seq.empty[Row].toIterator
        }.foreach(_ => ())
      }
    }

    implicit class MyCommonDFTouple(df: (DataFrame, DataFrame)) extends Logging {
      def ~>(f: (DataFrame, DataFrame) => DataFrame) = f(df._1, df._2)
      def ~|>(f: (DataFrame, DataFrame) => DataFrame) = { df._1.show(false); df._2.show(false); f(df._1, df._2) }
    }

    implicit class ToEdit(x: (String, Column)) {
      def e = Edit(x._1, x._2)
    }

    implicit class ToEditSeq(x: Seq[(String, Column)]) {
      def e = x.map(_.e)
    }

    implicit class ToToupleLogic(x: (String, String)) {
      def r = Rename(x._1, x._2)
      def c = CastByName(x._1, x._2)
      def cr = CastByRegex(x._1, x._2)
    }

    implicit class ToToupleLogicSeq1(x: Seq[(String, String)]) {
      def r = x.map(_.r)
      def c = x.map(_.c)
      def cr = x.map(_.cr)
    }

    implicit class ToToupleFunctionLogic(x: (String, Column => Column)) {
      def a = ApplyByName(x._1, x._2)
      def ar = ApplyByRegex(x._1, x._2)
    }

    implicit class ToToupleFunctionLogicSeq1(x: Seq[(String, Column => Column)]) {
      def a = x.map(_.a)
      def ar = x.map(_.ar)
    }

    implicit class ToRename2(x: ((String, String), Column)) {
      def r = Rename(x._1._1, x._1._2, x._2)
    }

    implicit class ToRenameSeq2(x: Seq[((String, String), Column)]) {
      def r = x.map(_.r)
    }

    implicit class ToDelete(x: String) {
      def d = Delete(x)
    }

    implicit class ToDeleteSeq(x: Seq[String]) {
      def d = x.map(_.d)
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package spark.common

import java.io.PrintWriter
import java.io.FileOutputStream
import java.io.OutputStreamWriter
import java.io.FileInputStream
import java.util.Properties
import org.apache.hadoop.fs.FileSystem
import org.apache.hadoop.fs.Path
import scala.reflect.io.Directory
import org.apache.spark.sql.Row

object FileCtl {
  def writeToFile(fileName: String, append: Boolean = false, charEnc: String = "MS932")(func: PrintWriter => Unit) {
    val outFile = new PrintWriter(new OutputStreamWriter(new FileOutputStream(fileName, append), charEnc))
    func(outFile)
    outFile.close()
  }

  def emptyPrintWriterCache = Map.empty[String, PrintWriter]

  def loanPrintWriterCache(func: Map[String, PrintWriter] => Map[String, PrintWriter]) {
    var pwMap = FileCtl.emptyPrintWriterCache
    try { pwMap = func(pwMap) } finally { pwMap.values.foreach(_.close) }
  }

  def addExtention(path: String, ext: String) = if (ext.isEmpty) path else s"${path}.${ext}"

  def writeToFileWithPartitionColumns(
    fileName: String, partitionIndex: Int = 0, charEnc: String = "MS932",
    partitionColumns: Seq[String] = Seq.empty[String], partitionExtention: String = "")(
      func: Row => String)(pwCache: Map[String, PrintWriter])(row: Row) = {
    val outPath = partitionColumns.map { col =>
      s"${col}=${row.getAs[String](col)}"
    }.mkString(s"${fileName}/", "/", "")

    FileCtl.createDirectory(outPath)

    val outFile = pwCache.getOrElse(outPath,
      new PrintWriter(new OutputStreamWriter(new FileOutputStream(
        addExtention(s"${outPath}/${partitionIndex}", partitionExtention), true), charEnc)))
    outFile.println(func(row))
    if (pwCache.isDefinedAt(outPath)) pwCache else pwCache.updated(outPath, outFile)
  }

  def loadEnv(filePath: String): Properties = {
    val env = new Properties()
    env.load(new FileInputStream(filePath))
    env
  }

  def exists(filePath: String): Boolean = {
    val conf = SparkContexts.sc.hadoopConfiguration
    val fs = FileSystem.get(conf)
    Option(fs.globStatus(new Path(filePath))).map(!_.isEmpty).getOrElse(false)
  }

  def createDirectory(fullPath: String) {
    Directory(fullPath).createDirectory(true, false)
  }

  def createDirectory(dirPath: String, filePath: String) {
    (Directory(dirPath) / filePath).createDirectory(true, false)
  }

  def deleteDirectory(fullPath: String) {
    Directory(fullPath).deleteRecursively
  }

  def deleteDirectory(dirPath: String, filePath: String) {
    (Directory(dirPath) / filePath).deleteRecursively
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package spark.common

import org.slf4j.LoggerFactory

trait Logging extends Serializable {
  val logger = LoggerFactory.getLogger(this.getClass)

  def errorLog(message: String, t: Throwable) = logger.error(message, t)

  def isDebugEnabled = logger.isDebugEnabled

  def elapse(message: String)(func: => Unit) = {
    logger.info(s" Start[${message}]")
    val startTime = System.currentTimeMillis
    func
    val endTime = System.currentTimeMillis
    val elapse = BigDecimal(endTime - startTime) / 1000
    logger.info(f"finish[${message}] elapse:${elapse}%,.3fs")
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package spark.common

import SparkContexts._
import org.apache.spark.sql.DataFrame
import scala.util.Try
import org.slf4j.LoggerFactory
import org.slf4j.Marker
import d2k.common.MakeResource

class PqCtl(val baseParquetFilePath: String) extends Serializable {
  val logger = LoggerFactory.getLogger(this.getClass)

  def readParquet(appendPath: String, strictMode: Boolean = true, readPqEmptySchema: Seq[(String, String)] = Seq.empty[(String, String)]) = {
    val appendPaths = appendPath.split(",").map(path => s"${baseParquetFilePath}/${path.trim}")
    if (strictMode) {
      context.read.parquet(appendPaths: _*)
    } else {
      try { context.read.parquet(appendPaths: _*) } catch {
        case t: org.apache.spark.sql.AnalysisException => {
          if (t.getMessage.startsWith("Path does not exist")) {
            logger.warn(s"Not Found Read Parquet[${appendPaths.mkString(",")}]")
            val schema = MakeResource.makeSchema(readPqEmptySchema.map(_._1), readPqEmptySchema.map(_._2), readPqEmptySchema.map(_ => "10"))
            context.createDataFrame(context.emptyDataFrame.rdd, schema)
          } else { throw t }
        }
      }
    }
  }

  object implicits {
    implicit class MyParquetDF(df: DataFrame) {

      def writeParquet(appendPath: String) = {
        df.write.mode("overwrite").parquet(s"${baseParquetFilePath}/${appendPath}")
      }

      def writeParquetWithPartitionBy(appendPath: String, partitionColumn: String*) = {
        try {
          df.write.mode("overwrite").partitionBy(partitionColumn: _*).parquet(s"${baseParquetFilePath}/${appendPath}")
        } catch {
          case t: NullPointerException => if (df.count() > 0) { df.show(); throw t } else { println(s"${baseParquetFilePath}/${appendPath} IS NO RECORD") }
          case t: Throwable            => throw t
        }
      }
    }
  }

  import implicits._

  def readParquetAndWriteParquet(readParquetPath: String, writeParquetPath: String)(proc: DataFrame => DataFrame = df => df) =
    proc(readParquet(readParquetPath)).writeParquet(writeParquetPath)

  def readParquetAndWriteParquetWithPartitionBy(readParquetPath: String, writeParquetPath: String, partitionColumn: String*)(proc: DataFrame => DataFrame = df => df) =
    proc(readParquet(readParquetPath)).writeParquetWithPartitionBy(writeParquetPath, partitionColumn: _*)

  def dbToPq(tableName: String, where: Array[String]) {
    dbToPq(tableName, tableName, where, DbCtl.dbInfo1)
  }

  def dbToPq(appendPath: String, tableName: String, where: Array[String], dbInfo: DbInfo) {
    new DbCtl(dbInfo).readTable(tableName, where).writeParquet(appendPath)
  }

  def dbToPq(appendPath: String, tableName: String, dbInfo: DbInfo) {
    new DbCtl(dbInfo).readTable(tableName).writeParquet(appendPath)
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package spark.common

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.SparkSession

object SparkContexts {
  val conf = new SparkConf()
  val session = SparkSession.builder.enableHiveSupport.config(conf).getOrCreate
  val sc = session.sparkContext
  val processTime = new java.sql.Timestamp(sc.startTime)
  val context = session.sqlContext
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.app.test.common

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter

class TestToolsTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "Domain Converter" should {
    "all space" in {
      val result = MakeDf.dc("test/dev/conf/import/projectId_items_test.conf").allSpace.collect.head
      result.getAs[String](0).size mustBe 0
      result.getAs[String](1).size mustBe 0
    }

    "all empty" in {
      val result = MakeDf.dc("test/dev/conf/import/projectId_items_test.conf").allEmpty.collect.head
      result.getAs[String](0).size mustBe 0
      result.getAs[String](1).size mustBe 0
    }
  }

  "Plain" should {
    "all space" in {
      val result = MakeDf("test/dev/conf/import/projectId_items_test.conf").allSpace.collect.head
      result.getAs[String](0).size mustBe 2
      result.getAs[String](1).size mustBe 3
    }

    "all empty" in {
      val result = MakeDf("test/dev/conf/import/projectId_items_test.conf").allEmpty.collect.head
      result.getAs[String](0).size mustBe 0
      result.getAs[String](1).size mustBe 0
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.dic

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import d2k.appdefdoc.gen.dic._
import scala.reflect.io.Directory
import scala.reflect.io.Path.string2path

class DictionaryGeneratorTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "GenerateTestCase" should {
    "be normal end" in {
      GenerateDictionary("http://10.47.148.28:8088/d2k_app_dev/d2k_docs")
        .generate("master")
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.src

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import d2k.appdefdoc.gen.src._
import scala.reflect.io.Directory
import scala.reflect.io.Path.string2path
import d2k.appdefdoc.parser.AppDefParser

class SourceGeneratorTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "generateItemConf" should {
    "be normal end" in {
      val baseUrl = "http://10.47.148.28:8088/d2k_app_dev/d2k_docs"
      val branch = "master"
      val appGroup = "mka"
      val appId = "MKA0690001001"
      val appdef = AppDefParser(baseUrl, branch, appGroup, appId).get
      val writeBase = s"data/srcGen/${appGroup}"
      val writePath = Directory(writeBase)
      writePath.createDirectory(true, false)
      val inputFiles = appdef.inputList.filter(_.srcType.toLowerCase.startsWith("file"))

      SourceGenerator.generateItemConf(baseUrl, branch, appGroup, appId, writePath)(inputFiles)
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.gen.test

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter

class GenerateTestCaseTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "GenerateTestCase" should {
    "xxx" in {
      GenerateTestCase("http://10.47.148.28:8088/d2k_app_dev/d2k_docs")
        .generate("master", "sha", "SHAMCA0003001")
        .write()
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.appdefdoc.parser

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter

class ComponentFlowParserTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "flow" should {
    "normal end" in {
      val testdata1 = """
"01_PqToDf\nFK向け顧客契約(中間)_PQ取得抽出" as 01_PqToDf --> "03_DfJoinToDf\nFK向け顧客契約ファイル料金プラン取付" as 03_DfJoinToDf

        """
      val testdata = """
"01_PqToDf\nFK向け顧客契約(中間)_PQ取得抽出" as 01_PqToDf --> "03_DfJoinToDf\nFK向け顧客契約ファイル料金プラン取付" as 03_DfJoinToDf
"02_PqToDf\nポスペ加入者台帳用料金系サービス情報PQ取得" as 02_PqToDf --> 03_DfJoinToDf
03_DfJoinToDf --> "05_DfJoinToDf\nFK向け顧客契約ファイルデータプリペ区分取付" as 05_DfJoinToDf
"04_DbToDf\nＬＴＥプリペイド料金プランテーブル取得" as 04_DbToDf --> 05_DfJoinToDf
05_DfJoinToDf --> "07_DfJoinToDf\nFK向け顧客契約ファイルデータ抽出" as 07_DfJoinToDf
"06_FileToDf\nmtGerberaFeePlnPrm_Gerbera料金プランパラメータファイル取得" as 06_FileToDf --> 07_DfJoinToDf
07_DfJoinToDf --> "08_DfToPq\nFK向け顧客契約Pqファイル出力" as 08_DfToPq
08_DfToPq --> (*)

        """
      ComponentFlowParser(testdata)
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter

class DateConverterTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "DateConverter" should {
    "success convert" when {
      val target = MakeDate.timestamp_yyyyMMddhhmmssSSS("00010101000000000")
      "yyyyMMddhhmmssSSS" in {
        import DateConverter.implicits._
        target.toYmdhmsS mustBe "00010101000000000"
      }

      "yyyyMMddhhmmss" in {
        import DateConverter.implicits._
        target.toYmdhms mustBe "00010101000000"
      }

      "yyyyMMdd" in {
        import DateConverter.implicits._
        target.toYmd mustBe "00010101"
      }
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.component.cmn

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter

import spark.common.SparkContexts
import SparkContexts.context.implicits._
import spark.common.DbCtl
import spark.common.DfCtl.implicits._
import d2k.common.df.DbConnectionInfo
import d2k.common.TestArgs
import org.apache.spark.sql.DataFrame
import scala.util.Try
import spark.common.PqCtl

case class PostCodeConverterTestData(CD_POST: String, CD_POST2: String = "")
case class PostCodeInputData(CD_ZIP7LEN: String, CD_KENCD: String, CD_DEMEGRPCD: String)
class PostCodeConverterTest extends WordSpec with MustMatchers with BeforeAndAfter {
  implicit val inArgs = TestArgs().toInputArgs

  val pqDf = Seq(
    PostCodeInputData("0010010", "01", "101"),
    PostCodeInputData("00200  ", "01", "102"),
    PostCodeInputData("003    ", "01", "103"),
    PostCodeInputData("0040000", "01", "104"),
    PostCodeInputData("004    ", "02", "204"),
    PostCodeInputData("005    ", "01", "105"),
    PostCodeInputData("0060010", "", ""),
    PostCodeInputData("0070010", null, null)).toDF
  pqDf.write.mode("overwrite").parquet("test/dev/data/output/MBA935.pq")

  "PostCodeConverter" should {
    "normal end" when {
      "7桁パターン" when {
        val df = Seq(PostCodeConverterTestData("0010010")).toDF.cache
        "県コードのみ出力" in {
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD")).collect.apply(0)
          result.schema.fields.size mustBe 3
          result.getAs[String]("CD_KENCD") mustBe "01"
        }

        "県コード/市区町村両方出力" in {
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result.schema.fields.size mustBe 4
          result.getAs[String]("CD_KENCD") mustBe "01"
          result.getAs[String]("CD_DEMEGRPCD") mustBe "101"
        }

        "複数回呼出" in {
          val pccnv = PostCodeConverter()

          val result = (df ~> pccnv.localGovernmentCode("CD_POST")("CD_KENCD")).collect.apply(0)
          result.schema.fields.size mustBe 3
          result.getAs[String]("CD_KENCD") mustBe "01"

          val result2 = (df ~> pccnv.localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result2.schema.fields.size mustBe 4
          result2.getAs[String]("CD_KENCD") mustBe "01"
          result2.getAs[String]("CD_DEMEGRPCD") mustBe "101"
        }
      }

      "5桁パターン" when {
        val df = Seq(PostCodeConverterTestData("00200  ")).toDF.cache
        "複数回呼出" in {
          val pccnv = PostCodeConverter()

          val result = (df ~> pccnv.localGovernmentCode("CD_POST")("CD_KENCD")).collect.apply(0)
          result.schema.fields.size mustBe 3
          result.getAs[String]("CD_KENCD") mustBe "01"

          val result2 = (df ~> pccnv.localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result2.schema.fields.size mustBe 4
          result2.getAs[String]("CD_KENCD") mustBe "01"
          result2.getAs[String]("CD_DEMEGRPCD") mustBe "102"
        }
      }

      "3桁パターン" when {
        val df = Seq(PostCodeConverterTestData("003    ")).toDF.cache
        "複数回呼出" in {
          val pccnv = PostCodeConverter()

          val result = (df ~> pccnv.localGovernmentCode("CD_POST")("CD_KENCD")).collect.apply(0)
          result.schema.fields.size mustBe 3
          result.getAs[String]("CD_KENCD") mustBe "01"

          val result2 = (df ~> pccnv.localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result2.schema.fields.size mustBe 4
          result2.getAs[String]("CD_KENCD") mustBe "01"
          result2.getAs[String]("CD_DEMEGRPCD") mustBe "103"
        }
      }

      "8桁パターン" when {
        val df = Seq(PostCodeConverterTestData("001-0010  ")).toDF.cache
        "複数回呼出" in {
          val pccnv = PostCodeConverter()

          val result = (df ~> pccnv.localGovernmentCode("CD_POST")("CD_KENCD")).collect.apply(0)
          result.schema.fields.size mustBe 3
          result.getAs[String]("CD_KENCD") mustBe "01"

          val result2 = (df ~> pccnv.localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result2.schema.fields.size mustBe 4
          result2.getAs[String]("CD_KENCD") mustBe "01"
          result2.getAs[String]("CD_DEMEGRPCD") mustBe "101"
        }
      }

      "親子分割パターン" when {
        val df = Seq(PostCodeConverterTestData("001  ", "0010  ")).toDF.cache
        "複数回呼出" in {
          val pccnv = PostCodeConverter()

          val result = (df ~> pccnv.localGovernmentCode("CD_POST", "CD_POST2")("CD_KENCD")).collect.apply(0)
          result.schema.fields.size mustBe 3
          result.getAs[String]("CD_KENCD") mustBe "01"

          val result2 = (df ~> pccnv.localGovernmentCode("CD_POST", "CD_POST2")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result2.schema.fields.size mustBe 4
          result2.getAs[String]("CD_KENCD") mustBe "01"
          result2.getAs[String]("CD_DEMEGRPCD") mustBe "101"
        }
      }
    }

    "next match" when {
      "7桁パターン" when {
        "0000パターンでマッチ" in {
          val df = Seq(PostCodeConverterTestData("0049999")).toDF.cache
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result.schema.fields.size mustBe 4
          result.getAs[String]("CD_KENCD") mustBe "01"
          result.getAs[String]("CD_DEMEGRPCD") mustBe "104"
        }

        "スペースパターンでマッチ" in {
          val df = Seq(PostCodeConverterTestData("0059999")).toDF.cache
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result.schema.fields.size mustBe 4
          result.getAs[String]("CD_KENCD") mustBe "01"
          result.getAs[String]("CD_DEMEGRPCD") mustBe "105"
        }
      }

      "8桁パターン" when {
        "0000パターンでマッチ" in {
          val df = Seq(PostCodeConverterTestData("004-9999")).toDF.cache
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result.schema.fields.size mustBe 4
          result.getAs[String]("CD_KENCD") mustBe "01"
          result.getAs[String]("CD_DEMEGRPCD") mustBe "104"
        }

        "スペースパターンでマッチ" in {
          val df = Seq(PostCodeConverterTestData("005-9999")).toDF.cache
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result.schema.fields.size mustBe 4
          result.getAs[String]("CD_KENCD") mustBe "01"
          result.getAs[String]("CD_DEMEGRPCD") mustBe "105"
        }
      }

      "5桁パターン" when {
        "スペースパターンでマッチ" in {
          val df = Seq(PostCodeConverterTestData("00499")).toDF.cache
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result.schema.fields.size mustBe 4
          result.getAs[String]("CD_KENCD") mustBe "02"
          result.getAs[String]("CD_DEMEGRPCD") mustBe "204"
        }
      }

      "3桁パターン" when {
        "スペースパターンでマッチ" in {
          val df = Seq(PostCodeConverterTestData("004")).toDF.cache
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result.schema.fields.size mustBe 4
          result.getAs[String]("CD_KENCD") mustBe "02"
          result.getAs[String]("CD_DEMEGRPCD") mustBe "204"
        }
      }
    }

    "unmatched end" when {
      "7桁パターン" when {
        "県コードのみ出力" in {
          val df = Seq(PostCodeConverterTestData("0069999")).toDF.cache
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD")).collect.apply(0)
          result.schema.fields.size mustBe 3
          result.getAs[String]("CD_KENCD") mustBe "99"
        }

        "県コード/市区町村両方出力" in {
          val df = Seq(PostCodeConverterTestData("0069999")).toDF.cache
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result.schema.fields.size mustBe 4
          result.getAs[String]("CD_KENCD") mustBe "99"
          result.getAs[String]("CD_DEMEGRPCD") mustBe "999"
        }
      }

      "8桁パターン" when {
        "県コードのみ出力" in {
          val df = Seq(PostCodeConverterTestData("006-9999")).toDF.cache
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD")).collect.apply(0)
          result.schema.fields.size mustBe 3
          result.getAs[String]("CD_KENCD") mustBe "99"
        }

        "県コード/市区町村両方出力" in {
          val df = Seq(PostCodeConverterTestData("006-9999")).toDF.cache
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result.schema.fields.size mustBe 4
          result.getAs[String]("CD_KENCD") mustBe "99"
          result.getAs[String]("CD_DEMEGRPCD") mustBe "999"
        }
      }

      "5桁パターン" when {
        "県コードのみ出力" in {
          val df = Seq(PostCodeConverterTestData("00699")).toDF.cache
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD")).collect.apply(0)
          result.schema.fields.size mustBe 3
          result.getAs[String]("CD_KENCD") mustBe "99"
        }

        "県コード/市区町村両方出力" in {
          val df = Seq(PostCodeConverterTestData("00699")).toDF.cache
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result.schema.fields.size mustBe 4
          result.getAs[String]("CD_KENCD") mustBe "99"
          result.getAs[String]("CD_DEMEGRPCD") mustBe "999"
        }
      }

      "3桁パターン" when {
        "県コードのみ出力" in {
          val df = Seq(PostCodeConverterTestData("006")).toDF.cache
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD")).collect.apply(0)
          result.schema.fields.size mustBe 3
          result.getAs[String]("CD_KENCD") mustBe "99"
        }

        "県コード/市区町村両方出力" in {
          val df = Seq(PostCodeConverterTestData("006")).toDF.cache
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result.schema.fields.size mustBe 4
          result.getAs[String]("CD_KENCD") mustBe "99"
          result.getAs[String]("CD_DEMEGRPCD") mustBe "999"
        }
      }

      "3,5,7,8桁以外" when {
        "県コードのみ出力" in {
          val df = Seq(PostCodeConverterTestData("01")).toDF.cache
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD")).collect.apply(0)
          result.schema.fields.size mustBe 3
          result.getAs[String]("CD_KENCD") mustBe "99"
        }

        "県コード/市区町村両方出力" in {
          val df = Seq(PostCodeConverterTestData("01")).toDF.cache
          val result = (df ~> PostCodeConverter().localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
          result.schema.fields.size mustBe 4
          result.getAs[String]("CD_KENCD") mustBe "99"
          result.getAs[String]("CD_DEMEGRPCD") mustBe "999"
        }
      }
    }

    "abnormal data" when {
      "empty string" in {
        val df = Seq(PostCodeConverterTestData("0060010")).toDF.cache
        val pccnv = PostCodeConverter()

        val result = (df ~> pccnv.localGovernmentCode("CD_POST")("CD_KENCD")).collect.apply(0)
        result.schema.fields.size mustBe 3
        result.getAs[String]("CD_KENCD") mustBe "99"

        val result2 = (df ~> pccnv.localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
        result2.schema.fields.size mustBe 4
        result2.getAs[String]("CD_KENCD") mustBe "99"
        result2.getAs[String]("CD_DEMEGRPCD") mustBe "999"
      }

      "null value" in {
        val df = Seq(PostCodeConverterTestData("0070010")).toDF.cache
        val pccnv = PostCodeConverter()

        val result = (df ~> pccnv.localGovernmentCode("CD_POST")("CD_KENCD")).collect.apply(0)
        result.schema.fields.size mustBe 3
        result.getAs[String]("CD_KENCD") mustBe "99"

        val result2 = (df ~> pccnv.localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
        result2.schema.fields.size mustBe 4
        result2.getAs[String]("CD_KENCD") mustBe "99"
        result2.getAs[String]("CD_DEMEGRPCD") mustBe "999"
      }

      "input null value" in {
        val df = Seq(PostCodeConverterTestData(null)).toDF.cache
        val pccnv = PostCodeConverter()

        val result = (df ~> pccnv.localGovernmentCode("CD_POST")("CD_KENCD")).collect.apply(0)
        result.schema.fields.size mustBe 3
        result.getAs[String]("CD_KENCD") mustBe "99"

        val result2 = (df ~> pccnv.localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
        result2.schema.fields.size mustBe 4
        result2.getAs[String]("CD_KENCD") mustBe "99"
        result2.getAs[String]("CD_DEMEGRPCD") mustBe "999"
      }

      "input null value. with child" in {
        val df = Seq(PostCodeConverterTestData(null, null)).toDF.cache
        val pccnv = PostCodeConverter()

        val result = (df ~> pccnv.localGovernmentCode("CD_POST")("CD_KENCD")).collect.apply(0)
        result.schema.fields.size mustBe 3
        result.getAs[String]("CD_KENCD") mustBe "99"

        val result2 = (df ~> pccnv.localGovernmentCode("CD_POST")("CD_KENCD", "CD_DEMEGRPCD")).collect.apply(0)
        result2.schema.fields.size mustBe 4
        result2.getAs[String]("CD_KENCD") mustBe "99"
        result2.getAs[String]("CD_DEMEGRPCD") mustBe "999"
      }
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.component.sh

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter

import spark.common.SparkContexts
import SparkContexts.context.implicits._
import spark.common.DbCtl
import spark.common.DfCtl.implicits._
import d2k.common.df.DbConnectionInfo
import d2k.common.TestArgs
import org.apache.spark.sql.DataFrame

object CommissionBaseChannelSelectorTest {
  case class Data(key: String, CD_CHNLGRPCD: String, CD_CHNLDETAILCD: String)
  implicit class ToData(t3: Seq[Tuple3[String, String, String]]) {
    def toDF = t3.map { case (a, b, c) => Data(a, b, c) }.toDF
  }
}
class CommissionBaseChannelSelectorTest extends WordSpec with MustMatchers with BeforeAndAfter {
  implicit val inArgs = TestArgs().toInputArgs
  import CommissionBaseChannelSelector._
  import CommissionBaseChannelSelectorTest._

  def check(key: String, DV_OUTOBJDIV: String, DV_TRICALCOBJDIV: String) = { (df: DataFrame) =>
    df.filter($"key" === key).collect.foreach { row =>
      row.getAs[String]("DV_OUTOBJDIV") mustBe DV_OUTOBJDIV
      row.getAs[String]("DV_TRICALCOBJDIV") mustBe DV_TRICALCOBJDIV
    }
    df
  }

  val df = Seq(
    ("a", "999", "999"),
    ("b", "600", "641"),
    ("c", "   ", "641"),
    ("d", "600", "   "),
    ("e", "   ", "   ")).toDF

  "comm試算" should {
    "normal end with key" when {
      val result = df ~> comm試算("key").run ~> ((_: DataFrame).cache)
      "N-01-01, N-01-02" in result ~> check("a", "1", "1")
      "N-02-01-01" in result ~> check("b", "0", "1")
      "N-02-01-02" in result ~> check("c", "0", "1")
      "N-02-01-03" in result ~> check("d", "1", "0")
      "N-02-01-04" in result ~> check("e", "1", "1")
    }

    "normal end without key" when {
      val result = df ~> comm試算.run ~> ((_: DataFrame).cache)
      "N-01-01, N-01-02" in result ~> check("a", "1", "1")
      "N-02-01-01" in result ~> check("b", "0", "1")
      "N-02-01-02" in result ~> check("c", "0", "1")
      "N-02-01-03" in result ~> check("d", "1", "0")
      "N-02-01-04" in result ~> check("e", "1", "1")
    }
  }

  "comm実績_月次手数料" should {
    "normal end with key" when {
      val result = df ~> comm実績_月次手数料("key").run ~> ((_: DataFrame).cache)
      "N-02-02-01" in result ~> check("b", "0", "2")
      "N-02-02-02" in result ~> check("c", "0", "2")
      "N-02-02-03" in result ~> check("d", "2", "0")
      "N-02-02-04" in result ~> check("e", "1", "1")
    }

    "normal end without key" when {
      val result = df ~> comm実績_月次手数料.run ~> ((_: DataFrame).cache)
      "N-02-02-01" in result ~> check("b", "0", "2")
      "N-02-02-02" in result ~> check("c", "0", "2")
      "N-02-02-03" in result ~> check("d", "2", "0")
      "N-02-02-04" in result ~> check("e", "1", "1")
    }
  }

  "comm実績_割賦充当" should {
    "normal end with key" when {
      val result = df ~> comm実績_割賦充当("key").run ~> ((_: DataFrame).cache)
      "N-02-03-01" in result ~> check("b", "0", "3")
      "N-02-03-02" in result ~> check("c", "0", "3")
      "N-02-03-03" in result ~> check("d", "3", "0")
      "N-02-03-04" in result ~> check("e", "1", "1")
    }

    "normal end without key" when {
      val result = df ~> comm実績_割賦充当.run ~> ((_: DataFrame).cache)
      "N-02-03-01" in result ~> check("b", "0", "3")
      "N-02-03-02" in result ~> check("c", "0", "3")
      "N-02-03-03" in result ~> check("d", "3", "0")
      "N-02-03-04" in result ~> check("e", "1", "1")
    }
  }

  "comm実績_直営店" should {
    "normal end with key" when {
      val result = df ~> comm実績_直営店("key").run ~> ((_: DataFrame).cache)
      "N-02-04-01" in result ~> check("b", "0", "4")
      "N-02-04-02" in result ~> check("c", "0", "4")
      "N-02-04-03" in result ~> check("d", "4", "0")
      "N-02-04-04" in result ~> check("e", "0", "0")
    }

    "normal end without key" when {
      val result = df ~> comm実績_直営店.run ~> ((_: DataFrame).cache)
      "N-02-04-01" in result ~> check("b", "0", "4")
      "N-02-04-02" in result ~> check("c", "0", "4")
      "N-02-04-03" in result ~> check("d", "4", "0")
      "N-02-04-04" in result ~> check("e", "0", "0")
    }
  }

  "comm毎月割一時金" should {
    "normal end with key" when {
      val result = df ~> comm毎月割一時金("key").run ~> ((_: DataFrame).cache)
      "N-02-05-01" in result ~> check("b", "0", "5")
      "N-02-05-02" in result ~> check("c", "0", "5")
      "N-02-05-03" in result ~> check("d", "5", "0")
      "N-02-05-04" in result ~> check("e", "1", "1")
    }

    "normal end without key" when {
      val result = df ~> comm毎月割一時金.run ~> ((_: DataFrame).cache)
      "N-02-05-01" in result ~> check("b", "0", "5")
      "N-02-05-02" in result ~> check("c", "0", "5")
      "N-02-05-03" in result ~> check("d", "5", "0")
      "N-02-05-04" in result ~> check("e", "1", "1")
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter

class DbConnectionInfoTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "DbConnectionInfoTest" should {
    "dm1" in {
      val dm1 = DbConnectionInfo.dm1
      dm1.url mustBe "dm1_url"
      dm1.user mustBe "dm1_user"
      dm1.password mustBe "dm1_password"
    }

    "dwh1" in {
      val dwh1 = DbConnectionInfo.dwh1
      dwh1.url mustBe "dwh1_url"
      dwh1.user mustBe "dwh1_user"
      dwh1.password mustBe "dwh1_password"
    }

    "dwh2" in {
      val dwh2 = DbConnectionInfo.dwh2
      dwh2.url mustBe "dwh2_url"
      dwh2.user mustBe "dwh2_user"
      dwh2.password mustBe "dwh2_password"
    }

    "bat1" in {
      val bat1 = DbConnectionInfo.bat1
      bat1.url mustBe "bat1_url"
      bat1.user mustBe "bat1_user"
      bat1.password mustBe "bat1_password"
    }

    "csp1" in {
      val csp1 = DbConnectionInfo.csp1
      csp1.url mustBe "csp1_url"
      csp1.user mustBe "csp1_user"
      csp1.password mustBe "csp1_password"
    }

    "hi1" in {
      val hi1 = DbConnectionInfo.hi1
      hi1.url mustBe "hi1_url"
      hi1.user mustBe "hi1_user"
      hi1.password mustBe "hi1_password"
    }

    "mth1" in {
      val mth1 = DbConnectionInfo.mth1
      mth1.url mustBe "mth1_url"
      mth1.user mustBe "mth1_user"
      mth1.password mustBe "mth1_password"
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.executor

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import d2k.common.TestArgs
import d2k.common.InputArgs

import spark.common.SparkContexts.context
import context.implicits._
import spark.common.SparkContexts

import d2k.common.df.template.DfToDf
import d2k.common.df.template.DfToDb
import org.apache.spark.sql.SaveMode
import spark.common.DbCtl

case class ConvNaData(ts: String, dt: String, str: String)
case class Sp01(DT: String, TSTMP: String, VC: String, CH: String)
class ConvNaTest extends WordSpec with MustMatchers with BeforeAndAfter {

  implicit val inArgs = TestArgs().toInputArgs
  import SparkContexts.context.implicits._

  "Date" should {
    "spaceが正しく変換される" in {
      val data = Seq(ConvNaData("", "", "")).toDF
      val target = new DfToDf with ConvNaDate {
        val componentId = "test"
        val dateColumns = Seq("dt")
      }

      val r = target.run(data).as[ConvNaData].collect.head
      r.ts mustBe ""
      r.dt mustBe "0001-01-01"
      r.str mustBe ""
    }

    "nullが正しく変換される" in {
      val data = Seq(ConvNaData(null, null, null)).toDF
      val target = new DfToDf with ConvNaDate {
        val componentId = "test"
        val dateColumns = Seq("dt")
      }

      val r = target.run(data).as[ConvNaData].collect.head
      r.ts mustBe null
      r.dt mustBe "0001-01-01"
      r.str mustBe null
    }
  }

  "Timestamp" should {
    "spaceが正しく変換される" in {
      val data = Seq(ConvNaData("", "", "")).toDF
      val target = new DfToDf with ConvNaTs {
        val componentId = "test"
        val tsColumns = Seq("ts")
      }

      val r = target.run(data).as[ConvNaData].collect.head
      r.ts mustBe "0001-01-01 00:00:00"
      r.dt mustBe ""
      r.str mustBe ""
    }

    "nullが正しく変換される" in {
      val data = Seq(ConvNaData(null, null, null)).toDF
      val target = new DfToDf with ConvNaTs {
        val componentId = "test"
        val tsColumns = Seq("ts")
      }

      val r = target.run(data).as[ConvNaData].collect.head
      r.ts mustBe "0001-01-01 00:00:00"
      r.dt mustBe null
      r.str mustBe null
    }
  }

  "Date and Timestamp" should {
    "spaceが正しく変換される" in {
      val data = Seq(ConvNaData("", "", "")).toDF
      val target = new DfToDf with ConvNa {
        val componentId = "test"
        val tsColumns = Seq("ts")
        val dateColumns = Seq("dt")
      }

      val r = target.run(data).as[ConvNaData].collect.head
      r.ts mustBe "0001-01-01 00:00:00"
      r.dt mustBe "0001-01-01"
      r.str mustBe ""
    }

    "nullが正しく変換される" in {
      val data = Seq(ConvNaData(null, null, null)).toDF
      val target = new DfToDf with ConvNa {
        val componentId = "test"
        val tsColumns = Seq("ts")
        val dateColumns = Seq("dt")
      }

      val r = target.run(data).as[ConvNaData].collect.head
      r.ts mustBe "0001-01-01 00:00:00"
      r.dt mustBe "0001-01-01"
      r.str mustBe null
    }
  }

  val dbCtl = new DbCtl()
  import dbCtl.implicits._

  "Db Write" should {
    "正しくDBに書き込まれる" in {
      val data = Seq(Sp01("", "", "", ""), Sp01(null, null, null, null)).toDF
      val target = new DfToDb with ConvNa {
        val componentId = "test"
        val tsColumns = Seq("TSTMP")
        val dateColumns = Seq("DT")

        override lazy val writeTableName = "sp01"
        override val writeDbSaveMode = SaveMode.Overwrite
        override val writeDbWithCommonColumn = false
      }

      target.run(data)

      val result = dbCtl.readTable("sp01").as[Sp01].collect

      {
        val r = result(0)
        r.DT mustBe "0001-01-01 00:00:00"
        r.TSTMP mustBe "0001-01-01 00:00:00"
        r.VC mustBe null
        r.CH mustBe null
      }

      {
        val r = result(1)
        r.DT mustBe "0001-01-01 00:00:00"
        r.TSTMP mustBe "0001-01-01 00:00:00"
        r.VC mustBe null
        r.CH mustBe null
      }
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.executor.face

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import spark.common.SparkContexts
import SparkContexts.context.implicits._
import spark.common.DbCtl
import d2k.common.df.DbConnectionInfo
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

case class Data(d01: String, d02: String, d03: String, d04: String,
                d05: String, d06: String, d07: String, d08: String,
                d09: String, d10: String)

import d2k.common.df.template.DfToDf
import d2k.common.TestArgs

class DomainConvereterTest extends WordSpec with MustMatchers with BeforeAndAfter {
  implicit val inArgs = TestArgs().toInputArgs

  "pattern1" should {
    val compo = new DfToDf with DomainConverter {
      val targetColumns = Set(
        ("d01", "年月日"), ("d02", "年月"), ("d03", "月日"), ("d04", "年"), ("d05", "月"),
        ("d06", "日"), ("d07", "年月日時分秒"), ("d08", "年月日時分"), ("d09", "年月日時分ミリ秒"), ("d10", "年月日時"))
    }

    "be success" when {
      "nomal data" in {
        val df = Seq(Data("20170101", "201702", "0201", "2018", "03", "31",
          "20170102030405", "201701030000", "20170104050607999", "2017010500")).toDF

        val result = compo.run(df).as[Data].collect.head

        result.toString mustBe Data("2017-01-01", "201702", "0201", "2018", "03", "31",
          "2017-01-02 03:04:05", "201701030000", "2017-01-04 05:06:07.999", "2017010500").toString
      }

      "nomal data2" in {
        val df = Seq(Data("00010102", "201702", "0201", "2018", "03", "31",
          "99991231235959", "201701030000", "99991231235959999", "2017010500")).toDF

        val result = compo.run(df).as[Data].collect.head

        result.toString mustBe Data("0001-01-02", "201702", "0201", "2018", "03", "31",
          "9999-12-31 23:59:59", "201701030000", "9999-12-31 23:59:59.999", "2017010500").toString
      }

      "invalid data" in {
        val df = Seq(Data("x", "x", "x", "x", "x", "x", "x", "x", "x", "x")).toDF

        val result = compo.run(df).as[Data].collect.head

        result.toString mustBe Data("0001-01-01", "000101", "0101", "0001", "01", "01",
          "0001-01-01 00:00:00", "000101010000", "0001-01-01 00:00:00", "0001010100").toString
      }

      "null data" in {
        val df = Seq(Data(null, null, null, null, null, null, null, null, null, null)).toDF

        val result = compo.run(df).as[Data].collect.head

        result.toString mustBe Data("0001-01-01", "000101", "0101", "0001", "01", "01",
          "0001-01-01 00:00:00", "000101010000", "0001-01-01 00:00:00", "0001010100").toString
      }
    }
  }

  "pattern2" should {
    val compo = new DfToDf with DomainConverter {
      val targetColumns = Set(
        ("d01", "時分秒"), ("d02", "時分ミリ秒"), ("d03", "時分"), ("d04", "時"), ("d05", "分"),
        ("d06", "秒"), ("d07", "時間"))
    }

    "be success" when {
      "nomal data" in {
        val df = Seq(Data("010101", "010102000", "0102", "03", "04", "05",
          "000006", "", "", "")).toDF

        val result = compo.run(df).as[Data].collect.head

        result.toString mustBe Data("010101", "010102000", "0102", "03", "04", "05",
          "000006", "", "", "").toString
      }

      "invalid data" in {
        val df = Seq(Data("x", "x", "x", "x", "x", "x", "x", "", "", "")).toDF

        val result = compo.run(df).as[Data].collect.head

        result.toString mustBe Data("000000", "000000000", "0000", "00", "00", "00",
          "000000", "", "", "").toString
      }

      "null data" in {
        val df = Seq(Data(null, null, null, null, null, null, null, null, null, null)).toDF

        val result = compo.run(df).as[Data].collect.head

        result.toString mustBe Data("000000", "000000000", "0000", "00", "00", "00",
          "000000", null, null, null).toString
      }
    }
  }

  "pattern3" should {
    val compo = new DfToDf with DomainConverter {
      val targetColumns = Set(
        ("d01", "文字列"), ("d02", "文字列_trim_半角"), ("d03", "文字列_trim_全角"), ("d04", "文字列_trim_全半角"), ("d05", "文字列_trim_無し"),
        ("d06", "全角文字列"), ("d07", "全角文字列_trim_全角"), ("d08", "全角文字列_trim_無し"), ("d09", "通信方式"))
    }

    "be success" when {
      "nomal data" in {
        val df = Seq(Data("x", " xx ", "　あ　", "　 xxx　 ", "　 xxxx　 ", "い",
          "　う　", "　え　", "   123  ", "")).toDF

        val result = compo.run(df).as[Data].collect.head

        result.toString mustBe Data("x", "xx", "あ", "xxx", "　 xxxx　 ", "い",
          "う", "　え　", "1", "").toString
      }
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.executor

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row
import spark.common.SparkContexts
import d2k.common.SparkApp
import d2k.common.InputArgs
import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame

class NothingTest extends WordSpec with MustMatchers with BeforeAndAfter {
  val structType = StructType(Seq(
    StructField("dummy", StringType)))
  val df = SparkContexts.context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
    Row("aa"))), structType)
  implicit val inArgs = InputArgs("", "", "", "data/test/conv/conf/COM_DATEFILE_SK0.txt")

  "NothingTest" should {
    "normal end" in {
      val result = df.collect
      result(0).length mustBe 1
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.executor

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row
import spark.common.SparkContexts
import d2k.common.SparkApp
import d2k.common.InputArgs
import d2k.common.df.Executor
import org.apache.spark.sql.DataFrame
import spark.common.SparkContexts.context

class PqCommonColumnRemoverTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "PqCommonColumnRemover test" in {
    implicit val inArgs = InputArgs("", "", "", "data/test/conv/conf/COM_DATEFILE_SK0.txt")
    val structType = StructType(Seq(StructField("DUMMY1", StringType), StructField("DUMMY2", StringType), StructField("ROW_ERR", StringType), StructField("ROW_ERR_MESSAGE", StringType)))
    val beforeDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(Row("dummy1", "dummy2", "false", "dummy error message"))), structType)
    val beforeArr = beforeDf.collect
    val afterDf = PqCommonColumnRemover.apply(beforeDf)(inArgs)
    val afterArr = afterDf.collect
    
    beforeArr(0).length mustBe 4
    afterArr(0).length mustBe 2
    afterArr(0).getAs[String]("DUMMY1") mustBe "dummy1"
    afterArr(0).getAs[String]("DUMMY2") mustBe "dummy2"
  }
} 
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter
import spark.common.SparkContexts.context
import context.implicits._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.Row
import d2k.common.InputArgs
import org.apache.spark.sql.types._
import spark.common.SparkContexts
import spark.common.DbCtl
import org.apache.spark.sql.SaveMode
import java.sql.Timestamp
import WriteDbMode._
import scala.io.Source
import d2k.common.df.WriteFileMode._
import d2k.common.TestArgs
import spark.common.DbInfo
import java.sql.Date
import org.joda.time.DateTime
import java.sql.Timestamp
import d2k.common.ResourceInfo
import org.scalatest.Ignore
import d2k.common.df.executor.Nothing

class HiRDB_readTest extends WordSpec with MustMatchers with BeforeAndAfter {
  def d2s(dateMill: Long) = new DateTime(dateMill).toString("yyyy-MM-dd")
  def d2s(date: Date) = new DateTime(date).toString("yyyy-MM-dd")
  def d2s(date: Timestamp) = new DateTime(date).toString("yyyy-MM-dd hh:mm:ss")

  //テスト用DDL:　test/dev/conf/ddl/hirdb-make-schema.sql
  "HiRDB" should {
    implicit val inArgs = TestArgs().toInputArgs
    val structType = StructType(Seq(
      StructField("KEY", StringType), StructField("TEST", StringType)))
    val dbCtl = new DbCtl(DbConnectionInfo.hi1)
    import dbCtl.implicits._

    "success read table" in {
      val result = dbCtl.readTable("cs_test").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 1

      result(0).getAs[String]("KEY") mustBe "key1"
      result(0).getAs[String]("TEST") mustBe "aaa"
    }

    "success data check" in {
      val result = dbCtl.readTable("type_test").collect.head
      result.getAs[String]("KEY") mustBe "key1"
      result.getAs[String]("CHR") mustBe "chr  "
      result.getAs[String]("MVCHR") mustBe "xx漢字xx"
      result.getAs[Integer]("INT") mustBe 10000
      result.getAs[Integer]("SINT") mustBe 20000
      result.getAs[java.math.BigDecimal]("DCML") mustBe new java.math.BigDecimal("30.333")
      result.getAs[Float]("FLT") mustBe (400.444).toFloat
      result.getAs[Double]("SFLT") mustBe (5.0).toDouble
      result.getAs[Date]("DATE") mustBe DateTime.parse("2016-01-01").toDate
      result.getAs[Timestamp]("TMSTMP") mustBe new Timestamp(new DateTime(2016, 1, 1, 11, 22, 33).getMillis)
    }

    "success type check" in {
      val result = dbCtl.readTable("type_test")
      val fields = result.schema.fields

      val checkList = Seq(("KEY", "StringType"),
        ("CHR", "StringType"),
        ("VCHR", "StringType"),
        ("MCHR", "StringType"),
        ("INT", "IntegerType"),
        ("SINT", "IntegerType"),
        ("DCML", "DecimalType(5,3)"),
        ("FLT", "FloatType"),
        ("SFLT", "DoubleType"),
        ("DATE", "DateType"),
        ("TMSTMP", "TimestampType"))

      for {
        (name, t) <- checkList
        f <- fields.filter(_.name == name)
      } {
        f.dataType.toString mustBe t
      }
    }

    "success date check" in {
      val result = dbCtl.readTable("date_test").show
    }

    "success time check" in {
      try {
        val result = dbCtl.readTable("time_test").show
        fail
      } catch {
        case t: Throwable => t.printStackTrace()
      }
    }

    "success timestamp check" in {
      val result = dbCtl.readTable("tmstmp_test").show
    }

    "success insert" ignore {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq.empty[Row]), structType)
      val target = new WriteDb {
        val componentId = "cs_test"
      }
      val insertDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("key1", "aaa"))), structType)
      target.writeDb(insertDf)

      dbCtl.readTable("cs_test").show
      val result = dbCtl.readTable("cs_test").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 1

      result(0).getAs[String]("TEST") mustBe "aaa"
      d2s(result(0).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "cs_test"
    }

    def dateAddHyphen(str: String) = s"${str.take(4)}-${str.drop(4).take(2)}-${str.drop(6)}"
    "success 'where' test for date" in {
      val fileToDf = new template.DbToDf with Nothing {
        val componentId = "where_test"
        override val readDbInfo = DbConnectionInfo.hi1
        override def readDbWhere(inArgs: InputArgs) = Array(s""""DATE" = '${dateAddHyphen(inArgs.runningDates(0))}'""")
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 1
      result.head.getAs[String]("KEY") mustBe "wheretest_date"
    }

    "success 'where' test for timestamp" in {
      val fileToDf = new template.DbToDf with Nothing {
        val componentId = "where_test"
        override val readDbInfo = DbConnectionInfo.hi1
        override def readDbWhere(inArgs: InputArgs) = Array(s""""TMSTMP" = '${dateAddHyphen(inArgs.runningDates(0))} 00:00:00'""")
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 1
      result.head.getAs[String]("KEY") mustBe "wheretest_timestamp"
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.mixIn

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter
import d2k.common.df.template._
import d2k.common.df.executor._
import d2k.app.test.common.TestArgs
import scala.io.Source
import reflect.io._
import Path._

class OraLoaderHdfsTest extends WordSpec with MustMatchers with BeforeAndAfter {
  import spark.common.SparkContexts.context.implicits._
  implicit val inArgs = TestArgs().toInputArgs

  val compo1 = new DfToFile with OraLoaderHdfs with Nothing {
    val componentId = "test_hdfs"
  }

  "OraLoader" should {
    "normal end" in {
      val df = Seq(OraLoaderTestData("""ｶﾅ"1""", "ｶﾅ2")).toDF
      compo1.run(df)

      val filePath = s"${sys.env("DB_LOADING_FILE_PATH")}/${compo1.componentId}"
      val fileName = filePath.toDirectory.files.map(_.name).filter(_.endsWith(".csv")).toSeq.head
      Source.fromFile(s"${filePath}/${fileName}").getLines.foreach { line =>
        line mustBe """"ｶﾅ""1","ｶﾅ2""""
      }
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.mixIn

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter
import d2k.common.df.template._
import d2k.common.df.executor._
import d2k.app.test.common.TestArgs
import scala.io.Source

case class OraLoaderTestData(s1: String, s2: String)
class OraLoaderTest extends WordSpec with MustMatchers with BeforeAndAfter {
  import spark.common.SparkContexts.context.implicits._
  implicit val inArgs = TestArgs().toInputArgs

  val compo1 = new DfToFile with OraLoader with Nothing {
    val componentId = "test"
  }

  "OraLoader" should {
    "normal end" in {
      val df = Seq(OraLoaderTestData("""ｶﾅ"1""", "ｶﾅ2")).toDF
      compo1.run(df)

      val filePath = s"${sys.env("DB_LOADING_FILE_PATH")}/test"
      Source.fromFile(filePath)("MS932").getLines.foreach { line =>
        line mustBe """"ｶﾅ""1","ｶﾅ2""""
      }
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import d2k.common.TestArgs
import d2k.common.InputArgs
import d2k.common.df.template.DbToDf
import d2k.common.df.executor.Nothing
import spark.common.DbCtl
import org.apache.spark.sql.SaveMode

class ReadDbTest extends WordSpec with MustMatchers with BeforeAndAfter {
  implicit val inArgs = TestArgs().toInputArgs
  "SingleReadDb" should {
    "defined columns" in {
      val readDb = new SingleReadDb {
        val componentId = "compo"
        override lazy val readTableName = "SP02"
        override val columns = Array("DT", "VC", "CH")
      }
      val result = readDb.readDb(inArgs).collect
      result.size mustBe 4
      result.head.schema.size mustBe 3
      val names = result.head.schema.map(_.name)
      names.exists(_ == "DT") mustBe true
      names.exists(_ == "NUM5") mustBe false
      names.exists(_ == "NUM52") mustBe false
      names.exists(_ == "TSTMP") mustBe false
      names.exists(_ == "VC") mustBe true
      names.exists(_ == "CH") mustBe true
    }

    "not defined columns" in {
      val readDb = new SingleReadDb {
        val componentId = "compo"
        override lazy val readTableName = "SP02"
      }
      val result = readDb.readDb.collect
      result.size mustBe 4
      result.head.schema.size mustBe 6
      val names = result.head.schema.map(_.name)
      names.exists(_ == "DT") mustBe true
      names.exists(_ == "NUM5") mustBe true
      names.exists(_ == "NUM52") mustBe true
      names.exists(_ == "TSTMP") mustBe true
      names.exists(_ == "VC") mustBe true
      names.exists(_ == "CH") mustBe true
    }

    "defined column and readDbWhere " in {
      val readDb = new SingleReadDb {
        val componentId = "compo"
        override lazy val readTableName = "SP02"
        override val columns = Array("DT", "NUM5", "VC", "CH")
        override val readDbWhere = Array("NUM5 = '2000'")
      }
      val result = readDb.readDb.collect
      result.size mustBe 1
      result.head.schema.size mustBe 4
      val names = result.head.schema.map(_.name)
      names.exists(_ == "DT") mustBe true
      names.exists(_ == "NUM5") mustBe true
      names.exists(_ == "NUM52") mustBe false
      names.exists(_ == "TSTMP") mustBe false
      names.exists(_ == "VC") mustBe true
      names.exists(_ == "CH") mustBe true

      result(0).getAs[java.math.BigDecimal]("NUM5").toString mustBe "2000"
    }

    "defined column and readDbWhereArgs " in {
      val readDb = new SingleReadDb {
        val componentId = "compo"
        override lazy val readTableName = "SP02"
        override val columns = Array("DT", "NUM5", "VC", "CH")
        override def readDbWhere(inArgs: InputArgs) = Array("NUM5 = '2000'")
      }
      val result = readDb.readDb.collect
      result.size mustBe 1
      result.head.schema.size mustBe 4
      val names = result.head.schema.map(_.name)
      names.exists(_ == "DT") mustBe true
      names.exists(_ == "NUM5") mustBe true
      names.exists(_ == "NUM52") mustBe false
      names.exists(_ == "TSTMP") mustBe false
      names.exists(_ == "VC") mustBe true
      names.exists(_ == "CH") mustBe true

      result(0).getAs[java.math.BigDecimal]("NUM5").toString mustBe "2000"
    }

    "not defined column and defined readDbWhere " in {
      val readDb = new SingleReadDb {
        val componentId = "compo"
        override lazy val readTableName = "SP02"
        override val readDbWhere = Array("NUM5 = '2000'")
      }
      val result = readDb.readDb.collect
      result.size mustBe 1
      result.head.schema.size mustBe 6
      val names = result.head.schema.map(_.name)
      names.exists(_ == "DT") mustBe true
      names.exists(_ == "NUM5") mustBe true
      names.exists(_ == "NUM52") mustBe true
      names.exists(_ == "TSTMP") mustBe true
      names.exists(_ == "VC") mustBe true
      names.exists(_ == "CH") mustBe true

      result(0).getAs[java.math.BigDecimal]("NUM5").toString mustBe "2000"
    }

    "not defined column and defined readDbWhereArgs " in {
      val readDb = new SingleReadDb {
        val componentId = "compo"
        override lazy val readTableName = "SP02"
        override def readDbWhere(inArgs: InputArgs) = Array("NUM5 = '2000'")
      }
      val result = readDb.readDb.collect
      result.size mustBe 1
      result.head.schema.size mustBe 6
      val names = result.head.schema.map(_.name)
      names.exists(_ == "DT") mustBe true
      names.exists(_ == "NUM5") mustBe true
      names.exists(_ == "NUM52") mustBe true
      names.exists(_ == "TSTMP") mustBe true
      names.exists(_ == "VC") mustBe true
      names.exists(_ == "CH") mustBe true

      result(0).getAs[java.math.BigDecimal]("NUM5").toString mustBe "2000"
    }
  }

  "MultiReadDb" should {
    val copyDb = new DbToDf with Nothing {
      val componentId = "compo"
      override lazy val readTableName = "SP02"
    }

    val dbCtl = new DbCtl(copyDb.readDbInfo)
    import dbCtl.implicits._
    copyDb.run(Unit).write.mode(SaveMode.Overwrite).jdbc(dbCtl.dbInfo.url, "SP02_tmp", dbCtl.props)

    "defined columns" in {
      val readDb = new MultiReadDb {
        val componentId = "compo"
        val readTableNames = Seq("SP02", "SP02_tmp")
        override val columns = Array("DT", "VC", "CH")
      }
      readDb.readDb(inArgs).foreach { data =>
        val result = data._2.collect
        result.size mustBe 4
        result.head.schema.size mustBe 3
        val names = result.head.schema.map(_.name)
        names.exists(_ == "DT") mustBe true
        names.exists(_ == "NUM5") mustBe false
        names.exists(_ == "NUM52") mustBe false
        names.exists(_ == "TSTMP") mustBe false
        names.exists(_ == "VC") mustBe true
        names.exists(_ == "CH") mustBe true
      }
    }

    "not defined columns" in {
      val readDb = new MultiReadDb {
        val componentId = "compo"
        val readTableNames = Seq("SP02", "SP02_tmp")
      }
      readDb.readDb(inArgs).foreach { data =>
        val result = data._2.collect
        result.size mustBe 4
        result.head.schema.size mustBe 6
        val names = result.head.schema.map(_.name)
        names.exists(_ == "DT") mustBe true
        names.exists(_ == "NUM5") mustBe true
        names.exists(_ == "NUM52") mustBe true
        names.exists(_ == "TSTMP") mustBe true
        names.exists(_ == "VC") mustBe true
        names.exists(_ == "CH") mustBe true
      }
    }

    "defined column and readDbWhere " in {
      val readDb = new MultiReadDb {
        val componentId = "compo"
        val readTableNames = Seq("SP02", "SP02_tmp")
        override val columns = Array("DT", "NUM5", "VC", "CH")
        override val readDbWhere = Array("NUM5 = '2000'")
      }
      readDb.readDb(inArgs).foreach { data =>
        val result = data._2.collect
        result.size mustBe 1
        result.head.schema.size mustBe 4
        val names = result.head.schema.map(_.name)
        names.exists(_ == "DT") mustBe true
        names.exists(_ == "NUM5") mustBe true
        names.exists(_ == "NUM52") mustBe false
        names.exists(_ == "TSTMP") mustBe false
        names.exists(_ == "VC") mustBe true
        names.exists(_ == "CH") mustBe true
        result(0).getAs[java.math.BigDecimal]("NUM5").toString mustBe "2000"
      }
    }

    "defined column and readDbWhereArgs " in {
      val readDb = new MultiReadDb {
        val componentId = "compo"
        val readTableNames = Seq("SP02", "SP02_tmp")
        override val columns = Array("DT", "NUM5", "VC", "CH")
        override def readDbWhere(inArgs: InputArgs) = Array("NUM5 = '2000'")
      }
      readDb.readDb(inArgs).foreach { data =>
        val result = data._2.collect
        result.size mustBe 1
        result.head.schema.size mustBe 4
        val names = result.head.schema.map(_.name)
        names.exists(_ == "DT") mustBe true
        names.exists(_ == "NUM5") mustBe true
        names.exists(_ == "NUM52") mustBe false
        names.exists(_ == "TSTMP") mustBe false
        names.exists(_ == "VC") mustBe true
        names.exists(_ == "CH") mustBe true

        result(0).getAs[java.math.BigDecimal]("NUM5").toString mustBe "2000"
      }
    }

    "not defined column and defined readDbWhere " in {
      val readDb = new MultiReadDb {
        val componentId = "compo"
        val readTableNames = Seq("SP02", "SP02_tmp")
        override val readDbWhere = Array("NUM5 = '2000'")
      }
      readDb.readDb(inArgs).foreach { data =>
        val result = data._2.collect
        result.size mustBe 1
        result.head.schema.size mustBe 6
        val names = result.head.schema.map(_.name)
        names.exists(_ == "DT") mustBe true
        names.exists(_ == "NUM5") mustBe true
        names.exists(_ == "NUM52") mustBe true
        names.exists(_ == "TSTMP") mustBe true
        names.exists(_ == "VC") mustBe true
        names.exists(_ == "CH") mustBe true

        result(0).getAs[java.math.BigDecimal]("NUM5").toString mustBe "2000"
      }
    }

    "not defined column and defined readDbWhereArgs " in {
      val readDb = new MultiReadDb {
        val componentId = "compo"
        val readTableNames = Seq("SP02", "SP02_tmp")
        override def readDbWhere(inArgs: InputArgs) = Array("NUM5 = '2000'")
      }
      readDb.readDb(inArgs).foreach { data =>
        val result = data._2.collect
        result.size mustBe 1
        result.head.schema.size mustBe 6
        val names = result.head.schema.map(_.name)
        names.exists(_ == "DT") mustBe true
        names.exists(_ == "NUM5") mustBe true
        names.exists(_ == "NUM52") mustBe true
        names.exists(_ == "TSTMP") mustBe true
        names.exists(_ == "VC") mustBe true
        names.exists(_ == "CH") mustBe true

        result(0).getAs[java.math.BigDecimal]("NUM5").toString mustBe "2000"
      }
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import d2k.common.TestArgs
import d2k.common.InputArgs
import spark.common.SparkContexts
import SparkContexts.context.implicits._
import d2k.common.df.template.PqToDf
import d2k.common.df.executor.Nothing
import d2k.common.df.template.MultiPqToMapDf
import spark.common.PqCtl

object ReadPqTest {
  case class MultiReadTest(s: String)
}

class ReadPqTest extends WordSpec with MustMatchers with BeforeAndAfter {
  import ReadPqTest._
  implicit val inArgs = TestArgs().toInputArgs
  "strict check mode" should {
    "trueで指定したファイルが存在しない場合Exceptionが発生する" in {
      val pqToDf = new PqToDf with Nothing {
        val componentId = "ReadPqTest"
        override val readPqStrictCheckMode = true
      }
      try {
        pqToDf.run(Unit)
        fail
      } catch {
        case t: Throwable => {
          t.getClass.getName mustBe "org.apache.spark.sql.AnalysisException"
          t.getMessage must startWith("Path does not exist: file:")
        }
      }
    }

    "falseで指定したファイルが存在しない場合Exceptionが発生しない" when {
      "readPqEmptySchema未指定" in {
        val pqToDf = new PqToDf with Nothing {
          val componentId = "ReadPqTest"
          override val readPqStrictCheckMode = false
        }
        pqToDf.run(Unit)
      }

      "readPqEmptySchema指定" in {
        val pqToDf = new PqToDf with Nothing {
          val componentId = "ReadPqTest"
          override val readPqStrictCheckMode = false
          override val readPqEmptySchema = Seq(("a", "string"), ("b", "decimal"), ("c", "date"))
        }
        val schema = pqToDf.run(Unit).schema.map(_.toString)
        schema(0) mustBe "StructField(a,StringType,true)"
        schema(1) mustBe "StructField(b,DecimalType(10,0),true)"
        schema(2) mustBe "StructField(c,DateType,true)"
      }

      "readPqEmptySchema指定1" in {
        val pqToDf = new PqToDf with Nothing {
          val componentId = "ReadPqTest"
          override val readPqStrictCheckMode = false
          override val readPqEmptySchema = Seq(("VC_ADDUPYM", "String"), ("VC_ADDUPDT", "String"), ("VC_ADDUPYMD", "String"), ("CD_SISYAPARTITAFTSISYACD", "String"), ("DV_MNGCSCDIV", "String"), ("DV_PDCTCTGRY1", "String"), ("DV_PDCTCTGRY2", "String"), ("DV_INFTELKIND", "String"), ("DV_DATAPRIDIV", "String"), ("DV_MNPTRNMTCOCD", "String"), ("DV_MNPTRNSKCOCD", "String"), ("NM_NEWSU", "BigDecimal"), ("NM_MDLCHGCTRCP", "BigDecimal"), ("NM_MDLCHGCTHOLD", "BigDecimal"), ("NM_MDLCHGDECRHOLD", "BigDecimal"), ("NM_CNCLSU", "BigDecimal"), ("NM_SUMTTLWRKSU", "BigDecimal"), ("NM_INCRCOUNT", "BigDecimal"), ("NM_AUSMAVALSUMTTLCONTSU", "BigDecimal"), ("NM_MNPNEWSU", "BigDecimal"), ("NM_MNPCNCLSU", "BigDecimal"), ("NM_TDYDTNEWSU", "BigDecimal"), ("NM_TDYDTMDLCHGCTRCP", "BigDecimal"), ("NM_TDYDTMDLCHGCTHOLD", "BigDecimal"), ("NM_TDYDTMDLCHGDECRHOLD", "BigDecimal"), ("NM_TDYDTCNCLSU", "BigDecimal"), ("NM_TDYDTINCRCOUNT", "BigDecimal"), ("NM_TDYDTAUSMAVALINCRCOUNT", "BigDecimal"), ("NM_TDYDTMNPNEWSU", "BigDecimal"), ("NM_TDYDTMNPCNCLSU", "BigDecimal"))
        }
        pqToDf.run(Unit).schema.foreach(println)
      }
    }
  }

  "Single Read" should {
    "指定したParquetをDataFrameに変換できる" in {
      import SparkContexts.context.implicits._

      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._
      Seq(MultiReadTest("a")).toDF.writeParquet("A.pq")
      val mapDf = new PqToDf with Nothing {
        val componentId = "ReadPqTest"
        override lazy val readPqName = "A.pq"
      }

      val result = mapDf.run(Unit)
      result.collect.foreach { row =>
        row.getAs[String]("s") mustBe "a"
      }
    }
  }

  "Multi Read" should {
    "複数指定したParquetをDataFrameに変換できる" in {
      import SparkContexts.context.implicits._

      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._
      Seq(MultiReadTest("b")).toDF.writeParquet("B.pq")
      Seq(MultiReadTest("c")).toDF.writeParquet("C.pq")
      val mapDf = new MultiPqToMapDf with Nothing {
        val componentId = "ReadPqTest"
        override val readPqNames = Seq("B.pq", "C.pq")
      }

      val result = mapDf.run(Unit)
      result("B.pq").collect.foreach { row =>
        row.getAs[String]("s") mustBe "b"
      }
      result("C.pq").collect.foreach { row =>
        row.getAs[String]("s") mustBe "c"
      }
    }
  }

  "Many Read" should {
    "be normal end" in {
      import SparkContexts.context.implicits._

      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._
      Seq(MultiReadTest("b")).toDF.writeParquet("B.pq")
      Seq(MultiReadTest("c")).toDF.writeParquet("C.pq")
      val df = new PqToDf with Nothing {
        val componentId = "ReadPqTest"
        override lazy val readPqName = "B.pq, C.pq"
      }

      val result = df.run(Unit).collect
      result(0).getAs[String]("s") mustBe "b"
      result(1).getAs[String]("s") mustBe "c"
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template.base

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter

import d2k.common.df.executor
import d2k.common.df.template

import org.apache.spark.sql.functions._
import spark.common.SparkContexts.context.implicits._
import spark.common.DfCtl._
import implicits._
import org.apache.spark.sql.DataFrame
import d2k.common.TestArgs

class TwoDfJoinToAnyTest extends WordSpec with MustMatchers with BeforeAndAfter {
  implicit val inArgs = TestArgs().toInputArgs

  case class TestA(key: String = "key", a: String = "a1")
  case class TestB(key: String = "key", a: String = "a2", b: String = "b2")

  "mergeDropDuplicate" should {
    val dfA = Seq(TestA()).toDF
    val dfB = Seq(TestB()).toDF
    "be success" in {
      val comp = new template.DfJoinToDf with executor.Nothing {
        val componentId = "test"
        def joinExprs(left: DataFrame, right: DataFrame) = left("key") === right("key")
        def select(left: DataFrame, right: DataFrame) = mergeDropDuplicate(left, right)
      }

      val result = comp.run(dfA, dfB).collect
      result.foreach { row =>
        row.schema.size mustBe 3
        row.getAs[String]("key") mustBe "key"
        row.getAs[String]("a") mustBe "a1"
        row.getAs[String]("b") mustBe "b2"
      }
    }
  }

  "mergeWithName" should {
    val dfA = Seq(TestA()).toDF
    val dfB = Seq(TestB()).toDF
    "be success" in {
      val comp = new template.DfJoinToDf with executor.Nothing {
        val componentId = "test"
        def joinExprs(left: DataFrame, right: DataFrame) = left("key") === right("key")
        def select(left: DataFrame, right: DataFrame) = mergeWithPrefix(left, right, "XXX")
      }

      val result = comp.run(dfA, dfB).collect
      result.foreach { row =>
        row.schema.size mustBe 5
        row.getAs[String]("key") mustBe "key"
        row.getAs[String]("a") mustBe "a1"
        row.getAs[String]("XXX_key") mustBe "key"
        row.getAs[String]("XXX_a") mustBe "a2"
        row.getAs[String]("XXX_b") mustBe "b2"
      }
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import spark.common.SparkContexts.context
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions._
import d2k.common.df.executor
import d2k.common.df.template
import d2k.common.df.template.base.JoinPqInfo
import d2k.common.TestArgs
import spark.common.PqCtl

class DfJoinPqToDfTest extends WordSpec with MustMatchers with BeforeAndAfter {
  case class Test(key: String, value: String)
  case class Test2(key1: String, key2: String, value: String)
  case class Test3(key: String, value: String, value1: String, value2: String)

  "DfJoinPqToDf" should {
    "be success single data" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinPqToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"
        val joinPqInfoList = Seq(
          JoinPqInfo("testPq1", col("org#key1") === col("testPq1#key")))
      }

      val df1 = context.createDataFrame(Seq(Test2("key1", "key2", "aaa"), Test2("key11", "key22", "xxx")))
      val df2 = context.createDataFrame(Seq(Test("key1", "bbb")))
      df2.writeParquet("testPq1")

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "key1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("testPq1#key") mustBe "key1"
      result(0).getAs[String]("testPq1#value") mustBe "bbb"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("testPq1#key") mustBe null
      result(1).getAs[String]("testPq1#value") mustBe null
    }

    "be success multi data" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinPqToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"
        val joinPqInfoList = Seq(
          JoinPqInfo("testPq1", col("org#key1") === col("testPq1#key")),
          JoinPqInfo("testPq2", col("org#key2") === col("testPq2#key")))
      }

      val df1 = context.createDataFrame(Seq(Test2("key1", "key2", "aaa"), Test2("key11", "key22", "xxx")))
      val df2 = context.createDataFrame(Seq(Test("key1", "bbb")))
      df2.writeParquet("testPq1")
      val df3 = context.createDataFrame(Seq(Test("key2", "ccc"), Test("key22", "ddd")))
      df3.writeParquet("testPq2")

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "key1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("testPq1#key") mustBe "key1"
      result(0).getAs[String]("testPq1#value") mustBe "bbb"
      result(0).getAs[String]("testPq2#key") mustBe "key2"
      result(0).getAs[String]("testPq2#value") mustBe "ccc"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("testPq1#key") mustBe null
      result(1).getAs[String]("testPq1#value") mustBe null
      result(1).getAs[String]("testPq2#key") mustBe "key22"
      result(1).getAs[String]("testPq2#value") mustBe "ddd"
    }

    "be success drop column" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinPqToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"
        val joinPqInfoList = Seq(
          JoinPqInfo("testPq1", col("org#key1") === col("testPq1#key")),
          JoinPqInfo("testPq2", col("org#key2") === col("testPq2#key"), Set("testPq2#value1")))
      }

      val df1 = context.createDataFrame(Seq(Test2("key1", "key2", "aaa"), Test2("key11", "key22", "xxx")))
      val df2 = context.createDataFrame(Seq(Test("key1", "bbb")))
      df2.writeParquet("testPq1")
      val df3 = context.createDataFrame(Seq(
        Test3("key2", "ccc", "ccc1", "ccc2"),
        Test3("key22", "ddd", "ddd1", "ddd2")))
      df3.writeParquet("testPq2")

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "key1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("testPq1#key") mustBe "key1"
      result(0).getAs[String]("testPq1#value") mustBe "bbb"
      result(0).getAs[String]("testPq2#key") mustBe "key2"
      result(0).getAs[String]("testPq2#value") mustBe "ccc"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("testPq1#key") mustBe null
      result(1).getAs[String]("testPq1#value") mustBe null
      result(1).getAs[String]("testPq2#key") mustBe "key22"
      result(1).getAs[String]("testPq2#value") mustBe "ddd"
    }

    "be success change prefix name" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinPqToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"
        val joinPqInfoList = Seq(
          JoinPqInfo("testPq1", col("org#key1") === col("pre#key"), prefixName = "pre"),
          JoinPqInfo("testPq2", col("org#key2") === col("testPq2#key")))
      }

      val df1 = context.createDataFrame(Seq(Test2("key1", "key2", "aaa"), Test2("key11", "key22", "xxx")))
      val df2 = context.createDataFrame(Seq(Test("key1", "bbb")))
      df2.writeParquet("testPq1")
      val df3 = context.createDataFrame(Seq(Test("key2", "ccc"), Test("key22", "ddd")))
      df3.writeParquet("testPq2")

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "key1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("pre#key") mustBe "key1"
      result(0).getAs[String]("pre#value") mustBe "bbb"
      result(0).getAs[String]("testPq2#key") mustBe "key2"
      result(0).getAs[String]("testPq2#value") mustBe "ccc"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("pre#key") mustBe null
      result(1).getAs[String]("pre#value") mustBe null
      result(1).getAs[String]("testPq2#key") mustBe "key22"
      result(1).getAs[String]("testPq2#value") mustBe "ddd"
    }

    "be success env input data" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinPqToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"
        val joinPqInfoList = Seq(
          JoinPqInfo("testPq3", col("org#key1") === col("testPq3#key")))
        override lazy val envName = "TEST"
      }

      val df1 = context.createDataFrame(Seq(Test2("key1", "key2", "aaa"), Test2("key11", "key22", "xxx")))
      val df2 = context.createDataFrame(Seq(Test("key1", "bbb")))
      df2.writeParquet("testPq3")

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "key1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("testPq3#key") mustBe "key1"
      result(0).getAs[String]("testPq3#value") mustBe "bbb"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("testPq3#key") mustBe null
      result(1).getAs[String]("testPq3#value") mustBe null
    }

  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions._

import spark.common.SparkContexts.context
import spark.common.PqCtl

import d2k.common.df.executor
import d2k.common.df.template
import d2k.common.TestArgs
import d2k.common.df.VariableJoin
import d2k.common.df.PqInfo
import d2k.common.df.FixedInfo
import d2k.common.df.CsvInfo
import d2k.common.df.TsvInfo
import d2k.common.df.VsvInfo
import d2k.common.df.SsvInfo

class DfJoinVariableToDfTest extends WordSpec with MustMatchers with BeforeAndAfter {
  case class Test(key: String, value: String)
  case class Test2(key1: String, key2: String, value: String)
  case class Test3(key: String, value: String, value1: String, value2: String)

  "parquet" should {
    "be success single join data" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinVariableToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"
        val joins = Seq(VariableJoin(PqInfo("testPq1"), col("org#key1") === col("testPq1#key")))
      }

      val df1 = context.createDataFrame(Seq(Test2("key1", "key2", "aaa"), Test2("key11", "key22", "xxx")))
      val df2 = context.createDataFrame(Seq(Test("key1", "bbb")))
      df2.writeParquet("testPq1")

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "key1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("testPq1#key") mustBe "key1"
      result(0).getAs[String]("testPq1#value") mustBe "bbb"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("testPq1#key") mustBe null
      result(1).getAs[String]("testPq1#value") mustBe null
    }

    "be success multi data" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinVariableToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"
        val joins = Seq(
          VariableJoin(PqInfo("testPq1"), col("org#key1") === col("testPq1#key")),
          VariableJoin(PqInfo("testPq2"), col("org#key2") === col("testPq2#key")))
      }

      val df1 = context.createDataFrame(Seq(Test2("key1", "key2", "aaa"), Test2("key11", "key22", "xxx")))
      val df2 = context.createDataFrame(Seq(Test("key1", "bbb")))
      df2.writeParquet("testPq1")
      val df3 = context.createDataFrame(Seq(Test("key2", "ccc"), Test("key22", "ddd")))
      df3.writeParquet("testPq2")

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "key1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("testPq1#key") mustBe "key1"
      result(0).getAs[String]("testPq1#value") mustBe "bbb"
      result(0).getAs[String]("testPq2#key") mustBe "key2"
      result(0).getAs[String]("testPq2#value") mustBe "ccc"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("testPq1#key") mustBe null
      result(1).getAs[String]("testPq1#value") mustBe null
      result(1).getAs[String]("testPq2#key") mustBe "key22"
      result(1).getAs[String]("testPq2#value") mustBe "ddd"
    }

    "be success drop column" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinVariableToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"
        val joins = Seq(
          VariableJoin(PqInfo("testPq1"), col("org#key1") === col("testPq1#key")),
          VariableJoin(PqInfo("testPq2"), col("org#key2") === col("testPq2#key"), dropCols = Set("testPq2#value1")))
      }

      val df1 = context.createDataFrame(Seq(Test2("key1", "key2", "aaa"), Test2("key11", "key22", "xxx")))
      val df2 = context.createDataFrame(Seq(Test("key1", "bbb")))
      df2.writeParquet("testPq1")
      val df3 = context.createDataFrame(Seq(
        Test3("key2", "ccc", "ccc1", "ccc2"),
        Test3("key22", "ddd", "ddd1", "ddd2")))
      df3.writeParquet("testPq2")

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "key1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("testPq1#key") mustBe "key1"
      result(0).getAs[String]("testPq1#value") mustBe "bbb"
      result(0).getAs[String]("testPq2#key") mustBe "key2"
      result(0).getAs[String]("testPq2#value") mustBe "ccc"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("testPq1#key") mustBe null
      result(1).getAs[String]("testPq1#value") mustBe null
      result(1).getAs[String]("testPq2#key") mustBe "key22"
      result(1).getAs[String]("testPq2#value") mustBe "ddd"
    }

    "be success change prefix name" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinVariableToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"
        val joins = Seq(
          VariableJoin(PqInfo("testPq1"), col("org#key1") === col("pre1#key"), "pre1"),
          VariableJoin(PqInfo("testPq2"), col("org#key2") === col("pre2#key"), prefixName = "pre2"))
      }

      val df1 = context.createDataFrame(Seq(Test2("key1", "key2", "aaa"), Test2("key11", "key22", "xxx")))
      val df2 = context.createDataFrame(Seq(Test("key1", "bbb")))
      df2.writeParquet("testPq1")
      val df3 = context.createDataFrame(Seq(Test("key2", "ccc"), Test("key22", "ddd")))
      df3.writeParquet("testPq2")

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "key1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("pre1#key") mustBe "key1"
      result(0).getAs[String]("pre1#value") mustBe "bbb"
      result(0).getAs[String]("pre2#key") mustBe "key2"
      result(0).getAs[String]("pre2#value") mustBe "ccc"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("pre1#key") mustBe null
      result(1).getAs[String]("pre1#value") mustBe null
      result(1).getAs[String]("pre2#key") mustBe "key22"
      result(1).getAs[String]("pre2#value") mustBe "ddd"
    }

    "be success env input data" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinVariableToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"
        val joins = Seq(VariableJoin(PqInfo("testPq3", "TEST"), col("org#key1") === col("testPq3#key")))
      }

      val df1 = context.createDataFrame(Seq(Test2("key1", "key2", "aaa"), Test2("key11", "key22", "xxx")))
      val df2 = context.createDataFrame(Seq(Test("key1", "bbb")))
      df2.writeParquet("testPq3")

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "key1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("testPq3#key") mustBe "key1"
      result(0).getAs[String]("testPq3#value") mustBe "bbb"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("testPq3#key") mustBe null
      result(1).getAs[String]("testPq3#value") mustBe null
    }
  }

  "file common funcs" should {
    "be success single join data" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinVariableToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"
        val joins = Seq(
          VariableJoin(FixedInfo(Set("fixed.dat"), itemConfId = "fixed_change_conf_id"), col("org#key1") === col("fixed_change_conf_id#item1")))
      }

      val df1 = context.createDataFrame(Seq(Test2("a1", "key2", "aaa"), Test2("key11", "key22", "xxx")))

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "a1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("fixed_change_conf_id#item1") mustBe "a1"
      result(0).getAs[String]("fixed_change_conf_id#item2") mustBe "bb1"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("fixed_change_conf_id#item1") mustBe null
      result(1).getAs[String]("fixed_change_conf_id#item2") mustBe null
    }

    "be success any joined data" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinVariableToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"
        val joins = Seq(
          VariableJoin(FixedInfo(Set("fixed.dat"), itemConfId = "fixed_change_conf_id"), col("org#key1") === col("fixed_change_conf_id#item1")),
          VariableJoin(FixedInfo(Set("fixed.dat"), itemConfId = "fixed_change_conf_id"), col("org#key1") === col("pre#item1"), "pre"))
      }

      val df1 = context.createDataFrame(Seq(Test2("a1", "key2", "aaa"), Test2("key11", "key22", "xxx")))

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "a1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("fixed_change_conf_id#item1") mustBe "a1"
      result(0).getAs[String]("fixed_change_conf_id#item2") mustBe "bb1"
      result(0).getAs[String]("pre#item1") mustBe "a1"
      result(0).getAs[String]("pre#item2") mustBe "bb1"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("fixed_change_conf_id#item1") mustBe null
      result(1).getAs[String]("fixed_change_conf_id#item2") mustBe null
      result(1).getAs[String]("pre#item1") mustBe null
      result(1).getAs[String]("pre#item2") mustBe null
    }

    "be success file not found" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinVariableToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"
        val joins = Seq(
          VariableJoin(FixedInfo(Set("notFound.dat"), itemConfId = "fixed_change_conf_id"), col("org#key1") === col("fixed_change_conf_id#item1")))
      }

      val df1 = context.createDataFrame(Seq(Test2("a1", "key2", "aaa"), Test2("key11", "key22", "xxx")))

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "a1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("fixed_change_conf_id#item1") mustBe null
      result(0).getAs[String]("fixed_change_conf_id#item2") mustBe null

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("fixed_change_conf_id#item1") mustBe null
      result(1).getAs[String]("fixed_change_conf_id#item2") mustBe null
    }

    "be success one found and one not found" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinVariableToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"
        val joins = Seq(
          VariableJoin(FixedInfo(Set("notFound.dat"), itemConfId = "fixed_change_conf_id"), col("org#key1") === col("fixed_change_conf_id#item1")),
          VariableJoin(FixedInfo(Set("fixed.dat"), itemConfId = "fixed_change_conf_id"), col("org#key1") === col("pre#item1"), "pre"))
      }

      val df1 = context.createDataFrame(Seq(Test2("a1", "key2", "aaa"), Test2("key11", "key22", "xxx")))

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "a1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("fixed_change_conf_id#item1") mustBe null
      result(0).getAs[String]("fixed_change_conf_id#item2") mustBe null
      result(0).getAs[String]("pre#item1") mustBe "a1"
      result(0).getAs[String]("pre#item2") mustBe "bb1"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("fixed_change_conf_id#item1") mustBe null
      result(1).getAs[String]("fixed_change_conf_id#item2") mustBe null
      result(1).getAs[String]("pre#item1") mustBe null
      result(1).getAs[String]("pre#item2") mustBe null
    }

    "be success non droped row error column" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinVariableToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"
        val joins = Seq(
          VariableJoin(FixedInfo(Set("fixed.dat"), itemConfId = "fixed_change_conf_id", dropRowError = false), col("org#key1") === col("fixed_change_conf_id#item1")))
      }

      val df1 = context.createDataFrame(Seq(Test2("a1", "key2", "aaa"), Test2("key11", "key22", "xxx")))

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "a1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("fixed_change_conf_id#item1") mustBe "a1"
      result(0).getAs[String]("fixed_change_conf_id#item2") mustBe "bb1"
      result(0).getAs[String]("fixed_change_conf_id#ROW_ERR") mustBe "false"
      result(0).getAs[String]("fixed_change_conf_id#ROW_ERR_MESSAGE") mustBe ""

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("fixed_change_conf_id#item1") mustBe null
      result(1).getAs[String]("fixed_change_conf_id#item2") mustBe null
      result(1).getAs[String]("fixed_change_conf_id#ROW_ERR") mustBe null
      result(1).getAs[String]("fixed_change_conf_id#ROW_ERR_MESSAGE") mustBe null
    }
  }

  "csv" should {
    "be success single join data" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinVariableToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"

        val joins = Seq(
          VariableJoin(CsvInfo(Set("csv.dat"), itemConfId = "csv"), col("org#key1") === col("pre#item1"), "pre"))
      }

      val df1 = context.createDataFrame(Seq(Test2("a1", "key2", "aaa"), Test2("key11", "key22", "xxx")))

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "a1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("pre#item1") mustBe "a1"
      result(0).getAs[String]("pre#item2") mustBe "bb1"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("pre#item1") mustBe null
      result(1).getAs[String]("pre#item2") mustBe null
    }
  }

  "tsv" should {
    "be success single join data" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinVariableToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"

        val joins = Seq(
          VariableJoin(TsvInfo(Set("tsv.dat"), itemConfId = "tsv"), col("org#key1") === col("pre#item1"), "pre"))
      }

      val df1 = context.createDataFrame(Seq(Test2("a1", "key2", "aaa"), Test2("key11", "key22", "xxx")))

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "a1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("pre#item1") mustBe "a1"
      result(0).getAs[String]("pre#item2") mustBe "bb1"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("pre#item1") mustBe null
      result(1).getAs[String]("pre#item2") mustBe null
    }
  }

  "vsv" should {
    "be success single join data" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinVariableToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"

        val joins = Seq(
          VariableJoin(VsvInfo(Set("vsv.dat"), itemConfId = "vsv"), col("org#key1") === col("pre#item1"), "pre"))
      }

      val df1 = context.createDataFrame(Seq(Test2("a1", "key2", "aaa"), Test2("key11", "key22", "xxx")))

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "a1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("pre#item1") mustBe "a1"
      result(0).getAs[String]("pre#item2") mustBe "bb1"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("pre#item1") mustBe null
      result(1).getAs[String]("pre#item2") mustBe null
    }
  }

  "ssv" should {
    "be success single join data" in {
      implicit val inArgs = TestArgs().toInputArgs
      val pqCtl = new PqCtl(inArgs.baseOutputFilePath)
      import pqCtl.implicits._

      val comp = new template.DfJoinVariableToDf with executor.Nothing {
        val componentId = "test"
        val prefixName = "org"

        val joins = Seq(
          VariableJoin(SsvInfo(Set("ssv.dat"), itemConfId = "csv"), col("org#key1") === col("pre#item1"), "pre"))
      }

      val df1 = context.createDataFrame(Seq(Test2("a1", "key2", "aaa"), Test2("key11", "key22", "xxx")))

      val result = comp.run(df1).sort("org#key1").collect
      result(0).getAs[String]("org#key1") mustBe "a1"
      result(0).getAs[String]("org#key2") mustBe "key2"
      result(0).getAs[String]("org#value") mustBe "aaa"
      result(0).getAs[String]("pre#item1") mustBe "a1"
      result(0).getAs[String]("pre#item2") mustBe "bb1"

      result(1).getAs[String]("org#key1") mustBe "key11"
      result(1).getAs[String]("org#key2") mustBe "key22"
      result(1).getAs[String]("org#value") mustBe "xxx"
      result(1).getAs[String]("pre#item1") mustBe null
      result(1).getAs[String]("pre#item2") mustBe null
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter
import d2k.common.df._
import d2k.common.InputArgs
import d2k.common.SparkApp
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._
import spark.common.PqCtl
import scala.io.Source
import spark.common.FileCtl
import org.apache.spark.sql._
import spark.common.SparkContexts.context
import org.apache.spark.sql.types._
import spark.common.SparkContexts
import d2k.common.df.executor.Nothing
import d2k.common.TestArgs

class DfToFileTest extends WordSpec with MustMatchers with BeforeAndAfter {
  val outConfPath = "data/test/conv/conf/mba_eo.conf"
  val applicationId = "APP-ID-001"
  implicit val inArgs = TestArgs().toInputArgs.copy(processId = "procId", applicationId = applicationId,
    runningDateFileFullPath = "data/test/conv/conf/COM_DATEFILE_SK0.txt", fileConvOutputFile = outConfPath)
  val outputFilePath = s"C:/tmp/output/${applicationId}.dat"

  object Test1 extends SparkApp {

    val structType = StructType(Seq(
      StructField("COL1", StringType),
      StructField("COL2", StringType),
      StructField("COL3", StringType)))

    val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(Row("AAA", "01", "一"))), structType)
    def targetComponent = new DfToFile with Nothing {
      val componentId = applicationId
      override val writeFileFunc = writeFixedFunc
    }
    def exec(implicit inArgs: InputArgs) = targetComponent.run(df)(inArgs)
  }

  val writeFixedFunc = (df: DataFrame, inArgs: InputArgs, conf: Map[String, String]) => {
    def rpad(target: String, len: Int, pad: String = " ") = {
      val str = if (target == null) { "" } else { target }
      val strSize = str.getBytes("MS932").size
      val padSize = len - strSize
      s"${str}${pad * padSize}"
    }

    val outputDir = conf(s"${inArgs.applicationId}.outputDir")
    val outputFile = conf(s"${inArgs.applicationId}.outputFile")

    val itemLens = conf(s"${inArgs.applicationId}.itemLengths")
    val itemLenList = itemLens.split(',').map { len => len.toInt }

    val itemLenListWithIdx = itemLenList.zipWithIndex
    def rowToFixedStr(row: Row) = {
      val line = itemLenListWithIdx.foldLeft("") { (acum, elem) =>
        {
          val (len, idx) = elem
          val str = row.getString(idx)
          acum + rpad(str, len)
        }
      }
      line + "\n"
    }
    FileCtl.writeToFile(s"${outputDir}/${outputFile}") { writer => df.rdd.map(rowToFixedStr).collect.foreach(writer.print) }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import spark.common.SparkContexts.context
import org.apache.spark.sql.Row
import d2k.common.df.executor
import d2k.common.df.template
import d2k.common.TestArgs

class DfUnionToDfTest extends WordSpec with MustMatchers with BeforeAndAfter {
  case class Test(a: String)

  "DfUnionToDfTest" should {
    "normal end" in {
      implicit val inArgs = TestArgs().toInputArgs
      val comp = new template.DfUnionToDf with executor.Nothing {
        val componentId = "test"
      }
      val df1 = context.createDataFrame(Seq(Test("aaa")))
      val df2 = context.createDataFrame(Seq(Test("bbb")))
      val result = comp.run(df1, df2).collect
      result(0).getAs[String]("a") mustBe "aaa"
      result(1).getAs[String]("a") mustBe "bbb"
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import d2k.common.df.CsvInfo

import d2k.common.TestArgs
import scala.util.Try
import d2k.common.df.template._
import d2k.common.df.executor.Nothing

class FileToDf_UTF8Test extends WordSpec with MustMatchers with BeforeAndAfter {
  implicit val inArgs = TestArgs().toInputArgs

  "read utf-8" should {
    "success" in {
      val compo = new FileToDf with Nothing {
        val componentId = "csv"
        override val fileInputInfo = CsvInfo(Set("csv_utf8.dat"), charSet = "UTF-8")
      }
      val df = compo.run(Unit)
      val r = df.collect
      r(0).getAs[String]("item1") mustBe "あ"
      r(0).getAs[String]("item2") mustBe "う1"
      r(1).getAs[String]("item1") mustBe "い"
      r(1).getAs[String]("item2") mustBe "え2"
      r(2).getAs[String]("item1") mustBe ""
      r(2).getAs[String]("item2") mustBe ""
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import d2k.common.TestArgs
import org.apache.spark.sql.types._
import spark.common.DbCtl
import spark.common.SparkContexts._
import org.apache.spark.sql.Row
import spark.common.PqCtl
import d2k.common.df.CsvInfo
import d2k.common.df.executor.PqCommonColumnRemover
import org.apache.spark.sql.SaveMode

class FileToPq_DbTest extends WordSpec with MustMatchers with BeforeAndAfter {
  implicit val inArgs = TestArgs().toInputArgs
  val structType = StructType(Seq(
    StructField("KEY", StringType), StructField("TEST", StringType)))
  val dbCtl = new DbCtl()
  import dbCtl.implicits._

  val pqCtl = new PqCtl(inArgs.baseOutputFilePath)

  "FileToPq_Db" should {
    "be success fileToDb" in {
      val target = new FileToDb with PqCommonColumnRemover {
        val componentId = "cs_xxx"
        val fileInputInfo = CsvInfo(Set("cs_xxx1.dat"))
        override lazy val writeTableName = "cs_test"
        override val writeDbSaveMode = SaveMode.Overwrite
      }
      val insertDf = context.createDataFrame(sc.makeRDD(Seq(
        Row("key1", "aaa"))), structType)
      target.run(Unit)

      val result = dbCtl.readTable("cs_test").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 2
      result(0).getAs[String]("TEST") mustBe "aaa"
      result(1).getAs[String]("TEST") mustBe "bbb"
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "cs_xxx"
    }

    "be same result" in {
      val target = new FileToPq_Db with PqCommonColumnRemover {
        val componentId = "cs_xxx"
        val fileInputInfo = CsvInfo(Set("cs_xxx2.dat"))
        override lazy val writeTableName = "cs_test"
        override val writeDbSaveMode = SaveMode.Overwrite
      }
      val insertDf = context.createDataFrame(sc.makeRDD(Seq(
        Row("key1", "aaa"))), structType)
      target.run(Unit)

      val result = dbCtl.readTable("cs_test").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 2
      result(0).getAs[String]("TEST") mustBe "ccc"
      result(1).getAs[String]("TEST") mustBe "ddd"
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "cs_xxx"

      val resultPq = pqCtl.readParquet("cs_test").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 2
      resultPq(0).getAs[String]("TEST") mustBe "ccc"
      result(1).getAs[String]("TEST") mustBe "ddd"
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "cs_xxx"
    }

  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df.template

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter
import spark.common.SparkContexts.context
import context.implicits._
import d2k.common.InputArgs
import d2k.common.TestArgs
import d2k.common.df._
import d2k.common.fileConv.FixedConverter
import d2k.common.fileConv.Converter
import d2k.common.df.FileInputInfoBase.CRLF
import d2k.common.df.FileInputInfoBase.CR

class FileToXxxTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "NoConfFileToXxx for csv" should {
    implicit val inArgs = TestArgs().toInputArgs

    "success simplePattern test" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "csv"
        val fileInputInfo = CsvInfo(Set("csv.dat"))
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success cnange conf id" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "csv"
        override lazy val itemConfId = "csv_change_conf_id"
        val fileInputInfo = CsvInfo(Set("csv.dat"))
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success file path from componentId and change conf id" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fptest"
        val fileInputInfo = CsvInfo(Set("csv.dat"))
        override lazy val itemConfId = "csv_change_conf_id"
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success change env name" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "csv"
        val fileInputInfo = CsvInfo(Set("csv.dat"), "ENV_TEST")
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success exist header" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "csv"
        val fileInputInfo = CsvInfo(Set("csv_h.dat"), header = true)
        override lazy val itemConfId = "csv_h"
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }
  }

  "NoConfFileToXxx for tsv" should {
    implicit val inArgs = TestArgs().toInputArgs

    "success simplePattern test" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "tsv"
        val fileInputInfo = TsvInfo(Set("tsv.dat"))
      }
      val result = fileToDf.run(Unit).collect
      result.foreach(println)
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success cnange conf id" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "tsv"
        override lazy val itemConfId = "tsv_change_conf_id"
        val fileInputInfo = TsvInfo(Set("tsv.dat"))
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success file path from componentId and change conf id" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fptest"
        val fileInputInfo = TsvInfo(Set("tsv.dat"))
        override lazy val itemConfId = "tsv_change_conf_id"
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success change env name" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "tsv"
        val fileInputInfo = TsvInfo(Set("tsv.dat"), "ENV_TEST")
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success drop double quote mode on" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "tsv"
        val fileInputInfo = TsvInfo(Set("tsv_strict.dat"), dropDoubleQuoteMode = true)
        override lazy val itemConfId = "tsv_h"
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success exist header" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "tsv"
        val fileInputInfo = TsvInfo(Set("tsv_h.dat"), header = true)
        override lazy val itemConfId = "tsv_h"
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }
  }

  "NoConfFileToXxx for vsv" should {
    implicit val inArgs = TestArgs().toInputArgs

    "success simplePattern test" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "vsv"
        val fileInputInfo = VsvInfo(Set("vsv.dat"))
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success cnange conf id" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "vsv"
        override lazy val itemConfId = "vsv_change_conf_id"
        val fileInputInfo = VsvInfo(Set("vsv.dat"))
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success file path from componentId and change conf id" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fptest"
        val fileInputInfo = VsvInfo(Set("vsv.dat"))
        override lazy val itemConfId = "vsv_change_conf_id"
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success change env name" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "vsv"
        val fileInputInfo = VsvInfo(Set("vsv.dat"), "ENV_TEST")
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success exist header" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "vsv"
        val fileInputInfo = VsvInfo(Set("vsv_h.dat"), header = true)
        override lazy val itemConfId = "vsv_h"
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }
  }

  "NoConfFileToXxx for ssv" should {
    implicit val inArgs = TestArgs().toInputArgs

    "success simplePattern test" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "csv"
        val fileInputInfo = SsvInfo(Set("ssv.dat"))
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success exist header" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "csv"
        val fileInputInfo = SsvInfo(Set("ssv_h.dat"), header = true)
        override lazy val itemConfId = "csv_h"
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }
  }

  "NoConfFileToXxx for fixed" should {
    implicit val inArgs = TestArgs().toInputArgs

    "success simplePattern test" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fixed"
        val fileInputInfo = FixedInfo(Set("fixed.dat"))
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success cnange conf id" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fixed"
        override lazy val itemConfId = "fixed_change_conf_id"
        val fileInputInfo = FixedInfo(Set("fixed.dat"))
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success file path from componentId and change conf id" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fptest"
        val fileInputInfo = FixedInfo(Set("fixed.dat"))
        override lazy val itemConfId = "fixed_change_conf_id"
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success change env name" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fixed"
        val fileInputInfo = FixedInfo(Set("fixed.dat"), "ENV_TEST")
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success exist header" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fixed"
        val fileInputInfo = FixedInfo(Set("fixed_h.dat"), header = true)
        override lazy val itemConfId = "fixed_h"
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success exist footer" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fixed"
        val fileInputInfo = FixedInfo(Set("fixed_f.dat"), footer = true)
        override lazy val itemConfId = "fixed_f"
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success exist header and footer" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fixed"
        val fileInputInfo = FixedInfo(Set("fixed_hf.dat"), header = true, footer = true)
        override lazy val itemConfId = "fixed_hf"
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    import Converter._
    "success withIndex test" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fixed"
        val fileInputInfo = FixedInfo(Set("fixed.dat"), withIndex = true)
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
      result(0).getAs[String](SYSTEM_COLUMN_NAME.RECORD_INDEX) mustBe "0"
      result(1).getAs[String](SYSTEM_COLUMN_NAME.RECORD_INDEX) mustBe "1"
      result(2).getAs[String](SYSTEM_COLUMN_NAME.RECORD_INDEX) mustBe "2"
    }

    "success record length check test" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fixed"
        val fileInputInfo = FixedInfo(Set("fixed.dat"), recordLengthCheck = true)
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
      result(0).getAs[String](SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR) mustBe "false"
      result(1).getAs[String](SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR) mustBe "false"
      result(2).getAs[String](SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR) mustBe "false"
    }

    "success record length check illegal size test" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fixed"
        val fileInputInfo = FixedInfo(Set("fixed_size_error.dat"), recordLengthCheck = true)
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 4
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
      result(0).getAs[String](SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR) mustBe "false"
      result(1).getAs[String](SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR) mustBe "true"
      result(2).getAs[String](SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR) mustBe "true"
      result(3).getAs[String](SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR) mustBe "true"
    }

    "success record length check illegal size and with index test" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fixed"
        val fileInputInfo = FixedInfo(Set("fixed_size_error.dat"), withIndex = true, recordLengthCheck = true)
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 4
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
      result(0).getAs[String](SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR) mustBe "false"
      result(0).getAs[String](SYSTEM_COLUMN_NAME.RECORD_INDEX) mustBe "0"
      result(1).getAs[String](SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR) mustBe "true"
      result(1).getAs[String](SYSTEM_COLUMN_NAME.RECORD_INDEX) mustBe "1"
      result(2).getAs[String](SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR) mustBe "true"
      result(2).getAs[String](SYSTEM_COLUMN_NAME.RECORD_INDEX) mustBe "2"
      result(3).getAs[String](SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR) mustBe "true"
      result(3).getAs[String](SYSTEM_COLUMN_NAME.RECORD_INDEX) mustBe "3"
    }

    "success record length throw Exception by linebreak test" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fixed"
        val fileInputInfo = FixedInfo(Set("fixed_size_error.dat"), recordLengthCheck = true, newLine = false)
      }
      try {
        fileToDf.run(Unit).collect
        fail
      } catch {
        case t: IllegalArgumentException => t.printStackTrace
      }
    }

    "success crlf test" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fixed"
        val fileInputInfo = FixedInfo(Set("fixed_crlf.dat"), newLineCode = CRLF)
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }

    "success cr test" in {
      val fileToDf = new template.FileToDf with executor.Nothing {
        val componentId = "fixed"
        val fileInputInfo = FixedInfo(Set("fixed_cr.dat"), newLineCode = CR)
      }
      val result = fileToDf.run(Unit).collect
      result.size mustBe 3
      result(0).getAs[String]("item1") mustBe "a1"
      result(0).getAs[String]("item2") mustBe "bb1"
    }
  }

  "preFilter" should {
    implicit val inArgs = TestArgs().toInputArgs

    "be success" when {
      "simple pattern" in {
        val fileToDf = new template.FileToDf with executor.Nothing {
          val componentId = "fixed"
          val fileInputInfo = FixedInfo(
            Set("fixed.dat"),
            preFilter = (Seq("item1"), m => m("item1") == "a1"))
        }
        val result = fileToDf.run(Unit).collect
        result.size mustBe 1
        result(0).getAs[String]("item1") mustBe "a1"
        result(0).getAs[String]("item2") mustBe "bb1"
      }

      import Converter._
      "success withIndex test" in {
        val fileToDf = new template.FileToDf with executor.Nothing {
          val componentId = "fixed"
          val fileInputInfo = FixedInfo(Set("fixed.dat"), withIndex = true,
            preFilter = (Seq("item1"), m => m("item1") == "a1" || m("item1") == "a2"))
        }
        val result = fileToDf.run(Unit).collect
        result.size mustBe 2
        result(0).getAs[String]("item1") mustBe "a1"
        result(0).getAs[String]("item2") mustBe "bb1"
        result(0).getAs[String](SYSTEM_COLUMN_NAME.RECORD_INDEX) mustBe "0"
        result(1).getAs[String](SYSTEM_COLUMN_NAME.RECORD_INDEX) mustBe "1"
      }

      "success record length check test" in {
        val fileToDf = new template.FileToDf with executor.Nothing {
          val componentId = "fixed"
          val fileInputInfo = FixedInfo(Set("fixed.dat"), recordLengthCheck = true,
            preFilter = (Seq("item1"), m => m("item1") == "a1" || m("item1") == "a2"))
        }
        val result = fileToDf.run(Unit).collect
        result.size mustBe 2
        result(0).getAs[String]("item1") mustBe "a1"
        result(0).getAs[String]("item2") mustBe "bb1"
        result(0).getAs[String](SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR) mustBe "false"
        result(1).getAs[String](SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR) mustBe "false"
      }

      "success record length check illegal size test" in {
        val fileToDf = new template.FileToDf with executor.Nothing {
          val componentId = "fixed"
          val fileInputInfo = FixedInfo(Set("fixed_size_error.dat"), recordLengthCheck = true,
            preFilter = (Seq("item2"), m => m("item2") == "bb1" || m("item2") == "b2X"))
        }
        fileToDf.run(Unit).show
        val result = fileToDf.run(Unit).collect
        result.size mustBe 2
        result(0).getAs[String]("item1") mustBe "a1"
        result(0).getAs[String]("item2") mustBe "bb1"
        result(0).getAs[String](SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR) mustBe "false"
        result(1).getAs[String](SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR) mustBe "true"
      }
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import d2k.common.TestArgs
import d2k.common.InputArgs

import java.sql.Date
import org.joda.time.DateTime
import java.sql.Timestamp

import spark.common.SparkContexts.context
import context.implicits._
import org.apache.spark.sql.Row
import org.apache.spark.sql.types._
import spark.common.SparkContexts
import spark.common.DbCtl
import d2k.common.df.WriteDbMode._
import org.apache.spark.sql.SaveMode
import java.time.LocalDateTime

case class Sp01(DT: Timestamp, NUM5: BigDecimal, NUM52: BigDecimal, TSTMP: Timestamp, VC: String, CH: String)
class WriteDbTest extends WordSpec with MustMatchers with BeforeAndAfter {
  def d2s(dateMill: Long) = new DateTime(dateMill).toString("yyyy-MM-dd")
  def d2s(date: Date) = new DateTime(date).toString("yyyy-MM-dd")
  def d2s(date: Timestamp) = new DateTime(date).toString("yyyy-MM-dd hh:mm:ss")

  implicit val inArgs = TestArgs().toInputArgs
  val structType = StructType(Seq(
    StructField("KEY", StringType), StructField("TEST", StringType)))
  val dbCtl = new DbCtl()
  import dbCtl.implicits._
  import context.implicits._

  "Insert" should {
    "be normal end" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq.empty[Row]), structType)
      DbCommonColumnAppender(df, "CS_TEST").autoCreateTable("CS_TEST")
      val target = new WriteDb {
        val componentId = "CS_TEST"
        override val writeDbMode = Insert
      }
      val insertDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"))), structType)
      target.writeDb(insertDf)

      val result = dbCtl.readTable("CS_TEST").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 1

      result(0).getAs[String]("TEST") mustBe "aaa"
      d2s(result(0).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
    }

    "insert not set CommonColumns" in {
      val target = new WriteDb {
        val componentId = "insertNotSetCommonColumns"
        override val writeDbMode = Insert
        override val writeDbWithCommonColumn = false
        override val writeDbSaveMode = SaveMode.Overwrite
      }

      implicit val inArgs = TestArgs().toInputArgs
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY2", "bbb"), Row("KEY3", "ccc"))), structType)
      target.writeDb(df)

      val dbCtl = new DbCtl()
      val schemas = dbCtl.readTable("insertNotSetCommonColumns").schema
      schemas.contains("DT_D2KMKDTTM") mustBe false
    }

    "be normal end　with CommitSize" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq.empty[Row]), structType)
      DbCommonColumnAppender(df, "CS_TEST").autoCreateTable("CS_TEST")
      val target = new WriteDb {
        val componentId = "CS_TEST"
        override val writeDbMode = Insert
        override val writeDbInfo = DbConnectionInfo.bat1.copy(commitSize = Some(2))
      }
      val insertDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"))), structType)
      target.writeDb(insertDf)

      val result = dbCtl.readTable("CS_TEST").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 1

      result(0).getAs[String]("TEST") mustBe "aaa"
      d2s(result(0).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
    }
  }

  "InsertAcc" should {
    "be normal end" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq.empty[Row]), structType)
      DbCommonColumnAppender(df, "CS_TEST").autoCreateTable("CS_TEST")
      val target = new WriteDb {
        val componentId = "CS_TEST"
        override val writeDbMode = InsertAcc
      }
      val insertDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"))), structType)
      target.writeDb(insertDf)

      val result = dbCtl.readTable("CS_TEST").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 1

      result(0).getAs[String]("TEST") mustBe "aaa"
      d2s(result(0).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
    }

    "insert not set CommonColumns" in {
      val target = new WriteDb {
        val componentId = "insertNotSetCommonColumns"
        override val writeDbMode = InsertAcc
        override val writeDbWithCommonColumn = false
        override val writeDbSaveMode = SaveMode.Overwrite
      }

      implicit val inArgs = TestArgs().toInputArgs
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY2", "bbb"), Row("KEY3", "ccc"))), structType)
      target.writeDb(df)

      val dbCtl = new DbCtl()
      val schemas = dbCtl.readTable("insertNotSetCommonColumns").schema
      schemas.contains("DT_D2KMKDTTM") mustBe false
    }

    "be normal end　with commitSize" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq.empty[Row]), structType)
      DbCommonColumnAppender(df, "CS_TEST").autoCreateTable("CS_TEST")
      val target = new WriteDb {
        val componentId = "CS_TEST"
        override val writeDbMode = InsertAcc
        override val writeDbInfo = DbConnectionInfo.bat1.copy(commitSize = Some(2))
      }
      val insertDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"))), structType)
      target.writeDb(insertDf)

      val result = dbCtl.readTable("CS_TEST").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 1

      result(0).getAs[String]("TEST") mustBe "aaa"
      d2s(result(0).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
    }
  }

  "InsertNotExist" should {
    "be normal end" in {
      val target = new WriteDb {
        val componentId = "insertNotExist"
        override val writeDbMode = InsertNotExists("KEY")
        override val writeDbWithCommonColumn = true
        override val writeDbSaveMode = SaveMode.Overwrite
      }
      val insertDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY1", "bbb"), Row("KEY2", "ccc"))), structType)
      target.writeDb(insertDf.repartition(1))
      val result = dbCtl.readTable("insertNotExist").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 2

      result(0).getAs[String]("TEST") mustBe "aaa"
      d2s(result(0).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "insertNotExist"

      result(1).getAs[String]("TEST") mustBe "ccc"
    }

    "append" in {
      val target = new WriteDb {
        val componentId = "insertNotExist"
        override val writeDbMode = InsertNotExists("KEY")
        override val writeDbSaveMode = SaveMode.Overwrite
      }
      val insertDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY1", "bbb"))), structType)
      target.writeDb(insertDf.repartition(1))

      val target2 = new WriteDb {
        val componentId = "insertNotExist"
        override val writeDbMode = InsertNotExists("KEY")
        override val writeDbSaveMode = SaveMode.Append
      }
      val insertDf2 = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY2", "ccc"), Row("KEY2", "ddd"))), structType)
      target2.writeDb(insertDf2.repartition(1))

      val result = dbCtl.readTable("insertNotExist").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 2

      result(0).getAs[String]("TEST") mustBe "aaa"
      result(1).getAs[String]("TEST") mustBe "ccc"
    }

    "InsertNotExist not set CommonColumns" in {
      val target = new WriteDb {
        val componentId = "insertNotExistNotSetColumns"
        override val writeDbMode = InsertNotExists("KEY")
        override val writeDbWithCommonColumn = false
        override val writeDbSaveMode = SaveMode.Overwrite
      }

      implicit val inArgs = TestArgs().toInputArgs
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY1", "bbb"))), structType)
      target.writeDb(df.repartition(1))

      val dbCtl = new DbCtl()
      val result = dbCtl.readTable("insertNotExistNotSetColumns").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 1

      result(0).getAs[String]("TEST") mustBe "aaa"
      result.contains("DT_D2KMKDTTM") mustBe false
    }

    "be normal end with commitSize" in {
      val target = new WriteDb {
        val componentId = "insertNotExist"
        override val writeDbMode = InsertNotExists("KEY")
        override val writeDbWithCommonColumn = true
        override val writeDbSaveMode = SaveMode.Overwrite
        override val writeDbInfo = DbConnectionInfo.bat1.copy(commitSize = Some(2))
      }
      val insertDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY1", "bbb"))), structType)
      target.writeDb(insertDf.repartition(1))

      val result = dbCtl.readTable("insertNotExist").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 1

      result(0).getAs[String]("TEST") mustBe "aaa"
      d2s(result(0).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "insertNotExist"
    }
  }

  val structTypeUpd = StructType(Seq(
    StructField("KEY", StringType), StructField("TEST", StringType), StructField("TEST2", StringType)))
  "Update" should {
    "be normal end" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY2", "bbb"), Row("KEY3", "ccc"))), structType)
      DbCommonColumnAppender(df, "compo").writeTable("CS_TEST", SaveMode.Overwrite)

      val target = new WriteDb {
        val componentId = "CS_TEST"
        override val writeDbMode = Update
        override val writeDbUpdateKeys = Set("KEY")
      }
      val updateDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx"), Row("KEY3", "yyy"))), structType)
      target.writeDb(updateDf)

      val result = dbCtl.readTable("CS_TEST").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 3
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("TEST") mustBe "xxx"
      d2s(result(0).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("TEST") mustBe "bbb"
      result(1).getAs[Timestamp]("DT_D2KUPDDTTM") ne inArgs.sysSQLDate
      result(1).getAs[String]("ID_D2KUPDUSR") mustBe "compo"
      result(2).getAs[String]("KEY") mustBe "KEY3"
      result(2).getAs[String]("TEST") mustBe "yyy"
      d2s(result(2).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(2).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
    }

    "Update not set CommonColumns" in {
      val insert = new WriteDb {
        val componentId = "updateNotSetCommonColumns"
        override val writeDbMode = Insert
        override val writeDbWithCommonColumn = false
        override val writeDbSaveMode = SaveMode.Overwrite
      }

      implicit val inArgs = TestArgs().toInputArgs
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY2", "bbb"), Row("KEY3", "ccc"))), structType)
      insert.writeDb(df)

      val target = new WriteDb {
        val componentId = "updateNotSetCommonColumns"
        override val writeDbMode = Update
        override val writeDbWithCommonColumn = false
        override val writeDbUpdateKeys = Set("KEY")
      }
      val updateDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx"), Row("KEY3", "yyy"))), structType)
      target.writeDb(updateDf)

      val dbCtl = new DbCtl()
      val schemas = dbCtl.readTable("updateNotSetCommonColumns").schema
      schemas.foreach(println)
      schemas.contains("DT_D2KMKDTTM") mustBe false
    }

    "Update ignore column　single" in {
      val dfdummy = context.createDataFrame(SparkContexts.sc.makeRDD(Seq.empty[Row]), structTypeUpd)
      DbCommonColumnAppender(dfdummy, "compo").autoCreateTable("cs_test_update_ignore")

      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa", "aaa2"), Row("KEY2", "bbb", "bbb2"), Row("KEY3", "ccc", "ccc2"))), structTypeUpd)
      DbCommonColumnAppender(df, "compo").writeTable("cs_test_update_ignore", SaveMode.Overwrite)

      val target = new WriteDb {
        val componentId = "cs_test_update_ignore"
        override val writeDbMode = Update
        override val writeDbUpdateKeys = Set("KEY")
        override val writeDbUpdateIgnoreColumns = Set("TEST2")
      }
      val updateDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx", "99999"), Row("KEY3", "yyy", "000000"))), structTypeUpd)
      target.writeDb(updateDf)

      val result = dbCtl.readTable("cs_test_update_ignore").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 3
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("TEST") mustBe "xxx"
      result(0).getAs[String]("TEST2") mustBe "aaa2"

      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("TEST") mustBe "bbb"
      result(1).getAs[String]("TEST2") mustBe "bbb2"

      result(2).getAs[String]("KEY") mustBe "KEY3"
      result(2).getAs[String]("TEST") mustBe "yyy"
      result(2).getAs[String]("TEST2") mustBe "ccc2"
    }

    "Update ignore column　multi" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa", "aaa2"), Row("KEY2", "bbb", "bbb2"), Row("KEY3", "ccc", "ccc2"))), structTypeUpd)
      DbCommonColumnAppender(df, "compo").writeTable("cs_test_update_ignore", SaveMode.Overwrite)

      val target = new WriteDb {
        val componentId = "cs_test_update_ignore"
        override val writeDbMode = Update
        override val writeDbUpdateKeys = Set("KEY")
        override val writeDbUpdateIgnoreColumns = Set("TEST", "TEST2")
      }
      val updateDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx", "99999"), Row("KEY3", "yyy", "000000"))), structTypeUpd)
      target.writeDb(updateDf)

      val result = dbCtl.readTable("cs_test_update_ignore").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 3
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("TEST") mustBe "aaa"
      result(0).getAs[String]("TEST2") mustBe "aaa2"

      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("TEST") mustBe "bbb"
      result(1).getAs[String]("TEST2") mustBe "bbb2"

      result(2).getAs[String]("KEY") mustBe "KEY3"
      result(2).getAs[String]("TEST") mustBe "ccc"
      result(2).getAs[String]("TEST2") mustBe "ccc2"
    }

    "be normal end with commitSize" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY2", "bbb"), Row("KEY3", "ccc"))), structType)
      DbCommonColumnAppender(df, "compo").writeTable("CS_TEST", SaveMode.Overwrite)

      val target = new WriteDb {
        val componentId = "CS_TEST"
        override val writeDbMode = Update
        override val writeDbUpdateKeys = Set("KEY")
        override val writeDbInfo = DbConnectionInfo.bat1.copy(commitSize = Some(2))
      }
      val updateDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx"), Row("KEY3", "yyy"))), structType)
      target.writeDb(updateDf)

      val result = dbCtl.readTable("CS_TEST").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 3
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("TEST") mustBe "xxx"
      d2s(result(0).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("TEST") mustBe "bbb"
      result(1).getAs[Timestamp]("DT_D2KUPDDTTM") ne inArgs.sysSQLDate
      result(1).getAs[String]("ID_D2KUPDUSR") mustBe "compo"
      result(2).getAs[String]("KEY") mustBe "KEY3"
      result(2).getAs[String]("TEST") mustBe "yyy"
      d2s(result(2).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(2).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
    }
  }

  "Upsert" should {
    "be normal end" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY2", "bbb"), Row("KEY3", "ccc"))), structType)
      DbCommonColumnAppender(df, "compo").writeTable("CS_TEST", SaveMode.Overwrite)

      val target = new WriteDb {
        val componentId = "CS_TEST"
        override val writeDbMode = Upsert
        override val writeDbUpdateKeys = Set("KEY")
      }
      val updateDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx"), Row("KEY3", "yyy"), Row("KEY4", "zzz"))), structType)
      target.writeDb(updateDf)

      val dbCtl = new DbCtl()
      val result = dbCtl.readTable("CS_TEST").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 4
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("TEST") mustBe "xxx"
      d2s(result(0).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("TEST") mustBe "bbb"
      result(1).getAs[Timestamp]("DT_D2KUPDDTTM") ne inArgs.sysSQLDate
      result(1).getAs[String]("ID_D2KUPDUSR") ne "CS_TEST"
      result(2).getAs[String]("KEY") mustBe "KEY3"
      result(2).getAs[String]("TEST") mustBe "yyy"
      d2s(result(2).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(2).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
      result(3).getAs[String]("KEY") mustBe "KEY4"
      result(3).getAs[String]("TEST") mustBe "zzz"
      d2s(result(3).getAs[Timestamp]("DT_D2KMKDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(3).getAs[String]("ID_D2KMKUSR") mustBe "CS_TEST"
      d2s(result(3).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(3).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
    }

    "Upsert not set CommonColumns" in {
      val insert = new WriteDb {
        val componentId = "upsertNotSetCommonColumns"
        override val writeDbMode = Insert
        override val writeDbWithCommonColumn = false
        override val writeDbSaveMode = SaveMode.Overwrite
      }

      implicit val inArgs = TestArgs().toInputArgs
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY2", "bbb"), Row("KEY3", "ccc"))), structType)
      insert.writeDb(df)

      val target = new WriteDb {
        val componentId = "upsertNotSetCommonColumns"
        override val writeDbMode = Upsert
        override val writeDbWithCommonColumn = false
        override val writeDbUpdateKeys = Set("KEY")
      }
      val updateDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx"), Row("KEY3", "yyy"))), structType)
      target.writeDb(updateDf)

      val dbCtl = new DbCtl()
      val schemas = dbCtl.readTable("upsertNotSetCommonColumns").schema
      schemas.foreach(println)
      schemas.contains("DT_D2KMKDTTM") mustBe false
    }

    "Upsert ignore column single" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa", "aaa2"), Row("KEY2", "bbb", "bbb2"), Row("KEY3", "ccc", "ccc2"))), structTypeUpd)
      DbCommonColumnAppender(df, "compo").writeTable("cs_test_update_ignore", SaveMode.Overwrite)

      val target = new WriteDb {
        val componentId = "cs_test_update_ignore"
        override val writeDbMode = Upsert
        override val writeDbUpdateKeys = Set("KEY")
        override val writeDbUpdateIgnoreColumns = Set("TEST2")
      }
      val updateDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx", "11111"), Row("KEY3", "yyy", "222222"), Row("KEY4", "zzz", "33333"))), structTypeUpd)
      target.writeDb(updateDf)

      val dbCtl = new DbCtl()
      val result = dbCtl.readTable("cs_test_update_ignore").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 4
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("TEST") mustBe "xxx"
      result(0).getAs[String]("TEST2") mustBe "aaa2"
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("TEST") mustBe "bbb"
      result(1).getAs[String]("TEST2") mustBe "bbb2"
      result(2).getAs[String]("KEY") mustBe "KEY3"
      result(2).getAs[String]("TEST") mustBe "yyy"
      result(2).getAs[String]("TEST2") mustBe "ccc2"
      result(3).getAs[String]("KEY") mustBe "KEY4"
      result(3).getAs[String]("TEST") mustBe "zzz"
      result(3).getAs[String]("TEST2") mustBe "33333"
    }

    "Upsert ignore column multi" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa", "aaa2"), Row("KEY2", "bbb", "bbb2"), Row("KEY3", "ccc", "ccc2"))), structTypeUpd)
      DbCommonColumnAppender(df, "compo").writeTable("cs_test_update_ignore", SaveMode.Overwrite)

      val target = new WriteDb {
        val componentId = "cs_test_update_ignore"
        override val writeDbMode = Upsert
        override val writeDbUpdateKeys = Set("KEY")
        override val writeDbUpdateIgnoreColumns = Set("TEST", "TEST2")
      }
      val updateDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx", "11111"), Row("KEY3", "yyy", "222222"), Row("KEY4", "zzz", "33333"))), structTypeUpd)
      target.writeDb(updateDf)

      val dbCtl = new DbCtl()
      val result = dbCtl.readTable("cs_test_update_ignore").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 4
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("TEST") mustBe "aaa"
      result(0).getAs[String]("TEST2") mustBe "aaa2"
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("TEST") mustBe "bbb"
      result(1).getAs[String]("TEST2") mustBe "bbb2"
      result(2).getAs[String]("KEY") mustBe "KEY3"
      result(2).getAs[String]("TEST") mustBe "ccc"
      result(2).getAs[String]("TEST2") mustBe "ccc2"
      result(3).getAs[String]("KEY") mustBe "KEY4"
      result(3).getAs[String]("TEST") mustBe "zzz"
      result(3).getAs[String]("TEST2") mustBe "33333"
    }

    "be normal end with commitSize" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY2", "bbb"), Row("KEY3", "ccc"))), structType)
      DbCommonColumnAppender(df, "compo").writeTable("CS_TEST", SaveMode.Overwrite)

      val target = new WriteDb {
        val componentId = "CS_TEST"
        override val writeDbMode = Upsert
        override val writeDbUpdateKeys = Set("KEY")
        override val writeDbInfo = DbConnectionInfo.bat1.copy(commitSize = Some(2))
      }
      val updateDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx"), Row("KEY3", "yyy"), Row("KEY4", "zzz"))), structType)
      target.writeDb(updateDf)

      val result = dbCtl.readTable("CS_TEST").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 4
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("TEST") mustBe "xxx"
      d2s(result(0).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("TEST") mustBe "bbb"
      result(1).getAs[Timestamp]("DT_D2KUPDDTTM") ne inArgs.sysSQLDate
      result(1).getAs[String]("ID_D2KUPDUSR") ne "CS_TEST"
      result(2).getAs[String]("KEY") mustBe "KEY3"
      result(2).getAs[String]("TEST") mustBe "yyy"
      d2s(result(2).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(2).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
      result(3).getAs[String]("KEY") mustBe "KEY4"
      result(3).getAs[String]("TEST") mustBe "zzz"
      d2s(result(3).getAs[Timestamp]("DT_D2KMKDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(3).getAs[String]("ID_D2KMKUSR") mustBe "CS_TEST"
      d2s(result(3).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(3).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
    }
  }

  "delete Logical" should {
    "be normal end" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY2", "bbb"), Row("KEY3", "ccc"))), structType)
      DbCommonColumnAppender(df, "compo").writeTable("CS_TEST", SaveMode.Overwrite)

      val target = new WriteDb {
        val componentId = "CS_TEST"
        override val writeDbMode = DeleteLogical
        override val writeDbUpdateKeys = Set("KEY")
      }
      val updateDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx"), Row("KEY3", "yyy"), Row("KEY4", "zzz"))), structType)
      target.writeDb(updateDf)

      val result = dbCtl.readTable("CS_TEST").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 3
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("TEST") mustBe "aaa"
      result(0).getAs[String]("FG_D2KDELFLG") mustBe "1"
      d2s(result(0).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("TEST") mustBe "bbb"
      result(1).getAs[String]("FG_D2KDELFLG") mustBe "0"
      result(2).getAs[String]("KEY") mustBe "KEY3"
      result(2).getAs[String]("TEST") mustBe "ccc"
      result(2).getAs[String]("FG_D2KDELFLG") mustBe "1"
      d2s(result(2).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(2).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
    }

    "delete Logical not set CommonColumns" in {
      val insert = new WriteDb {
        val componentId = "deleteLNotSetCommonColumns"
        override val writeDbMode = Insert
        override val writeDbWithCommonColumn = false
        override val writeDbSaveMode = SaveMode.Overwrite
      }

      implicit val inArgs = TestArgs().toInputArgs
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY2", "bbb"), Row("KEY3", "ccc"))), structType)
      insert.writeDb(df)

      val target = new WriteDb {
        val componentId = "deleteLNotSetCommonColumns"
        override val writeDbMode = DeleteLogical
        override val writeDbWithCommonColumn = false
        override val writeDbUpdateKeys = Set("KEY")
      }
      val updateDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx"), Row("KEY3", "yyy"))), structType)

      try {
        target.writeDb(updateDf)
        fail
      } catch {
        case t: Throwable => t.toString.contains("DeleteLogical and writeDbWithCommonColumn == false can not used be togather") mustBe true
      }
    }

    "be normal end with commitSize" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY2", "bbb"), Row("KEY3", "ccc"))), structType)
      DbCommonColumnAppender(df, "compo").writeTable("CS_TEST", SaveMode.Overwrite)

      val target = new WriteDb {
        val componentId = "CS_TEST"
        override val writeDbMode = DeleteLogical
        override val writeDbUpdateKeys = Set("KEY")
        override val writeDbInfo = DbConnectionInfo.bat1.copy(commitSize = Some(2))
      }
      val updateDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx"), Row("KEY3", "yyy"), Row("KEY4", "zzz"))), structType)
      target.writeDb(updateDf)

      val result = dbCtl.readTable("CS_TEST").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 3
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("TEST") mustBe "aaa"
      result(0).getAs[String]("FG_D2KDELFLG") mustBe "1"
      d2s(result(0).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("TEST") mustBe "bbb"
      result(1).getAs[String]("FG_D2KDELFLG") mustBe "0"
      result(2).getAs[String]("KEY") mustBe "KEY3"
      result(2).getAs[String]("TEST") mustBe "ccc"
      result(2).getAs[String]("FG_D2KDELFLG") mustBe "1"
      d2s(result(2).getAs[Timestamp]("DT_D2KUPDDTTM")) mustBe d2s(inArgs.sysSQLDate)
      result(2).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
    }
  }

  "delete Physical" should {
    "be normal end" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY2", "bbb"), Row("KEY3", "ccc"))), structType)
      DbCommonColumnAppender(df, "compo").writeTable("CS_TEST", SaveMode.Overwrite)

      val target = new WriteDb {
        val componentId = "CS_TEST"
        override val writeDbMode = DeletePhysical
        override val writeDbUpdateKeys = Set("KEY")
      }
      val updateDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx"), Row("KEY3", "yyy"), Row("KEY4", "zzz"))), structType)
      target.writeDb(updateDf)

      val result = dbCtl.readTable("CS_TEST").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 1
      result(0).getAs[String]("KEY") mustBe "KEY2"
      result(0).getAs[String]("TEST") mustBe "bbb"
    }

    "delete Physical not set CommonColumns" in {
      val insert = new WriteDb {
        val componentId = "delPNotSetCommonColumns"
        override val writeDbMode = Insert
        override val writeDbWithCommonColumn = false
        override val writeDbSaveMode = SaveMode.Overwrite
      }

      implicit val inArgs = TestArgs().toInputArgs
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "aaa"), Row("KEY2", "bbb"), Row("KEY3", "ccc"))), structType)
      insert.writeDb(df)

      val target = new WriteDb {
        val componentId = "delPNotSetCommonColumns"
        override val writeDbMode = DeletePhysical
        override val writeDbWithCommonColumn = false
        override val writeDbUpdateKeys = Set("KEY")
      }
      val updateDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx"), Row("KEY3", "yyy"))), structType)
      target.writeDb(updateDf)

      val dbCtl = new DbCtl()
      val result = dbCtl.readTable("delPNotSetCommonColumns").collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 1
      result(0).getAs[String]("KEY") mustBe "KEY2"
      result(0).getAs[String]("TEST") mustBe "bbb"
    }
  }

  "DbCommonColumnAppender" should {
    "be normal end" in {
      implicit val inArgs = TestArgs().toInputArgs
      val target = new WriteDb {
        val componentId = "CS_TEST"
        override val writeDbSaveMode = SaveMode.Overwrite
      }
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx"))), structType)
      target.writeDb(df)

      val result = dbCtl.readTable("CS_TEST").collect.toList.sortBy(_.getAs[String]("KEY"))
      result(0).getAs[Timestamp]("DT_D2KMKDTTM").toString.isEmpty mustBe false
      result(0).getAs[String]("ID_D2KMKUSR") mustBe "CS_TEST"
      result(0).getAs[Timestamp]("DT_D2KUPDDTTM").toString.isEmpty mustBe false
      result(0).getAs[String]("ID_D2KUPDUSR") mustBe "CS_TEST"
      result(0).getAs[String]("NM_D2KUPDTMS") mustBe "0"
      result(0).getAs[String]("FG_D2KDELFLG") mustBe "0"
    }
  }

  "writeDbConvNaMode" should {
    val ts = Timestamp.valueOf(LocalDateTime.of(2000, 1, 1, 0, 0, 0))
    val df = Seq(
      Sp01(ts, 10, 100, ts, "xx", "yy"),
      Sp01(null, null, null, null, null, null),
      Sp01(null, null, null, null, "", null),
      Sp01(null, null, null, null, null, "")).toDF.repartition(1)

    "trueの場合 Stringのnull及び空文字をspace1文字に変換する" in {
      val target = new WriteDb {

        override val writeDbConvNaMode: Boolean = true

        val componentId = "sp01"
        override val writeDbMode = Insert
        override val writeDbSaveMode: SaveMode = SaveMode.Overwrite
        override val writeDbWithCommonColumn: Boolean = false
      }

      target.writeDb(df)

      val result = dbCtl.readTable("sp01").as[Sp01].collect
      result(0) mustBe Sp01(ts, 10, 100.00, ts, "xx", "yy   ")
      result(1) mustBe Sp01(null, null, null, null, " ", "     ")
      result(2) mustBe Sp01(null, null, null, null, " ", "     ")
      result(3) mustBe Sp01(null, null, null, null, " ", "     ")
    }

    "falseの場合変換しない" in {
      val target = new WriteDb {

        override val writeDbConvNaMode: Boolean = false

        val componentId = "sp01"
        override val writeDbMode = Insert
        override val writeDbSaveMode: SaveMode = SaveMode.Overwrite
        override val writeDbWithCommonColumn: Boolean = false
      }

      target.writeDb(df)

      val result = dbCtl.readTable("sp01").as[Sp01].collect
      result(0) mustBe Sp01(ts, 10, 100.00, ts, "xx", "yy   ")
      result(1) mustBe Sp01(null, null, null, null, null, null)
      result(2) mustBe Sp01(null, null, null, null, null, null)
      result(3) mustBe Sp01(null, null, null, null, null, null)
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import d2k.common.TestArgs
import d2k.common.InputArgs
import spark.common.SparkContexts
import SparkContexts.context.implicits._
import scala.io.Source
import d2k.common.df.WriteFileMode._
import org.apache.spark.sql.types._
import spark.common.SparkContexts.context
import context.implicits._
import org.apache.spark.sql.Row
import spark.common.DbCtl
import scala.reflect.io.Directory

class WriteFilePartitionTest extends WordSpec with MustMatchers with BeforeAndAfter {
  implicit val inArgs = TestArgs().toInputArgs

  val testData = (1 to 100).map(idx => TestData("あ～", s"$idx")).toDF.repartition(10)
  "Fixed WriteFileTest" should {
    "be normal end." in {
      val writeFile = new WriteFile {
        val componentId = "paraWrite1"
        override val writeFileMode = partition.Fixed(5, 5)
      }

      writeFile.writeFile(testData)

      val d = Directory(s"${inArgs.baseOutputFilePath}/paraWrite1")
      val l = d.files.toList

      l.size mustBe 10

      val recs = Source.fromInputStream(l(0).inputStream, "MS932").getLines.toList
      recs.size mustBe 10
      recs(0).getBytes("MS932").size mustBe 10
      recs(0).contains("あ～") mustBe true
      recs(1).contains("あ～") mustBe true

      val recs2 = Source.fromInputStream(l(1).inputStream, "MS932").getLines.toList
      recs2.size mustBe 10
      recs(0).getBytes("MS932").size mustBe 10
      recs2(0).contains("あ～") mustBe true
      recs2(1).contains("あ～") mustBe true
    }

    "add Extention" in {
      val writeFile = new WriteFile {
        val componentId = "paraWrite1_2"
        override val writeFileMode = partition.Fixed(5, 5)
        override val writeFilePartitionExtention = "ext"
      }

      writeFile.writeFile(testData)

      Directory(s"${inArgs.baseOutputFilePath}/paraWrite1_2")
        .files.foreach(_.name.takeRight(4) mustBe ".ext")
    }
  }

  "Csv WriteFileTest" should {
    "be normal end. arg pattern" in {
      val writeFile = new WriteFile {
        val componentId = "paraWrite2"
        override val writeFileMode = partition.Csv("a")
      }
      writeFile.writeFile(testData)

      val d = Directory(s"${inArgs.baseOutputFilePath}/paraWrite2")
      val l = d.files.toList

      l.size mustBe 10

      val recs = Source.fromInputStream(l(0).inputStream, "MS932").getLines.toList
      recs.size mustBe 10
      recs(0).contains("\"あ～\"") mustBe true
      recs(0).contains(",") mustBe true
      recs(1).contains("\"あ～\"") mustBe true
      recs(1).contains(",") mustBe true

      val recs2 = Source.fromInputStream(l(1).inputStream, "MS932").getLines.toList
      recs2.size mustBe 10
      recs2(0).contains("\"あ～\"") mustBe true
      recs2(0).contains(",") mustBe true
      recs2(1).contains("\"あ～\"") mustBe true
      recs2(1).contains(",") mustBe true
    }

    "add Extention" in {
      val writeFile = new WriteFile {
        val componentId = "paraWrite2_2"
        override val writeFileMode = partition.Csv("a")
        override val writeFilePartitionExtention = "ext"
      }
      writeFile.writeFile(testData)

      Directory(s"${inArgs.baseOutputFilePath}/paraWrite2_2")
        .files.foreach(_.name.takeRight(4) mustBe ".ext")
    }

    "be normal end. Csv with empty arg pattern" in {
      val writeFile = new WriteFile {
        val componentId = "writeFile21"
        override val writeFileMode = partition.Csv()
      }
      writeFile.writeFile(testData)

      val d = Directory(s"${inArgs.baseOutputFilePath}/writeFile21")
      val l = d.files.toList

      l.size mustBe 10

      val recs = Source.fromInputStream(l(0).inputStream, "MS932").getLines.toList
      recs.size mustBe 10
      recs(0).contains("あ～") mustBe true
      recs(0).contains(",") mustBe true
      recs(1).contains("あ～") mustBe true
      recs(1).contains(",") mustBe true

      val recs2 = Source.fromInputStream(l(1).inputStream, "MS932").getLines.toList
      recs2.size mustBe 10
      recs2(0).contains("あ～") mustBe true
      recs2(0).contains(",") mustBe true
      recs2(1).contains("あ～") mustBe true
      recs2(1).contains(",") mustBe true
    }
  }

  "Tsv WriteFileTest" should {
    "be normal end." in {
      val writeFile = new WriteFile {
        val componentId = "paraWriteTsv"
        override val writeFileMode = partition.Tsv
      }
      writeFile.writeFile(testData)

      val d = Directory(s"${inArgs.baseOutputFilePath}/paraWriteTsv")
      val l = d.files.toList

      l.size mustBe 10

      val recs = Source.fromInputStream(l(0).inputStream, "MS932").getLines.toList
      recs.size mustBe 10
      recs(0).contains("あ～") mustBe true
      recs(0).contains("\t") mustBe true
      recs(1).contains("あ～") mustBe true
      recs(1).contains("\t") mustBe true

      val recs2 = Source.fromInputStream(l(1).inputStream, "MS932").getLines.toList
      recs2.size mustBe 10
      recs2(0).contains("あ～") mustBe true
      recs2(0).contains("\t") mustBe true
      recs2(1).contains("あ～") mustBe true
      recs2(1).contains("\t") mustBe true
    }
  }

  "writeCharEncoding by UTF-8" should {
    "csv" in {
      val writeFile = new WriteFile {
        val componentId = "paraWriteCsvByUTF8"
        override val writeFileMode = partition.Csv
        override val writeCharEncoding = "UTF-8"
      }
      writeFile.writeFile(testData)

      val d = Directory(s"${inArgs.baseOutputFilePath}/${writeFile.componentId}")
      val l = d.files.toList

      l.size mustBe 10

      val recs = Source.fromInputStream(l(0).inputStream, "UTF-8").getLines.toList
      recs.size mustBe 10
      recs(0).contains("あ～") mustBe true
      recs(0).contains(",") mustBe true
      recs(1).contains("あ～") mustBe true
      recs(1).contains(",") mustBe true

      val recs2 = Source.fromInputStream(l(1).inputStream, "UTF-8").getLines.toList
      recs2.size mustBe 10
      recs2(0).contains("あ～") mustBe true
      recs2(0).contains(",") mustBe true
      recs2(1).contains("あ～") mustBe true
      recs2(1).contains(",") mustBe true
    }

    "tsv" in {
      val writeFile = new WriteFile {
        val componentId = "paraWriteTsvByUTF8"
        override val writeFileMode = partition.Tsv
        override val writeCharEncoding = "UTF-8"
      }
      writeFile.writeFile(testData)

      val d = Directory(s"${inArgs.baseOutputFilePath}/${writeFile.componentId}")
      val l = d.files.toList

      l.size mustBe 10

      val recs = Source.fromInputStream(l(0).inputStream, "UTF-8").getLines.toList
      recs.size mustBe 10
      recs(0).contains("あ～") mustBe true
      recs(0).contains("\t") mustBe true
      recs(1).contains("あ～") mustBe true
      recs(1).contains("\t") mustBe true

      val recs2 = Source.fromInputStream(l(1).inputStream, "UTF-8").getLines.toList
      recs2.size mustBe 10
      recs2(0).contains("あ～") mustBe true
      recs2(0).contains("\t") mustBe true
      recs2(1).contains("あ～") mustBe true
      recs2(1).contains("\t") mustBe true
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import d2k.common.TestArgs
import d2k.common.InputArgs
import spark.common.SparkContexts
import SparkContexts.context.implicits._
import scala.io.Source
import d2k.common.df.WriteFileMode._
import org.apache.spark.sql.types._
import spark.common.SparkContexts.context
import context.implicits._
import org.apache.spark.sql.Row
import spark.common.DbCtl

case class TestData(a: String, b: String)
class WriteFileTest extends WordSpec with MustMatchers with BeforeAndAfter {
  implicit val inArgs = TestArgs().toInputArgs
  val df = SparkContexts.sc.makeRDD(Seq(TestData("aaa", "bbb"), TestData("ccccc", "ddddd"), TestData(null, null))).toDF

  "WriteFileTest" should {
    "be normal end. Fixed with arg pattern" in {
      val writeFile = new WriteFile {
        val componentId = "writeFile1"
        override val writeFileMode = Fixed(2, 5)
      }
      writeFile.writeFile(df)
      val lines = Source.fromFile(s"${inArgs.baseOutputFilePath}/writeFile1").getLines.toList
      lines(0) mustBe "aabbb  "
      lines(1) mustBe "ccddddd"
      lines(2) mustBe "       "
    }

    "be normal end. Fixed with empty arg pattern" in {
      val writeFile = new WriteFile {
        val componentId = "writeFile11"
        override val writeFileMode = Fixed()
      }
      writeFile.writeFile(df)
      val lines = Source.fromFile(s"${inArgs.baseOutputFilePath}/writeFile11").getLines.toList
      lines(0) mustBe ""
      lines(1) mustBe ""
      lines(2) mustBe ""
    }

    "be normal end. Csv with arg pattern. writeFileVariableWrapDoubleQuote = true(default)" in {
      val writeFile = new WriteFile {
        val componentId = "writeFile2"
        override val writeFileMode = Csv("a")
      }
      writeFile.writeFile(df)
      val lines = Source.fromFile(s"${inArgs.baseOutputFilePath}/writeFile2").getLines.toList
      lines(0) mustBe """"aaa",bbb"""
      lines(1) mustBe """"ccccc",ddddd"""
      lines(2) mustBe """"","""
    }

    "be normal end. Csv with arg pattern. writeFileVariableWrapDoubleQuote = false" in {
      val writeFile = new WriteFile {
        val componentId = "writeFile2-2"
        override val writeFileMode = Csv("a")
        override val writeFileVariableWrapDoubleQuote = false
      }
      writeFile.writeFile(df)
      val lines = Source.fromFile(s"${inArgs.baseOutputFilePath}/writeFile2-2").getLines.toList
      lines(0) mustBe """"aaa",bbb"""
      lines(1) mustBe """"ccccc",ddddd"""
      lines(2) mustBe ""","""
    }

    "be normal end. Csv with empty arg pattern" in {
      val writeFile = new WriteFile {
        val componentId = "writeFile2x"
        override val writeFileMode = Csv()
      }
      writeFile.writeFile(df)
      val lines = Source.fromFile(s"${inArgs.baseOutputFilePath}/writeFile2x").getLines.toList
      lines(0) mustBe """aaa,bbb"""
      lines(1) mustBe """ccccc,ddddd"""
      lines(2) mustBe ""","""
    }
  }

  "CommonServices.writeFile CSV" should {
    val structType = StructType(Seq(
      StructField("KEY", StringType), StructField("TEST", StringType)))

    "normal end" in {
      implicit val inArgs = TestArgs().toInputArgs
      val forInsert = new WriteFile {
        val componentId = "csv"
        override val writeFileMode = Csv
      }
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("key1", "あああ～"), Row("key2", """bb"b"""), Row("key3", """c"c"c"""))), structType)
      forInsert.writeFile(df)

      val result = Source.fromFile(s"${inArgs.baseOutputFilePath}/csv")("MS932").getLines.toList.sorted
      result(0) mustBe """"key1","あああ～""""
      result(1) mustBe """"key2","bb""b""""
      result(2) mustBe """"key3","c""c""c""""
    }

    "changeFileName" in {
      implicit val inArgs = TestArgs().toInputArgs
      val forInsert = new WriteFile {
        val componentId = "csv"
        override val writeFileMode = Csv
        override lazy val writeFileName = "xxx"
      }
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("key1", "あああ～"))), structType)
      forInsert.writeFile(df)

      val result = Source.fromFile(s"${inArgs.baseOutputFilePath}/xxx")("MS932").getLines.toList.sorted
      result(0) mustBe """"key1","あああ～""""
    }

    "change Non Double Quote" in {
      implicit val inArgs = TestArgs().toInputArgs
      val forInsert = new WriteFile {
        val componentId = "csv2"
        override val writeFileMode = Csv
        override val writeFileVariableWrapDoubleQuote = false
      }
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("key1", "あああ～"))), structType)
      forInsert.writeFile(df)

      val result = Source.fromFile(s"${inArgs.baseOutputFilePath}/csv2")("MS932").getLines.toList.sorted
      result(0) mustBe """key1,あああ～"""
    }

    "empty DF" in {
      implicit val inArgs = TestArgs().toInputArgs
      val common = new WriteFile {
        val componentId = "csv"
        override val writeFileMode = Csv
        override lazy val writeFileName = "test2"
      }
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq.empty[Row]), structType)
      common.writeFile(df)

      try {
        Source.fromFile(s"${inArgs.baseOutputFilePath}/csv2")("MS932")
        fail
      } catch {
        case t: Throwable => t.printStackTrace()
      }
    }

    "null column" in {
      implicit val inArgs = TestArgs().toInputArgs
      val common = new WriteFile {
        val componentId = "csv"
        override val writeFileMode = Csv
        override lazy val writeFileName = "test3"
      }
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("key1", null))), structType)
      common.writeFile(df)

      val result = Source.fromFile(s"${inArgs.baseOutputFilePath}/test3")("MS932").getLines.toList.sorted
      result(0) mustBe """"key1","""""
    }
  }

  "CommonServices.writeFile TSV" should {
    val structType = StructType(Seq(
      StructField("KEY", StringType), StructField("TEST", StringType)))

    "normal end" in {
      implicit val inArgs = TestArgs().toInputArgs
      val forInsert = new WriteFile {
        val componentId = "tsv"
        override val writeFileMode = Tsv
      }
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("key1", "あああ～"), Row("key2", """bb"b"""), Row("key3", """c"c"c"""))), structType)
      forInsert.writeFile(df)

      val result = Source.fromFile(s"${inArgs.baseOutputFilePath}/tsv")("MS932").getLines.toList.sorted
      result(0) mustBe "\"key1\"\t\"あああ～\""
      result(1) mustBe "\"key2\"\t\"bb\"\"b\""
      result(2) mustBe "\"key3\"\t\"c\"\"c\"\"c\""
    }

    "changeFileName" in {
      implicit val inArgs = TestArgs().toInputArgs
      val forInsert = new WriteFile {
        val componentId = "tsv"
        override val writeFileMode = Tsv
        override lazy val writeFileName = "xxx2"
      }
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("key1", "あああ～"))), structType)
      forInsert.writeFile(df)

      val result = Source.fromFile(s"${inArgs.baseOutputFilePath}/xxx2")("MS932").getLines.toList.sorted
      result(0) mustBe "\"key1\"\t\"あああ～\""
    }

    "change Non Double Quote" in {
      implicit val inArgs = TestArgs().toInputArgs
      val forInsert = new WriteFile {
        val componentId = "tsv2"
        override val writeFileMode = Tsv
        override val writeFileVariableWrapDoubleQuote = false
      }
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("key1", "あああ～"))), structType)
      forInsert.writeFile(df)

      val result = Source.fromFile(s"${inArgs.baseOutputFilePath}/tsv2")("MS932").getLines.toList.sorted
      result(0) mustBe "key1\tあああ～"
    }

    "empty DF" in {
      implicit val inArgs = TestArgs().toInputArgs
      val common = new WriteFile {
        val componentId = "tsv"
        override val writeFileMode = Tsv
        override lazy val writeFileName = "test2"
      }
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq.empty[Row]), structType)
      common.writeFile(df)

      try {
        Source.fromFile(s"${inArgs.baseOutputFilePath}/tsv2")("MS932")
        fail
      } catch {
        case t: Throwable => t.printStackTrace()
      }
    }
  }

  "CommonServices.DbInfo" should {
    implicit val inArgs = TestArgs().toInputArgs
    "not defined(Default settting)" in {
      val target = new WriteDb with ReadDb {
        val componentId = "dbInfoTest"
      }
      target.readDbInfo mustBe DbCtl.dbInfo1
      target.writeDbInfo mustBe DbCtl.dbInfo1
    }

    "readDbInf" in {
      val target = new WriteDb with ReadDb {
        val componentId = "dbInfoTest"
        override val readDbInfo = DbCtl.dbInfo2
      }
      target.readDbInfo mustBe DbCtl.dbInfo2
      target.writeDbInfo mustBe DbCtl.dbInfo1
    }

    "writeDbInf" in {
      val target = new WriteDb with ReadDb {
        val componentId = "dbInfoTest"
        override val writeDbInfo = DbCtl.dbInfo2
      }
      target.readDbInfo mustBe DbCtl.dbInfo1
      target.writeDbInfo mustBe DbCtl.dbInfo2
    }

    "both" in {
      val target = new WriteDb with ReadDb {
        val componentId = "dbInfoTest"
        override val readDbInfo = DbCtl.dbInfo2
        override val writeDbInfo = DbCtl.dbInfo2
      }
      target.readDbInfo mustBe DbCtl.dbInfo2
      target.writeDbInfo mustBe DbCtl.dbInfo2
    }
  }

  "Override ResourcePath" should {
    val structType = StructType(Seq(
      StructField("KEY", StringType), StructField("TEST", StringType)))
    "normal end" in {
      implicit val inArgs = TestArgs().toInputArgs
      val forInsert = new WritePq with SingleReadPq with WriteFile {
        val componentId = "csv"
        override val writeFileMode = Csv
        override def readPqPath(implicit inArgs: InputArgs): String = s"test/dev/data/mypath"
        override def writePqPath(implicit inArgs: InputArgs): String = s"test/dev/data/mypath"
        override def writeFilePath(implicit inArgs: InputArgs): String = s"test/dev/data/mypath2"
      }
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("key1", "あああ～"), Row("key2", """bb"b"""), Row("key3", """c"c"c"""))), structType)
      forInsert.writeFile(df)
      forInsert.writeParquet(df)
      forInsert.readParquet

      val result = Source.fromFile(s"test/dev/data/mypath2/csv")("MS932").getLines.toList.sorted
      result(0) mustBe """"key1","あああ～""""
      result(1) mustBe """"key2","bb""b""""
      result(2) mustBe """"key3","c""c""c""""
    }

    "shared Impliments" in {
      implicit val inArgs = TestArgs().toInputArgs
      val forInsert = new WritePq with SingleReadPq with WriteFile {
        val componentId = "csv2"
        override def readPqPath(implicit inArgs: InputArgs): String = s"test/dev/data/mypath"
        override def writePqPath(implicit inArgs: InputArgs): String = s"test/dev/data/mypath"
        override def writeFilePath(implicit inArgs: InputArgs): String = s"test/dev/data/mypath"
        override val writeFileMode = Csv
        override lazy val readPqName = "pq"
        override lazy val writePqName = "pq"
      }
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("key11", "あああ～"), Row("key12", """bb"b"""), Row("key13", """c"c"c"""))), structType)
      forInsert.writeFile(df)
      forInsert.writeParquet(df)
      forInsert.readParquet
    }
  }

  "writeCharEncoding by UTF-8" should {
    val testData = Seq(TestData("aaa", "bbb"), TestData("あ～", "い～"), TestData("", "")).toDF
    "csv" in {
      val writeFile = new WriteFile {
        val componentId = "utf8TestForCsv"
        override val writeFileMode = Csv
        override val writeCharEncoding = "UTF-8"
      }
      val df = writeFile.writeFile(testData)
      val lines = Source.fromFile(s"${inArgs.baseOutputFilePath}/${writeFile.componentId}")("UTF-8").getLines.toList
      lines(0) mustBe "\"aaa\",\"bbb\""
      lines(1) mustBe "\"あ～\",\"い～\""
      lines(2) mustBe "\"\",\"\""
    }

    "tsv" in {
      val writeFile = new WriteFile {
        val componentId = "utf8TestForTsv"
        override val writeFileMode = Tsv
        override val writeCharEncoding = "UTF-8"
      }
      val df = writeFile.writeFile(testData)
      val lines = Source.fromFile(s"${inArgs.baseOutputFilePath}/${writeFile.componentId}")("UTF-8").getLines.toList
      lines(0) mustBe "\"aaa\"\t\"bbb\""
      lines(1) mustBe "\"あ～\"\t\"い～\""
      lines(2) mustBe "\"\"\t\"\""
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.df

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import d2k.common.TestArgs
import d2k.common.InputArgs

import spark.common.SparkContexts.context.implicits._
import spark.common.PqCtl

case class WritePqData(a: String, b: String, c: String)
class WritePqTest extends WordSpec with MustMatchers with BeforeAndAfter {
  implicit val inArgs = TestArgs().toInputArgs
  "Single Write" should {
    "be success." in {

      val wq = new WritePq { val componentId = "writeaPqTest" }

      val df = Seq(WritePqData("a1", "b1", "c1"), WritePqData("a2", "b2", "c2")).toDF

      wq.writeParquet(df)

      val pqCtl = new PqCtl(wq.writePqPath)
      import pqCtl.implicits._
      val r = pqCtl.readParquet(wq.writePqName).as[WritePqData].collect

      r(0) mustBe WritePqData("a1", "b1", "c1")
      r(1) mustBe WritePqData("a2", "b2", "c2")

    }
  }

  "partition Write" should {
    "be success." in {

      val wq = new WritePq {
        val componentId = "writeaPqPartitionTest"
        override val writePqPartitionColumns = Seq("b", "c")
      }

      val df = Seq(
        WritePqData("a1", "b1", "c1"),
        WritePqData("a2", "b1", "c2"),
        WritePqData("a3", "b1", "c2"),
        WritePqData("a4", "b2", "c1"),
        WritePqData("a5", "b2", "c2"),
        WritePqData("a6", "b3", "c1"),
        WritePqData("a7", "b3", "c2")).toDF

      wq.writeParquet(df)

      val pqCtl = new PqCtl(wq.writePqPath)
      import pqCtl.implicits._

      val r1 = pqCtl.readParquet(s"${wq.writePqName}/b=b1/c=c1").as[String].collect.sorted
      r1.size mustBe 1
      r1(0) mustBe "a1"

      val r2 = pqCtl.readParquet(s"${wq.writePqName}/b=b1/c=c2").as[String].collect.sorted
      r2.size mustBe 2
      r2(0) mustBe "a2"
      r2(1) mustBe "a3"

      val r3 = pqCtl.readParquet(s"${wq.writePqName}/b=b3/c=c2").as[String].collect.sorted
      r3.size mustBe 1
      r3(0) mustBe "a7"

      val r4 = pqCtl.readParquet(s"${wq.writePqName}/b=b1").as[(String, String)].collect.sortBy(_._1)
      r4.size mustBe 3
      r4(0) mustBe ("a1", "c1")
      r4(1) mustBe ("a2", "c2")
      r4(2) mustBe ("a3", "c2")

      val r5 = pqCtl.readParquet(s"${wq.writePqName}/*/c=c1").as[String].collect.sorted
      r5.size mustBe 3
      r5(0) mustBe "a1"
      r5(1) mustBe "a4"
      r5(2) mustBe "a6"

      val r6 = pqCtl.readParquet(s"${wq.writePqName}/b=b1/*").as[String].collect.sorted
      r6.size mustBe 3
      r6(0) mustBe "a1"
      r6(1) mustBe "a2"
      r6(2) mustBe "a3"

      val r7 = pqCtl.readParquet(s"${wq.writePqName}/*/*").as[String].collect.sorted
      r7.size mustBe 7
      r7(0) mustBe "a1"
      r7(1) mustBe "a2"
      r7(2) mustBe "a3"
      r7(3) mustBe "a4"
      r7(4) mustBe "a5"
      r7(5) mustBe "a6"
      r7(6) mustBe "a7"
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.file.output

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter
import spark.common.SparkContexts.context
import context.implicits._
import scala.reflect.io.Directory
import spark.common.SparkContexts
import scala.io.Source
import d2k.common.TestArgs
import scala.reflect.io.Path
import org.apache.hadoop.io.NullWritable
import org.apache.hadoop.io.BytesWritable
import scala.util.Try

import spark.common.DfCtl
import DfCtl._
import DfCtl.implicits._

case class TestData(a: String, b: String)
case class TestData2(a: String, b: String, c: String, d: String, e: String, f: String, g: String)
case class TestData3(z: String, b: String, x: String, d: String, e: String, y: String, g: String)
class FixedFileTest extends WordSpec with MustMatchers with BeforeAndAfter {
  implicit val inArgs = TestArgs().toInputArgs
  val df = SparkContexts.sc.makeRDD(Seq(TestData("aaa", "bbb"), TestData("ccccc", "ddddd"))).toDF
  val dfemptydata = SparkContexts.sc.makeRDD(Seq(TestData("", "bbb"), TestData("ccccc", ""))).toDF
  val dfnulldata = SparkContexts.sc.makeRDD(Seq(TestData(null, "bbb"), TestData("ccccc", null))).toDF
  val dfkanjidata = SparkContexts.sc.makeRDD(Seq(TestData("あい", "うえお"), TestData("かきくけこ", "さしすせそ"))).toDF

  val outputDir = "test/dev/data/output2"
  def mkFilePath(fileName: String) = s"${outputDir}/${fileName}"

  "writeSingle_MS932" should {
    "be normal end. pattern1" in {
      new FixedFile(mkFilePath("writeSingle_normal1")).writeSingle_MS932(Seq(2, 6))(df)
      val lines = Source.fromFile(mkFilePath("writeSingle_normal1")).getLines.toList
      lines(0) mustBe "aabbb   "
      lines(1) mustBe "ccddddd "
    }

    "be normal end. pattern2" in {
      new FixedFile(mkFilePath("writeSingle_normal2")).writeSingle_MS932(Seq(3, 3))(df)
      val lines = Source.fromFile(mkFilePath("writeSingle_normal2")).getLines.toList
      lines(0) mustBe "aaabbb"
      lines(1) mustBe "cccddd"
    }

    "be normal end. pattern3" in {
      new FixedFile(mkFilePath("writeSingle_normal3")).writeSingle_MS932(Seq(5, 5))(df)
      val lines = Source.fromFile(mkFilePath("writeSingle_normal3")).getLines.toList
      lines(0) mustBe "aaa  bbb  "
      lines(1) mustBe "cccccddddd"
    }

    "be normal end. no data" in {
      new FixedFile(mkFilePath("writeSingle_normal_nodata4")).writeSingle_MS932(Seq(2, 6))(df.filter(df("a") === "xx"))
      val lines = Source.fromFile(mkFilePath("writeSingle_normal_nodata4")).getLines.toList
      lines.size mustBe 0
    }

    "be normal end. empty data" in {
      new FixedFile(mkFilePath("writeSingle_normal5")).writeSingle_MS932(Seq(2, 6))(dfemptydata)
      val lines = Source.fromFile(mkFilePath("writeSingle_normal5")).getLines.toList
      lines(0) mustBe "  bbb   "
      lines(1) mustBe "cc      "
    }

    "be normal end. null data" in {
      new FixedFile(mkFilePath("writeSingle_normal6")).writeSingle_MS932(Seq(2, 6))(dfnulldata)
      val lines = Source.fromFile(mkFilePath("writeSingle_normal6")).getLines.toList
      lines(0) mustBe "  bbb   "
      lines(1) mustBe "cc      "
    }

    "be normal end. set size zero" in {
      new FixedFile(mkFilePath("writeSingle_normal7")).writeSingle_MS932(Seq(0, 0))(df)
      val lines = Source.fromFile(mkFilePath("writeSingle_normal7")).getLines.toList
      lines(0) mustBe ""
      lines(1) mustBe ""
    }

    "be normal end. kanji pattern1" in {
      new FixedFile(mkFilePath("writeSingle_normal8")).writeSingle_MS932(Seq(4, 6))(dfkanjidata)
      val lines = Source.fromFile(mkFilePath("writeSingle_normal8"), "MS932").getLines.toList
      lines(0) mustBe "あいうえお"
      lines(1) mustBe "かきさしす"
    }

    "be normal end. kanji pattern2" in {
      new FixedFile(mkFilePath("writeSingle_normal9")).writeSingle_MS932(Seq(10, 10))(dfkanjidata)
      val lines = Source.fromFile(mkFilePath("writeSingle_normal9"), "MS932").getLines.toList
      lines(0) mustBe "あい      うえお    "
      lines(1) mustBe "かきくけこさしすせそ"
    }

    "writeSingle" when {
      "partitionColumns" in {
        val fileName = s"${inArgs.baseOutputFilePath}/fixedFileTest/ptt1_1"
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => Test1(x.toString, cnt, cnt.toString))).toDF
        new FixedFile(fileName, Seq("a", "b")).writeSingle_MS932(Seq(1, 1, 1))(df)

        ('a' to 'c').flatMap(x => (1 to 3).map(cnt => s"${fileName}/a=${x}/b=${cnt}/0")).foreach { path =>
          withClue(path) { Path(path).exists mustBe true }
        }
      }

      "with partitionColumns add Extention" in {
        val fileName = s"${inArgs.baseOutputFilePath}/fixedFileTest/ptt1_2"
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => Test1(x.toString, cnt, cnt.toString))).toDF
        new FixedFile(fileName, Seq("a", "b"), "ext").writeSingle_MS932(Seq(1, 1, 1))(df)

        ('a' to 'c').flatMap(x => (1 to 3).map(cnt => s"${fileName}/a=${x}/b=${cnt}/0.ext")).foreach { path =>
          withClue(path) { Path(path).exists mustBe true }
        }
      }
    }

    "writePartition" when {
      "partitionColumns" in {
        val fileName = s"${inArgs.baseOutputFilePath}/fixedFileTest/ptt2_1"
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => Test1(x.toString, cnt, cnt.toString))).toDF
        new FixedFile(fileName, Seq("a", "b")).writePartition_MS932(Seq(1, 1, 1))(df)
        Path(s"${fileName}/a=a/b=1/0").exists mustBe true
        Path(s"${fileName}/a=a/b=2/0").exists mustBe true
        Path(s"${fileName}/a=a/b=3/1").exists mustBe true
        Path(s"${fileName}/a=b/b=1/1").exists mustBe true
        Path(s"${fileName}/a=b/b=2/2").exists mustBe true
        Path(s"${fileName}/a=b/b=3/2").exists mustBe true
        Path(s"${fileName}/a=c/b=1/3").exists mustBe true
        Path(s"${fileName}/a=c/b=2/3").exists mustBe true
        Path(s"${fileName}/a=c/b=3/3").exists mustBe true
      }

      "add Extention" in {
        val fileName = s"${inArgs.baseOutputFilePath}/fixedFileTest/ptt2_2"
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => Test1(x.toString, cnt, cnt.toString))).toDF
        new FixedFile(fileName, writeFilePartitionExtention = "ext").writePartition_MS932(Seq(1, 1, 1))(df)
        Path(s"${fileName}/0.ext").exists mustBe true
        Path(s"${fileName}/0.ext").exists mustBe true
        Path(s"${fileName}/1.ext").exists mustBe true
        Path(s"${fileName}/1.ext").exists mustBe true
        Path(s"${fileName}/2.ext").exists mustBe true
        Path(s"${fileName}/2.ext").exists mustBe true
        Path(s"${fileName}/3.ext").exists mustBe true
        Path(s"${fileName}/3.ext").exists mustBe true
        Path(s"${fileName}/3.ext").exists mustBe true
      }

      "partitionColumns add Extention" in {
        val fileName = s"${inArgs.baseOutputFilePath}/fixedFileTest/ptt2_3"
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => Test1(x.toString, cnt, cnt.toString))).toDF
        new FixedFile(fileName, Seq("a", "b"), writeFilePartitionExtention = "ext").writePartition_MS932(Seq(1, 1, 1))(df)
        Path(s"${fileName}/a=a/b=1/0.ext").exists mustBe true
        Path(s"${fileName}/a=a/b=2/0.ext").exists mustBe true
        Path(s"${fileName}/a=a/b=3/1.ext").exists mustBe true
        Path(s"${fileName}/a=b/b=1/1.ext").exists mustBe true
        Path(s"${fileName}/a=b/b=2/2.ext").exists mustBe true
        Path(s"${fileName}/a=b/b=3/2.ext").exists mustBe true
        Path(s"${fileName}/a=c/b=1/3.ext").exists mustBe true
        Path(s"${fileName}/a=c/b=2/3.ext").exists mustBe true
        Path(s"${fileName}/a=c/b=3/3.ext").exists mustBe true
      }
    }

    "writeSequence" ignore {
      val fileName = s"${inArgs.baseOutputFilePath}/fixedFileSequence/pt1"
      val df = ('a' to 'c').map(x => Test1(s"あ${x}", 1, x.toString)).toDF
      new FixedFile(fileName, Seq("a", "b")).writeSequence_MS932(Seq(3, 1, 2))(df)

      val result = SparkContexts.sc.sequenceFile(fileName, NullWritable.get.getClass, Test1.getClass).map(_.toString).collect.sorted
      result(0) mustBe "((null),82 a0 61 31 61 20)"
      result(1) mustBe "((null),82 a0 62 31 62 20)"
      result(2) mustBe "((null),82 a0 63 31 63 20)"
    }
  }

  "writeHdfs" should {
    "be normal end" in {
      val fileName = s"${inArgs.baseOutputFilePath}/fixedFileHdfs/pt1"
      val df = ('a' to 'c').map(x => Test1(s"あ${x}", 1, x.toString)).toDF
      new FixedFile(fileName).writeHdfs_MS932(Seq(4, 2, 3))(df)
      val result = SparkContexts.context.read.text(fileName).collect
      result(0).getAs[String](0) mustBe "あa 1 a  "
      result(1).getAs[String](0) mustBe "あb 1 b  "
      result(2).getAs[String](0) mustBe "あc 1 c  "
    }

    "empty check" in {
      val fileName = s"${inArgs.baseOutputFilePath}/fixedFileHdfs/pt1-1"
      val df = ('a' to 'c').map(x => Test1("", 1, "")).toDF
      new FixedFile(fileName).writeHdfs_MS932(Seq(4, 2, 3))(df)
      val result = SparkContexts.context.read.text(fileName).collect
      result(0).getAs[String](0) mustBe "    1    "
      result(1).getAs[String](0) mustBe "    1    "
      result(2).getAs[String](0) mustBe "    1    "
    }

    "null check" in {
      val fileName = s"${inArgs.baseOutputFilePath}/fixedFileHdfs/pt1-2"
      val df = ('a' to 'c').map(x => Test1(null, 1, null)).toDF
      new FixedFile(fileName).writeHdfs_MS932(Seq(4, 2, 3))(df)
      val result = SparkContexts.context.read.text(fileName).collect
      result(0).getAs[String](0) mustBe "    1    "
      result(1).getAs[String](0) mustBe "    1    "
      result(2).getAs[String](0) mustBe "    1    "
    }

    "writeHdfs partition write mode" in {
      val fileName = s"${inArgs.baseOutputFilePath}/fixedFileHdfs/pt2"
      val df = ('a' to 'c').map(x => Test1(s"あ${x}", 1, x.toString)).toDF
      new FixedFile(fileName, Seq("a", "b")).writeHdfs_MS932(Seq(3))(df)
      val result = SparkContexts.context.read.text(fileName).select("value").sort("value").collect
      result(0).getAs[String](0) mustBe "a  "
      result(1).getAs[String](0) mustBe "b  "
      result(2).getAs[String](0) mustBe "c  "
    }

    "writeHdfs partition write mode. null test" in {
      val fileName = s"${inArgs.baseOutputFilePath}/fixedFileHdfs/pt2"
      val df = ('a' to 'c').map(x => Test1(s"あ${x}", 1, null)).toDF
      new FixedFile(fileName, Seq("a", "b")).writeHdfs_MS932(Seq(3))(df)
      val result = SparkContexts.context.read.text(fileName).select("value").sort("value").collect
      result(0).getAs[String](0) mustBe "   "
      result(1).getAs[String](0) mustBe "   "
      result(2).getAs[String](0) mustBe "   "
    }

    "many columns" when {
      "sorted column name" in {
        val fileName = s"${inArgs.baseOutputFilePath}/fixedFileHdfs/pt3"
        val dfx = ('a' to 'c').map(x => TestData2(s"あ${x}", "1", "2", "3", "4", "5", "6")).toDF

        new FixedFile(fileName, Seq("b", "d", "f")).writeHdfs_MS932(Seq(4, 1, 2, 3))(dfx)
        val result = SparkContexts.context.read.text(fileName).collect
        result(0).getAs[String](0) mustBe "あa 24 6  "
        result(1).getAs[String](0) mustBe "あb 24 6  "
        result(2).getAs[String](0) mustBe "あc 24 6  "
      }

      "random column name" in {
        val fileName = s"${inArgs.baseOutputFilePath}/fixedFileHdfs/pt3"
        val dfx = ('a' to 'c').map(x => TestData3(s"あ${x}", "1", "2", "3", "4", "5", "6")).toDF

        new FixedFile(fileName, Seq("e", "g", "b", "d")).writeHdfs_MS932(Seq(4, 1, 2))(dfx)
        val result = SparkContexts.context.read.text(fileName).collect
        result(0).getAs[String](0) mustBe "あa 25 "
        result(1).getAs[String](0) mustBe "あb 25 "
        result(2).getAs[String](0) mustBe "あc 25 "
      }
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.file.output

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter
import spark.common.SparkContexts.context
import context.implicits._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.Row
import d2k.common.InputArgs
import org.apache.spark.sql.types._
import spark.common.SparkContexts
import scala.io.Source
import d2k.common.TestArgs
import scala.reflect.io.Path
import org.apache.hadoop.io.NullWritable

case class Test1(a: String, b: Int, c: String)
case class Test2(str1: String,str2: String, sqlDate: java.sql.Date, sqlTimestamp: java.sql.Timestamp,decimal:BigDecimal )
class VariableFileTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "VariableFile" should {
    implicit val inArgs = TestArgs().toInputArgs
    "mkOutputStr Wrap Double Quate normal end" in {
      val wrapDoubleQuote = true
      VariableFile.mkOutputStr(Row("key1", "あああ～"), ",", wrapDoubleQuote, "\"") mustBe """"key1","あああ～""""
      VariableFile.mkOutputStr(Row("key1", "a\"b"), ",", wrapDoubleQuote, "\"") mustBe """"key1","a""b""""
      VariableFile.mkOutputStr(Row("key1", ""), ",", wrapDoubleQuote, "\"") mustBe """"key1","""""
      VariableFile.mkOutputStr(Row("key1", "\""), ",", wrapDoubleQuote, "\"") mustBe """"key1","""""""
      VariableFile.mkOutputStr(Row("key1", "\"\""), ",", wrapDoubleQuote, "\"") mustBe """"key1","""""""""
      VariableFile.mkOutputStr(Row("key1", null), ",", wrapDoubleQuote, "\"") mustBe """"key1","""""
    }

    "mkOutputStr UnWrap Double Quate normal end" in {
      val wrapDoubleQuote = false
      VariableFile.mkOutputStr(Row("key1", "あああ～"), ",", wrapDoubleQuote, "\"") mustBe """key1,あああ～"""
      VariableFile.mkOutputStr(Row("key1", "a\"b"), ",", wrapDoubleQuote, "\"") mustBe """key1,a"b"""
      VariableFile.mkOutputStr(Row("key1", ""), ",", wrapDoubleQuote, "\"") mustBe """key1,"""
      VariableFile.mkOutputStr(Row("key1", "\""), ",", wrapDoubleQuote, "\"") mustBe """key1,""""
      VariableFile.mkOutputStr(Row("key1", "\"\""), ",", wrapDoubleQuote, "\"") mustBe """key1,"""""
      VariableFile.mkOutputStr(Row("key1", null), ",", wrapDoubleQuote, "\"") mustBe """key1,"""
    }

    "writeSingleCsvWithDoubleQuote_MS932. 1 wrapped pattern. wrapDoubleQuote = true" in {
      implicit val inArgs = TestArgs().toInputArgs
      val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/1"
      val structType = StructType(Seq(
        StructField("A", StringType), StructField("B", StringType), StructField("C", StringType), StructField("D", DecimalType(5, 0))))
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(
        Seq(Row("a1", "b1", "c1", BigDecimal(100)), Row("a2", "b\"2", "c2", BigDecimal(200)), Row(null, null, null, null))), structType)
      new VariableFile(fileName, true, "\"", "MS932").writeSingleCsvWithDoubleQuote(Set("B"))(df)
      val result = Source.fromFile(fileName).getLines.toList
      result(0) mustBe """a1,"b1",c1,100"""
      result(1) mustBe """a2,"b""2",c2,200"""
      result(2) mustBe ""","",,"""
    }

    "writeSingleCsvWithDoubleQuote_MS932. 1 wrapped pattern. wrapDoubleQuote = false" in {
      implicit val inArgs = TestArgs().toInputArgs
      val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/1-2"
      val structType = StructType(Seq(
        StructField("A", StringType), StructField("B", StringType), StructField("C", StringType), StructField("D", DecimalType(5, 0))))
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(
        Seq(Row("a1", "b1", "c1", BigDecimal(100)), Row("a2", "b\"2", "c2", BigDecimal(200)), Row(null, null, null, null))), structType)
      new VariableFile(fileName, false, "\"", "MS932").writeSingleCsvWithDoubleQuote(Set("B"))(df)
      val result = Source.fromFile(fileName).getLines.toList
      result(0) mustBe """a1,"b1",c1,100"""
      result(1) mustBe """a2,"b""2",c2,200"""
      result(2) mustBe """,,,"""
    }

    "writeSingleCsvWithDoubleQuote_MS932. no wrapped pattern. wrapDoubleQuote = true" in {
      implicit val inArgs = TestArgs().toInputArgs
      val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/no"
      val structType = StructType(Seq(
        StructField("A", StringType), StructField("B", StringType), StructField("C", StringType), StructField("D", DecimalType(5, 0))))
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(
        Seq(Row("a1", "b1", "c1", BigDecimal(100)), Row("a2", "b\"2", "c2", BigDecimal(200)), Row(null, null, null, null))), structType)
      new VariableFile(fileName, true, "\"", "MS932").writeSingleCsvWithDoubleQuote(Set.empty[String])(df)
      val result = Source.fromFile(fileName).getLines.toList
      result(0) mustBe """a1,b1,c1,100"""
      result(1) mustBe """a2,b"2,c2,200"""
      result(2) mustBe """,,,"""
    }

    "writeSingleCsvWithDoubleQuote_MS932. no wrapped pattern. wrapDoubleQuote = false" in {
      implicit val inArgs = TestArgs().toInputArgs
      val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/no1"
      val structType = StructType(Seq(
        StructField("A", StringType), StructField("B", StringType), StructField("C", StringType), StructField("D", DecimalType(5, 0))))
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(
        Seq(Row("a1", "b1", "c1", BigDecimal(100)), Row("a2", "b\"2", "c2", BigDecimal(200)), Row(null, null, null, null))), structType)
      new VariableFile(fileName, false, "\"", "MS932").writeSingleCsvWithDoubleQuote(Set.empty[String])(df)
      val result = Source.fromFile(fileName).getLines.toList
      result(0) mustBe """a1,b1,c1,100"""
      result(1) mustBe """a2,b"2,c2,200"""
      result(2) mustBe """,,,"""
    }

    "writeSingleCsvWithDoubleQuote_MS932. all wrapped pattern. wrapDoubleQuote = true" in {
      implicit val inArgs = TestArgs().toInputArgs
      val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/2"
      val structType = StructType(Seq(
        StructField("A", StringType), StructField("B", StringType), StructField("C", StringType), StructField("D", DecimalType(5, 0))))
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(
        Seq(Row("a1", "b1", "c1", BigDecimal(100)), Row("a2", "b\"2", "c2", BigDecimal(200)), Row(null, null, null, null))), structType)
      new VariableFile(fileName, true, "\"", "MS932").writeSingleCsvWithDoubleQuote(Set("D", "C", "B", "A"))(df)
      val result = Source.fromFile(fileName).getLines.toList
      result(0) mustBe """"a1","b1","c1","100""""
      result(1) mustBe """"a2","b""2","c2","200""""
      result(2) mustBe """"","","","""""
    }

    "writeSingleCsvWithDoubleQuote_MS932. all wrapped pattern. wrapDoubleQuote = false" in {
      implicit val inArgs = TestArgs().toInputArgs
      val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/2-2"
      val structType = StructType(Seq(
        StructField("A", StringType), StructField("B", StringType), StructField("C", StringType), StructField("D", DecimalType(5, 0))))
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(
        Seq(Row("a1", "b1", "c1", BigDecimal(100)), Row("a2", "b\"2", "c2", BigDecimal(200)), Row(null, null, null, null))), structType)
      new VariableFile(fileName, false, "\"", "MS932").writeSingleCsvWithDoubleQuote(Set("D", "C", "B", "A"))(df)
      val result = Source.fromFile(fileName).getLines.toList
      result(0) mustBe """"a1","b1","c1","100""""
      result(1) mustBe """"a2","b""2","c2","200""""
      result(2) mustBe """,,,"""
    }

    "writeSingleCsvWithDoubleQuote_MS932. 5 wrapped pattern. wrapDoubleQuote = true" in {
      implicit val inArgs = TestArgs().toInputArgs
      val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/3"
      val structType = StructType(Seq(
        StructField("A", StringType), StructField("B", StringType), StructField("C", StringType),
        StructField("D", StringType), StructField("E", StringType), StructField("F", StringType), StructField("G", DecimalType(5, 0))))
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(
        Seq(Row("a1", "b1", "c1", "d1", "e1", "f1", BigDecimal(100)), Row("a2", "b\"2", "c2", "d2", "e2", "f2", BigDecimal(200)),
          Row(null, null, null, null, null, null, null))), structType)
      new VariableFile(fileName, true, "\"", "MS932").writeSingleCsvWithDoubleQuote(Set("A", "B", "C", "D", "E", "G"))(df)
      val result = Source.fromFile(fileName).getLines.toList
      result(0) mustBe """"a1","b1","c1","d1","e1",f1,"100""""
      result(1) mustBe """"a2","b""2","c2","d2","e2",f2,"200""""
      result(2) mustBe """"","","","","",,"""""
    }

    "writeSingleCsvWithDoubleQuote_MS932. 5 wrapped pattern. wrapDoubleQuote = false" in {
      implicit val inArgs = TestArgs().toInputArgs
      val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/3-2"
      val structType = StructType(Seq(
        StructField("A", StringType), StructField("B", StringType), StructField("C", StringType),
        StructField("D", StringType), StructField("E", StringType), StructField("F", StringType), StructField("G", DecimalType(5, 0))))
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(
        Seq(Row("a1", "b1", "c1", "d1", "e1", "f1", BigDecimal(100)), Row("a2", "b\"2", "c2", "d2", "e2", "f2", BigDecimal(200)),
          Row(null, null, null, null, null, null, null))), structType)
      new VariableFile(fileName, false, "\"", "MS932").writeSingleCsvWithDoubleQuote(Set("A", "B", "C", "D", "E", "G"))(df)
      val result = Source.fromFile(fileName).getLines.toList
      result(0) mustBe """"a1","b1","c1","d1","e1",f1,"100""""
      result(1) mustBe """"a2","b""2","c2","d2","e2",f2,"200""""
      result(2) mustBe """,,,,,,"""
    }

    "writeSingle" when {
      "partitionColumns" in {
        val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/ptt1_1"
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => Test1(x.toString, cnt, cnt.toString))).toDF
        new VariableFile(fileName, false, "\"", "MS932", Seq("a", "b")).writeSingle(",")(df)
        ('a' to 'c').flatMap(x => (1 to 3).map(cnt => s"${fileName}/a=${x}/b=${cnt}/0")).foreach { path =>
          withClue(path) { Path(path).exists mustBe true }
        }
      }

      "partitionColumns add Extention" in {
        val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/ptt1_2"
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => Test1(x.toString, cnt, cnt.toString))).toDF
        new VariableFile(fileName, false, "\"", "MS932", Seq("a", "b"), "ext").writeSingle(",")(df)
        ('a' to 'c').flatMap(x => (1 to 3).map(cnt => s"${fileName}/a=${x}/b=${cnt}/0.ext")).foreach { path =>
          withClue(path) { Path(path).exists mustBe true }
        }
      }
    }

    "writeSingleCsvWithDoubleQuote_MS932" when {
      "partitionColumns" in {
        val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/ptt2_1"
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => Test1(x.toString, cnt, cnt.toString))).toDF
        new VariableFile(fileName, false, "MS932", "\"", Seq("a", "b")).writeSingleCsvWithDoubleQuote(Set("c"))(df)
        ('a' to 'c').flatMap(x => (1 to 3).map(cnt => s"${fileName}/a=${x}/b=${cnt}/0")).foreach { path =>
          withClue(path) { Path(path).exists mustBe true }
        }
      }

      "partitionColumns add Extention" in {
        val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/ptt2_2"
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => Test1(x.toString, cnt, cnt.toString))).toDF
        new VariableFile(fileName, false, "MS932", "\"", Seq("a", "b"), "ext").writeSingleCsvWithDoubleQuote(Set("c"))(df)
        ('a' to 'c').flatMap(x => (1 to 3).map(cnt => s"${fileName}/a=${x}/b=${cnt}/0.ext")).foreach { path =>
          withClue(path) { Path(path).exists mustBe true }
        }
      }
    }

    "writePartition" when {
      "partitionColumns" in {
        val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/ptt3_1"
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => Test1(x.toString, cnt, cnt.toString))).toDF
        new VariableFile(fileName, false, "\"", "MS932", Seq("a", "b")).writePartition(",")(df)
        Path(s"${fileName}/a=a/b=1/0").exists mustBe true
        Path(s"${fileName}/a=a/b=2/0").exists mustBe true
        Path(s"${fileName}/a=a/b=3/1").exists mustBe true
        Path(s"${fileName}/a=b/b=1/1").exists mustBe true
        Path(s"${fileName}/a=b/b=2/2").exists mustBe true
        Path(s"${fileName}/a=b/b=3/2").exists mustBe true
        Path(s"${fileName}/a=c/b=1/3").exists mustBe true
        Path(s"${fileName}/a=c/b=2/3").exists mustBe true
        Path(s"${fileName}/a=c/b=3/3").exists mustBe true
      }

      "add Extention" in {
        val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/ptt3_2"
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => Test1(x.toString, cnt, cnt.toString))).toDF
        new VariableFile(fileName, false, "\"", "MS932", writeFilePartitionExtention = "ext").writePartition(",")(df)
        Path(s"${fileName}/0.ext").exists mustBe true
        Path(s"${fileName}/0.ext").exists mustBe true
        Path(s"${fileName}/1.ext").exists mustBe true
        Path(s"${fileName}/1.ext").exists mustBe true
        Path(s"${fileName}/2.ext").exists mustBe true
        Path(s"${fileName}/2.ext").exists mustBe true
        Path(s"${fileName}/3.ext").exists mustBe true
        Path(s"${fileName}/3.ext").exists mustBe true
        Path(s"${fileName}/3.ext").exists mustBe true
      }

      "partitionColumns add Extention" in {
        val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/ptt3_3"
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => Test1(x.toString, cnt, cnt.toString))).toDF
        new VariableFile(fileName, false, "\"", "MS932", Seq("a", "b"), "ext").writePartition(",")(df)
        Path(s"${fileName}/a=a/b=1/0.ext").exists mustBe true
        Path(s"${fileName}/a=a/b=2/0.ext").exists mustBe true
        Path(s"${fileName}/a=a/b=3/1.ext").exists mustBe true
        Path(s"${fileName}/a=b/b=1/1.ext").exists mustBe true
        Path(s"${fileName}/a=b/b=2/2.ext").exists mustBe true
        Path(s"${fileName}/a=b/b=3/2.ext").exists mustBe true
        Path(s"${fileName}/a=c/b=1/3.ext").exists mustBe true
        Path(s"${fileName}/a=c/b=2/3.ext").exists mustBe true
        Path(s"${fileName}/a=c/b=3/3.ext").exists mustBe true
      }
    }

    "writePartitionCsvWithDoubleQuote_MS932" when {
      "partitionColumns" in {
        val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/ptt4_1"
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => Test1(x.toString, cnt, cnt.toString))).toDF
        new VariableFile(fileName, false, "\"", "MS932", Seq("a", "b")).writePartitionCsvWithDoubleQuote(Set("c"))(df)
        Path(s"${fileName}/a=a/b=1/0").exists mustBe true
        Path(s"${fileName}/a=a/b=2/0").exists mustBe true
        Path(s"${fileName}/a=a/b=3/1").exists mustBe true
        Path(s"${fileName}/a=b/b=1/1").exists mustBe true
        Path(s"${fileName}/a=b/b=2/2").exists mustBe true
        Path(s"${fileName}/a=b/b=3/2").exists mustBe true
        Path(s"${fileName}/a=c/b=1/3").exists mustBe true
        Path(s"${fileName}/a=c/b=2/3").exists mustBe true
        Path(s"${fileName}/a=c/b=3/3").exists mustBe true
      }

      "add Extention" in {
        val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/ptt4_2"
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => Test1(x.toString, cnt, cnt.toString))).toDF
        new VariableFile(fileName, false, "\"", "MS932", writeFilePartitionExtention = "ext").writePartitionCsvWithDoubleQuote(Set("c"))(df)
        Path(s"${fileName}/0.ext").exists mustBe true
        Path(s"${fileName}/0.ext").exists mustBe true
        Path(s"${fileName}/1.ext").exists mustBe true
        Path(s"${fileName}/1.ext").exists mustBe true
        Path(s"${fileName}/2.ext").exists mustBe true
        Path(s"${fileName}/2.ext").exists mustBe true
        Path(s"${fileName}/3.ext").exists mustBe true
        Path(s"${fileName}/3.ext").exists mustBe true
        Path(s"${fileName}/3.ext").exists mustBe true
      }

      "partitionColumns add Extention" in {
        val fileName = s"${inArgs.baseOutputFilePath}/variableFileTest/ptt4_3"
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => Test1(x.toString, cnt, cnt.toString))).toDF
        new VariableFile(fileName, false, "\"", "MS932", Seq("a", "b"), "ext").writePartitionCsvWithDoubleQuote(Set("c"))(df)
        Path(s"${fileName}/a=a/b=1/0.ext").exists mustBe true
        Path(s"${fileName}/a=a/b=2/0.ext").exists mustBe true
        Path(s"${fileName}/a=a/b=3/1.ext").exists mustBe true
        Path(s"${fileName}/a=b/b=1/1.ext").exists mustBe true
        Path(s"${fileName}/a=b/b=2/2.ext").exists mustBe true
        Path(s"${fileName}/a=b/b=3/2.ext").exists mustBe true
        Path(s"${fileName}/a=c/b=1/3.ext").exists mustBe true
        Path(s"${fileName}/a=c/b=2/3.ext").exists mustBe true
        Path(s"${fileName}/a=c/b=3/3.ext").exists mustBe true
      }
    }

    "writeHdfs" when {
      "csv" in {
        val fileName = s"${inArgs.baseOutputFilePath}/variableFileHdfs/pt1"
        val df = ('a' to 'c').map(x => Test1(s"あ${x}", 1, x.toString)).toDF
        new VariableFile(fileName, true, "", "UTF-8").writeHdfs(",")(df)
        val result = SparkContexts.context.read.csv(fileName).collect
        result(0).getAs[String](0) mustBe "あa"
        result(0).getAs[String](1) mustBe "1"
        result(0).getAs[String](2) mustBe "a"
      }

      "csv without double quote" in {
        val fileName = s"${inArgs.baseOutputFilePath}/variableFileHdfs/pt2"
        val df = ('a' to 'c').map(x => Test1(s"あ${x}", 1, x.toString)).toDF
        new VariableFile(fileName, false, "", "UTF-8").writeHdfs(",")(df)
        val result = SparkContexts.context.read.csv(fileName).collect
        result(0).getAs[String](0) mustBe "あa"
        result(0).getAs[String](1) mustBe "1"
        result(0).getAs[String](2) mustBe "a"
      }

      "trim無し確認" in {
        val fileName = s"${inArgs.baseOutputFilePath}/variableFileHdfs/pt3"
        val df = ('a' to 'c').map(x => Test1(s"   あ${x}  ", 1, x.toString)).toDF
        new VariableFile(fileName, false, "", "UTF-8").writeHdfs(",")(df)
        val result = SparkContexts.context.read.csv(fileName).collect
        result(0).getAs[String](0) mustBe "   あa  "
        result(0).getAs[String](1) mustBe "1"
        result(0).getAs[String](2) mustBe "a"
      }

      "空値、null値のquoteなし確認" in {
        val fileName = s"${inArgs.baseOutputFilePath}/variableFileHdfs/pt4"
        val df = List(Test2("", null, null, null, null)).toDF
        new VariableFile(fileName, false, "", "UTF-8").writeHdfs(",")(df)
        val result = SparkContexts.context.read.text(fileName).collect
        result(0).getAs[String](0) mustBe ",,,,"
      }

    }

    "writeSequence" ignore {
      val fileName = s"${inArgs.baseOutputFilePath}/variableFileSequence/pt1"
      val df = ('a' to 'c').map(x => Test1(s"あ${x}", 1, x.toString)).toDF
      new VariableFile(fileName, false, "", "MS932", Seq("a", "b")).writeSequence(",")(df)

      val result = SparkContexts.sc.sequenceFile(fileName, NullWritable.get.getClass, Test1.getClass).map(_.toString).collect.sorted
      result(0) mustBe "((null),82 a0 61 2c 31 2c 61)"
      result(1) mustBe "((null),82 a0 62 2c 31 2c 62)"
      result(2) mustBe "((null),82 a0 63 2c 31 2c 63)"
    }

    "writeSequenceCsvWithDoubleQuote_MS932" ignore {
      val fileName = s"${inArgs.baseOutputFilePath}/variableFileSequence/pt2"
      val df = ('a' to 'c').map(x => Test1(s"あ${x}", 1, x.toString)).toDF
      new VariableFile(fileName, false, "", "MS932", Seq("a", "b")).writeSequenceCsvWithDoubleQuote(Set("c"))(df)

      val result = SparkContexts.sc.sequenceFile(fileName, NullWritable.get.getClass, Test1.getClass).map(_.toString).collect.sorted
      result.foreach(println)
      result(0) mustBe "((null),82 a0 61 2c 31 2c 22 61 22)"
      result(1) mustBe "((null),82 a0 62 2c 31 2c 22 62 22)"
      result(2) mustBe "((null),82 a0 63 2c 31 2c 22 63 22)"
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.fileConv

import org.apache.spark.sql.functions._
import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import d2k.common.TestArgs
import d2k.common.df.executor.BinaryRecordConverter
import d2k.common.df.template._
import d2k.common.df.executor.Nothing
import d2k.common.df.FixedInfo

import spark.common.DfCtl.implicits._
import scala.util.Try
import d2k.common.df.executor.PqCommonColumnRemover

class BinaryRecordTest extends WordSpec with MustMatchers with BeforeAndAfter {
  implicit val inArgs = TestArgs().toInputArgs

  "fixed with Binary Record" should {
    "be normal end" when {
      val binrecName = "binrec"
      "MS932" in {
        val compo = new FileToDf with Nothing {
          val componentId = "fixed"
          override val fileInputInfo = FixedInfo(Set("fixed_binrec.dat"), itemConfId = "binrecPre", withBinaryRecord = binrecName)
        }
        val df = compo.run(Unit)
        val result = df.collect
        result.foreach { row =>
          row.getAs[String]("pre_data_div") + row.getAs[String]("pre_item1") + row.getAs[String]("pre_item2") mustBe
            new String(row.getAs[Array[Byte]](binrecName), "MS932").trim
        }

        val converted = df ~> BinaryRecordConverter(binrecName, "binrecPost", "MS932") _
        converted.collect.foreach { row =>
          row.getAs[String]("pre_data_div") mustBe row.getAs[String]("data_div")
          row.getAs[String]("pre_item1") mustBe row.getAs[String]("item1")
          row.getAs[String]("pre_item2") mustBe row.getAs[String]("item2")
        }
      }

      "JEF" in {
        val fileToDf = new FileToDf with Nothing {
          val componentId = "fixed_jef_sample_pre"
          val fileInputInfo = FixedInfo(Set("org.dat"), "JEF_SAMPLE", newLine = false, charSet = "JEF", withBinaryRecord = binrecName)
        }
        val df = fileToDf.run(Unit)

        val converted = df ~> BinaryRecordConverter(binrecName, "fixed_jef_sample_post", "JEF") _
        converted.collect.foreach { row =>
          (1 to 26).map(d => row.getAs[String](s"pre_item${d}") mustBe row.getAs[String](s"item${d}"))
        }
      }

      "JEF" when {
        "Parquet read and write" in {
          val fileToPq = new FileToPq with Nothing {
            val componentId = "fixed_jef_sample_pre"
            val fileInputInfo = FixedInfo(Set("org.dat"), "JEF_SAMPLE", newLine = false, charSet = "JEF", withBinaryRecord = binrecName)
          }

          val pqToDf = new PqToDf with Nothing {
            val componentId = "fixed_jef_sample_pre"
          }

          fileToPq.run(Unit)
          val df = pqToDf.run(Unit)
          val converted = df ~> BinaryRecordConverter(binrecName, "fixed_jef_sample_post", "JEF") _
          converted.collect.foreach { row =>
            (1 to 26).map(d => row.getAs[String](s"pre_item${d}") mustBe row.getAs[String](s"item${d}"))
          }
        }
      }

      "Executor pattern" in {
        val compo = new FileToDf with Nothing {
          val componentId = "fixed"
          override val fileInputInfo = FixedInfo(Set("fixed_binrec.dat"), itemConfId = "binrecPre", withBinaryRecord = binrecName)
        }
        val df = compo.run(Unit)
        val result = df.collect
        result.foreach { row =>
          row.getAs[String]("pre_data_div") + row.getAs[String]("pre_item1") + row.getAs[String]("pre_item2") mustBe
            new String(row.getAs[Array[Byte]](binrecName), "MS932").trim
        }

        val dfToDf = new DfToDf with BinaryRecordConverter {
          val binaryRecordName = binrecName
          val itemConfId = "binrecPost"
          val charEnc = "MS932"
        }

        val converted = df ~> dfToDf.run _
        converted.collect.foreach { row =>
          row.getAs[String]("pre_data_div") mustBe row.getAs[String]("data_div")
          row.getAs[String]("pre_item1") mustBe row.getAs[String]("item1")
          row.getAs[String]("pre_item2") mustBe row.getAs[String]("item2")
        }
      }
    }

    "normal end withIndex mode" when {
      val binrecName = "binrec"
      "MS932" in {
        val compo = new FileToDf with Nothing {
          val componentId = "fixed"
          override val fileInputInfo = FixedInfo(Set("fixed_binrec.dat"), itemConfId = "binrecPre", withBinaryRecord = binrecName,
            withIndex = true)
        }
        val df = compo.run(Unit)
        val result = df.collect
        result.foreach { row =>
          row.getAs[String]("pre_data_div") + row.getAs[String]("pre_item1") + row.getAs[String]("pre_item2") mustBe
            new String(row.getAs[Array[Byte]](binrecName), "MS932").trim
        }

        val converted = df ~> BinaryRecordConverter(binrecName, "binrecPost", "MS932") _
        converted.collect.foreach { row =>
          row.getAs[String]("pre_data_div") mustBe row.getAs[String]("data_div")
          row.getAs[String]("pre_item1") mustBe row.getAs[String]("item1")
          row.getAs[String]("pre_item2") mustBe row.getAs[String]("item2")
        }
      }

      "JEF" in {
        val fileToDf = new FileToDf with Nothing {
          val componentId = "fixed_jef_sample_pre"
          val fileInputInfo = FixedInfo(Set("org.dat"), "JEF_SAMPLE", newLine = false, charSet = "JEF", withBinaryRecord = binrecName,
            withIndex = true)
        }
        Try {
          fileToDf.run(Unit)
          fail
        }.failed.get.getMessage mustBe "JEF CharEnc is not supportted"
      }
    }

    "normal end recordLengthCheck on" when {
      val binrecName = "binrec"
      "MS932" in {
        val compo = new FileToDf with Nothing {
          val componentId = "fixed"
          override val fileInputInfo = FixedInfo(Set("fixed_binrec.dat"), itemConfId = "binrecPre", withBinaryRecord = binrecName,
            recordLengthCheck = true)
        }
        val df = compo.run(Unit)
        val result = df.collect
        result.foreach { row =>
          row.getAs[String]("pre_data_div") + row.getAs[String]("pre_item1") + row.getAs[String]("pre_item2") mustBe
            new String(row.getAs[Array[Byte]](binrecName), "MS932").trim
        }

        val converted = df ~> BinaryRecordConverter(binrecName, "binrecPost", "MS932") _
        converted.collect.foreach { row =>
          row.getAs[String]("pre_data_div") mustBe row.getAs[String]("data_div")
          row.getAs[String]("pre_item1") mustBe row.getAs[String]("item1")
          row.getAs[String]("pre_item2") mustBe row.getAs[String]("item2")
        }
      }

      "JEF throw Exception" in {
        val fileToDf = new FileToDf with Nothing {
          val componentId = "fixed_jef_sample_pre"
          val fileInputInfo = FixedInfo(Set("org.dat"), "JEF_SAMPLE", newLine = false, charSet = "JEF", withBinaryRecord = binrecName,
            recordLengthCheck = true)
        }
        Try {
          fileToDf.run(Unit)
          fail
        }.failed.get.getMessage mustBe "JEF CharEnc is not supportted"
      }
    }

    "normal end recordLengthCheck on and withIndex" when {
      val binrecName = "binrec"
      "MS932" in {
        val compo = new FileToDf with Nothing {
          val componentId = "fixed"
          override val fileInputInfo = FixedInfo(Set("fixed_binrec.dat"), itemConfId = "binrecPre", withBinaryRecord = binrecName,
            recordLengthCheck = true, withIndex = true)
        }
        val df = compo.run(Unit)
        val result = df.collect
        result.foreach { row =>
          row.getAs[String]("pre_data_div") + row.getAs[String]("pre_item1") + row.getAs[String]("pre_item2") mustBe
            new String(row.getAs[Array[Byte]](binrecName), "MS932").trim
        }

        val converted = df ~> BinaryRecordConverter(binrecName, "binrecPost", "MS932") _
        converted.collect.foreach { row =>
          row.getAs[String]("pre_data_div") mustBe row.getAs[String]("data_div")
          row.getAs[String]("pre_item1") mustBe row.getAs[String]("item1")
          row.getAs[String]("pre_item2") mustBe row.getAs[String]("item2")
        }
      }

      "JEF throw Exception" in {
        val fileToDf = new FileToDf with Nothing {
          val componentId = "fixed_jef_sample_pre"
          val fileInputInfo = FixedInfo(Set("org.dat"), "JEF_SAMPLE", newLine = false, charSet = "JEF", withBinaryRecord = binrecName,
            recordLengthCheck = true, withIndex = true)
        }
        Try {
          fileToDf.run(Unit)
          fail
        }.failed.get.getMessage mustBe "JEF CharEnc is not supportted"
      }
    }
  }

  "MCA0130071 test pattern" should {
    "be normal end" in {
      val binrecName = "binrec"
      val c1 = new FileToPq with PqCommonColumnRemover {
        val componentId = "fixed1"
        override val fileInputInfo = FixedInfo(Set("fixed_binrec.dat"), itemConfId = "binrecPreNoItem", withBinaryRecord = binrecName)
      }
      val c2 = new FileToPq with PqCommonColumnRemover {
        val componentId = "fixed2"
        override val fileInputInfo = FixedInfo(Set("fixed_binrec2.dat"), itemConfId = "binrecPreNoItem", withBinaryRecord = binrecName)
      }
      val c3 = new PqToDf with Nothing {
        val componentId = "fixed1"
      }
      val c4 = new PqToDf with Nothing {
        val componentId = "fixed2"
      }

      c1.run(Unit)
      c2.run(Unit)
      val df1 = c3.run(Unit)
      val df2 = c4.run(Unit)
      val df = df1.except(df2)

      val converted = df ~> BinaryRecordConverter(binrecName, "binrecPost", "MS932") _
      val row = converted.collect
      row.size mustBe 1
      row(0).getAs[String]("data_div") mustBe "D"
      row(0).getAs[String]("item1") mustBe "a3"
      row(0).getAs[String]("item2") mustBe "bb3"
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.fileConv

import org.apache.spark.sql.functions._
import d2k.common.InputArgs
import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import spark.common.PqCtl
import scala.io.Source
import d2k.common.TestArgs
import scala.reflect.io.Directory

class ConfParserTest extends WordSpec with MustMatchers with BeforeAndAfter {
  implicit val inputArgs = TestArgs().toInputArgs.copy(
    fileConvInputFile = "data/test/conv/conf/mdb_app.conf")

  "test Normal" should {
    val confs = ConfParser.parse("data/test/conv/conf/mdb_app.conf").toSeq
    val conf = confs.find { x => x.appConf.AppId == "APP012" }.get
    "appConf check" in {
      conf.appConf.AppName mustBe "固定長コンバータ"
      conf.appConf.destSystem mustBe "ALICE"
      conf.appConf.fileFormat mustBe "fixed"
      conf.appConf.newline mustBe true
      conf.appConf.header mustBe false
      conf.appConf.footer mustBe false
      conf.appConf.storeType mustBe "all"
      conf.appConf.inputFiles mustBe "convFixed0?.dat"
      conf.appConf.comment mustBe "備考"

      val tsvNewLine = confs.find { x => x.appConf.AppId == "APP002" }.get.appConf.newline
      tsvNewLine mustBe false

      val csvNewLine = confs.find { x => x.appConf.AppId == "APP005" }.get.appConf.newline
      tsvNewLine mustBe false

      val vsvNewLine = confs.find { x => x.appConf.AppId == "APP011" }.get.appConf.newline
      tsvNewLine mustBe false

      val csvStrictNewLine = confs.find { x => x.appConf.AppId == "APP008" }.get.appConf.newline
      tsvNewLine mustBe false

      val itemConfs = conf.itemConfs.toArray
      val itemConf1 = itemConfs(0)
      itemConf1.itemId mustBe "rcd_div"
      itemConf1.itemName mustBe "レコード区分"
      itemConf1.length mustBe "2"
      itemConf1.cnvType mustBe "文字列"
      itemConf1.extractTarget mustBe false
      itemConf1.comment mustBe "備考"

      val itemConf2 = itemConfs(1)
      itemConf2.itemId mustBe "nw_service"
      itemConf2.itemName mustBe "ＮＷサービス"
      itemConf2.length mustBe "2"
      itemConf2.cnvType mustBe "数字_PD"
      itemConf2.extractTarget mustBe false
      itemConf2.comment mustBe ""
    }
  }
  "read itemConf from resource" should {
    "app conf" in {
      val confs = ConfParser.parse("projectId_app.conf").toSeq
      confs.head.appConf.AppId mustBe "res"
      confs.head.appConf.inputFiles mustBe "res.dat"
    }

    "item Conf" in {
      val confs = ConfParser.parseItemConf(Directory("data/xxx"), "projectId", "res").toSeq

      {
        val r = confs(0)
        r.itemId mustBe "xxx1"
      }

      {
        val r = confs(1)
        r.itemId mustBe "xxx2"
      }

    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.fileConv

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter

class DomainProcessorJefTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "JEF 全角" should {
    "be normal end" in {
      val data = Array[Short](0xA3, 0xCA, 0x40, 0x40, 0xA3, 0xC5, 0x40, 0x40, 0xA3, 0xC6).map(_.toByte)
      DomainProcessor.execArrayByte("全角文字列", data, "JEF").right.get mustBe "Ｊ　Ｅ　Ｆ"
    }

    "全角 space" in {
      val data = Array[Short](0x40, 0x40).map(_.toByte)
      DomainProcessor.execArrayByte("全角文字列", data, "JEF").right.get mustBe ""
    }

    "全角 space trim無し" in {
      val data = Array[Short](0x40, 0x40).map(_.toByte)
      DomainProcessor.execArrayByte("全角文字列_trim_無し", data, "JEF").right.get mustBe "　"
    }

    "全角 trim全角" in {
      val data = Array[Short](0x40, 0x40).map(_.toByte)
      DomainProcessor.execArrayByte("全角文字列_trim_全角", data, "JEF").right.get mustBe ""
    }

    "null文字" in {
      val data = Array[Short](0x00, 0x00).map(_.toByte)
      DomainProcessor.execArrayByte("全角文字列", data, "JEF").right.get mustBe "■"
    }
  }

  "JEF 半角" should {
    "年月日" in {
      val domain = "年月日"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0xf0, 0xf1, 0xf0, 0xf2).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "20160102"
    }
    "年月日_SL" in {
      val domain = "年月日_SL"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0x61, 0xf0, 0xf1, 0x61, 0xf0, 0xf2).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "20160102"
    }
    "年月日_HY" in {
      val domain = "年月日_HY"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0x60, 0xf0, 0xf1, 0x60, 0xf0, 0xf2).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "20160102"
    }
    "年月" in {
      val domain = "年月"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0xf0, 0xf2).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "201602"
    }
    "年月_SL" in {
      val domain = "年月_SL"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0x61, 0xf0, 0xf2).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "201602"
    }
    "月日" in {
      val domain = "月日"
      val data = Array[Short](0xf0, 0xf1, 0xf0, 0xf2).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "0102"
    }
    "年" in {
      val domain = "年"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "2016"
    }
    "月" in {
      val domain = "月"
      val data = Array[Short](0xf0, 0xf1).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "01"
    }
    "日" in {
      val domain = "日"
      val data = Array[Short](0xf0, 0xf2).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "02"
    }
    "年月日時分秒" in {
      val domain = "年月日時分秒"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0xf0, 0xf1, 0xf0, 0xf2, 0xf0, 0xf1, 0xf2, 0xf3, 0xf4, 0xf5).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "20160102012345"
    }
    "年月日時分秒_HC" in {
      val domain = "年月日時分秒_HC"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0x60, 0xf0, 0xf1, 0x60, 0xf0, 0xf2, 0x40, 0xf0, 0xf1, 0x7a, 0xf2, 0xf3, 0x7a, 0xf4, 0xf5).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "20160102012345"
    }
    "年月日時分秒_SC" in {
      val domain = "年月日時分秒_SC"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0x61, 0xf0, 0xf1, 0x61, 0xf0, 0xf2, 0x40, 0xf0, 0xf1, 0x7a, 0xf2, 0xf3, 0x7a, 0xf4, 0xf5).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "20160102012345"
    }
    "年月日時分秒_CO" in {
      val domain = "年月日時分秒_CO"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0x7a, 0xf0, 0xf1, 0x7a, 0xf0, 0xf2, 0x40, 0xf0, 0xf1, 0x7a, 0xf2, 0xf3, 0x7a, 0xf4, 0xf5).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "20160102012345"
    }
    "年月日時分ミリ秒" in {
      val domain = "年月日時分ミリ秒"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0xf0, 0xf1, 0xf0, 0xf2, 0xf0, 0xf1, 0xf2, 0xf3, 0xf4, 0xf5, 0xf6, 0xf7, 0xf8).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "20160102012345678"
    }
    "年月日時分ミリ秒/ミリ秒小数点付加" in {
      val domain = "年月日時分ミリ秒/ミリ秒小数点付加"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0xf0, 0xf1, 0xf0, 0xf2, 0xf0, 0xf1, 0xf2, 0xf3, 0xf4, 0xf5, 0xf6, 0xf7, 0xf8).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "20160102012345.678"
    }
    "年月日時" in {
      val domain = "年月日時"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0xf0, 0xf1, 0xf0, 0xf2, 0xf0, 0xf1).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "2016010201"
    }
    "年月日時分" in {
      val domain = "年月日時分"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0xf0, 0xf1, 0xf0, 0xf2, 0xf0, 0xf1, 0xf2, 0xf3).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "201601020123"
    }
    "年月日時分_SC" in {
      val domain = "年月日時分_SC"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0x61, 0xf0, 0xf1, 0x61, 0xf0, 0xf2, 0x40, 0xf0, 0xf1, 0x7a, 0xf2, 0xf3).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "201601020123"
    }
    "時分秒" in {
      val domain = "時分秒"
      val data = Array[Short](0xf0, 0xf1, 0xf2, 0xf3, 0xf4, 0xf5).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "012345"
    }
    "時分秒_CO" in {
      val domain = "時分秒_CO"
      val data = Array[Short](0xf0, 0xf1, 0x7a, 0xf2, 0xf3, 0x7a, 0xf4, 0xf5).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "012345"
    }
    "時分ミリ秒" in {
      val domain = "時分ミリ秒"
      val data = Array[Short](0xf0, 0xf1, 0xf2, 0xf3, 0xf4, 0xf5, 0xf6, 0xf7, 0xf8).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "012345678"
    }
    "時分ミリ秒/ミリ秒小数点付加" in {
      val domain = "時分ミリ秒/ミリ秒小数点付加"
      val data = Array[Short](0xf0, 0xf1, 0xf2, 0xf3, 0xf4, 0xf5, 0xf6, 0xf7, 0xf8).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "012345.678"
    }
    "時分" in {
      val domain = "時分"
      val data = Array[Short](0xf0, 0xf1, 0xf2, 0xf3).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "0123"
    }
    "時" in {
      val domain = "時"
      val data = Array[Short](0xf0, 0xf1).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "01"
    }
    "分" in {
      val domain = "分"
      val data = Array[Short](0xf0, 0xf2).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "02"
    }
    "秒" in {
      val domain = "秒"
      val data = Array[Short](0xf0, 0xf3).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "03"
    }
    "時間" in {
      val domain = "時間"
      val data = Array[Short](0xf1, 0xf2, 0xf3, 0xf4, 0xf5, 0xf6).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "123456"
    }
    "数字" in {
      val domain = "数字"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0xf0, 0xf1, 0xf0, 0xf1).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "20160101"
    }
    "数字_SIGNED" in {
      val domain = "数字_SIGNED"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0xf0, 0xf1, 0xf0, 0xf1).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "20160101"
    }
    "文字列" in {
      val domain = "文字列"
      val data = Array[Short](0xd1, 0xc5, 0xc6).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "JEF"
    }
    "文字列_trim_無し" in {
      val domain = "文字列_trim_無し"
      val data = Array[Short](0x40, 0xd1, 0x40, 0xc5, 0x40, 0xc6, 0x40).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe " J E F "
    }
    "文字列_trim_半角" in {
      val domain = "文字列_trim_半角"
      val data = Array[Short](0x40, 0x8d, 0xbe, 0x40, 0x51, 0x40, 0x9f, 0x40).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "ｼﾞ ｪ ﾌ"
    }
    "レコード区分_NUMBER" in {
      val domain = "レコード区分_NUMBER"
      val data = Array[Short](0xf1).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "H"
    }
    "レコード区分_ALPHABET" in {
      val domain = "レコード区分_ALPHABET"
      val data = Array[Short](0xf2, 0xf0, 0xf1, 0xf6, 0xf0, 0xf1, 0xf0, 0xf1).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "20160101"
    }
    "通信方式" in {
      val domain = "通信方式"
      val data = Array[Short](0xc1, 0xc2).map(_.toByte)
      DomainProcessor.execArrayByte(domain, data, "JEF").right.get mustBe "A"
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.fileConv

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter

class DomainProcessorTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "format test" should {
    "YYYYMMDD" in {
      val domain_name = "年月日"
      val empty = "        "
      val invalid = "20151232"
      val max = "99991231"
      val max_over = "99991232"
      val min = "00010101"
      val min_under = "00010100"
      val not_digit = "0001010 "
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid)
    }

    "YYYYMMDD_SLASH" in {
      val domain_name = "年月日_SL"
      val empty = "          "
      val invalid = "2015/12/32"
      val max = "9999/12/31"
      val max_over = "9999/12/32"
      val min = "0001/01/01"
      val min_under = "0001/01/00"
      val not_digit = "0001/01/0 "
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid)
    }

    "YYYYMMDD_Hyphen" in {
      val domain_name = "年月日_HY"
      val empty = "          "
      val invalid = "2015-12-32"
      val max = "9999-12-31"
      val max_over = "9999-12-32"
      val min = "0001-01-01"
      val min_under = "0001-01-00"
      val not_digit = "0001-01-0 "
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid)
    }

    "YYYYMM" in {
      val domain_name = "年月"
      val empty = "      "
      val invalid = "201513"
      val max = "999912"
      val max_over = "999913"
      val min = "000101"
      val min_under = "000100"
      val not_digit = "00010 "
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid)
    }

    "MMDD" in {
      val domain_name = "月日"
      val empty = "    "
      val invalid = "1032"
      val max = "1231"
      val max_over = "1232"
      val min = "0101"
      val min_under = "0100"
      val not_digit = "010 "
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid)
    }

    "YYYYMMDDHHMMSS" in {
      val domain_name = "年月日時分秒"
      val empty = "              "
      val invalid = "20151231235960"
      val max = "99991231235959"
      val max_over = "99991231235960"
      val min = "00010101000000"
      val min_under = "00010100000000"
      val not_digit = "0001010000000 "
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid)
    }

    "年月日時分秒_HC" in {
      val domain_name = "年月日時分秒_HC"
      execRight(domain_name, "2016-06-01 12:34:56") mustBe "20160601123456"

      execRight(domain_name, "                   ") mustBe "00010101000000"
      execRight(domain_name, "") mustBe "00010101000000"
      execRight(domain_name, "2016-6-1 12:34:56") mustBe "00010101000000"
    }

    "年月日時分秒_SC" in {
      val domain_name = "年月日時分秒_SC"
      execRight(domain_name, "2016/06/01 12:34:56") mustBe "20160601123456"

      execRight(domain_name, "                   ") mustBe "00010101000000"
      execRight(domain_name, "") mustBe "00010101000000"
      execRight(domain_name, "2016/6/1 12:34:56") mustBe "00010101000000"
    }

    "YYYYMMDDHHMMSSMS" in {
      val domain_name = "年月日時分ミリ秒"
      val empty = "                 "
      val invalid = "20151231235960999"
      val max = "99991231235959999"
      val max_over = "99991231235960999"
      val min = "00010101000000000"
      val min_under = "00010101000000000"
      val not_digit = "0001010100000000 "
      val max_exp = "99991231235959999"
      val min_exp = "00010101000000000"
      timestampTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid, max_exp, min_exp)
    }

    "年月日時分ミリ秒/ミリ秒小数点付加" in {
      val domain_name = "年月日時分ミリ秒/ミリ秒小数点付加"
      execRight(domain_name, "20160601123456123") mustBe "20160601123456.123"

      execRight(domain_name, "                   ") mustBe "00010101000000.000"
      execRight(domain_name, "") mustBe "00010101000000.000"
      execRight(domain_name, "2016/6/1 12:34:56") mustBe "00010101000000.000"
    }

    "YYYYMMDDHHMMSSMS_len15" in {
      val domain_name = "年月日時分ミリ秒"
      val empty = "               "
      val invalid = "201512312359609"
      val max = "999912312359599"
      val max_over = "999912312359609"
      val min = "000101010000000"
      val min_under = "000101010000000"
      val not_digit = "00010101000000 "
      val max_exp = "99991231235959900"
      val min_exp = "00010101000000000"
      timestampTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid, max_exp, min_exp)
    }

    "年月日時分ミリ秒/ミリ秒小数点付加 len15" in {
      val domain_name = "年月日時分ミリ秒/ミリ秒小数点付加"
      execRight(domain_name, "201606011234561") mustBe "20160601123456.100"

      execRight(domain_name, "                 ") mustBe "00010101000000.000"
      execRight(domain_name, "") mustBe "00010101000000.000"
      execRight(domain_name, "2016/6/1 12:34:56") mustBe "00010101000000.000"
    }

    "YYYYMMDDHHMMSSMS_len16" in {
      val domain_name = "年月日時分ミリ秒"
      val empty = "                "
      val invalid = "2015123123596099"
      val max = "9999123123595999"
      val max_over = "9999123123596099"
      val min = "0001010100000000"
      val min_under = "0001010100000000"
      val not_digit = "000101010000000 "
      val max_exp = "99991231235959990"
      val min_exp = "00010101000000000"
      timestampTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid, max_exp, min_exp)
    }

    "年月日時分ミリ秒/ミリ秒小数点付加 len16" in {
      val domain_name = "年月日時分ミリ秒/ミリ秒小数点付加"
      execRight(domain_name, "2016060112345612") mustBe "20160601123456.120"

      execRight(domain_name, "                  ") mustBe "00010101000000.000"
      execRight(domain_name, "") mustBe "00010101000000.000"
      execRight(domain_name, "2016/6/1 12:34:56") mustBe "00010101000000.000"
    }

    "YYYYMMDDHH" in {
      val domain_name = "年月日時"
      val empty = "          "
      val invalid = "2015123124"
      val max = "9999123123"
      val max_over = "9999123124"
      val min = "0001010100"
      val min_under = "0001010000"
      val not_digit = "000101000 "
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid)
    }

    "HHMMSS" in {
      val domain_name = "時分秒"
      val empty = "      "
      val invalid = "225960"
      val max = "235959"
      val max_over = "235960"
      val min = "000000"
      val min_under = null
      val not_digit = "00000 "
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid)
    }

    "HHMMSS_COLON" in {
      val domain_name = "時分秒_CO"
      val empty = "      "
      val invalid = "22:59:60"
      val max = "23:59:59"
      val max_over = "23:59:60"
      val min = "00:00:00"
      val min_under = null
      val not_digit = "00:00:0 "
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid)
    }

    "HHMMSSMS" in {
      val domain_name = "時分ミリ秒"
      val empty = "         "
      val invalid = "225960999"
      val max = "235959990"
      val max_over = "235960999"
      val min = "000000000"
      val min_under = null
      val not_digit = "00000000 "
      val max_exp = "235959990"
      val min_exp = "000000000"
      timestampTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid, max_exp, min_exp)
    }

    "時分ミリ秒/ミリ秒小数点付加" in {
      val domain_name = "時分ミリ秒/ミリ秒小数点付加"
      execRight(domain_name, "123456123") mustBe "123456.123"

      execRight(domain_name, "                   ") mustBe "000000.000"
      execRight(domain_name, "") mustBe "000000.000"
      execRight(domain_name, "2016/6/1 12:34:56") mustBe "000000.000"
    }

    "HHMMSSMS_len8" in {
      val domain_name = "時分ミリ秒"
      val empty = "        "
      val invalid = "22596099"
      val max = "23595990"
      val max_over = "23596099"
      val min = "00000000"
      val min_under = null
      val not_digit = "0000000 "
      val max_exp = "235959900"
      val min_exp = "000000000"
      timestampTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid, max_exp, min_exp)
    }

    "時分ミリ秒/ミリ秒小数点付加　len8" in {
      val domain_name = "時分ミリ秒/ミリ秒小数点付加"
      execRight(domain_name, "1234561") mustBe "123456.100"

      execRight(domain_name, "                 ") mustBe "000000.000"
      execRight(domain_name, "") mustBe "000000.000"
      execRight(domain_name, "2016/6/1 12:34:56") mustBe "000000.000"
    }

    "HHMMSSMS_len7" in {
      val domain_name = "時分ミリ秒"
      val empty = "       "
      val invalid = "2259609"
      val max = "2359599"
      val max_over = "2359609"
      val min = "0000000"
      val min_under = null
      val not_digit = "000000 "
      val max_exp = "235959900"
      val min_exp = "000000000"
      timestampTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid, max_exp, min_exp)
    }

    "時分ミリ秒/ミリ秒小数点付加　len7" in {
      val domain_name = "時分ミリ秒/ミリ秒小数点付加"
      execRight(domain_name, "12345612") mustBe "123456.120"

      execRight(domain_name, "                  ") mustBe "000000.000"
      execRight(domain_name, "") mustBe "000000.000"
      execRight(domain_name, "2016/6/1 12:34:56") mustBe "000000.000"
    }

    "HHMM" in {
      val domain_name = "時分"
      val empty = "    "
      val invalid = "2260"
      val max = "2359"
      val max_over = "2400"
      val min = "0000"
      val min_under = null
      val not_digit = "000 "
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid)
    }

    "YYYY" in {
      val domain_name = "年"
      val empty = "     "
      val max = "9999"
      val max_over = "10000"
      val min = "0001"
      val min_under = "0000"
      val not_digit = "000 "
      val invalid = null
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid)
    }

    "MM" in {
      val domain_name = "月"
      val empty = "  "
      val max = "12"
      val max_over = "13"
      val min = "01"
      val min_under = "00"
      val not_digit = "0 "
      val invalid = null
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid)
    }

    "DD" in {
      val domain_name = "日"
      val empty = "  "
      val max = "31"
      val max_over = "32"
      val min = "01"
      val min_under = "00"
      val not_digit = "0 "
      val invalid = null
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid)
    }

    "HH" in {
      val domain_name = "時"
      val empty = "  "
      val max = "23"
      val max_over = "24"
      val min = "00"
      val min_under = null
      val not_digit = "0 "
      val invalid = null
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid)
    }

    "MI" in {
      val domain_name = "分"
      val empty = "  "
      val max = "59"
      val max_over = "60"
      val min = "00"
      val min_under = null
      val not_digit = "0 "
      val invalid = null
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid)
    }

    "SS" in {
      val domain_name = "秒"
      val empty = "  "
      val max = "59"
      val max_over = "60"
      val min = "00"
      val min_under = null
      val not_digit = "0 "
      val invalid = null
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid)
    }

    "TIME" in {
      val domain_name = "時間"
      val empty = "      "
      val invalid = "985960"
      val max = "995959"
      val max_over = "995960"
      val min = "000000"
      val min_under = null
      val not_digit = "00000 "
      dateTest(domain_name, empty, max, max_over, min, min_under, not_digit, invalid, DomainProcessor.ERR_MSG_INVALID_VALUE)
    }

    "success 年月_SL convert" in {
      val domain_name = "年月_SL"
      execRight(domain_name, "2016/01") mustBe "201601"
      execRight(domain_name, "0000/00") mustBe "000101"
      execRight(domain_name, "9999/99") mustBe "999912"
    }

    "success 年月日時分 convert" in {
      val domain_name = "年月日時分"
      execRight(domain_name, "201601020304") mustBe "201601020304"
      execRight(domain_name, "000000000000") mustBe "000101010000"
      execRight(domain_name, "999999999999") mustBe "999912312359"
    }

    "success 年月日時分_SC convert" in {
      val domain_name = "年月日時分_SC"
      execRight(domain_name, "2016/01/02 03:04") mustBe "201601020304"
      execRight(domain_name, "0000/00/00 00:00") mustBe "000101010000"
      execRight(domain_name, "9999/99/99 99:99") mustBe "999912312359"
    }

    "success 年月日時分秒_CO convert" in {
      val domain_name = "年月日時分秒_CO"
      execRight(domain_name, "20160102 03:04:05") mustBe "20160102030405"
      execRight(domain_name, "00000000 00:00:00") mustBe "00010101000000"
      execRight(domain_name, "99999999 99:99:99") mustBe "99991231235959"
    }

    "PD_DATE" in {
      val domain_name = "年月日_PD"
      execRight2(domain_name, Array(0x00, 0x10, 0x00, 0x10, 0x1c)) mustBe "01000101"
    }

    "PD_DATE2" in {
      val domain_name = "年月日_PD"
      execRight2(domain_name, Array(0x01, 0x00, 0x70, 0x10, 0x1c)) mustBe "10070101"
      execRight2(domain_name, Array(0x01, 0x00, 0x80, 0x10, 0x1c)) mustBe "10080101"

      val null_exp = execRight2(domain_name, Array(0x00, 0x00))
      null_exp mustBe "00010101"
    }

    "PD" in {
      val domain_name = "数字_PD"
      val nullStr = new String(Array[Byte](0x00, 0x00), "MS932")
      val null_exp = execRight2(domain_name, Array(0x00, 0x00))
      println(null_exp)
      null_exp mustBe nullStr

      val minus = execRight2(domain_name, Array(0x00, 0x01, 0x20, 0x00, 0x3d))
      minus mustBe "-120003"

      val minus2 = execRight2(domain_name, Array(0x00, 0x01, 0x20, 0x0d))
      minus2 mustBe "-1200"

      val plus = execRight2(domain_name, Array(0x21, 0x3c))
      plus mustBe "213"

      val empty = execLeft2(domain_name, Array(0x20, 0x20))
      empty mustBe DomainProcessor.ERR_MSG_INVALID_VALUE

      val invalidSign = execLeft2(domain_name, Array(0x20, 0x00))
      invalidSign mustBe DomainProcessor.ERR_MSG_INVALID_VALUE

      val notDigit = execLeft2(domain_name, Array(0x0f, 0x0d))
      notDigit mustBe DomainProcessor.ERR_MSG_INVALID_VALUE
    }

    "文字列_PD" in {
      val domain_name = "文字列_PD"
      execRight2(domain_name, Array(0x21, 0x3c)) mustBe "213"
      execRight2(domain_name, Array(0x02, 0x3c)) mustBe "023"
      execRight2(domain_name, Array(0x00, 0x21, 0x3c)) mustBe "00213"
      execRight2(domain_name, Array(0x01, 0x20, 0x0d)) mustBe "-01200"
      execRight2(domain_name, Array(0x00, 0x01, 0x20, 0x00, 0x3d)) mustBe "-000120003"
    }

    "識別子_PD" in {
      val domain_name = "識別子_PD"
      execRight2(domain_name, Array(0x21, 0x3c)) mustBe "213"
      execRight2(domain_name, Array(0x02, 0x3c)) mustBe "023"
      execRight2(domain_name, Array(0x00, 0x21, 0x3c)) mustBe "00213"
      execRight2(domain_name, Array(0x01, 0x20, 0x0d)) mustBe "01200"
      execRight2(domain_name, Array(0x00, 0x01, 0x20, 0x00, 0x3d)) mustBe "000120003"
    }

    "ZD" in {
      val domain_name = "数字_ZD"
      val nullStr = new String(Array[Byte](0x00, 0x00), "MS932")
      val null_exp = execRight2(domain_name, Array(0x00, 0x00))
      null_exp mustBe nullStr

      val minus = execRight2(domain_name, Array(0xf1, 0xf2, 0xf0, 0xf0, 0xf0, 0xd3))
      minus mustBe "-120003"

      val plus = execRight2(domain_name, Array(0xf2, 0xf1, 0xf3))
      plus mustBe "213"

      val plus2 = execRight2(domain_name, Array(0xf2, 0xf1, 0xc3))
      plus2 mustBe "213"

      val empty = execLeft2(domain_name, Array(0x40, 0x40))
      empty mustBe DomainProcessor.ERR_MSG_INVALID_VALUE

      val invalidSign = execLeft2(domain_name, Array(0x40, 0x00))
      invalidSign mustBe DomainProcessor.ERR_MSG_INVALID_VALUE

      val notDigit = execLeft2(domain_name, Array(0x0f, 0x0d))
      notDigit mustBe DomainProcessor.ERR_MSG_INVALID_VALUE
    }

    "文字列_ZD" in {
      val domain_name = "文字列_ZD"
      execRight2(domain_name, Array(0xf2, 0xf1, 0xc3)) mustBe "213"
      execRight2(domain_name, Array(0xf0, 0xf2, 0xf1, 0xc3)) mustBe "0213"
      execRight2(domain_name, Array(0xf0, 0xf0, 0xf2, 0xf1, 0xc3)) mustBe "00213"
      execRight2(domain_name, Array(0xf1, 0xf2, 0xf0, 0xf0, 0xf0, 0xd3)) mustBe "-120003"
      execRight2(domain_name, Array(0xf0, 0xf1, 0xf2, 0xf0, 0xf0, 0xf0, 0xd3)) mustBe "-0120003"
      execRight2(domain_name, Array(0xf0, 0xf0, 0xf1, 0xf2, 0xf0, 0xf0, 0xf0, 0xd3)) mustBe "-00120003"
    }

    "識別子_ZD" in {
      val domain_name = "識別子_ZD"
      execRight2(domain_name, Array(0xf2, 0xf1, 0xc3)) mustBe "213"
      execRight2(domain_name, Array(0xf0, 0xf2, 0xf1, 0xc3)) mustBe "0213"
      execRight2(domain_name, Array(0xf0, 0xf0, 0xf2, 0xf1, 0xc3)) mustBe "00213"
      execRight2(domain_name, Array(0xf1, 0xf2, 0xf0, 0xf0, 0xf0, 0xd3)) mustBe "120003"
      execRight2(domain_name, Array(0xf0, 0xf1, 0xf2, 0xf0, 0xf0, 0xf0, 0xd3)) mustBe "0120003"
      execRight2(domain_name, Array(0xf0, 0xf0, 0xf1, 0xf2, 0xf0, 0xf0, 0xf0, 0xd3)) mustBe "00120003"
    }

    "Digit" in {
      val domain_name = "数字"

      val empty = execLeft2(domain_name, Array(0x20, 0x20))
      empty mustBe DomainProcessor.ERR_MSG_INVALID_VALUE

      val notDigit = execLeft(domain_name, "AA")
      notDigit mustBe DomainProcessor.ERR_MSG_INVALID_VALUE

      val nullStr = new String(Array[Byte](0x00, 0x00), "MS932")
      val null_exp = execRight(domain_name, nullStr)
      null_exp mustBe nullStr

    }

    "数字_SIGNED" in {
      val domain_name = "数字_SIGNED"
      execRight(domain_name, "+0") mustBe "0"
      execRight(domain_name, "-0") mustBe "0"
      execRight(domain_name, "+10000") mustBe "10000"
      execRight(domain_name, "-10000") mustBe "-10000"
      execRight(domain_name, "+00001") mustBe "1"
      execRight(domain_name, "-00001") mustBe "-1"
      execRight(domain_name, " 0") mustBe "0"
      execRight(domain_name, "-A") mustBe "0"
      execRight(domain_name, "  ") mustBe "0"
    }

    "文字列" in {
      val domain_name = "文字列"
      execRight(domain_name, "AB") mustBe "AB"
      execRight(domain_name, "AB  ") mustBe "AB"
      execRight(domain_name, "  AB") mustBe "AB"
      execRight(domain_name, "  AB  ") mustBe "AB"
      execRight(domain_name, "20151232") mustBe "20151232"

      //半角
      execRight(domain_name, "AB  ") mustBe "AB"
      execRight(domain_name, "  AB") mustBe "AB"
      execRight(domain_name, "  AB  ") mustBe "AB"
      //全角
      execRight(domain_name, "AB　　") mustBe "AB　　"
      execRight(domain_name, "　　AB") mustBe "　　AB"
      execRight(domain_name, "　　AB　　") mustBe "　　AB　　"
      //全半角混在
      execRight(domain_name, "　 　 AB　　") mustBe "　 　 AB　　"
      execRight(domain_name, "　 　 AB") mustBe "　 　 AB"
      execRight(domain_name, " 　 　 AB　 　 ") mustBe "　 　 AB　 　"
      execRight(domain_name, "20151232") mustBe "20151232"
    }

    "文字列_trim_無し" in {
      val domain_name = "文字列_trim_無し"
      execRight(domain_name, "AB") mustBe "AB"

      //半角
      execRight(domain_name, "AB  ") mustBe "AB  "
      execRight(domain_name, "  AB") mustBe "  AB"
      execRight(domain_name, "  AB  ") mustBe "  AB  "
      //全角
      execRight(domain_name, "AB　　") mustBe "AB　　"
      execRight(domain_name, "　　AB") mustBe "　　AB"
      execRight(domain_name, "　　AB　　") mustBe "　　AB　　"
      //全半角混在
      execRight(domain_name, "　 　 AB　　") mustBe "　 　 AB　　"
      execRight(domain_name, "　 　 AB") mustBe "　 　 AB"
      execRight(domain_name, " 　 　 AB　 　 ") mustBe " 　 　 AB　 　 "
      execRight(domain_name, "20151232") mustBe "20151232"
    }

    "文字列_trim_半角" in {
      val domain_name = "文字列_trim_半角"
      execRight(domain_name, "AB") mustBe "AB"
      execRight(domain_name, "AB  ") mustBe "AB"
      execRight(domain_name, "  AB") mustBe "AB"
      execRight(domain_name, "  AB  ") mustBe "AB"
      execRight(domain_name, "20151232") mustBe "20151232"

      //半角
      execRight(domain_name, "AB  ") mustBe "AB"
      execRight(domain_name, "  AB") mustBe "AB"
      execRight(domain_name, "  AB  ") mustBe "AB"
      //全角
      execRight(domain_name, "AB　　") mustBe "AB　　"
      execRight(domain_name, "　　AB") mustBe "　　AB"
      execRight(domain_name, "　　AB　　") mustBe "　　AB　　"
      //全半角混在
      execRight(domain_name, "　 　 AB　　") mustBe "　 　 AB　　"
      execRight(domain_name, "　 　 AB") mustBe "　 　 AB"
      execRight(domain_name, " 　 　 AB　 　 ") mustBe "　 　 AB　 　"
      execRight(domain_name, "20151232") mustBe "20151232"
    }

    "文字列_trim_全角" in {
      val domain_name = "文字列_trim_全角"
      execRight(domain_name, "AB") mustBe "AB"
      execRight(domain_name, "AB　　") mustBe "AB"
      execRight(domain_name, "　　AB") mustBe "AB"
      execRight(domain_name, "　　AB　　") mustBe "AB"
      execRight(domain_name, "20151232") mustBe "20151232"

      //半角
      execRight(domain_name, "AB  ") mustBe "AB  "
      execRight(domain_name, "  AB") mustBe "  AB"
      execRight(domain_name, "  AB  ") mustBe "  AB  "
      //全角
      execRight(domain_name, "AB　　") mustBe "AB"
      execRight(domain_name, "　　AB") mustBe "AB"
      execRight(domain_name, "　　AB　　") mustBe "AB"
      //全半角混在
      execRight(domain_name, "　 　 AB　　") mustBe " 　 AB"
      execRight(domain_name, "　 　 AB") mustBe " 　 AB"
      execRight(domain_name, " 　 　 AB　 　 ") mustBe " 　 　 AB　 　 "
      execRight(domain_name, "20151232") mustBe "20151232"
    }

    "文字列_trim_全半角" in {
      val domain_name = "文字列_trim_全半角"
      //半角
      execRight(domain_name, "AB  ") mustBe "AB"
      execRight(domain_name, "  AB") mustBe "AB"
      execRight(domain_name, "  AB  ") mustBe "AB"
      //全角
      execRight(domain_name, "AB　　") mustBe "AB"
      execRight(domain_name, "　　AB") mustBe "AB"
      execRight(domain_name, "　　AB　　") mustBe "AB"
      //全半角混在
      execRight(domain_name, "　 　 AB　　") mustBe "AB"
      execRight(domain_name, "　 　 AB") mustBe "AB"
      execRight(domain_name, " 　 　 AB　 　 ") mustBe "AB"
      execRight(domain_name, "20151232") mustBe "20151232"
    }

    "全角文字列" in {
      val domain_name = "全角文字列"
      execRight(domain_name, "あいう　　") mustBe "あいう"
      execRight(domain_name, "　　あいう") mustBe "あいう"
      execRight(domain_name, "　　あいう　　") mustBe "あいう"
    }

    "全角文字列_trim_無し" in {
      val domain_name = "全角文字列_trim_無し"
      //全角
      execRight(domain_name, "あいう　　") mustBe "あいう　　"
      execRight(domain_name, "　　あいう") mustBe "　　あいう"
      execRight(domain_name, "　　あいう　　") mustBe "　　あいう　　"
    }

    "全角文字列_trim_全角" in {
      val domain_name = "全角文字列_trim_全角"
      execRight(domain_name, "あいう　　") mustBe "あいう"
      execRight(domain_name, "　　あいう") mustBe "あいう"
      execRight(domain_name, "　　あいう　　") mustBe "あいう"
    }

    "DataDiv_ALPHABET" in {
      val domain_name = "レコード区分_ALPHABET"

      val head = execRight(domain_name, "H")
      head mustBe "H"

      val data = execRight(domain_name, "D")
      data mustBe "D"

      val foot = execRight(domain_name, "T")
      foot mustBe "T"

    }
    "DataDiv_NUMBER" in {
      val domain_name = "レコード区分_NUMBER"

      val head = execRight(domain_name, "1")
      head mustBe "H"

      val data = execRight(domain_name, "2")
      data mustBe "D"

      val foot = execRight(domain_name, "3")
      foot mustBe "T"

      try {
        execRight(domain_name, "4")
        fail
      } catch {
        case t: RuntimeException => t.getMessage mustBe s"${DomainProcessor.ERR_MSG_INVALID_DATA_DIV}:4"
        case t: Throwable        => fail
      }

    }

    "Communication Method NULL" in {
      val domain_name = "通信方式"

      val result = execRight(domain_name, "")
      result mustBe ""

    }

    "Communication Method Right Space" in {
      val domain_name = "通信方式"

      val result = execRight(domain_name, "ABC    ")
      result mustBe "A"

    }

    "Communication Method Left Space" in {
      val domain_name = "通信方式"

      val result = execRight(domain_name, "   ABC")
      result mustBe "A"

    }

    "InvalidDomain" in {
      val domain_name = "HOGE"
      try {
        execRight(domain_name, "hoge")
        fail
      } catch {
        case t: RuntimeException => t.getMessage mustBe s"${DomainProcessor.ERR_MSG_INVALID_DOMAIN}:${domain_name}"
        case t: Throwable        => fail
      }
    }

    /*
   * 日付系 共通
   */
    def dateTest(domain_name: String, empty: String, max: String, max_over: String, min: String, min_under: String, not_digit: String, invalid: String, INVALID_DATE_MSG: String = null) {
      val max_exp = max.replaceAll("[-:/]", "")
      val min_exp = min.replaceAll("[-:/]", "")

      val actual_empty = execRight(domain_name, empty)
      actual_empty mustBe min_exp

      val nullString = new String(Array[Byte](0x00))
      val actual_null = execRight(domain_name, empty.replaceAll(" ", nullString))
      actual_null mustBe min_exp

      val actual_max = execRight(domain_name, max)
      actual_max mustBe max_exp

      val actual_min = execRight(domain_name, min)
      actual_min mustBe min_exp

      if (min_under != null) {
        val actual_min_under = execRight(domain_name, min_under)
        actual_min_under mustBe min_exp
      }
    }

    "Byte配列" should {
      "ISO-8859-1の文字列として格納されている" in {
        val domainName = "Byte配列"
        execRight(domainName, "あいう").getBytes("ISO-8859-1") mustBe "あいう".getBytes("ISO-8859-1")
        execRight2(domainName, Array(0x01, 0x02, 0x03)).getBytes("ISO-8859-1") mustBe Array(0x01, 0x02, 0x03)
      }
    }

    /*
   * タイムスタンプ
   */
    def timestampTest(domain_name: String, empty: String, max: String, max_over: String, min: String, min_under: String, not_digit: String, invalid: String, max_exp: String, min_exp: String) {

      val actual_empty = execRight(domain_name, empty)
      actual_empty mustBe min_exp

      val nullString = new String(Array[Byte](0x00))
      val actual_null = execRight(domain_name, empty.replaceAll(" ", nullString))
      actual_null mustBe min_exp

      val actual_max = execRight(domain_name, max)
      actual_max mustBe max_exp

      val actual_min = execRight(domain_name, min)
      actual_min mustBe min_exp

      if (min_under != null) {
        val actual_min_under = execRight(domain_name, min_under)
        actual_min_under mustBe min_exp
      }

    }

    def execLeft(domain_name: String, data: String) =
      checkLeft(DomainProcessor.exec(domain_name, data))

    def execLeft2(domain_name: String, data: Array[Int], charEnc: String = "MS932") =
      checkLeft(DomainProcessor.execArrayByte(domain_name, data.map(_.toByte), charEnc))

    def checkLeft(result: Either[String, String]) =
      if (result.isLeft) {
        result.left.get
      } else {
        "UNEXPECTED RIGHT VALUE:" + result.right.get
      }

    def execRight(domain_name: String, data: String) =
      checkRight(DomainProcessor.exec(domain_name, data))

    def execRight2(domain_name: String, data: Array[Int], charEnc: String = "MS932") =
      checkRight(DomainProcessor.execArrayByte(domain_name, data.map(_.toByte), charEnc))

    def checkRight(result: Either[String, String]) =
      if (result.isRight) {
        result.right.get
      } else {
        "UNEXPECTED LEFT VALUE:" + result.left.get
      }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.fileConv

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import d2k.common.df.CsvInfo

import d2k.common.TestArgs
import scala.util.Try

class FileConvPartition1Test extends WordSpec with MustMatchers with BeforeAndAfter {
  System.setProperty("spark.cores.max", "16")

  implicit val inArgs = TestArgs().toInputArgs
  "FileConv variable File Partition number" should {
    "equal core number" in {
      val compoId = "res"
      val fileNames = (1 to 9).map(s => s"csv${s}.dat").toSet
      val fc = new FileConv(compoId, CsvInfo(fileNames), compoId)
      val df = fc.makeDf
      val result = df.collect
      df.rdd.partitions.size mustBe 9
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.fileConv

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import d2k.common.df.CsvInfo

import d2k.common.TestArgs
import scala.util.Try

class FileConvPartition2Test extends WordSpec with MustMatchers with BeforeAndAfter {
  System.setProperty("spark.executor.cores", "16")

  implicit val inArgs = TestArgs().toInputArgs
  "FileConv variable File Partition number" should {
    "equal core number" in {
      val compoId = "res"
      val fileNames = (1 to 9).map(s => s"csv${s}.dat").toSet
      val fc = new FileConv(compoId, CsvInfo(fileNames), compoId)
      val df = fc.makeDf
      val result = df.collect
      df.rdd.partitions.size mustBe 9
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common.fileConv

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import d2k.common.df.CsvInfo

import d2k.common.TestArgs
import scala.util.Try

class FileConvTest extends WordSpec with MustMatchers with BeforeAndAfter {
  implicit val inArgs = TestArgs().toInputArgs

  "FileConv" should {
    "file not exists mkEmptyDfWhenFileNotExists is false(default)" in {
      val compoId = "csv"
      val fc = new FileConv(compoId, CsvInfo(Set("csv.da")), compoId)
      val df = try {
        fc.makeDf
        fail
      } catch {
        case ex: org.apache.spark.sql.AnalysisException => ex.getMessage.startsWith("Path does not exist:") mustBe true
      }
    }

    "file exists mkEmptyDfWhenFileNotExists is false(default)" in {
      val compoId = "csv"
      val fc = new FileConv(compoId, CsvInfo(Set("csv.dat")), compoId)
      val df = fc.makeDf
      df.count mustBe 3
      df.schema.map(_.name).mkString(",") mustBe "item1,item2,ROW_ERR,ROW_ERR_MESSAGE"
    }

    "file not exists mkEmptyDfWhenFileNotExists is true" in {
      val compoId = "csv"
      val fc = new FileConv(compoId, CsvInfo(Set("csv.da")), compoId, true)
      val df = fc.makeDf
      df.count mustBe 0
      df.schema.map(_.name).mkString(",") mustBe "item1,item2,ROW_ERR,ROW_ERR_MESSAGE"
    }

    "file exists mkEmptyDfWhenFileNotExists is true" in {
      val compoId = "csv"
      val fc = new FileConv(compoId, CsvInfo(Set("csv.dat")), compoId, true)
      val df = fc.makeDf
      df.count mustBe 3
      df.schema.map(_.name).mkString(",") mustBe "item1,item2,ROW_ERR,ROW_ERR_MESSAGE"
    }

    "read UTF-8 data" in {
      val compoId = "csv"
      val fc = new FileConv(compoId, CsvInfo(Set("csv_utf8.dat"), charSet = "UTF-8"), compoId, true)
      val df = fc.makeDf
      df.count mustBe 3

      val r = df.collect
      r(0).getAs[String]("item1") mustBe "あ"
      r(0).getAs[String]("item2") mustBe "う1"
      r(1).getAs[String]("item1") mustBe "い"
      r(1).getAs[String]("item2") mustBe "え2"
      r(2).getAs[String]("item1") mustBe ""
      r(2).getAs[String]("item2") mustBe ""
    }
  }

  "read from Resource" should {
    "success read" in {
      val compoId = "res"
      val fc = new FileConv(compoId, CsvInfo(Set("res.dat")), compoId)
      val result = fc.makeDf.collect

      {
        val r = result(0)
        r.getAs[String]("xxx1") mustBe "res1"
        r.getAs[String]("xxx2") mustBe "bb1"
      }

      {
        val r = result(1)
        r.getAs[String]("xxx1") mustBe "res2"
        r.getAs[String]("xxx2") mustBe "bb2"
      }
    }
  }

  "FileConv variable File Partition number" should {
    "equal core number" in {
      val compoId = "res"
      val fileNames = (1 to 9).map(s => s"csv${s}.dat").toSet
      val fc = new FileConv(compoId, CsvInfo(fileNames), compoId)
      val df = fc.makeDf
      val result = df.collect
      df.rdd.partitions.size mustBe 3
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter
import java.sql.DriverManager

class HiRDBTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "HiRDB" should {
    "connect db" in {
      Class.forName("JP.co.Hitachi.soft.HiRDB.JDBC.HiRDBDriver")
      val con = DriverManager.getConnection("jdbc:hitachi:hirdb://DBID=22200,DBHOST=localhost","USER1","USER1")
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter
import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils
import d2k.common.df.DbConnectionInfo
import spark.common.SparkContexts
import spark.common.DbCtl
import SparkContexts.context.implicits._
import java.sql.Timestamp
import java.time.LocalDateTime
import java.util.Properties
import org.apache.spark.sql._
import d2k.common.df.template.DbToDb
import d2k.common.df.template.DbToDb
import scala.util.Try
import org.apache.spark.sql.functions._
import java.sql.Date
import d2k.common.df.executor.Nothing

case class LastUpdateTime(ID_TBLID: String, DT_FROMUPDYMDTM: Timestamp,
                          DT_TOUPDYMDTM: Timestamp, DT_UPDYMDTM: Timestamp)
case class T1(ID: String, TMSTMP: Timestamp)
case class T2(a: String, b: String)
case class T3(aa: String, bb: String)
class InputArgsTest extends WordSpec with MustMatchers with BeforeAndAfter {
  implicit val inArgs = TestArgs().toInputArgs

  "InputArgs" should {
    "normal end" in {
      inArgs.tableNameMapper.isEmpty mustBe true
    }

    "baseInputFilePath" in {
      inArgs.baseInputFilePath mustBe "test/dev/data/output"
    }

    "be success read runningdate" in {
      import inArgs.runningDate._

      MANG_DT mustBe "20151001"
      YST_DY mustBe "20150930"
      NXT_DT mustBe "20150305"
      BEF_MO mustBe "201502"
      BEF_MO_FRST_MTH_DT mustBe "20150201"
      BEF_MO_MTH_DT mustBe "20150228"
      CURR_MO mustBe "201503"
      CURR_MO_FRST_MTH_DT mustBe "20150301"
      CURR_MO_MTH_DT mustBe "20150331"
      NXT_MO mustBe "201504"
      NXT_MO_FRST_MTH_DT mustBe "20150401"
      NXT_MO_MTH_DT mustBe "20150430"
    }
  }

  "InputArgs　RunningEnv daily execute cnt" should {
    "be normal end" in {
      inArgs.runningEnv.DAILY_EXECUTE_CNT mustBe "999"
    }
  }

  def timestamp_yyyymmdd(yyyy: Int, mm: Int, dd: Int) =
    Timestamp.valueOf(LocalDateTime.of(yyyy, mm, dd, 0, 0, 0))

  implicit class MakeDate(str: String) {
    def toTm =
      timestamp_yyyymmdd(str.take(4).toInt, str.drop(4).take(2).toInt, str.drop(6).take(2).toInt)
  }

  "差分抽出時刻管理テーブルデータ取得" should {
    val readDbInfo = DbConnectionInfo.bat1
    val dbCtl = new DbCtl(readDbInfo)
    Try { dbCtl.dropTable("MOP012") }
    println(readDbInfo.toOptions.tableOrQuery)
    JdbcUtils.createConnectionFactory(readDbInfo.toOptions)()
      .prepareStatement(
        """create table MOP012(
          |  ID_TBLID VARCHAR2(6),
          |  DT_FROMUPDYMDTM TIMESTAMP,
          |  DT_TOUPDYMDTM TIMESTAMP,
          |  DT_UPDYMDTM TIMESTAMP)
          |""".stripMargin).executeUpdate

    import dbCtl.implicits._

    "be success lastUpdateTime." in {
      Seq(
        LastUpdateTime("X1", "20000101".toTm, "20010101".toTm, "20170101".toTm),
        LastUpdateTime("X2", "20010102".toTm, "20020102".toTm, "20170102".toTm)).toDF.writeTable("MOP012")

      val x1 = inArgs.lastUpdateTime("X1")
      x1.from mustBe "20000101".toTm
      x1.to mustBe "20010101".toTm

      val x2 = inArgs.lastUpdateTime("X2")
      x2.from mustBe "20010102".toTm
      x2.to mustBe "20020102".toTm
    }

    "tableNameが未定義の場合IllegalArgumentExceptionがthrowされる" in {
      Seq(
        LastUpdateTime("X1", "20000101".toTm, "20010101".toTm, "20170101".toTm),
        LastUpdateTime("X2", "20010102".toTm, "20020102".toTm, "20170102".toTm)).toDF.writeTable("MOP012")

      try {
        inArgs.lastUpdateTime("othertable")
        fail
      } catch {
        case t: Throwable => t.getClass.getName mustBe "java.lang.IllegalArgumentException"
      }
    }

    "template実装 sample" in {
      Try { dbCtl.dropTable("T1") }
      JdbcUtils.createConnectionFactory(readDbInfo.toOptions)()
        .prepareStatement(
          """create table T1(
            |  ID VARCHAR2(2),
            |  TMSTMP TIMESTAMP)
            |""".stripMargin).executeUpdate

      Try { dbCtl.dropTable("X2") }
      JdbcUtils.createConnectionFactory(readDbInfo.toOptions)()
        .prepareStatement(
          """create table X2(
            |  ID VARCHAR2(2),
            |  TMSTMP TIMESTAMP)
            |""".stripMargin).executeUpdate

      (1 to 10).map { idx =>
        T1(idx.toString, f"200101${idx}%02d".toTm)
      }.toDF.writeTable("T1")

      Seq(
        LastUpdateTime("X1", "20010101".toTm, "20010101".toTm, "20170101".toTm),
        LastUpdateTime("X2", "20010103".toTm, "20010105".toTm, "20170102".toTm)).toDF.writeTable("MOP012", SaveMode.Overwrite)

      object LastUpdateTimeApp extends SparkApp {
        def exec(implicit inArgs: InputArgs) = {
          dbTodb.run(Unit)
        }

        val dbTodb = new DbToDb with Nothing {
          val componentId = "test1"
          override lazy val readTableName = "T1"
          override def readDbWhere(inArgs: InputArgs) = {
            val ut = inArgs.lastUpdateTime(writeTableName)
            Array(s"TMSTMP >= '${ut.from}' and TMSTMP <= '${ut.to}'")
          }

          override lazy val writeTableName = "X2"
          override val writeDbWithCommonColumn = false
        }
      }

      LastUpdateTimeApp.exec

      val result = dbCtl.readTable("X2").as[T1].collect
      result.size mustBe 3

      result(0).ID mustBe "3"
      result(1).ID mustBe "4"
      result(2).ID mustBe "5"
    }

    "template実装 tableName未定義" in {
      object LastUpdateTimeApp2 extends SparkApp {
        def exec(implicit inArgs: InputArgs) = {
          dbTodb.run(Unit)
        }

        val dbTodb = new DbToDb with Nothing {
          val componentId = "test"
          override lazy val readTableName = "T1"
          override def readDbWhere(inArgs: InputArgs) =
            Array(s"TMSTMP >= '${inArgs.lastUpdateTime("xxx")}'")
        }
      }

      try {
        LastUpdateTimeApp2.exec
        fail
      } catch {
        case t: Throwable => t.getClass.getName mustBe "java.lang.IllegalArgumentException"
      }
    }

    "InputArgs can be Broadcasting" when {
      "join" in {
        val inputArgs: InputArgs = TestArgs().toInputArgs
        val data1 = (1 to 100).map(cnt => T2(s"a${cnt}", s"b${cnt}")).toDF
        val data2 = (1 to 100).map(cnt => T3(s"a${cnt}", s"bb${cnt}")).toDF

        data1.join(data2, $"a" === $"aa").withColumn("b", lit(inputArgs.runningDate.MANG_DT)).show
        succeed
      }

      "repartition" in {
        val inputArgs: InputArgs = TestArgs().toInputArgs
        val data = (1 to 100).map(cnt => T2(s"a${cnt}", s"b${cnt}")).toDF

        data.repartition(10).withColumn("b", lit(inputArgs.runningDate.MANG_DT)).show
        succeed
      }
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter

class JefConverterTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "JEF to UTF8" should {
    "be normal end." in {
      val data = Array[Short](0xA3, 0xCA, 0x40, 0x40, 0xA3, 0xC5, 0x40, 0x40, 0xA3, 0xC6).map(_.toByte)
      JefConverter.convJefToUtfFull(data) mustBe "Ｊ　Ｅ　Ｆ"
    }

    "illegal data pattern" in {
      val data = Array[Short](0xA3, 0xCA, 0xFF, 0xFF, 0xA3, 0xC5, 0x00, 0x00, 0xA3, 0xC6).map(_.toByte)
      JefConverter.convJefToUtfFull(data) mustBe "Ｊ■Ｅ■Ｆ"
    }

    "normal char code" in {
      val data = Array[Short](0xB9, 0xC2, 0xB3, 0xEB, 0xC6, 0xEA, 0x7F, 0xD9, 0x7F, 0xEA).map(_.toByte)
      JefConverter.convJefToUtfFull(data) mustBe "溝葛楢∨♯"
    }

    "kddi original char code" in {
      val data = Array[Short](0x57, 0xC6, 0x61, 0xB5, 0x70, 0xC5).map(_.toByte)
      JefConverter.convJefToUtfFull(data) mustBe "溝葛楢"
    }

    "kddi original char code 塚(旧文字)" in {
      val data = Array[Short](0xC4, 0xCD).map(_.toByte)
      JefConverter.convJefToUtfFull(data) mustBe "塚"
    }

    "0x00 half char to space" in {
      val data = Array[Short](0x00).map(_.toByte)
      JefConverter.convJefToUtfHalf(data) mustBe " "
    }

    "0x28 half char to space" in {
      val data = Array[Short](0x28).map(_.toByte)
      JefConverter.convJefToUtfHalf(data) mustBe ""
    }

    "0x29 half char to space" in {
      val data = Array[Short](0x29).map(_.toByte)
      JefConverter.convJefToUtfHalf(data) mustBe ""
    }

    "0x38 half char to space" in {
      val data = Array[Short](0x38).map(_.toByte)
      JefConverter.convJefToUtfHalf(data) mustBe ""
    }

    "0x05 half char to tab" in {
      val data = Array[Short](0x05).map(_.toByte)
      JefConverter.convJefToUtfHalf(data) mustBe "\t"
    }

    "0x4B half char to period" in {
      val data = Array[Short](0x4B).map(_.toByte)
      JefConverter.convJefToUtfHalf(data) mustBe "."
    }

    "0x79 half char to tab" in {
      val data = Array[Short](0x79).map(_.toByte)
      JefConverter.convJefToUtfHalf(data) mustBe "`"
    }

    "0x7D half char to single quote" in {
      val data = Array[Short](0x7D).map(_.toByte)
      JefConverter.convJefToUtfHalf(data) mustBe "'"
    }

    "0x7E half char to equal" in {
      val data = Array[Short](0x7E).map(_.toByte)
      JefConverter.convJefToUtfHalf(data) mustBe "="
    }

    "0x7F half char to double quote" in {
      val data = Array[Short](0x7F).map(_.toByte)
      JefConverter.convJefToUtfHalf(data) mustBe "\""
    }

    "illegal data pattern　half" in {
      val data = Array[Short](0xF1, 0x00, 0xFF, 0xF2).map(_.toByte)
      JefConverter.convJefToUtfHalf(data) mustBe "1 *2"
    }

    "all null data" in {
      val data = Array[Short](0x00, 0x00, 0x00, 0x00).map(_.toByte)
      JefConverter.convJefToUtfHalf(data) mustBe "    "
    }
  }

  import JefConverter.implicits._
  "UTF8 to JEF Full" should {
    "be normal end." in {
      val binData = Array[Short](0xA3, 0xCA, 0x40, 0x40, 0xA3, 0xC5, 0x40, 0x40, 0xA3, 0xC6).map(_.toByte).map(x => f"$x%02x")
      "Ｊ　Ｅ　Ｆ".toJefFull.map(x => f"$x%02x") mustBe binData
    }
  }

  "UTF8 to JEF Half" should {
    "be normal end." in {
      val binData = Array[Short](0xD1, 0x40, 0xC5, 0x40, 0xC6).map(_.toByte).map(x => f"$x%02x")
      "J E F".toJefHalf.map(x => f"$x%02x") mustBe binData
    }

    "check tab" in {
      val binData = Array[Short](0xD1, 0x05, 0xC5, 0x05, 0xC6).map(_.toByte).map(x => f"$x%02x")
      "J\tE\tF".toJefHalf.map(x => f"$x%02x") mustBe binData
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter

class KanaConvTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "KanaConv" should {
    "be convert kana char" in {
      KanaConverter("アイウエオ ") mustBe "ｱｲｳｴｵ "
    }

    "be convert kana char　for select" in {
      KanaConverter.select("アイウエオ ") mustBe "ｱｲｳｴｵ"
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter

class MakeResourceTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "makeFixedData" should {
    "be normal end." in {
      val mr = MakeResource("test/dev/data/output")
      mr.readMdTable("makeResourceTest/fixed.md").toFixed("output.fixed")
    }
  }

  "makeJefData" should {
    "be normal end." in {
      val mr = MakeResource("test/dev/data/output")
      mr.readMdTable("makeResourceTest/jef.md").toJef("output.jef")
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter

class PostCodeNormalizerTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "PostCodeNormalizer" should {
    "success convert" when {
      "Parent Child PostCode" in {
        (PostCodeNormalizer.parent("300"), PostCodeNormalizer.child("1")) mustBe ("300", "")
        (PostCodeNormalizer.parent("300"), PostCodeNormalizer.child("11")) mustBe ("300", "11")
        (PostCodeNormalizer.parent("300"), PostCodeNormalizer.child("111")) mustBe ("300", "")
        (PostCodeNormalizer.parent("300"), PostCodeNormalizer.child("1111")) mustBe ("300", "1111")
        (PostCodeNormalizer.parent("300"), PostCodeNormalizer.child("11111")) mustBe ("300", "")

        (PostCodeNormalizer.parent(""), PostCodeNormalizer.child("1")) mustBe ("", "")
        (PostCodeNormalizer.parent("3"), PostCodeNormalizer.child("11")) mustBe ("", "11")
        (PostCodeNormalizer.parent("30"), PostCodeNormalizer.child("111")) mustBe ("", "")
        (PostCodeNormalizer.parent("30"), PostCodeNormalizer.child("1111")) mustBe ("", "1111")
        (PostCodeNormalizer.parent("3001"), PostCodeNormalizer.child("1111")) mustBe ("", "1111")
        (PostCodeNormalizer.parent("30011"), PostCodeNormalizer.child("1111")) mustBe ("", "1111")
        (PostCodeNormalizer.parent("300111"), PostCodeNormalizer.child("1111")) mustBe ("", "1111")
        (PostCodeNormalizer.parent("3001111"), PostCodeNormalizer.child("1111")) mustBe ("", "1111")
      }

      "Single PostCode" in {
        PostCodeNormalizer("300-") mustBe "300"
        PostCodeNormalizer("300-1") mustBe "300"
        PostCodeNormalizer("300-11") mustBe "300-11"
        PostCodeNormalizer("300-111") mustBe "300"
        PostCodeNormalizer("300-1111") mustBe "300-1111"
        PostCodeNormalizer("300-11111") mustBe "300"
      }

      "Single PostCode no hyphen" in {
        PostCodeNormalizer("3") mustBe ""
        PostCodeNormalizer("30") mustBe ""
        PostCodeNormalizer("300") mustBe "300"
        PostCodeNormalizer("3001") mustBe "300"
        PostCodeNormalizer("30011") mustBe "30011"
        PostCodeNormalizer("300111") mustBe "300"
        PostCodeNormalizer("3001111") mustBe "3001111"
        PostCodeNormalizer("30011111") mustBe "300"
      }
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter
import spark.common.SparkContexts
import spark.common.DfCtl._
import spark.common.DfCtl.implicits._
import org.apache.spark.sql.DataFrame
import d2k.common.df.Executor
import d2k.common.df.template.DfToDf
import d2k.common.df.executor._

object SparkAppTestObj extends SparkApp {
  case class Aaa(a: String, b: String)
  import SparkContexts.context.implicits._

  def exec(implicit inArgs: InputArgs) = {
    val df = Seq(Aaa("a", "b")).toDF
    comp1.run(df)
  }

  val comp1 = new DfToDf with Executor {
    val componentId = "MCA018121"
    def invoke(df: DataFrame)(implicit inArgs: InputArgs) =
      df ~> f01

    def f01 = (_: DataFrame).filter($"a" === "a")
  }
}

class SparkAppTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "SparkApp" should {
    "be closing session" in {
      SparkAppTestObj.main(TestArgs().toArray)
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

object TestArgs {
  def apply: TestArgs = TestArgs()
}

case class TestArgs(confPath: String = "conf", dataPath: String = "data",
                    projectId: String = "projectId", processId: String = "processId", applicationId: String = "appId",
                    runningDateFileFullPath: String = "test/dev/RUNNING_DATE.txt") {
  def toArray = Array("test", "dev", confPath, dataPath, projectId, processId,
    applicationId, runningDateFileFullPath)
  def toInputArgs = InputArgs("test", "dev", confPath, dataPath, projectId, processId,
    applicationId, runningDateFileFullPath)
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package d2k.common

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter
import spark.common.SparkContexts.context
import context.implicits._
import spark.common.SparkContexts
import spark.common.SparkContexts
import org.apache.spark.sql.functions._
import java.util.Date
import org.joda.time.DateTime
import org.joda.time.format.DateTimeFormatter
import org.joda.time.format.DateTimeFormat
import org.apache.spark.sql.DataFrame
import java.util.Calendar
import java.sql.Timestamp
import org.apache.spark.sql.expressions.UserDefinedFunction
import java.text.SimpleDateFormat

case class Test(str: String)
case class DateTest(str: String, sqlDate: java.sql.Date, sqlTimestamp: java.sql.Timestamp)
case class DateTest2(test1: DateTest, test2: DateTest)

case class DateRangeTestDateType(DT_BEGIN: java.sql.Date, DT_END: java.sql.Date, DT_DELETE: java.sql.Date)
case class DateRangeTestStringType(DT_BEGIN: String, DT_END: String, DT_DELETE: String)

case class DateDiffStr(target: String, target2: String = "")
case class DateDiffDate(target: java.sql.Date, target2: java.sql.Date)
case class DateDiffTimestamp(target: java.sql.Timestamp, target2: java.sql.Timestamp)

case class PostCodeNormalizeData(POSTCODE1: String, POSTCODE2: String = "")

class UdfsTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "Kana Converter" should {
    "be call udf kana converter" in {
      val df = context.createDataFrame(Seq(Test("アイウエオ")))
      val result = df.withColumn("result", Udfs.kanaConv(df("str"))).collect
      result(0).getAs[String]("result") mustBe "ｱｲｳｴｵ"
    }

    "be call udf kana converter　for Search" in {
      val df = context.createDataFrame(Seq(Test("アイウエオ")))
      val result = df.withColumn("result", Udfs.kanaConvSearch(df("str"))).collect
      result(0).getAs[String]("result") mustBe "ｱｲｳｴｵ"
    }
  }
  "DateCalc" should {
    "normal end" in {
      Udfs.dateCalc(DateTime.parse("2016-01-01T00:00:00"), 1, 0) mustBe DateTime.parse("2016-02-01T00:00:00")
      Udfs.dateCalc(DateTime.parse("2016-01-01T00:00:00"), -1, 0) mustBe DateTime.parse("2015-12-01T00:00:00")

      Udfs.dateCalc(DateTime.parse("2016-01-01T00:00:00"), 0, 1) mustBe DateTime.parse("2016-01-02T00:00:00")
      Udfs.dateCalc(DateTime.parse("2016-01-01T00:00:00"), 0, -1) mustBe DateTime.parse("2015-12-31T00:00:00")

      Udfs.dateCalc(DateTime.parse("2016-01-01T00:00:00"), 1, 1) mustBe DateTime.parse("2016-02-02T00:00:00")
      Udfs.dateCalc(DateTime.parse("2016-01-01T00:00:00"), -1, -1) mustBe DateTime.parse("2015-11-30T00:00:00")

      Udfs.dateCalc(DateTime.parse("2016-01-01T00:00:00"), 1, -1) mustBe DateTime.parse("2016-01-31T00:00:00")
      Udfs.dateCalc(DateTime.parse("2016-01-01T00:00:00"), -1, 1) mustBe DateTime.parse("2015-12-02T00:00:00")
    }

    "success null Check" in {
      Udfs.dateCalc(null, 1, 0) mustBe null
    }
  }

  "DateDuration" should {
    "success by Days" in {
      Udfs.dateDurationByDays(DateTime.parse("2016-01-01T00:00:00"), DateTime.parse("2016-01-01T00:00:00")) mustBe 0
      Udfs.dateDurationByDays(DateTime.parse("2016-01-02T00:00:00"), DateTime.parse("2016-01-01T00:00:00")) mustBe -1
      Udfs.dateDurationByDays(DateTime.parse("2015-12-31T00:00:00"), DateTime.parse("2016-01-01T00:00:00")) mustBe 1
      Udfs.dateDurationByDays(DateTime.parse("2016-01-01T00:00:00"), DateTime.parse("2016-02-01T00:00:00")) mustBe 31
    }

    "success by Months" in {
      Udfs.dateDurationByMonths(DateTime.parse("2016-01-01T00:00:00"), DateTime.parse("2016-01-01T00:00:00")) mustBe 0
      Udfs.dateDurationByMonths(DateTime.parse("2016-01-01T00:00:00"), DateTime.parse("2016-02-01T00:00:00")) mustBe 1
      Udfs.dateDurationByMonths(DateTime.parse("2016-01-01T00:00:00"), DateTime.parse("2015-12-01T00:00:00")) mustBe -1
      Udfs.dateDurationByMonths(DateTime.parse("2016-01-01T00:00:00"), DateTime.parse("2017-01-01T00:00:00")) mustBe 12
    }
  }

  "udf calc" should {
    "success ym String calc" in {
      val patternYYYYMM = DateTimeFormat.forPattern("yyyyMMdd HHmmss")

      import SparkContexts.context.implicits._
      val date = DateTime.parse("20160101 000000", patternYYYYMM)
      val sqlDate = new java.sql.Date(date.getMillis)
      val sqlTimestamp = new java.sql.Timestamp(date.getMillis)
      val df = SparkContexts.sc.makeRDD(Seq(DateTest("201601", null, null))).toDF

      import Udfs.date.ym.string._
      val result = df.withColumn("result", calc(df("str"), lit(0))).collect
      result(0).getAs[String]("result") mustBe "201601"

      val result2 = df.withColumn("result", calc(df("str"), lit(2))).collect
      result2(0).getAs[String]("result") mustBe "201603"

      val result3 = df.withColumn("result", calc(df("str"), lit(-2))).collect
      result3(0).getAs[String]("result") mustBe "201511"
    }

    "success ym String duration" in {
      val patternYYYYMM = DateTimeFormat.forPattern("yyyyMMdd HHmmss")

      import SparkContexts.context.implicits._
      val date = DateTime.parse("20160101 000000", patternYYYYMM)
      val sqlDate = new java.sql.Date(date.getMillis)
      val sqlTimestamp = new java.sql.Timestamp(date.getMillis)
      val df = SparkContexts.sc.makeRDD(
        Seq(DateTest2(DateTest("201601", null, null), DateTest("201603", null, null)))).toDF

      import Udfs.date.ym.string._
      val result = df.withColumn("result", duration(df("test1.str"), df("test2.str"))).collect
      result(0).getAs[Integer]("result") mustBe 2

      val result2 = df.withColumn("result", duration(df("test2.str"), df("test1.str"))).collect
      result2(0).getAs[Integer]("result") mustBe -2
    }

    "success ymd String calc" in {
      val patternYYYYMM = DateTimeFormat.forPattern("yyyyMMdd HHmmss")

      import SparkContexts.context.implicits._
      val date = DateTime.parse("20160101 000000", patternYYYYMM)
      val sqlDate = new java.sql.Date(date.getMillis)
      val sqlTimestamp = new java.sql.Timestamp(date.getMillis)
      val df = SparkContexts.sc.makeRDD(Seq(DateTest("20160101", null, null))).toDF

      import Udfs.date.ymd.string._
      val result = df.withColumn("result", calc(df("str"), lit(0))).collect
      result(0).getAs[String]("result") mustBe "20160101"

      val result2 = df.withColumn("result", calc(df("str"), lit(2))).collect
      result2(0).getAs[String]("result") mustBe "20160103"

      val result3 = df.withColumn("result", calc(df("str"), lit(-2))).collect
      result3(0).getAs[String]("result") mustBe "20151230"
    }

    "success ymd String duration" in {
      val patternYYYYMM = DateTimeFormat.forPattern("yyyyMMdd HHmmss")

      import SparkContexts.context.implicits._
      val date = DateTime.parse("20160101 000000", patternYYYYMM)
      val sqlDate = new java.sql.Date(date.getMillis)
      val sqlTimestamp = new java.sql.Timestamp(date.getMillis)
      val df = SparkContexts.sc.makeRDD(
        Seq(DateTest2(DateTest("20160101", null, null), DateTest("20160103", null, null)))).toDF

      import Udfs.date.ymd.string._
      val result = df.withColumn("result", duration(df("test1.str"), df("test2.str"))).collect
      result(0).getAs[Integer]("result") mustBe 2

      val result2 = df.withColumn("result", duration(df("test2.str"), df("test1.str"))).collect
      result2(0).getAs[Integer]("result") mustBe -2
    }
  }

  "hashAuId" should {
    "input is null" in {
      execAssertEquals(null, null, Udfs.hashAuId)
    }

    "input is zero length String" in {
      execAssertEquals("", "", Udfs.hashAuId)
    }

    "input is all space" in {
      val input = "                             "
      execAssertEquals(input, input, Udfs.hashAuId)
    }

  }

  "addPointToDateString" should {
    "normal 15Length" in {
      execAssertEquals("201601011122334", "20160101112233.4", Udfs.addPointToDateString)
    }

    "normal 17Length" in {
      execAssertEquals("20160101112233456", "20160101112233.456", Udfs.addPointToDateString)
    }

    "input is null" in {
      execAssertEquals(null, null, Udfs.addPointToDateString)
    }

    "input is zero length String" in {
      execAssertEquals("", "", Udfs.addPointToDateString)
    }
  }

  "addHyphenToDateString" should {
    "normal date format" in {
      execAssertEquals("20160123", "2016-01-23", Udfs.addHyphenToDateString)
    }

    "normal Timestamp format" in {
      execAssertEquals("20160123112233444", "2016-01-23", Udfs.addHyphenToDateString)
    }

    "input is null" in {
      execAssertEquals(null, null, Udfs.addHyphenToDateString)
    }

    "input is zero length String" in {
      execAssertEquals("", "", Udfs.addHyphenToDateString)
    }
  }

  "blankToZero" should {
    "normal " in {
      execAssertEquals("123", "123", Udfs.blankToZero)
    }

    "input is null" in {
      execAssertEquals(null, "0", Udfs.blankToZero)
    }

    "input is zero length String" in {
      execAssertEquals("", "0", Udfs.blankToZero)
    }
  }

  "trancateAfterPoint" should {
    "normal with fraction part" in {
      execAssertEquals("123.99", "123", Udfs.trancateAfterPoint)
    }

    "normal with fraction part , sign" in {
      execAssertEquals("+123.00", "+123", Udfs.trancateAfterPoint)
    }

    "normal with point , sign " in {
      execAssertEquals("-555.", "-555", Udfs.trancateAfterPoint)
    }

    "normal without fraction part" in {
      execAssertEquals("999", "999", Udfs.trancateAfterPoint)
    }

    "input is null" in {
      execAssertEquals(null, null, Udfs.trancateAfterPoint)
    }

    "input is zero length String" in {
      execAssertEquals("", "", Udfs.trancateAfterPoint)
    }
  }

  //String引数1個のUDF検証用
  def execAssertEquals(input: String, expected: String, targetUdf: UserDefinedFunction) {
    val df = context.createDataFrame(Seq(Test(input)))
    val result = df.withColumn("result", targetUdf(df("str"))).collect
    result(0).getAs[String]("result") mustBe expected
  }

  val FMT_YMD = DateTimeFormat.forPattern("yyyyMMdd")
  val MANG_DT_STR_TODAY = "20151001"
  val MANG_DT_STR_TOMORROW = "20151002"
  val MANG_DT_STR_INIT_HP = "0001-01-01"
  val MANG_DT_STR_INIT = "00010101"
  val MANG_DT_DATE_TODAY = new java.sql.Date(DateTime.parse(MANG_DT_STR_TODAY, FMT_YMD).getMillis)
  val MANG_DT_DATE_TOMORROW = new java.sql.Date(DateTime.parse(MANG_DT_STR_TOMORROW, FMT_YMD).getMillis)

  import Udfs.DateRangeStatus._
  "DateRangeStatus.getStatus" should {
    "normal string 0" in {
      execGetStatus(makeStringDf(MANG_DT_STR_TOMORROW, MANG_DT_STR_TOMORROW, null), STATUS_BEFORE)
    }
    "normal string 9" in {
      execGetStatus(makeStringDf(MANG_DT_STR_TODAY, MANG_DT_STR_TODAY, null), STATUS_END)
    }
    "normal string 1" in {
      execGetStatus(makeStringDf(MANG_DT_STR_TODAY, MANG_DT_STR_TOMORROW, null), STATUS_ON)
    }
    "input is null string" in {
      execGetStatus(makeStringDf(null, null, null), STATUS_ON)
    }
    "input is zero length string" in {
      execGetStatus(makeStringDf("", "", null), STATUS_END)
    }

    "normal date 0" in {
      execGetStatus(makeDateDf(MANG_DT_DATE_TOMORROW, MANG_DT_DATE_TOMORROW, null), STATUS_BEFORE)
    }
    "normal date 9" in {
      execGetStatus(makeDateDf(MANG_DT_DATE_TODAY, MANG_DT_DATE_TODAY, null), STATUS_END)
    }
    "normal date 1" in {
      execGetStatus(makeDateDf(MANG_DT_DATE_TODAY, MANG_DT_DATE_TOMORROW, null), STATUS_ON)
    }
    "input is null date" in {
      execGetStatus(makeDateDf(null, null, null), STATUS_ON)
    }
  }

  "DateRangeStatus.getStatusWithDeleteDate" should {
    "normal string 0" in {
      execGetStatusWithDeleteDate(makeStringDf(MANG_DT_STR_TOMORROW, MANG_DT_STR_TOMORROW, MANG_DT_STR_INIT), STATUS_BEFORE)
    }
    "normal string 9" in {
      execGetStatusWithDeleteDate(makeStringDf(MANG_DT_STR_TODAY, MANG_DT_STR_TODAY, MANG_DT_STR_INIT_HP), STATUS_END)
    }
    "normal string 1" in {
      execGetStatusWithDeleteDate(makeStringDf(MANG_DT_STR_TODAY, MANG_DT_STR_TOMORROW, ""), STATUS_ON)
    }
    "delete string" in {
      execGetStatusWithDeleteDate(makeStringDf(MANG_DT_STR_TODAY, MANG_DT_STR_TOMORROW, MANG_DT_STR_TODAY), STATUS_END)
    }
    "input is null string" in {
      execGetStatusWithDeleteDate(makeStringDf(null, null, null), STATUS_ON)
    }
    "input is zero length string" in {
      execGetStatusWithDeleteDate(makeStringDf("", "", ""), STATUS_END)
    }

    "normal date 0" in {
      execGetStatusWithDeleteDate(makeDateDf(MANG_DT_DATE_TOMORROW, MANG_DT_DATE_TOMORROW, null), STATUS_BEFORE)
    }
    "normal date 9" in {
      execGetStatusWithDeleteDate(makeDateDf(MANG_DT_DATE_TODAY, MANG_DT_DATE_TODAY, null), STATUS_END)
    }
    "normal date 1" in {
      execGetStatusWithDeleteDate(makeDateDf(MANG_DT_DATE_TODAY, MANG_DT_DATE_TOMORROW, null), STATUS_ON)
    }
    "delete date" in {
      execGetStatusWithDeleteDate(makeDateDf(MANG_DT_DATE_TODAY, MANG_DT_DATE_TOMORROW, MANG_DT_DATE_TODAY), STATUS_END)
    }
    "input is null date" in {
      execGetStatusWithDeleteDate(makeDateDf(null, null, null), STATUS_ON)
    }
  }

  "DateRangeStatus.getStatusWithBlankReplace" should {
    "normal string 0" in {
      execGetStatusWithBlankReplace(makeStringDf(MANG_DT_STR_TOMORROW, MANG_DT_STR_TOMORROW, null), STATUS_BEFORE)
    }
    "normal string 9" in {
      execGetStatusWithBlankReplace(makeStringDf(MANG_DT_STR_TODAY, MANG_DT_STR_TODAY, null), STATUS_END)
    }
    "normal string 1" in {
      execGetStatusWithBlankReplace(makeStringDf(MANG_DT_STR_TODAY, MANG_DT_STR_TOMORROW, null), STATUS_ON)
    }

    "input is all null string" in {
      execGetStatusWithBlankReplace(makeStringDf(null, null, null), STATUS_BEFORE)
    }
    "begin is null string" in {
      execGetStatusWithBlankReplace(makeStringDf(null, MANG_DT_STR_TODAY, null), STATUS_BEFORE)
    }
    "end is all null string" in {
      execGetStatusWithBlankReplace(makeStringDf(MANG_DT_STR_TODAY, null, null), STATUS_ON)
    }

    "input is all zero length string" in {
      execGetStatusWithBlankReplace(makeStringDf("", "", null), STATUS_BEFORE)
    }
    "begin is zero length string" in {
      execGetStatusWithBlankReplace(makeStringDf("", MANG_DT_STR_TODAY, null), STATUS_BEFORE)
    }
    "end is zero length string" in {
      execGetStatusWithBlankReplace(makeStringDf(MANG_DT_STR_TODAY, "", null), STATUS_ON)
    }

    "normal date 0" in {
      execGetStatusWithBlankReplace(makeDateDf(MANG_DT_DATE_TOMORROW, MANG_DT_DATE_TOMORROW, null), STATUS_BEFORE)
    }
    "normal date 9" in {
      execGetStatusWithBlankReplace(makeDateDf(MANG_DT_DATE_TODAY, MANG_DT_DATE_TODAY, null), STATUS_END)
    }
    "normal date 1" in {
      execGetStatusWithBlankReplace(makeDateDf(MANG_DT_DATE_TODAY, MANG_DT_DATE_TOMORROW, null), STATUS_ON)
    }

    "input is all null date" in {
      execGetStatusWithBlankReplace(makeDateDf(null, null, null), STATUS_BEFORE)
    }
    "begin is null date" in {
      execGetStatusWithBlankReplace(makeDateDf(null, MANG_DT_DATE_TODAY, null), STATUS_BEFORE)
    }
    "end is all null date" in {
      execGetStatusWithBlankReplace(makeDateDf(MANG_DT_DATE_TODAY, null, null), STATUS_ON)
    }
  }

  "DateRangeStatus.getStatusWithBlankReplaceAndDeleteDate" should {
    "normal string 0" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeStringDf(MANG_DT_STR_TOMORROW, MANG_DT_STR_TOMORROW, MANG_DT_STR_INIT), STATUS_BEFORE)
    }
    "normal string 9" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeStringDf(MANG_DT_STR_TODAY, MANG_DT_STR_TODAY, MANG_DT_STR_INIT_HP), STATUS_END)
    }
    "normal string 1" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeStringDf(MANG_DT_STR_TODAY, MANG_DT_STR_TOMORROW, ""), STATUS_ON)
    }
    "delete string" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeStringDf(MANG_DT_STR_TODAY, MANG_DT_STR_TOMORROW, MANG_DT_STR_TODAY), STATUS_END)
    }

    "input is all null string" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeStringDf(null, null, null), STATUS_ON)
    }
    "begin is null string" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeStringDf(null, MANG_DT_STR_TODAY, null), STATUS_END)
    }
    "end is all null string" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeStringDf(MANG_DT_STR_TODAY, null, null), STATUS_ON)
    }

    "input is all zero length string" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeStringDf("", "", null), STATUS_ON)
    }
    "begin is zero length string" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeStringDf("", MANG_DT_STR_TODAY, null), STATUS_END)
    }
    "end is zero length string" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeStringDf(MANG_DT_STR_TODAY, "", null), STATUS_ON)
    }

    "normal date 0" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeDateDf(MANG_DT_DATE_TOMORROW, MANG_DT_DATE_TOMORROW, null), STATUS_BEFORE)
    }
    "normal date 9" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeDateDf(MANG_DT_DATE_TODAY, MANG_DT_DATE_TODAY, null), STATUS_END)
    }
    "normal date 1" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeDateDf(MANG_DT_DATE_TODAY, MANG_DT_DATE_TOMORROW, null), STATUS_ON)
    }
    "delete date" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeDateDf(MANG_DT_DATE_TODAY, MANG_DT_DATE_TOMORROW, MANG_DT_DATE_TODAY), STATUS_END)
    }
    "input is all null date" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeDateDf(null, null, null), STATUS_ON)
    }
    "begin is null date" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeDateDf(null, MANG_DT_DATE_TODAY, null), STATUS_END)
    }
    "end is all null date" in {
      execGetStatusWithBlankReplaceAndDeleteDate(makeDateDf(MANG_DT_DATE_TODAY, null, null), STATUS_ON)
    }
  }
  // 日付ハサミコミ判定のみ
  def execGetStatus(df: DataFrame, expected: String) =
    assertEquals(df.withColumn("result", getStatus(df("DT_BEGIN"), df("DT_END"), lit(MANG_DT_STR_TODAY))), expected)

  // 削除年月日判定+日付ハサミコミ判定
  def execGetStatusWithDeleteDate(df: DataFrame, expected: String) =
    assertEquals(df.withColumn("result", getStatusWithDeleteDate(df("DT_BEGIN"), df("DT_END"), df("DT_DELETE"), lit(MANG_DT_STR_TODAY))), expected)

  // 初期値=>最大値 置換後、ハサミコミ判定
  def execGetStatusWithBlankReplace(df: DataFrame, expected: String) =
    assertEquals(df.withColumn("result", getStatusWithBlankReplace(df("DT_BEGIN"), df("DT_END"), lit(MANG_DT_STR_TODAY))), expected)

  // 初期値=>最大値 置換(endDateのみ)後、削除年月日判定+日付ハサミコミ判定
  def execGetStatusWithBlankReplaceAndDeleteDate(df: DataFrame, expected: String) =
    assertEquals(df.withColumn("result", getStatusWithBlankReplaceAndDeleteDate(df("DT_BEGIN"), df("DT_END"), df("DT_DELETE"), lit(MANG_DT_STR_TODAY))), expected)

  def assertEquals(df: DataFrame, expected: String) {
    val actual = df.collect()(0).getAs[String]("result")
    actual mustBe expected
  }
  def makeStringDf(DT_BEGIN: String, DT_END: String, DT_DELETE: String) =
    context.createDataFrame(Seq(DateRangeTestStringType(DT_BEGIN, DT_END, DT_DELETE)))

  def makeDateDf(DT_BEGIN: java.sql.Date, DT_END: java.sql.Date, DT_DELETE: java.sql.Date) =
    context.createDataFrame(Seq(DateRangeTestDateType(DT_BEGIN, DT_END, DT_DELETE)))

  "udf calcAndDiff" should {
    "normal date" in {
      import SparkContexts.context.implicits._
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("20160101"))).toDF
      import Udfs.date.ymd.string._
      val result = df.withColumn("result", calcAndDiff("20160101", 0, 0)(df("target"))).collect
      result(0).getAs[Boolean]("result") mustBe true
    }

    "be success beginDay pattern" in {
      import SparkContexts.context.implicits._
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("20160101"))).toDF
      import Udfs.date.ymd.string._
      val result = df.withColumn("result", calcAndDiff("20160101", -1)(df("target"))).collect
      result(0).getAs[Boolean]("result") mustBe true
    }

    "be fail beginDay pattern false" in {
      import SparkContexts.context.implicits._
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("20160101"))).toDF
      import Udfs.date.ymd.string._
      val result = df.withColumn("result", calcAndDiff("20160101", 1)(df("target"))).collect
      result(0).getAs[Boolean]("result") mustBe false
    }

    "be success endDay pattern" in {
      import SparkContexts.context.implicits._
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("20160101"))).toDF
      import Udfs.date.ymd.string._
      val result = df.withColumn("result", calcAndDiff("20160101", 0, 1)(df("target"))).collect
      result(0).getAs[Boolean]("result") mustBe true
    }

    "be success endDay pattern false" in {
      import SparkContexts.context.implicits._
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("20160101"))).toDF
      import Udfs.date.ymd.string._
      val result = df.withColumn("result", calcAndDiff("20160101", 0, -1)(df("target"))).collect
      result(0).getAs[Boolean]("result") mustBe false
    }

    "be success beginDay pattern multi target" in {
      import SparkContexts.context.implicits._
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("20160101", "20151230"))).toDF
      import Udfs.date.ymd.string._
      val result = df.withColumn("result", calcAndDiff("20160101", -1)(df("target"), df("target2"))).collect
      result(0).getAs[Boolean]("result") mustBe true
    }

    "be success beginDay pattern multi target false" in {
      import SparkContexts.context.implicits._
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("20151229", "20151230"))).toDF
      import Udfs.date.ymd.string._
      val result = df.withColumn("result", calcAndDiff("20160101", -1)(df("target"), df("target2"))).collect
      result(0).getAs[Boolean]("result") mustBe false
    }

    "be success endDay pattern multi target" in {
      import SparkContexts.context.implicits._
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("20160101", "20160102"))).toDF
      import Udfs.date.ymd.string._
      val result = df.withColumn("result", calcAndDiff("20160101", -1, 1)(df("target"), df("target2"))).collect
      result(0).getAs[Boolean]("result") mustBe true
    }

    "be success endDay pattern multi target false" in {
      import SparkContexts.context.implicits._
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("20151229", "20160103"))).toDF
      import Udfs.date.ymd.string._
      val result = df.withColumn("result", calcAndDiff("20160101", -1, 1)(df("target"), df("target2"))).collect
      result(0).getAs[Boolean]("result") mustBe false
    }

    "be true by null value in targets" in {
      import SparkContexts.context.implicits._
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr(null))).toDF
      import Udfs.date.ymd.string._
      val result = df.withColumn("result", calcAndDiff("20160101", 0, 0)(df("target"))).collect
      result(0).getAs[Boolean]("result") mustBe true
    }

    "be true by empty value in targets" in {
      import SparkContexts.context.implicits._
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr(""))).toDF
      import Udfs.date.ymd.string._
      val result = df.withColumn("result", calcAndDiff("20160101", 0, 0)(df("target"))).collect
      result(0).getAs[Boolean]("result") mustBe true
    }

    "be true by illegal format in targets" in {
      import SparkContexts.context.implicits._
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("xxxx"))).toDF
      import Udfs.date.ymd.string._
      val result = df.withColumn("result", calcAndDiff("20160101", 0, 0)(df("target"))).collect
      result(0).getAs[Boolean]("result") mustBe true
    }

    "be true by null value in base" in {
      import SparkContexts.context.implicits._
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("20160101"))).toDF
      import Udfs.date.ymd.string._
      val result = df.withColumn("result", calcAndDiff(null, 0, 0)(df("target"))).collect
      result(0).getAs[Boolean]("result") mustBe true
    }

    "be true by empty value in base" in {
      import SparkContexts.context.implicits._
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("20160101"))).toDF
      import Udfs.date.ymd.string._
      val result = df.withColumn("result", calcAndDiff("", 0, 0)(df("target"))).collect
      result(0).getAs[Boolean]("result") mustBe true
    }

    "be true by illegal format in base" in {
      import SparkContexts.context.implicits._
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("20160101"))).toDF
      import Udfs.date.ymd.string._
      val result = df.withColumn("result", calcAndDiff("xxx", 0, 0)(df("target"))).collect
      result(0).getAs[Boolean]("result") mustBe true
    }
  }

  "dateCnvJpToEu" should {
    "normal" in {
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("H", "281019"))).toDF
      val result = df.withColumn("result", Udfs.dateCnvJpToEu(df("target"), df("target2"))).collect
      result(0).getAs[String]("result") mustBe "20161019"
    }
    "normal2" in {
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("", "281019"))).toDF
      val result = df.withColumn("result", Udfs.dateCnvJpToEu(df("target"), df("target2"))).collect
      result(0).getAs[String]("result") mustBe "00010101"
    }
    "normal3" in {
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("H", ""))).toDF
      val result = df.withColumn("result", Udfs.dateCnvJpToEu(df("target"), df("target2"))).collect
      result(0).getAs[String]("result") mustBe "00010101"
    }
    "normal4" in {
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("", ""))).toDF
      val result = df.withColumn("result", Udfs.dateCnvJpToEu(df("target"), df("target2"))).collect
      result(0).getAs[String]("result") mustBe "00010101"
    }
    "normal5" in {
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("S", "131019"))).toDF
      val result = df.withColumn("result", Udfs.dateCnvJpToEu(df("target"), df("target2"))).collect
      result(0).getAs[String]("result") mustBe "19381019"
    }
    "normal6" in {
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("T", "131019"))).toDF
      val result = df.withColumn("result", Udfs.dateCnvJpToEu(df("target"), df("target2"))).collect
      result(0).getAs[String]("result") mustBe "19241019"
    }
    "normal7" in {
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("M", "131019"))).toDF
      val result = df.withColumn("result", Udfs.dateCnvJpToEu(df("target"), df("target2"))).collect
      result(0).getAs[String]("result") mustBe "18801019"
    }
    "normal8" in {
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("3", "281019"))).toDF
      val result = df.withColumn("result", Udfs.dateCnvJpToEu(df("target"), df("target2"))).collect
      result(0).getAs[String]("result") mustBe "20161019"
    }
    "normal9" in {
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("2", "131019"))).toDF
      val result = df.withColumn("result", Udfs.dateCnvJpToEu(df("target"), df("target2"))).collect
      result(0).getAs[String]("result") mustBe "19381019"
    }
    "normal10" in {
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("1", "131019"))).toDF
      val result = df.withColumn("result", Udfs.dateCnvJpToEu(df("target"), df("target2"))).collect
      result(0).getAs[String]("result") mustBe "19241019"
    }
    "normal11" in {
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("0", "131019"))).toDF
      val result = df.withColumn("result", Udfs.dateCnvJpToEu(df("target"), df("target2"))).collect
      result(0).getAs[String]("result") mustBe "18801019"
    }
    "normal12" in {
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("3", ""))).toDF
      val result = df.withColumn("result", Udfs.dateCnvJpToEu(df("target"), df("target2"))).collect
      result(0).getAs[String]("result") mustBe "00010101"
    }

    "new era" when {
      "normal input new era number" in {
        val df = SparkContexts.sc.makeRDD(
          Seq(
            DateDiffStr("4", "010101"),
            DateDiffStr("4", "010430"),
            DateDiffStr("4", "010501"),
            DateDiffStr("4", "991231"),
            DateDiffStr("4", ""))).toDF
        val result = df.withColumn("result", Udfs.dateCnvJpToEu(df("target"), df("target2"))).collect
        result(0).getAs[String]("result") mustBe "20190101"
        result(1).getAs[String]("result") mustBe "20190430"
        result(2).getAs[String]("result") mustBe "20190501"
        result(3).getAs[String]("result") mustBe "21171231"
        result(4).getAs[String]("result") mustBe "00010101"
      }

      "normal input new era character" in {
        val df = SparkContexts.sc.makeRDD(
          Seq(
            DateDiffStr("K", "010101"),
            DateDiffStr("K", "010430"),
            DateDiffStr("K", "010501"),
            DateDiffStr("K", "991231"),
            DateDiffStr("K", ""))).toDF
        val result = df.withColumn("result", Udfs.dateCnvJpToEu(df("target"), df("target2"))).collect
        result(0).getAs[String]("result") mustBe "20190101"
        result(1).getAs[String]("result") mustBe "20190430"
        result(2).getAs[String]("result") mustBe "20190501"
        result(3).getAs[String]("result") mustBe "21171231"
        result(4).getAs[String]("result") mustBe "00010101"
      }
    }
  }

  "dateCnvJpToEuWithDefault" should {
    "normal" in {
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr("H", "281019"))).toDF
      val result = df.withColumn("result", Udfs.dateCnvJpToEuWithDefault(df("target"), df("target2"), lit("29991231"))).collect
      result(0).getAs[String]("result") mustBe "20161019"
    }

    "abnormal" in {
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr(" ", "281019"))).toDF
      val result = df.withColumn("result", Udfs.dateCnvJpToEuWithDefault(df("target"), df("target2"), lit("29991231"))).collect
      result(0).getAs[String]("result") mustBe "29991231"
    }
    "abnormal1" in {
      val df = SparkContexts.sc.makeRDD(
        Seq(DateDiffStr(" ", "281019"))).toDF
      val result = df.withColumn("result", Udfs.dateCnvJpToEuWithDefault(df("target"), df("target2"), lit("00010101"))).collect
      result(0).getAs[String]("result") mustBe "00010101"
    }
  }

  "udf isNaOrNull" should {
    "be normal end" in {
      import SparkContexts.context.implicits._

      val df = SparkContexts.sc.makeRDD(
        Seq(Test(""), Test(null))).toDF
      val result = df.filter(Udfs.isNaOrNull(col("str"))).collect
      result(0).getAs[String]("str") mustBe null
      result.size mustBe 1
    }
  }

  "udf isNotNaAndNotNull" should {
    "be normal end" in {
      import SparkContexts.context.implicits._

      val df = SparkContexts.sc.makeRDD(
        Seq(Test(""), Test(null))).toDF
      val result = df.filter(Udfs.isNotNaAndNotNull(col("str"))).collect
      result(0).getAs[String]("str") mustBe ""
      result.size mustBe 1
    }
  }

  def testCutLimitStr(input: String, cutLen: Int, exp: String) = {
    import SparkContexts.context.implicits._

    val df = SparkContexts.sc.makeRDD(
      Seq(Test(input))).toDF
    val result = df.withColumn("result", Udfs.cutLimitStr(col("str"), lit(cutLen))).collect
    result(0).getAs[String]("result") mustBe exp

  }

  "udf cutLimitStr" should {
    "success not half" in { testCutLimitStr("ほげ", 2, "ほ") }
    "success half" in { testCutLimitStr("ほげ", 3, "ほ") }
    "success full" in { testCutLimitStr("ほげ", 4, "ほげ") }
    "success over" in { testCutLimitStr("ほげ", 5, "ほげ") }
    "success null" in { testCutLimitStr(null, 1, null) }
    "success empty" in { testCutLimitStr("", 1, "") }
  }

  def testCalcSchoolAge(birthY: Int, birthM: Int, birthD: Int, runningYMD: String, exp: Int) = {
    import SparkContexts.context.implicits._
    val cal = Calendar.getInstance
    cal.set(Calendar.YEAR, birthY)
    cal.set(Calendar.MONTH, birthM)
    cal.set(Calendar.DATE, birthD)

    val df = SparkContexts.sc.makeRDD(
      Seq(DateTest(runningYMD, null, new java.sql.Timestamp(cal.getTime.getTime)))).toDF
    val result = df.withColumn("result", Udfs.calcSchoolAge(df("sqlTimestamp"), df("str"))).collect
    result(0).getAs[Integer]("result") mustBe exp
  }

  "udf calcSchoolAge" should {
    "success birth 0402 run 0402" in { testCalcSchoolAge(2000, 3, 2, "20170402", 17) }
    "success birth 1231 run 1231" in { testCalcSchoolAge(2000, 11, 31, "20101231", 10) }
    "success birth 0101 run 0402" in { testCalcSchoolAge(2000, 0, 1, "20170402", 18) }
    "success birth 0401 run 1231" in { testCalcSchoolAge(2000, 3, 1, "20101231", 11) }
    "success birth 0402 run 0101" in { testCalcSchoolAge(2000, 3, 2, "20090101", 8) }
    "success birth 1231 run 0401" in { testCalcSchoolAge(2000, 11, 31, "20090401", 8) }
    "success birth 0101 run 0101" in { testCalcSchoolAge(2000, 0, 1, "20090101", 9) }
    "success  birth 0401 run 0401" in { testCalcSchoolAge(2000, 3, 1, "20090401", 9) }
    "success MAX EQUAL" in { testCalcSchoolAge(2000, 3, 2, "29990402", 999) }
    "success MAX OVER" in { testCalcSchoolAge(2000, 3, 2, "30000402", 999) }

    "illegal output" when {
      "schoolAge is under 0" in {
        testCalcSchoolAge(2017, 4, 2, "20160402", 999)
      }
    }

    "success input null" in {
      import SparkContexts.context.implicits._

      val df = SparkContexts.sc.makeRDD(
        Seq(DateTest("20150101", null, null))).toDF
      val result = df.withColumn("result", Udfs.calcSchoolAge(df("sqlTimestamp"), df("str"))).collect
      result(0).getAs[Integer]("result") mustBe 999
    }
  }

  def testCalcAge(birthYMD: String, runningYMD: String, exp: Int) = {
    var simpleDateFormat = new SimpleDateFormat("yyyyMMdd")
    var date = simpleDateFormat.parse(birthYMD)
    val df = SparkContexts.sc.makeRDD(
      Seq(DateTest(runningYMD, null, new java.sql.Timestamp(date.getTime)))).toDF
    val result = df.withColumn("result", Udfs.calcAge(df("sqlTimestamp"), df("str"))).collect
    result(0).getAs[Integer]("result") mustBe exp
  }

  "udf calcAge" should {
    "success birth 0401 run 0401" in { testCalcAge("20000401", "20170401", 17) }
    "success birth 0401 run 0501" in { testCalcAge("20000401", "20170501", 17) }
    "success birth 0501 run 0401" in { testCalcAge("20000501", "20170401", 16) }
    "success MAX EQUAL" in { testCalcAge("20000401", "29990501", 999) }
    "success MAX OVER" in { testCalcAge("20000401", "30000501", 999) }

    "illegal output" when {
      "calcAge reslt is under 0" in {
        testCalcAge("20000401", "19990401", 999)
      }
    }

    "success input null" in {
      import SparkContexts.context.implicits._

      val df = SparkContexts.sc.makeRDD(
        Seq(DateTest("20150101", null, null))).toDF
      val result = df.withColumn("result", Udfs.calcAge(df("sqlTimestamp"), df("str"))).collect
      result(0).getAs[Integer]("result") mustBe 999
    }
  }

  "domainConvert" should {
    "call Domain Converter" in {
      val df = Seq(Test("x")).toDF
      val result = df.withColumn("str", Udfs.domainConvert(col("str"), lit("年月日"))).collect
      result(0).getAs[String]("str") mustBe "00010101"
    }
  }

  "MakeDate date_yyyyMMdd" should {
    "be success　convert" in {
      val df = Seq(Test("20170102")).toDF
      val result = df.withColumn("str", Udfs.MakeDate.date_yyyyMMdd(col("str"))).collect
      result(0).getAs[Date]("str").toString mustBe "2017-01-02"
    }
  }

  "MakeDate timestamp_yyyyMMdd" should {
    "be success　convert" in {
      val df = Seq(Test("20170102")).toDF
      val result = df.withColumn("str", Udfs.MakeDate.timestamp_yyyyMMdd(col("str"))).collect
      result(0).getAs[Timestamp]("str").toString mustBe "2017-01-02 00:00:00.0"
    }
  }

  "MakeDate timestamp_yyyyMMddhhmmss" should {
    "be success　convert" in {
      val df = Seq(Test("20170102030405")).toDF
      val result = df.withColumn("str", Udfs.MakeDate.timestamp_yyyyMMddhhmmss(col("str"))).collect
      result(0).getAs[Timestamp]("str").toString mustBe "2017-01-02 03:04:05.0"
    }
  }

  "MakeDate timestamp_yyyyMMddhhmmssSSS" should {
    "be success　convert" in {
      val df = Seq(Test("20170102030405006")).toDF
      val result = df.withColumn("str", Udfs.MakeDate.timestamp_yyyyMMddhhmmssSSS(col("str"))).collect
      result(0).getAs[Timestamp]("str").toString mustBe "2017-01-02 03:04:05.006"
    }
  }

  "PostCodeNormalizer" should {
    import d2k.common.Udfs.PostCodeNormalizer._
    "normal end" when {
      "single" in {
        val df = Seq(PostCodeNormalizeData("300-1")).toDF
        val result = df.withColumn("RESULT", single($"POSTCODE1")).collect
        result(0).getAs[String]("RESULT") mustBe "300"
      }

      "parent" in {
        val df = Seq(PostCodeNormalizeData("400")).toDF
        val result = df.withColumn("RESULT", parent($"POSTCODE1")).collect
        result(0).getAs[String]("RESULT") mustBe "400"
      }

      "child" in {
        val df = Seq(PostCodeNormalizeData("55")).toDF
        val result = df.withColumn("RESULT", child($"POSTCODE1")).collect
        result(0).getAs[String]("RESULT") mustBe "55"
      }
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package org.apache.spark.sql

import org.scalatest.MustMatchers
import org.scalatest.WordSpec
import org.scalatest.BeforeAndAfter
import spark.common.SparkContexts
import org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.types.StringType
import spark.common.DbCtl
import org.apache.spark.sql.execution.datasources.jdbc._
import org.apache.spark.sql.sources.Filter
import java.sql.Timestamp
import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.sql.types.DecimalType
import org.apache.spark.sql.types.TimestampType
import org.apache.spark.sql.sources._

class JdbcCtlTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "JDBCRDD" should {
    "success connect db" in {
      val tableName = "sp02"
      val columns = Array("DT", "NUM5", "NUM52", "TSTMP", "VC", "CH")
      val dbCtl = new DbCtl()
      val result = JdbcCtl.readTable(dbCtl, tableName, columns).collect
      result.size mustBe 4
      result.head.schema.size mustBe 6
      val names = result.head.schema.map(_.name)
      names.exists(_ == "DT") mustBe true
      names.exists(_ == "NUM5") mustBe true
      names.exists(_ == "NUM52") mustBe true
      names.exists(_ == "TSTMP") mustBe true
      names.exists(_ == "VC") mustBe true
      names.exists(_ == "CH") mustBe true
    }

    "success connect db select columns" in {
      val tableName = "sp02"
      val columns = Array("DT", "VC", "CH")
      val dbCtl = new DbCtl()
      val result = JdbcCtl.readTable(dbCtl, tableName, columns).collect
      result.size mustBe 4
      result.head.schema.size mustBe 3
      val names = result.head.schema.map(_.name)
      names.exists(_ == "DT") mustBe true
      names.exists(_ == "NUM5") mustBe false
      names.exists(_ == "NUM52") mustBe false
      names.exists(_ == "TSTMP") mustBe false
      names.exists(_ == "VC") mustBe true
      names.exists(_ == "CH") mustBe true
    }

    "success connect db limit condition by where" in {
      val tableName = "sp02"
      val columns = Array("DT", "NUM5", "VC", "CH")
      val where = Array("NUM5 = '1000'", "NUM5 = '2000'")
      val dbCtl = new DbCtl()
      val result = JdbcCtl.readTable(dbCtl, tableName, columns, where).collect
      result.size mustBe 2
      result(0).getAs[java.math.BigDecimal]("NUM5").toString mustBe "1000"
      result(1).getAs[java.math.BigDecimal]("NUM5").toString mustBe "2000"
    }

    "success connect db limit condition by filter" in {
      val tableName = "sp02"
      val columns = Array("DT", "NUM5", "NUM52", "TSTMP", "VC", "CH")
      val where = Array("NUM5 = '1000'", "NUM5 = '2000'")
      val filter: Array[Filter] = Array(EqualTo("NUM52", "10.3"))
      val dbCtl = new DbCtl()
      val result = JdbcCtl.readTable(dbCtl, tableName, columns, where, filter).collect
      result.size mustBe 1
      result(0).getAs[java.math.BigDecimal]("NUM5").toString mustBe "1000"
      result(0).getAs[java.math.BigDecimal]("NUM52").toString mustBe "10.30"
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package spark.common

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import spark.common.SparkContexts.context
import context.implicits._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.Row
import org.apache.spark.sql.SaveMode
import scala.util.Try
import org.joda.time.DateTime
import java.sql.Timestamp
import java.sql.Date
import org.joda.time.format.DateTimeFormatter
import org.joda.time.format.DateTimeFormat
import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils
import java.util.Properties

class DbCtlTest extends WordSpec with MustMatchers with BeforeAndAfter {
  import org.apache.spark.sql.types._
  val currentTime = System.currentTimeMillis
  def d2s(dateMill: Long) = new DateTime(dateMill).toString("yyyy-MM-dd")
  def date2s(date: Date) = new DateTime(date).toString("yyyy-MM-dd")
  def d2s(date: Timestamp) = new DateTime(date).toString("yyyy-MM-dd")

  val dbtarget = DbCtl.dbInfo1

  val structTypeAnyType = StructType(Seq(
    StructField("KEY1", StringType), StructField("KEY2", DateType), StructField("KEY3", TimestampType),
    StructField("KEY4", IntegerType), StructField("KEY5", LongType),
    StructField("KEY6", DecimalType(5, 1)), StructField("VALUE", StringType)))

  val structType5 = StructType(Seq(
    StructField("TEST", StringType), StructField("TEST2", StringType), StructField("TEST3", StringType),
    StructField("TEST4", StringType), StructField("TEST5", StringType), StructField("DDATE", DateType), StructField("DTIMESTAMP", TimestampType)))
  val tableName5 = "deleteTest5"

  "DbCtl.insertAccelerated" should {
    val tableName = "insertNotExist3"
    "insert recs" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("AAA1", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff1"),
        Row("AAA2", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff2"),
        Row("AAA3", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff3"))), structTypeAnyType)

      val dbCtl = new DbCtl(dbtarget)
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
      val result = dbCtl.readTable(tableName).sort("KEY1").collect.toList
      result.length mustBe 3
      result(0).getAs[String]("KEY1") mustBe "AAA1"
      result(0).getAs[String]("VALUE") mustBe "fff1"
      result(1).getAs[String]("KEY1") mustBe "AAA2"
      result(1).getAs[String]("VALUE") mustBe "fff2"
      result(2).getAs[String]("KEY1") mustBe "AAA3"
      result(2).getAs[String]("VALUE") mustBe "fff3"
    }

    "insert recs with commit size" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("AAA1", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff1"),
        Row("AAA2", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff2"),
        Row("AAA3", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff3"))), structTypeAnyType)

      val dbCtl = new DbCtl(dbtarget.copy(commitSize = Some(2)))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
      val result = dbCtl.readTable(tableName).sort("KEY1").collect.toList
      result.length mustBe 3
      result(0).getAs[String]("KEY1") mustBe "AAA1"
      result(0).getAs[String]("VALUE") mustBe "fff1"
      result(1).getAs[String]("KEY1") mustBe "AAA2"
      result(1).getAs[String]("VALUE") mustBe "fff2"
      result(2).getAs[String]("KEY1") mustBe "AAA3"
      result(2).getAs[String]("VALUE") mustBe "fff3"
    }

    "be normal end. insert recs null pattern test." in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", null, null, null, null, null, null))), structTypeAnyType)

      val dbCtl = new DbCtl(dbtarget)
      dbCtl.insertAccelerated(df, "insertAcc", SaveMode.Overwrite)
      val result = dbCtl.readTable("insertAcc").sort("KEY1").collect.toList
      result.length mustBe 1
      result(0).getAs[String]("KEY1") mustBe "KEY1"
      result(0).getAs[Date]("KEY2") mustBe null
      result(0).getAs[Timestamp]("KEY3") mustBe null
      result(0).getAs[Integer]("KEY4") mustBe null
      result(0).getAs[Number]("KEY5") mustBe null
      result(0).getAs[Decimal]("KEY6") mustBe null
      result(0).getAs[String]("VALUE") mustBe null
    }

    "be normal end. insert recs null pattern no type test." in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("key1", null, null, null, null, null, null))), structTypeAnyType)
        .drop("KEY2").drop("KEY3").drop("KEY4").drop("KEY5").drop("KEY6").drop("VALUE")
        .withColumn("KEY2", lit(null))
        .withColumn("KEY3", lit(null))
        .withColumn("KEY4", lit(null))
        .withColumn("KEY5", lit(null))
        .withColumn("KEY6", lit(null))
        .withColumn("VALUE", lit(null))

      val dbCtl = new DbCtl(dbtarget)
      dbCtl.insertAccelerated(df, "insertAcc", SaveMode.Overwrite)
      val result = dbCtl.readTable("insertAcc").sort("KEY1").collect.toList
      result.length mustBe 1
      result(0).getAs[Date]("KEY2") mustBe null
      result(0).getAs[Timestamp]("KEY3") mustBe null
      result(0).getAs[Integer]("KEY4") mustBe null
      result(0).getAs[Number]("KEY5") mustBe null
      result(0).getAs[Decimal]("KEY6") mustBe null
      result(0).getAs[String]("VALUE") mustBe null
    }
  }

  "DbCtl.insertAccelerated with Direct Path Insert Mode" should {
    val tableName = "directInsert"
    "insert recs" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("AAA1", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff1"),
        Row("AAA2", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff2"),
        Row("AAA3", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff3"))), structTypeAnyType)

      val dbCtl = new DbCtl(dbtarget.copy(isDirectPathInsertMode = true))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
      val result = dbCtl.readTable(tableName).sort("KEY1").collect.toList
      result.length mustBe 3
      result(0).getAs[String]("KEY1") mustBe "AAA1"
      result(0).getAs[String]("VALUE") mustBe "fff1"
      result(1).getAs[String]("KEY1") mustBe "AAA2"
      result(1).getAs[String]("VALUE") mustBe "fff2"
      result(2).getAs[String]("KEY1") mustBe "AAA3"
      result(2).getAs[String]("VALUE") mustBe "fff3"
    }

    "insert recs with commit size" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("AAA1", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff1"),
        Row("AAA2", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff2"),
        Row("AAA3", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff3"))), structTypeAnyType)

      val dbCtl = new DbCtl(dbtarget.copy(commitSize = Some(2), isDirectPathInsertMode = true))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
      val result = dbCtl.readTable(tableName).sort("KEY1").collect.toList
      result.length mustBe 3
      result(0).getAs[String]("KEY1") mustBe "AAA1"
      result(0).getAs[String]("VALUE") mustBe "fff1"
      result(1).getAs[String]("KEY1") mustBe "AAA2"
      result(1).getAs[String]("VALUE") mustBe "fff2"
      result(2).getAs[String]("KEY1") mustBe "AAA3"
      result(2).getAs[String]("VALUE") mustBe "fff3"
    }
  }

  "DbCtl.insertNotExist" should {
    val tableName = "insertNotExist1"
    val tableName2 = "insertNotExist2"
    val tableName3 = "insertNotExist3"
    val tableName4 = "insertNotExist4"
    val structType = StructType(Seq(
      StructField("KEY1", StringType), StructField("KEY2", StringType), StructField("KEY3", StringType),
      StructField("KEY4", StringType), StructField("KEY5", StringType), StructField("VALUE", StringType)))
    val structTypeAnyType = StructType(Seq(
      StructField("KEY1", StringType), StructField("KEY2", DateType), StructField("KEY3", TimestampType),
      StructField("KEY4", IntegerType), StructField("KEY5", LongType),
      StructField("KEY6", DecimalType(5, 1)), StructField("VALUE", StringType)))

    "insert recs" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("AAA1", "BBB", "CCC", "ddd", "eee", "fff1"),
        Row("AAA2", "BBB", "CCC", "ddd", "eee", "fff2"),
        Row("AAA3", "BBB", "CCC", "ddd", "eee", "fff3"),
        Row("AAA1", "BBB", "CCC", "ddd", "eee", "fff4"),
        Row("AAA2", "BBB", "CCC", "ddd", "eee", "fff5"),
        Row("AAA3", "BBB", "CCC", "ddd", "eee", "fff6"))), structType)

      val dbCtl = new DbCtl(dbtarget)
      dbCtl.insertNotExists(df.repartition(1), tableName, Seq("KEY1", "KEY2", "KEY3", "KEY4", "KEY5"), SaveMode.Overwrite)
      val result = dbCtl.readTable(tableName).sort("KEY1").collect.toList
      result.length mustBe 3
      result(0).getAs[String]("KEY1") mustBe "AAA1"
      result(0).getAs[String]("VALUE") mustBe "fff1"
      result(1).getAs[String]("KEY1") mustBe "AAA2"
      result(1).getAs[String]("VALUE") mustBe "fff2"
      result(2).getAs[String]("KEY1") mustBe "AAA3"
      result(2).getAs[String]("VALUE") mustBe "fff3"
    }

    "insert recs. normal end pattern1" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("AAA1", "BBB", "CCC", "ddd", "eee", "fff1"),
        Row("AAA2", "BBB", "CCC", "ddd", "eee", "fff2"),
        Row("AAA3", "BBB", "CCC", "ddd", "eee", "fff3"),
        Row("AAA1", "BBB", "CCC", "ddd", "eee", "fff4"),
        Row("AAA2", "BBB", "CCC", "ddd", "eee", "fff5"),
        Row("AAA3", "BBB", "CCC", "ddd", "eee", "fff6"))), structType)

      val dbCtl = new DbCtl(dbtarget)
      dbCtl.insertNotExists(df.repartition(1), tableName, Seq("KEY1"), SaveMode.Overwrite)
      val result = dbCtl.readTable(tableName).sort("KEY1").collect.toList
      result.length mustBe 3
      result(0).getAs[String]("KEY1") mustBe "AAA1"
      result(0).getAs[String]("VALUE") mustBe "fff1"
      result(1).getAs[String]("KEY1") mustBe "AAA2"
      result(1).getAs[String]("VALUE") mustBe "fff2"
      result(2).getAs[String]("KEY1") mustBe "AAA3"
      result(2).getAs[String]("VALUE") mustBe "fff3"
    }

    "insert recs. normal end pattern2" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("AAA1", "BBB", "CCC", "ddd", "eee", "fff1"),
        Row("AAA2", "BBB", "CCC", "ddd", "eee", "fff2"),
        Row("AAA3", "BBB", "CCC", "ddd", "eee", "fff3"),
        Row("AAA1", "BBB", "CCC", "ddd", "eee", "fff4"),
        Row("AAA2", "BBB", "CCC", "ddd", "eee", "fff5"),
        Row("AAA3", "BBB", "CCC", "ddd", "eee", "fff6"))), structType)

      val dbCtl = new DbCtl(dbtarget)
      dbCtl.insertNotExists(df.repartition(1), tableName, Seq("KEY1", "KEY2"), SaveMode.Overwrite)
      val result = dbCtl.readTable(tableName).sort("KEY1").collect.toList
      result.length mustBe 3
      result(0).getAs[String]("KEY1") mustBe "AAA1"
      result(0).getAs[String]("VALUE") mustBe "fff1"
      result(1).getAs[String]("KEY1") mustBe "AAA2"
      result(1).getAs[String]("VALUE") mustBe "fff2"
      result(2).getAs[String]("KEY1") mustBe "AAA3"
      result(2).getAs[String]("VALUE") mustBe "fff3"
    }

    "insert recs. illegal pattern1" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("AAA1", "BBB", "CCC", "ddd", "eee", "fff1"),
        Row("AAA2", "BBB", "CCC", "ddd", "eee", "fff2"),
        Row("AAA3", "BBB", "CCC", "ddd", "eee", "fff3"),
        Row("AAA1", "BBB", "CCC", "ddd", "eee", "fff4"),
        Row("AAA2", "BBB", "CCC", "ddd", "eee", "fff5"),
        Row("AAA3", "BBB", "CCC", "ddd", "eee", "fff6"))), structType)

      val dbCtl = new DbCtl(dbtarget)

      try {
        dbCtl.insertNotExists(df.repartition(1), tableName, Seq("KEY1", "KEY3"), SaveMode.Overwrite)
        fail
      } catch {
        case t: Throwable => t.printStackTrace()
      }
    }

    "insert recs. illegal pattern2" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("AAA1", "BBB", "CCC", "ddd", "eee", "fff1"),
        Row("AAA2", "BBB", "CCC", "ddd", "eee", "fff2"),
        Row("AAA3", "BBB", "CCC", "ddd", "eee", "fff3"),
        Row("AAA1", "BBB", "CCC", "ddd", "eee", "fff4"),
        Row("AAA2", "BBB", "CCC", "ddd", "eee", "fff5"),
        Row("AAA3", "BBB", "CCC", "ddd", "eee", "fff6"))), structType)

      val dbCtl = new DbCtl(dbtarget)

      try {
        dbCtl.insertNotExists(df.repartition(1), tableName, Seq("KEY2"), SaveMode.Overwrite)
        fail
      } catch {
        case t: Throwable => t.printStackTrace()
      }
    }

    "insert recs.　reverse　keys sequence test" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("AAA1", "BBB", "CCC", "ddd", "eee", "fff1"),
        Row("AAA2", "BBB", "CCC", "ddd", "eee", "fff2"),
        Row("AAA3", "BBB", "CCC", "ddd", "eee", "fff3"),
        Row("AAA1", "BBB", "CCC", "ddd", "eee", "fff4"),
        Row("AAA2", "BBB", "CCC", "ddd", "eee", "fff5"),
        Row("AAA3", "BBB", "CCC", "ddd", "eee", "fff6"))), structType)

      val dbCtl = new DbCtl(dbtarget)
      dbCtl.insertNotExists(df.repartition(1), tableName2, Seq("KEY5", "KEY4", "KEY3", "KEY2", "KEY1"), SaveMode.Overwrite)
      val result = dbCtl.readTable(tableName2).sort("KEY1").collect.toList
      result.length mustBe 3
      result(0).getAs[String]("KEY1") mustBe "AAA1"
      result(0).getAs[String]("VALUE") mustBe "fff1"
      result(1).getAs[String]("KEY1") mustBe "AAA2"
      result(1).getAs[String]("VALUE") mustBe "fff2"
      result(2).getAs[String]("KEY1") mustBe "AAA3"
      result(2).getAs[String]("VALUE") mustBe "fff3"
    }

    "insert recs.　any type test." in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("AAA1", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff1"),
        Row("AAA2", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff2"),
        Row("AAA3", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff3"),
        Row("AAA1", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff4"),
        Row("AAA2", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff5"),
        Row("AAA3", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff6"))), structTypeAnyType)

      val dbCtl = new DbCtl(dbtarget)
      dbCtl.insertNotExists(df.repartition(1), tableName3, Seq("KEY1", "KEY2", "KEY3", "KEY4", "KEY5", "KEY6"), SaveMode.Overwrite)
      val result = dbCtl.readTable(tableName3).sort("KEY1").collect.toList
      result.length mustBe 3
      result(0).getAs[String]("KEY1") mustBe "AAA1"
      result(0).getAs[String]("VALUE") mustBe "fff1"
      result(1).getAs[String]("KEY1") mustBe "AAA2"
      result(1).getAs[String]("VALUE") mustBe "fff2"
      result(2).getAs[String]("KEY1") mustBe "AAA3"
      result(2).getAs[String]("VALUE") mustBe "fff3"
    }

    "insert recs.　append test." in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("AAA1", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff1"),
        Row("AAA2", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff2"),
        Row("AAA3", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff3"),
        Row("AAA1", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff4"),
        Row("AAA2", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff5"),
        Row("AAA3", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff6"))), structTypeAnyType)

      val dbCtl = new DbCtl(dbtarget)
      dbCtl.insertNotExists(df.repartition(1), tableName3, Seq("KEY1", "KEY2", "KEY3", "KEY4", "KEY5", "KEY6"), SaveMode.Overwrite)

      val df2 = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("AAA4", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff7"),
        Row("AAA5", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff8"),
        Row("AAA6", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fff9"),
        Row("AAA4", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fffA"),
        Row("AAA5", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fffB"),
        Row("AAA6", new Date(currentTime), new Timestamp(currentTime), 10, 100L, BigDecimal(100.1), "fffC"))), structTypeAnyType)

      dbCtl.insertNotExists(df2.repartition(1), tableName3, Seq("KEY1", "KEY2", "KEY3", "KEY4", "KEY5", "KEY6"), SaveMode.Append)
      val result = dbCtl.readTable(tableName3).sort("KEY1").collect.toList
      result.length mustBe 6
      result(0).getAs[String]("KEY1") mustBe "AAA1"
      result(0).getAs[String]("VALUE") mustBe "fff1"
      result(1).getAs[String]("KEY1") mustBe "AAA2"
      result(1).getAs[String]("VALUE") mustBe "fff2"
      result(2).getAs[String]("KEY1") mustBe "AAA3"
      result(2).getAs[String]("VALUE") mustBe "fff3"

      result(3).getAs[String]("KEY1") mustBe "AAA4"
      result(3).getAs[String]("VALUE") mustBe "fff7"
      result(4).getAs[String]("KEY1") mustBe "AAA5"
      result(4).getAs[String]("VALUE") mustBe "fff8"
      result(5).getAs[String]("KEY1") mustBe "AAA6"
      result(5).getAs[String]("VALUE") mustBe "fff9"
    }

    "be normal end. insert recs null pattern test." in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", null, null, null, null, null, null))), structTypeAnyType)
      val dbCtl = new DbCtl(dbtarget)
      dbCtl.insertAccelerated(df, tableName4, SaveMode.Overwrite)
      val df2 = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY2", null, null, null, null, null, null))), structTypeAnyType)

      dbCtl.insertNotExists(df2.repartition(1), tableName4, Seq("KEY1"), SaveMode.Append)
      val result = dbCtl.readTable(tableName4).sort("KEY1").collect.toList
      result.length mustBe 2
      result(1).getAs[String]("KEY1") mustBe "KEY2"
      result(1).getAs[Date]("KEY2") mustBe null
      result(1).getAs[Timestamp]("KEY3") mustBe null
      result(1).getAs[Integer]("KEY4") mustBe null
      result(1).getAs[Number]("KEY5") mustBe null
      result(1).getAs[Decimal]("KEY6") mustBe null
      result(1).getAs[String]("VALUE") mustBe null
    }
  }

  "DbCtl.deleteRecords" should {
    val tableName = "DELETETEST"
    val structType = StructType(Seq(
      StructField("TEST", StringType), StructField("DDATE", DateType), StructField("DTIMESTAMP", TimestampType)))
    "delete one rec" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("AAA", new Date(new DateTime().getMillis), new Timestamp(new DateTime().getMillis)),
          Row("BBB", new Date(new DateTime().getMillis), new Timestamp(new DateTime().getMillis)),
          Row("CCC", new Date(new DateTime().getMillis), new Timestamp(new DateTime().getMillis)))), structType)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("BBB", new Date(new DateTime().getMillis), new Timestamp(new DateTime().getMillis)))), structType)
      dbCtl.deleteRecords(delDf, tableName, Set("TEST"))

      val result = dbCtl.readTable(tableName).sort("TEST").collect.toList
      result.length mustBe 2
      result(0).getAs[String]("TEST") mustBe "AAA"
      result(1).getAs[String]("TEST") mustBe "CCC"
    }

    "delete one rec. over 5 columns" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("AAA", "AAA2", "AAA3", "AAA4", "AAA5", new Date(new DateTime().getMillis), new Timestamp(new DateTime().getMillis)),
          Row("BBB", "BBB2", "BBB3", "BBB4", "BBB5", new Date(new DateTime().getMillis), new Timestamp(new DateTime().getMillis)),
          Row("CCC", "CCC2", "CCC3", "CCC4", "CCC5", new Date(new DateTime().getMillis), new Timestamp(new DateTime().getMillis)))), structType5)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.autoCreateTable(tableName5)
        df.writeTable(tableName5, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("BBB", "BBB2", "BBB3", "BBB4", "BBB5",
          new Date(new DateTime().getMillis), new Timestamp(new DateTime().getMillis)))), structType5)
      dbCtl.deleteRecords(delDf, tableName5, Set("TEST", "TEST2", "TEST3", "TEST4", "TEST5"))

      val result = dbCtl.readTable(tableName5).sort("TEST").collect.toList
      result.length mustBe 2
      result(0).getAs[String]("TEST") mustBe "AAA"
      result(1).getAs[String]("TEST") mustBe "CCC"
    }

    "delete all recs" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("AAA", new Date(new DateTime().getMillis), new Timestamp(new DateTime().getMillis)),
          Row("BBB", new Date(new DateTime().getMillis), new Timestamp(new DateTime().getMillis)),
          Row("CCC", new Date(new DateTime().getMillis), new Timestamp(new DateTime().getMillis)))), structType)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName, SaveMode.Overwrite)
        dbCtl.readTable(tableName).show
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("AAA"), Row("BBB"), Row("CCC"))), StructType(Seq(StructField("TEST", StringType))))
      dbCtl.deleteRecords(delDf, tableName, Set("TEST"))

      val result = dbCtl.readTable(tableName).collect
      result.length mustBe 0
    }

    "delete Key by date" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("AAA", new Date(currentTime - 10000000000L), new Timestamp(currentTime - 1000)),
          Row("BBB", new Date(currentTime), new Timestamp(currentTime - 1000)),
          Row("CCC", new Date(currentTime - 10000000000L), new Timestamp(currentTime - 1000)))), structType)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("BBB", new Date(currentTime), new Timestamp(currentTime)))), structType)
      dbCtl.deleteRecords(delDf, tableName, Set("DDATE"))

      val result = dbCtl.readTable(tableName).collect.toList.sortBy(_.getAs[String]("TEST"))
      result.length mustBe 2
      result(0).getAs[String]("TEST") mustBe "AAA"
      result(1).getAs[String]("TEST") mustBe "CCC"
    }

    "delete Key by timestamp" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("AAA", new Date(currentTime - 10000000000L), new Timestamp(currentTime - 1000)),
          Row("BBB", new Date(currentTime - 10000000000L), new Timestamp(currentTime)),
          Row("CCC", new Date(currentTime - 10000000000L), new Timestamp(currentTime - 1000)))), structType)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.repartition(1).writeTable(tableName, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("BBB", new Date(currentTime), new Timestamp(currentTime)))), structType)
      dbCtl.deleteRecords(delDf.repartition(1), tableName, Set("DTIMESTAMP"))

      val result = dbCtl.readTable(tableName).collect.toList.sortBy(_.getAs[String]("TEST"))
      result.length mustBe 2
      result(0).getAs[String]("TEST") mustBe "AAA"
      result(1).getAs[String]("TEST") mustBe "CCC"
    }
  }

  "DbCtl.updateRecords" should {
    val tableName = "updatetest"
    val tableName2 = "updatetest2"
    val stringStructType = StructType(Seq(
      StructField("KEY", StringType),
      StructField("V1", StringType),
      StructField("V2", StringType)))

    val anyStructType = StructType(Seq(
      StructField("KEY", StringType),
      StructField("DDATE", DateType),
      StructField("DTIMESTAMP", TimestampType)))

    val stringStructType7Columns = StructType(Seq(
      StructField("KEY", StringType),
      StructField("V1", StringType),
      StructField("V2", StringType),
      StructField("V3", StringType),
      StructField("V4", StringType),
      StructField("V5", StringType),
      StructField("V6", StringType)))

    val datetimeFormatter = DateTimeFormat.forPattern("yyyyMMdd")
    "update one rec" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", new Date(currentTime), new Timestamp(currentTime)),
          Row("KEY2", new Date(currentTime), new Timestamp(currentTime)),
          Row("KEY3", new Date(currentTime), new Timestamp(currentTime)))), anyStructType)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName2, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY2", new Date(currentTime - 1000000000L), new Timestamp(currentTime - 1000L)))), anyStructType)
      dbCtl.updateRecords(delDf, tableName2, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName2).sort("KEY").collect.toList
      result.length mustBe 3
      result(0).getAs[String]("KEY") mustBe "KEY1"
      d2s(result(0).getAs[Timestamp]("DDATE")) mustBe d2s(currentTime)
      result(0).getAs[Timestamp]("DTIMESTAMP").getTime mustBe currentTime
      result(1).getAs[String]("KEY") mustBe "KEY2"
      d2s(result(1).getAs[Timestamp]("DDATE")) mustBe d2s(currentTime - 1000000000L)
      result(1).getAs[Timestamp]("DTIMESTAMP").getTime mustBe (currentTime - 1000L)
      result(2).getAs[String]("KEY") mustBe "KEY3"
      d2s(result(2).getAs[Timestamp]("DDATE")) mustBe d2s(currentTime)
      result(2).getAs[Timestamp]("DTIMESTAMP").getTime mustBe currentTime
    }

    def testDate(day: Int) = DateTime.parse(s"2016-1-${day}").getMillis
    "update one rec. over 5 columns" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("AAA", "AAA2", "AAA3", "AAA4", "AAA5", new Date(testDate(1)), new Timestamp(testDate(10))),
          Row("BBB", "BBB2", "BBB3", "BBB4", "BBB5", new Date(testDate(2)), new Timestamp(testDate(20))),
          Row("CCC", "CCC2", "CCC3", "CCC4", "CCC5", new Date(testDate(3)), new Timestamp(testDate(30))))), structType5)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.autoCreateTable(tableName5)
        df.writeTable(tableName5, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val updDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("BBB", "BBB2", "BBB3", "BBB4", "BBB5",
          new Date(testDate(15)), new Timestamp(testDate(25))))), structType5)
      dbCtl.updateRecords(updDf, tableName5, Set("TEST", "TEST2", "TEST3", "TEST4", "TEST5"), Set.empty, "")

      def day(date: Timestamp) = new DateTime(date.getTime).getDayOfMonth
      val result = dbCtl.readTable(tableName5).sort("TEST").collect.toList
      result.length mustBe 3
      result(0).getAs[String]("TEST") mustBe "AAA"
      day(result(0).getAs[Timestamp]("DDATE")) mustBe 1
      day(result(0).getAs[Timestamp]("DTIMESTAMP")) mustBe 10
      result(1).getAs[String]("TEST") mustBe "BBB"
      day(result(1).getAs[Timestamp]("DDATE")) mustBe 15
      day(result(1).getAs[Timestamp]("DTIMESTAMP")) mustBe 25
      result(2).getAs[String]("TEST") mustBe "CCC"
      day(result(2).getAs[Timestamp]("DDATE")) mustBe 3
      day(result(2).getAs[Timestamp]("DTIMESTAMP")) mustBe 30
    }

    val tblName7 = "col7"
    "update one rec. over 5 columns ignore keys add #10943" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", "a01", "a02", "a03", "a04", "a05", "a06"),
          Row("KEY2", "b01", "b02", "b03", "b04", "b05", "b07"))), stringStructType7Columns)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.autoCreateTable(tblName7)
        df.writeTable(tblName7, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val updDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "x01", "x02", "x03", "x04", "x05", "x06"),
        Row("KEY2", "y01", "y02", "y03", "y04", "y05", null))), stringStructType7Columns)
      dbCtl.updateRecords(updDf, tblName7, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tblName7).sort("KEY").collect.toList
      result.length mustBe 2
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("V1") mustBe "x01"
      result(0).getAs[String]("V2") mustBe "x02"
      result(0).getAs[String]("V3") mustBe "x03"
      result(0).getAs[String]("V4") mustBe "x04"
      result(0).getAs[String]("V5") mustBe "x05"
      result(0).getAs[String]("V6") mustBe "x06"

      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("V1") mustBe "y01"
      result(1).getAs[String]("V2") mustBe "y02"
      result(1).getAs[String]("V3") mustBe "y03"
      result(1).getAs[String]("V4") mustBe "y04"
      result(1).getAs[String]("V5") mustBe "y05"
      result(1).getAs[String]("V6") mustBe null
    }

    "update multi keys" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", new Date(currentTime - 9000000000L), new Timestamp(currentTime)),
          Row("KEY1", new Date(currentTime - 1000000000L), new Timestamp(currentTime)),
          Row("KEY1", new Date(currentTime), new Timestamp(currentTime)))), anyStructType)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName2, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", new Date(currentTime - 1000000000L), new Timestamp(currentTime - 1000L)),
        Row("KEY1", new Date(currentTime - 9000000000L), new Timestamp(currentTime - 2000L)))), anyStructType)
      dbCtl.updateRecords(delDf, tableName2, Set("KEY", "DDATE"), Set.empty, "")

      val result = dbCtl.readTable(tableName2).sort("KEY", "DDATE").collect.toList
      result.length mustBe 3
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[Timestamp]("DTIMESTAMP").getTime mustBe (currentTime - 2000L)
      result(1).getAs[String]("KEY") mustBe "KEY1"
      result(1).getAs[Timestamp]("DTIMESTAMP").getTime mustBe (currentTime - 1000L)
      result(2).getAs[String]("KEY") mustBe "KEY1"
      result(2).getAs[Timestamp]("DTIMESTAMP").getTime mustBe currentTime
    }

    "update all rec" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", "a1", "a2"), Row("KEY2", "b1", "b2"), Row("KEY3", "c1", "c2"))), stringStructType)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx1", "yyy1"), Row("KEY2", "xxx2", "yyy2"), Row("KEY3", "xxx3", "yyy3"))), stringStructType)
      dbCtl.updateRecords(delDf, tableName, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName).collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 3
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("V1") mustBe "xxx1"
      result(0).getAs[String]("V2") mustBe "yyy1"
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("V1") mustBe "xxx2"
      result(1).getAs[String]("V2") mustBe "yyy2"
      result(2).getAs[String]("KEY") mustBe "KEY3"
      result(2).getAs[String]("V1") mustBe "xxx3"
      result(2).getAs[String]("V2") mustBe "yyy3"
    }

    "update deleted one column" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", "a1", "a2"), Row("KEY2", "b1", "b2"), Row("KEY3", "c1", "c2"))), stringStructType)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx1", "yyy1"), Row("KEY2", "xxx2", "yyy2"), Row("KEY3", "xxx3", "yyy3"))), stringStructType)
        .drop("V1")
      dbCtl.updateRecords(delDf, tableName, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName).collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 3
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("V1") mustBe "a1"
      result(0).getAs[String]("V2") mustBe "yyy1"
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("V1") mustBe "b1"
      result(1).getAs[String]("V2") mustBe "yyy2"
      result(2).getAs[String]("KEY") mustBe "KEY3"
      result(2).getAs[String]("V1") mustBe "c1"
      result(2).getAs[String]("V2") mustBe "yyy3"
    }

    "nothing" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", "a1", "a2"), Row("KEY2", "b1", "b2"), Row("KEY3", "c1", "c2"))), stringStructType)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY4", "xxx1", "yyy1"), Row("KEY5", "xxx2", "yyy2"), Row("KEY6", "xxx3", "yyy3"))), stringStructType)
      dbCtl.updateRecords(delDf, tableName, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName).collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 3
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("V1") mustBe "a1"
      result(0).getAs[String]("V2") mustBe "a2"
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("V1") mustBe "b1"
      result(1).getAs[String]("V2") mustBe "b2"
      result(2).getAs[String]("KEY") mustBe "KEY3"
      result(2).getAs[String]("V1") mustBe "c1"
      result(2).getAs[String]("V2") mustBe "c2"
    }

    "be normal end. null pattern test." in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", null, null, null, null, null, null))), structTypeAnyType)
      val dbCtl = new DbCtl(dbtarget)
      dbCtl.insertAccelerated(df, "insertAcc", SaveMode.Overwrite)
      val df2 = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", null, null, null, null, null, null))), structTypeAnyType)

      dbCtl.updateRecords(df2, "insertAcc", Set("KEY1"), Set.empty, "")
      val result = dbCtl.readTable("insertAcc").sort("KEY1").collect.toList
      result.length mustBe 1
      result(0).getAs[String]("KEY1") mustBe "KEY1"
      result(0).getAs[Date]("KEY2") mustBe null
      result(0).getAs[Timestamp]("KEY3") mustBe null
      result(0).getAs[Integer]("KEY4") mustBe null
      result(0).getAs[Number]("KEY5") mustBe null
      result(0).getAs[Decimal]("KEY6") mustBe null
      result(0).getAs[String]("VALUE") mustBe null
    }

    "be normal end. null pattern no type test." in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("key1", null, null, null, null, null, null))), structTypeAnyType)
      val dbCtl = new DbCtl(dbtarget)
      dbCtl.insertAccelerated(df, "insertAcc", SaveMode.Overwrite)
      val df2 = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("key1", null, null, null, null, null, null))), structTypeAnyType)
        .drop("KEY2").drop("KEY3").drop("KEY4").drop("KEY5").drop("KEY6").drop("VALUE")
        .withColumn("KEY2", lit(null))
        .withColumn("KEY3", lit(null))
        .withColumn("KEY4", lit(null))
        .withColumn("KEY5", lit(null))
        .withColumn("KEY6", lit(null))
        .withColumn("VALUE", lit(null))

      dbCtl.updateRecords(df2, "insertAcc", Set("KEY1"), Set.empty, "")
      val result = dbCtl.readTable("insertAcc").sort("KEY1").collect.toList
      result.length mustBe 1
      result(0).getAs[Date]("KEY2") mustBe null
      result(0).getAs[Timestamp]("KEY3") mustBe null
      result(0).getAs[Integer]("KEY4") mustBe null
      result(0).getAs[Number]("KEY5") mustBe null
      result(0).getAs[Decimal]("KEY6") mustBe null
      result(0).getAs[String]("VALUE") mustBe null
    }
  }

  "DbCtl.upsertRecords" should {
    val tableName = "upsertTest1"
    val tableName2 = "upsertTest2"
    val structType = StructType(Seq(
      StructField("KEY", StringType),
      StructField("V1", StringType),
      StructField("V2", StringType)))

    val anyStructType = StructType(Seq(
      StructField("KEY", StringType),
      StructField("DDATE", DateType),
      StructField("DTIMESTAMP", TimestampType)))
    "update one rec" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", new Date(currentTime), new Timestamp(currentTime)),
          Row("KEY2", new Date(currentTime), new Timestamp(currentTime)),
          Row("KEY3", new Date(currentTime), new Timestamp(currentTime)))), anyStructType)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName2, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY2", new Date(currentTime - 1000000000L), new Timestamp(currentTime - 1000L)))), anyStructType)
      dbCtl.upsertRecords(delDf, tableName2, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName2).sort("KEY").collect.toList
      result.length mustBe 3
      result(0).getAs[String]("KEY") mustBe "KEY1"
      d2s(result(0).getAs[Timestamp]("DDATE")) mustBe d2s(currentTime)
      result(0).getAs[Timestamp]("DTIMESTAMP").getTime mustBe currentTime
      result(1).getAs[String]("KEY") mustBe "KEY2"
      d2s(result(1).getAs[Timestamp]("DDATE")) mustBe d2s(currentTime - 1000000000L)
      result(1).getAs[Timestamp]("DTIMESTAMP").getTime mustBe (currentTime - 1000L)
      result(2).getAs[String]("KEY") mustBe "KEY3"
      d2s(result(2).getAs[Timestamp]("DDATE")) mustBe d2s(currentTime)
      result(2).getAs[Timestamp]("DTIMESTAMP").getTime mustBe currentTime
    }

    "update multi keys" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", "a1", "a2"), Row("KEY1", "b1", "b2"), Row("KEY1", "c1", "c2"))), structType)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "a1", "yyy"))), structType)
      dbCtl.upsertRecords(delDf, tableName, Set("KEY", "V1"), Set.empty, "")

      val result = dbCtl.readTable(tableName).collect.toList.sortBy(x => (x.getAs[String]("KEY"), x.getAs[String]("V1")))
      result.length mustBe 3
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("V1") mustBe "a1"
      result(0).getAs[String]("V2") mustBe "yyy"
      result(1).getAs[String]("KEY") mustBe "KEY1"
      result(1).getAs[String]("V1") mustBe "b1"
      result(1).getAs[String]("V2") mustBe "b2"
      result(2).getAs[String]("KEY") mustBe "KEY1"
      result(2).getAs[String]("V1") mustBe "c1"
      result(2).getAs[String]("V2") mustBe "c2"
    }

    "update deleted one column and Insert" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", "a1", "a2"), Row("KEY2", "b1", "b2"), Row("KEY3", "c1", "c2"))), structType)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx1", "yyy1"), Row("KEY2", "xxx2", "yyy2"), Row("KEY3", "xxx3", "yyy3"), Row("KEY4", "xxx4", "yyy4"))), structType)
        .drop("v1")
      dbCtl.upsertRecords(delDf, tableName, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName).collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 4
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("V1") mustBe "a1"
      result(0).getAs[String]("V2") mustBe "yyy1"
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("V1") mustBe "b1"
      result(1).getAs[String]("V2") mustBe "yyy2"
      result(2).getAs[String]("KEY") mustBe "KEY3"
      result(2).getAs[String]("V1") mustBe "c1"
      result(2).getAs[String]("V2") mustBe "yyy3"
      result(3).getAs[String]("KEY") mustBe "KEY4"
      result(3).getAs[String]("V1") mustBe " "
      result(3).getAs[String]("V2") mustBe "yyy4"
    }

    "update all rec" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", "a1", "a2"), Row("KEY2", "b1", "b2"), Row("KEY3", "c1", "c2"))), structType)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx1", "yyy1"), Row("KEY2", "xxx2", "yyy2"), Row("KEY3", "xxx3", "yyy3"))), structType)
      dbCtl.upsertRecords(delDf, tableName, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName).collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 3
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("V1") mustBe "xxx1"
      result(0).getAs[String]("V2") mustBe "yyy1"
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("V1") mustBe "xxx2"
      result(1).getAs[String]("V2") mustBe "yyy2"
      result(2).getAs[String]("KEY") mustBe "KEY3"
      result(2).getAs[String]("V1") mustBe "xxx3"
      result(2).getAs[String]("V2") mustBe "yyy3"
    }

    "update ignore column" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", "a1", "a2"), Row("KEY2", "b1", "b2"), Row("KEY3", "c1", "c2"))), structType)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", "xxx1", "yyy1"), Row("KEY2", "xxx2", "yyy2"), Row("KEY3", "xxx3", "yyy3"))), structType)
      dbCtl.upsertRecords(delDf, tableName, Set("KEY"), Set("V1"), "")

      val result = dbCtl.readTable(tableName).collect.toList.sortBy(_.getAs[String]("KEY"))
      result.length mustBe 3
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[String]("V1") mustBe "a1"
      result(0).getAs[String]("V2") mustBe "yyy1"
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[String]("V1") mustBe "b1"
      result(1).getAs[String]("V2") mustBe "yyy2"
      result(2).getAs[String]("KEY") mustBe "KEY3"
      result(2).getAs[String]("V1") mustBe "c1"
      result(2).getAs[String]("V2") mustBe "yyy3"
    }

    "insert" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", new Date(currentTime), new Timestamp(currentTime)))), anyStructType)

        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName2, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY2", new Date(currentTime + 1000L), new Timestamp(currentTime + 10000000000L)))), anyStructType)
      dbCtl.upsertRecords(delDf, tableName2, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName2).sort("KEY").collect.toList
      result.length mustBe 2
      result(0).getAs[String]("KEY") mustBe "KEY1"
      d2s(result(0).getAs[Timestamp]("DDATE")) mustBe d2s(currentTime)
      result(0).getAs[Timestamp]("DTIMESTAMP").getTime mustBe (currentTime)
      result(1).getAs[String]("KEY") mustBe "KEY2"
      d2s(result(1).getAs[Timestamp]("DDATE")) mustBe d2s(currentTime + 1000L)
      result(1).getAs[Timestamp]("DTIMESTAMP").getTime mustBe (currentTime + 10000000000L)
    }

    "throw db exception" in {
      val structType = StructType(Seq(
        StructField("KEY", StringType),
        StructField("nullCol", StringType)))

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY01", "AAA"), Row("KEY02", null))), structType)
      try {
        dbCtl.upsertRecords(delDf, "nulltest", Set("KEY"), Set.empty, "")
        fail()
      } catch {
        case t: Throwable => t.printStackTrace()
      }
    }

    "be normal end. insert null pattern test." in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", null, null, null, null, null, null))), structTypeAnyType)
      val dbCtl = new DbCtl(dbtarget)
      dbCtl.insertAccelerated(df, "insertAcc", SaveMode.Overwrite)
      val df2 = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY2", null, null, null, null, null, null))), structTypeAnyType)

      dbCtl.upsertRecords(df2, "insertAcc", Set("KEY1"), Set.empty, "")
      val result = dbCtl.readTable("insertAcc").sort("KEY1").collect.toList
      result.length mustBe 2
      result(1).getAs[String]("KEY1") mustBe "KEY2"
      result(1).getAs[Date]("KEY2") mustBe null
      result(1).getAs[Timestamp]("KEY3") mustBe null
      result(1).getAs[Integer]("KEY4") mustBe null
      result(1).getAs[Number]("KEY5") mustBe null
      result(1).getAs[Decimal]("KEY6") mustBe null
      result(1).getAs[String]("VALUE") mustBe null
    }

    "be normal end. update null pattern test." in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", null, null, null, null, null, null))), structTypeAnyType)
      val dbCtl = new DbCtl(dbtarget)
      dbCtl.insertAccelerated(df, "insertAcc", SaveMode.Overwrite)
      val df2 = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", null, null, null, null, null, null))), structTypeAnyType)

      dbCtl.upsertRecords(df2, "insertAcc", Set("KEY1"), Set.empty, "")
      val result = dbCtl.readTable("insertAcc").sort("KEY1").collect.toList
      result.length mustBe 1
      result(0).getAs[String]("KEY1") mustBe "KEY1"
      result(0).getAs[Date]("KEY2") mustBe null
      result(0).getAs[Timestamp]("KEY3") mustBe null
      result(0).getAs[Integer]("KEY4") mustBe null
      result(0).getAs[Number]("KEY5") mustBe null
      result(0).getAs[Decimal]("KEY6") mustBe null
      result(0).getAs[String]("VALUE") mustBe null
    }

    "be normal end. null pattern no type test." in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("key1", null, null, null, null, null, null))), structTypeAnyType)
      val dbCtl = new DbCtl(dbtarget)
      dbCtl.insertAccelerated(df, "insertAcc", SaveMode.Overwrite)
      val df2 = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("key1", null, null, null, null, null, null))), structTypeAnyType)
        .drop("KEY2").drop("KEY3").drop("KEY4").drop("KEY5").drop("KEY6").drop("VALUE")
        .withColumn("KEY2", lit(null))
        .withColumn("KEY3", lit(null))
        .withColumn("KEY4", lit(null))
        .withColumn("KEY5", lit(null))
        .withColumn("KEY6", lit(null))
        .withColumn("VALUE", lit(null))

      dbCtl.upsertRecords(df2, "insertAcc", Set("KEY1"), Set.empty, "")
      val result = dbCtl.readTable("insertAcc").sort("KEY1").collect.toList
      result.length mustBe 1
      result(0).getAs[Date]("KEY2") mustBe null
      result(0).getAs[Timestamp]("KEY3") mustBe null
      result(0).getAs[Integer]("KEY4") mustBe null
      result(0).getAs[Number]("KEY5") mustBe null
      result(0).getAs[Decimal]("KEY6") mustBe null
      result(0).getAs[String]("VALUE") mustBe null
    }

  }

  "DateType Test" should {
    val tableName1 = "datetype"
    val structType = StructType(Seq(
      StructField("KEY", StringType), StructField("DDATE", DateType)))
    "insert" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", new Date(currentTime)))), structType)
      val dbCtl = new DbCtl(dbtarget)
      import dbCtl.implicits._
      df.writeTable(tableName1, SaveMode.Overwrite)

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 1
      d2s(result(0).getAs[Timestamp]("DDATE")) mustBe d2s(currentTime)
    }

    "update" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", new Date(currentTime)))), structType)
        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName1, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", new Date(currentTime - 1000L)))), structType)
      dbCtl.updateRecords(delDf, tableName1, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 1
      d2s(result(0).getAs[Timestamp]("DDATE")) mustBe d2s(currentTime - 1000L)
    }

    "upsert" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", new Date(currentTime)))), structType)
        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName1, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", new Date(currentTime - 1000L)),
        Row("KEY2", new Date(currentTime - 2000L)))), structType)
      dbCtl.upsertRecords(delDf, tableName1, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 2
      result(0).getAs[String]("KEY") mustBe "KEY1"
      d2s(result(0).getAs[Timestamp]("DDATE")) mustBe d2s(currentTime - 1000L)
      result(1).getAs[String]("KEY") mustBe "KEY2"
      d2s(result(1).getAs[Timestamp]("DDATE")) mustBe d2s(currentTime - 2000L)
    }

    "delete" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", new Date(currentTime)))), structType)
        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName1, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", new Date(currentTime)))), structType)
      dbCtl.deleteRecords(delDf, tableName1, Set("DDATE"))

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 0
    }
  }

  "TimestampTypeTest" should {
    val tableName1 = "DTIMESTAMP"
    val structType = StructType(Seq(
      StructField("KEY", StringType), StructField("DTIMESTAMP", TimestampType)))
    "insert" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", new Timestamp(currentTime)))), structType)
      val dbCtl = new DbCtl(dbtarget)
      import dbCtl.implicits._
      df.writeTable(tableName1, SaveMode.Overwrite)

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 1
      d2s(result(0).getAs[Timestamp]("DTIMESTAMP")) mustBe d2s(currentTime)
    }

    "update" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", new Timestamp(currentTime)))), structType)
        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName1, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", new Timestamp(currentTime - 1000L)))), structType)
      dbCtl.updateRecords(delDf, tableName1, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 1
      d2s(result(0).getAs[Timestamp]("DTIMESTAMP")) mustBe d2s(currentTime - 1000L)
    }

    "upsert" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", new Timestamp(currentTime)))), structType)
        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName1, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", new Timestamp(currentTime - 1000L)),
        Row("KEY2", new Timestamp(currentTime - 2000L)))), structType)
      dbCtl.upsertRecords(delDf, tableName1, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 2
      result(0).getAs[String]("KEY") mustBe "KEY1"
      d2s(result(0).getAs[Timestamp]("DTIMESTAMP")) mustBe d2s(currentTime - 1000L)
      result(1).getAs[String]("KEY") mustBe "KEY2"
      d2s(result(1).getAs[Timestamp]("DTIMESTAMP")) mustBe d2s(currentTime - 2000L)
    }

    "delete" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", new Timestamp(currentTime)))), structType)
        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName1, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", new Timestamp(currentTime)))), structType)
      dbCtl.deleteRecords(delDf, tableName1, Set("DTIMESTAMP"))

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 0
    }
  }

  "DecimalType Test" should {
    val tableName1 = "decimaltype"
    val structType = StructType(Seq(
      StructField("KEY", StringType), StructField("DDECIMAL", DecimalType(8, 0))))
    "insert" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", BigDecimal(100).bigDecimal))), structType)
      val dbCtl = new DbCtl(dbtarget)
      import dbCtl.implicits._
      df.writeTable(tableName1, SaveMode.Overwrite)

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 1
      result(0).getAs[java.math.BigDecimal]("DDECIMAL") mustBe BigDecimal(100).bigDecimal
    }

    "update" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", BigDecimal(100).bigDecimal))), structType)
        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName1, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", BigDecimal(200).bigDecimal))), structType)
      dbCtl.updateRecords(delDf, tableName1, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 1
      result(0).getAs[java.math.BigDecimal]("DDECIMAL") mustBe BigDecimal(200).bigDecimal
    }

    "upsert" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", BigDecimal(100).bigDecimal))), structType)
        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName1, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", BigDecimal(200).bigDecimal),
        Row("KEY2", BigDecimal(300).bigDecimal))), structType)
      dbCtl.upsertRecords(delDf, tableName1, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 2
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[java.math.BigDecimal]("DDECIMAL") mustBe BigDecimal(200).bigDecimal
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[java.math.BigDecimal]("DDECIMAL") mustBe BigDecimal(300).bigDecimal
    }

    "delete" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", BigDecimal(100).bigDecimal))), structType)
        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName1, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", BigDecimal(100).bigDecimal))), structType)
      dbCtl.deleteRecords(delDf, tableName1, Set("DDECIMAL"))

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 0
    }
  }

  "IntegerType Test" should {
    val tableName1 = "integertype"
    val structType = StructType(Seq(
      StructField("KEY", StringType), StructField("DINTEGER", IntegerType)))
    "insert" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", 100))), structType)
      val dbCtl = new DbCtl(dbtarget)
      import dbCtl.implicits._
      df.writeTable(tableName1, SaveMode.Overwrite)

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 1
      result(0).getAs[java.math.BigDecimal]("DINTEGER").toString mustBe "100"
    }

    "update" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", 100))), structType)
        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName1, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", 200))), structType)
      dbCtl.updateRecords(delDf, tableName1, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 1
      result(0).getAs[java.math.BigDecimal]("DINTEGER").toString mustBe "200"
    }

    "upsert" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", 100))), structType)
        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName1, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", 200),
        Row("KEY2", 300))), structType)
      dbCtl.upsertRecords(delDf, tableName1, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 2
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[java.math.BigDecimal]("DINTEGER").toString mustBe "200"
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[java.math.BigDecimal]("DINTEGER").toString mustBe "300"
    }

    "delete" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", 100))), structType)
        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName1, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", 100))), structType)
      dbCtl.deleteRecords(delDf, tableName1, Set("DINTEGER"))

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 0
    }
  }

  "LongType Test" should {
    val tableName1 = "LONGTYPE"
    val structType = StructType(Seq(
      StructField("KEY", StringType), StructField("DLONG", LongType)))
    "insert" in {
      val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", 100L))), structType)
      val dbCtl = new DbCtl(dbtarget)
      import dbCtl.implicits._
      df.writeTable(tableName1, SaveMode.Overwrite)

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 1
      result(0).getAs[Long]("DLONG").toString mustBe "100.0000000000"
    }

    "update" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", 100L))), structType)
        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName1, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", 200L))), structType)
      dbCtl.updateRecords(delDf, tableName1, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 1
      result(0).getAs[Long]("DLONG").toString mustBe "200.0000000000"
    }

    "upsert" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", 100L))), structType)
        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName1, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", 200L),
        Row("KEY2", 300L))), structType)
      dbCtl.upsertRecords(delDf, tableName1, Set("KEY"), Set.empty, "")

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 2
      result(0).getAs[String]("KEY") mustBe "KEY1"
      result(0).getAs[Long]("DLONG").toString mustBe "200.0000000000"
      result(1).getAs[String]("KEY") mustBe "KEY2"
      result(1).getAs[Long]("DLONG").toString mustBe "300.0000000000"
    }

    "delete" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("KEY1", 100L))), structType)
        val dbCtl = new DbCtl(dbtarget)
        import dbCtl.implicits._
        df.writeTable(tableName1, SaveMode.Overwrite)
      }

      val dbCtl = new DbCtl(dbtarget)
      val delDf = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
        Row("KEY1", 100L))), structType)
      dbCtl.deleteRecords(delDf, tableName1, Set("DLONG"))

      val result = dbCtl.readTable(tableName1).sort("KEY").collect.toList
      result.length mustBe 0
    }
  }

  "DbCtl.writeTable" should {
    val tableName = "notExist"
    val structType = StructType(Seq(
      StructField("test", StringType)))
    "not Exist Table for Overwrite" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("AAA"), Row("BBB"), Row("CCC"))), structType)

        val dbCtl = new DbCtl
        import dbCtl.implicits._
        try {
          df.writeTable(tableName, SaveMode.Overwrite)
          fail
        } catch {
          case t: Throwable => t.printStackTrace()
        }
      }
    }

    "not Exist Table for Append" in {
      {
        val df = context.createDataFrame(SparkContexts.sc.makeRDD(Seq(
          Row("AAA"), Row("BBB"), Row("CCC"))), structType)

        val dbCtl = new DbCtl
        import dbCtl.implicits._
        try {
          df.writeTable(tableName, SaveMode.Overwrite)
          fail
        } catch {
          case t: Throwable => t.printStackTrace()
        }
      }
    }
  }

  "DbCtl readTable" should {
    "be success require column mode read all data　on single thread" in {
      val dbCtl = new DbCtl
      val tableName = "SP01"
      val columns = Array("DT", "NUM5", "NUM52", "TSTMP", "VC", "CH")
      val result = dbCtl.readTable(tableName, columns, DbCtl.readAllData).collect
      result.size mustBe 4
      result.head.schema.size mustBe 6
      val names = result.head.schema.map(_.name)
      names.exists(_ == "DT") mustBe true
      names.exists(_ == "NUM5") mustBe true
      names.exists(_ == "NUM52") mustBe true
      names.exists(_ == "TSTMP") mustBe true
      names.exists(_ == "VC") mustBe true
      names.exists(_ == "CH") mustBe true

      result.size mustBe 4
      result(0).getAs[java.math.BigDecimal]("NUM5").toString mustBe "1000"
      result(1).getAs[java.math.BigDecimal]("NUM5").toString mustBe "2000"
      result(2).getAs[java.math.BigDecimal]("NUM5").toString mustBe "3000"
    }

    "be success require column mode using where" in {
      val dbCtl = new DbCtl
      val tableName = "SP01"
      val columns = Array("DT", "NUM5", "NUM52", "TSTMP", "VC", "CH")
      val where = Array("NUM5 = '1000'", "NUM5 = '2000'")
      val result = dbCtl.readTable(tableName, columns, where).collect
      result.size mustBe 2
      result.head.schema.size mustBe 6
      val names = result.head.schema.map(_.name)
      names.exists(_ == "DT") mustBe true
      names.exists(_ == "NUM5") mustBe true
      names.exists(_ == "NUM52") mustBe true
      names.exists(_ == "TSTMP") mustBe true
      names.exists(_ == "VC") mustBe true
      names.exists(_ == "CH") mustBe true

      result.size mustBe 2
      result(0).getAs[java.math.BigDecimal]("NUM5").toString mustBe "1000"
      result(1).getAs[java.math.BigDecimal]("NUM5").toString mustBe "2000"
    }
  }

  "DbCtl truncateTable" should {
    val tableOwner = "TRANCATE_TEST_USER"
    val tableName1 = "TRANCATE_TEST_TABLE1"
    val tableName2 = "TRANCATE_TEST_TABLE2"
    val tableName3 = "TRANCATE_TEST_TABLE3"
    val errTableName = "DELETE_TEST_TABLE"
    val dbCtl = new DbCtl

    before {
      Try {
        dbCtl.execSql(tableOwner, s"CREATE USER ${tableOwner} IDENTIFIED BY ${tableOwner}")
      }

      Try {
        dbCtl.execSql(tableName1, s"CREATE TABLE ${tableName1} AS SELECT * FROM DUAL")
        dbCtl.execSql(tableName1, s"CREATE SYNONYM ${tableName1} FOR ${tableOwner}.${tableName1}")
        dbCtl.execSql(tableName1, s"CREATE PUBLIC SYNONYM ${tableName1} FOR ${tableOwner}.${tableName1}")
      }

      Try {
        dbCtl.execSql(tableName2, s"CREATE TABLE ${tableName2} AS SELECT * FROM DUAL")
      }

      Try {
        dbCtl.execSql(tableName3, s"CREATE TABLE ${tableName3} AS SELECT * FROM DUAL")
        dbCtl.execSql(tableName3, s"CREATE SYNONYM ${tableName3} FOR ${tableOwner}.${tableName3}")
        dbCtl.execSql(tableName3, s"CREATE PUBLIC SYNONYM ${tableName3} FOR ${tableOwner}.${tableName3}")
      }
    }

    "exists SYNONYM" in {
      dbCtl.clearTable(tableName1)
    }

    "not exists SYNONYM" in {
      dbCtl.clearTable(tableName2)
    }

    "Owner.TableName" in {
      dbCtl.clearTable(tableName3)
    }

    "delete execution" in {
      intercept[Exception] {
        dbCtl.clearTable(errTableName)
      }
    }
  }

  "DbCtl columnTypes" should {
    "be success" in {
      val props = new Properties
      props.put("user", dbtarget.user)
      props.put("password", dbtarget.password)
      props.put("charSet", dbtarget.charSet)
      val conn = JdbcUtils.createConnectionFactory(dbtarget.toOptions)()

      val dbCtl = new DbCtl(dbtarget)
      val result = dbCtl.columnTypes(conn, "SP01")
      println(result)
      import java.sql.{ Types => t }
      result("NUM52") mustBe t.DECIMAL
      result("VC") mustBe t.VARCHAR
      result("TSTMP") mustBe t.TIMESTAMP
      result("NUM5") mustBe t.DECIMAL
      result("CH") mustBe t.CHAR
      result("DT") mustBe t.TIMESTAMP
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package spark.common

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import spark.common.SparkContexts.context
import context.implicits._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.Row
import org.apache.spark.sql.SaveMode
import scala.util.Try
import org.joda.time.DateTime
import java.sql.Timestamp
import java.sql.Date
import org.joda.time.format.DateTimeFormatter
import org.joda.time.format.DateTimeFormat
import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils
import java.util.Properties

object DbCtlWithHintTest {
  case class Data(key: String, data: String)
}

class DbCtlWithHintTest extends WordSpec with MustMatchers with BeforeAndAfter {
  import DbCtlWithHintTest._
  import org.apache.spark.sql.types._
  val currentTime = System.currentTimeMillis
  val dbtarget = DbCtl.dbInfo1

  "with hint" should {
    val tableName = "hinttest"
    val dbCtl = new DbCtl(dbtarget)
    "insertAcc" in {
      dbCtl.insertAccelerated(Seq(Data("key", "d")).toDF, tableName, SaveMode.Overwrite, "TEST_HINT_INSERTACC")
    }

    "insertNotExists" in {
      dbCtl.insertNotExists(Seq(Data("key", "d")).toDF, tableName, Seq("key"), SaveMode.Overwrite, "TEST_HINT_NOT_EXISTS")
    }

    "insertDirectPathInsert" in {
      val dbCtl = new DbCtl(dbtarget.copy(isDirectPathInsertMode = true))
      dbCtl.insertAccelerated(Seq(Data("key", "d")).toDF, tableName, SaveMode.Overwrite, "TEST_HINT_DIRECTPATH")
    }

    "delete" in {
      val df = Seq(Data("key", "d")).toDF
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
      dbCtl.deleteRecords(df.select("key"), tableName, Set("key"), "TEST_HINT_DELETE")
    }

    "update" in {
      val df = Seq(Data("key", "d")).toDF
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
      dbCtl.updateRecords(df.withColumn("data", lit("d2")), tableName, Set("key"), Set.empty, "TEST_HINT_UPDATE")
    }

    "upsert" in {
      val df = Seq(Data("key", "d")).toDF
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
      val df2 = Seq(Data("key", "dx"), Data("key2", "d2")).toDF
      dbCtl.upsertRecords(df2, tableName, Set("key"), Set.empty, "TEST_HINT_UPSERT")
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package spark.common

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import org.apache.spark.sql.SaveMode

class DbInfoTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "from env read" should {
    "be success" in {
      println(DbCtl.dbInfo1.commitSize)
      println(DbCtl.dbInfo1.isDirectPathInsertMode)
    }
  }

  "prop test" should {
    val df = SparkContexts.context.emptyDataFrame
    val tableName = "largeData"
    "set env dpi" in {
      val dbCtl = new DbCtl(DbCtl.dbInfo1)
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
    }

    "set source dpi" in {
      val dbCtl = new DbCtl(DbCtl.dbInfo1.copy(isDirectPathInsertMode = false))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
    }

    "set source and prop" in {
      System.setProperty("DPI_MODE", "true")
      val dbCtl = new DbCtl(DbCtl.dbInfo1.copy(commitSize = Some(1000), isDirectPathInsertMode = false))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
    }

    "set env commitSize" in {
      val dbCtl = new DbCtl(DbCtl.dbInfo1.copy(isDirectPathInsertMode = false))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
    }

    "set source commitSize" in {
      val dbCtl = new DbCtl(DbCtl.dbInfo1.copy(commitSize = Some(1000), isDirectPathInsertMode = false))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
    }

    "set source and prop commitSize" in {
      System.setProperty("COMMIT_SIZE", "9999")
      val dbCtl = new DbCtl(DbCtl.dbInfo1.copy(commitSize = Some(1000), isDirectPathInsertMode = false))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
    }

    "set source fetchSize" in {
      val dbCtl = new DbCtl(DbCtl.dbInfo1.copy(fetchSize = Some(1000)))
      dbCtl.readTable(tableName)
    }

    "set source and prop fetchSize" in {
      System.setProperty("FETCH_SIZE", "10000")
      val dbCtl = new DbCtl(DbCtl.dbInfo1.copy(fetchSize = Some(1000)))
      dbCtl.readTable(tableName)
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package spark.common

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter

import org.apache.spark.sql.functions._
import SparkContexts.context.implicits._
import spark.common.DfCtl._
import implicits._
import scala.reflect.io.Path
import org.apache.spark.sql.Column
import scala.io.Source

case class DfCtlData(
  a: String = "a", b: String = "b", c: String = "c", d: String = "d", e: String = "e",
  a1: Int = 1, b2: Int = 2)
case class DfCtlCastData(
  NM_A: String = "100", NM_B: String = "200", STR_C: String = "300", STR_D: String = "400", E: String = "500")
class DfCtlTest extends WordSpec with MustMatchers with BeforeAndAfter {
  val df = Seq(DfCtlData()).toDF
  "implicit conv editors" when {
    "edit" should {
      "be success" in {
        val d = Seq(
          ("a", col("c")).e,
          ("b", $"c").e)
        val r = df ~> editColumns(d)
        r.collect.foreach { row =>
          row.getAs[String]("a") mustBe "c"
          row.getAs[String]("b") mustBe "c"
        }
      }
    }

    "edit seq" should {
      "be success" in {
        val d = Seq(
          ("a", col("c")),
          ("b", $"c")).e
        val r = df ~> editColumns(d)
        r.collect.foreach { row =>
          row.getAs[String]("a") mustBe "c"
          row.getAs[String]("b") mustBe "c"
        }
      }
    }

    "rename" should {
      "be success" in {
        val d = Seq(
          ("a" -> "x").r,
          ("b" -> "y").r)
        val r = df ~> editColumns(d)
        r.collect.foreach { row =>
          row.getAs[String]("x") mustBe "a"
          row.getAs[String]("y") mustBe "b"
          val names = row.schema.map(_.name)
          names.contains("a") mustBe false
          names.contains("b") mustBe false
        }
      }
    }

    "rename seq" should {
      "be success" in {
        val d = Seq(
          ("a" -> "x"),
          ("b" -> "y")).r
        val r = df ~> editColumns(d)
        r.collect.foreach { row =>
          row.getAs[String]("x") mustBe "a"
          row.getAs[String]("y") mustBe "b"
          val names = row.schema.map(_.name)
          names.contains("a") mustBe false
          names.contains("b") mustBe false
        }
      }
    }

    "delete" should {
      "be success" in {
        val d = Seq(
          "a".d,
          "b".d)
        val r = df ~> editColumns(d)
        r.collect.foreach { row =>
          val names = row.schema.map(_.name)
          names.contains("a") mustBe false
          names.contains("b") mustBe false
        }
      }
    }

    "delete seq" should {
      "be success" in {
        val d = Seq(
          "a",
          "b").d
        val r = df ~> editColumns(d)
        r.collect.foreach { row =>
          val names = row.schema.map(_.name)
          names.contains("a") mustBe false
          names.contains("b") mustBe false
        }
      }
    }

    "cast by name" should {
      val df = Seq(DfCtlCastData()).toDF
      "be success" when {
        "direct" in {
          val d = Seq(
            ("NM_A", "bigint").c,
            ("NM_B", "int").c)
          val r = df ~> editColumns(d)
          val scm = r.schema
          scm("NM_A").dataType.typeName mustBe "long"
          scm("NM_B").dataType.typeName mustBe "integer"
          scm("STR_C").dataType.typeName mustBe "string"
          scm("STR_D").dataType.typeName mustBe "string"
          scm("E").dataType.typeName mustBe "string"
        }

        "regex" in {
          val d = Seq(
            ("NM_.*", "bigint").cr)
          val r = df ~> editColumns(d)
          val scm = r.schema
          scm("NM_A").dataType.typeName mustBe "long"
          scm("NM_B").dataType.typeName mustBe "long"
          scm("STR_C").dataType.typeName mustBe "string"
          scm("STR_D").dataType.typeName mustBe "string"
          scm("E").dataType.typeName mustBe "string"
        }

        "mix" in {
          val d = Seq(
            ("NM_A", "bigint").c,
            ("STR_.*", "decimal").cr,
            ("NM_B", "int").c)

          val r = df ~> editColumns(d)
          val scm = r.schema
          scm("NM_A").dataType.typeName mustBe "long"
          scm("NM_B").dataType.typeName mustBe "integer"
          scm("STR_C").dataType.typeName mustBe "decimal(10,0)"
          scm("STR_D").dataType.typeName mustBe "decimal(10,0)"
          scm("E").dataType.typeName mustBe "string"
        }
      }
    }

    "cast seq by name" should {
      val df = Seq(DfCtlCastData()).toDF
      "be success" when {
        "direct" in {
          val d = Seq(
            ("NM_A", "bigint"),
            ("NM_B", "int")).c
          val r = df ~> editColumns(d)
          val scm = r.schema
          scm("NM_A").dataType.typeName mustBe "long"
          scm("NM_B").dataType.typeName mustBe "integer"
          scm("STR_C").dataType.typeName mustBe "string"
          scm("STR_D").dataType.typeName mustBe "string"
          scm("E").dataType.typeName mustBe "string"
        }

        "regex" in {
          val d = Seq(
            ("NM_.*", "bigint"),
            ("STR_.*", "int")).cr
          val r = df ~> editColumns(d)
          val scm = r.schema
          scm("NM_A").dataType.typeName mustBe "long"
          scm("NM_B").dataType.typeName mustBe "long"
          scm("STR_C").dataType.typeName mustBe "integer"
          scm("STR_D").dataType.typeName mustBe "integer"
          scm("E").dataType.typeName mustBe "string"
        }
      }
    }

    "applyAll by name" should {
      val df = Seq(DfCtlCastData()).toDF
      "be success" when {
        "direct" in {
          val d = Seq(
            ("NM_A", (_: Column).cast("bigint")).a,
            ("NM_B", (_: Column).cast("int")).a)
          val r = df ~> editColumns(d)
          val scm = r.schema
          scm("NM_A").dataType.typeName mustBe "long"
          scm("NM_B").dataType.typeName mustBe "integer"
          scm("STR_C").dataType.typeName mustBe "string"
          scm("STR_D").dataType.typeName mustBe "string"
          scm("E").dataType.typeName mustBe "string"
        }

        "regex" in {
          val d = Seq(
            ("NM_.*", (_: Column).cast("bigint")).ar)
          val r = df ~> editColumns(d)
          val scm = r.schema
          scm("NM_A").dataType.typeName mustBe "long"
          scm("NM_B").dataType.typeName mustBe "long"
          scm("STR_C").dataType.typeName mustBe "string"
          scm("STR_D").dataType.typeName mustBe "string"
          scm("E").dataType.typeName mustBe "string"
        }

        "mix" in {
          val d = Seq(
            ("NM_A", (_: Column).cast("bigint")).a,
            ("STR_.*", (_: Column).cast("decimal")).ar,
            ("NM_B", (_: Column).cast("int")).a)

          val r = df ~> editColumns(d)
          val scm = r.schema
          scm("NM_A").dataType.typeName mustBe "long"
          scm("NM_B").dataType.typeName mustBe "integer"
          scm("STR_C").dataType.typeName mustBe "decimal(10,0)"
          scm("STR_D").dataType.typeName mustBe "decimal(10,0)"
          scm("E").dataType.typeName mustBe "string"
        }
      }
    }

    "applyAll seq by name" should {
      val df = Seq(DfCtlCastData()).toDF
      "be success" when {
        "direct" in {
          val d = Seq(
            ("NM_A", (_: Column).cast("bigint")),
            ("NM_B", (_: Column).cast("int"))).a
          val r = df ~> editColumns(d)
          val scm = r.schema
          scm("NM_A").dataType.typeName mustBe "long"
          scm("NM_B").dataType.typeName mustBe "integer"
          scm("STR_C").dataType.typeName mustBe "string"
          scm("STR_D").dataType.typeName mustBe "string"
          scm("E").dataType.typeName mustBe "string"
        }

        "regex" in {
          val d = Seq(
            ("NM_.*", (_: Column).cast("bigint")),
            ("STR_.*", (_: Column).cast("int"))).ar
          val r = df ~> editColumns(d)
          val scm = r.schema
          scm("NM_A").dataType.typeName mustBe "long"
          scm("NM_B").dataType.typeName mustBe "long"
          scm("STR_C").dataType.typeName mustBe "integer"
          scm("STR_D").dataType.typeName mustBe "integer"
          scm("E").dataType.typeName mustBe "string"
        }
      }
    }
  }

  "editColumns" should {
    "success" in {
      val editData =
        Seq(
          ("a", col("c")).e,
          ("b", $"d").e,
          ("c" -> "y").r,
          ("d" -> "z", lit("000")).r,
          "e".d)
      val r = df ~> editColumns(editData)
      r.schema.length mustBe 6
      r.collect.foreach { row =>
        row.getAs[String]("a") mustBe "c"
        row.getAs[String]("b") mustBe "d"
        row.getAs[String]("y") mustBe "c"
        row.getAs[String]("z") mustBe "000"
        val names = row.schema.map(_.name)
        names.contains("c") mustBe false
        names.contains("d") mustBe false
        names.contains("e") mustBe false
      }
    }
  }

  "editColumnsAndSelect" should {
    "success" in {
      val d = Seq(
        ("a", col("c")).e,
        ("b", $"d").e,
        ("c" -> "y").r,
        ("d" -> "z", lit("000")).r,
        "e".d)
      val r = df ~> editColumnsAndSelect(d)
      r.schema.length mustBe 4
      r.collect.foreach { row =>
        row.getAs[String]("a") mustBe "c"
        row.getAs[String]("b") mustBe "d"
        row.getAs[String]("y") mustBe "c"
        row.getAs[String]("z") mustBe "000"
        val names = row.schema.map(_.name)
        names.contains("c") mustBe false
        names.contains("d") mustBe false
        names.contains("e") mustBe false
      }
    }
  }

  "selectMaxValue" should {
    val df = Seq(DfCtlData(a1 = 0), DfCtlData(a1 = 100)).toDF
    "success" in {
      val r = df ~> selectMaxValue(Seq("a"), Seq($"a1".desc))
      r.count mustBe 1
      r.collect.foreach { row =>
        row.getAs[Int]("a1") mustBe 100
      }
    }
  }

  "groupingAgg" should {
    val df = Seq(DfCtlData(a1 = 0), DfCtlData(a1 = 100)).toDF
    "success" in {
      val r = df ~> groupingAgg(Seq("a"), Seq(avg("a1") as "a1"))
      r.count mustBe 1
      r.collect.foreach { row =>
        row.getAs[Int]("a1") mustBe 50
      }
    }
  }

  "groupingSum" should {
    val df = Seq(DfCtlData(a1 = 1), DfCtlData(a1 = 100)).toDF
    "success" in {
      val r = df ~> groupingSum(Seq("a"), Seq("a1"))
      r.count mustBe 1
      r.collect.foreach { row =>
        row.getAs[Int]("a1") mustBe 101
      }
    }
  }

  "addColumnPrefix" should {
    val df = Seq(DfCtlData(a1 = 1), DfCtlData(a1 = 100)).toDF
    "success" in {
      val r = df ~> addColumnPrefix("XXX")
      r.collect.foreach { row =>
        val names = row.schema.map(_.name)
        names.contains("XXX_a") mustBe true
        names.contains("XXX_b") mustBe true
      }
    }
  }

  "dropColumnPrefix" should {
    val df = Seq(DfCtlData(a1 = 1), DfCtlData(a1 = 100)).toDF
    val addPrefixCol = df ~> editColumns(Seq(("XXX_a", $"a"), ("XXX_b", $"b")).e)
    "success" in {
      val r = addPrefixCol ~> dropColumnPrefix("XXX")
      r.collect.foreach { row =>
        val names = row.schema.map(_.name)
        names.contains("XXX_a") mustBe false
        names.contains("XXX_b") mustBe false
      }
    }
  }

  def checkData(path: String, target: String) = {
    val basePath = Path(path)
    val files = basePath.jfile.listFiles
    val recs = files.flatMap(name => Source.fromFile(name).getLines)
    recs.contains(target)
  }
  "partitionWriteToFileWithPartitionColumns" should {
    "be normal end" when {
      "single column" in {
        val outPath = "test/ptt/t1"
        import FileCtl._
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => FileCtlTest1(x.toString, cnt, cnt))).toDF
        df.partitionWriteToFileWithPartitionColumns(outPath, Seq("x"))(_.mkString)
        checkData(s"${outPath}/x=a", "a11.000000000000000000") mustBe true
        checkData(s"${outPath}/x=a", "a22.000000000000000000") mustBe true
        checkData(s"${outPath}/x=a", "a33.000000000000000000") mustBe true
        checkData(s"${outPath}/x=b", "b11.000000000000000000") mustBe true
        checkData(s"${outPath}/x=b", "b22.000000000000000000") mustBe true
        checkData(s"${outPath}/x=b", "b33.000000000000000000") mustBe true
        checkData(s"${outPath}/x=c", "c11.000000000000000000") mustBe true
        checkData(s"${outPath}/x=c", "c22.000000000000000000") mustBe true
        checkData(s"${outPath}/x=c", "c33.000000000000000000") mustBe true
      }

      "multi column" in {
        val outPath = "test/ptt/t2"
        import FileCtl._
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => FileCtlTest1(x.toString, cnt, cnt))).toDF
        df.partitionWriteToFileWithPartitionColumns(outPath, Seq("x", "y"))(_.mkString)
        checkData(s"${outPath}/x=a/y=1", "a11.000000000000000000") mustBe true
        checkData(s"${outPath}/x=a/y=2", "a22.000000000000000000") mustBe true
        checkData(s"${outPath}/x=a/y=3", "a33.000000000000000000") mustBe true
        checkData(s"${outPath}/x=b/y=1", "b11.000000000000000000") mustBe true
        checkData(s"${outPath}/x=b/y=2", "b22.000000000000000000") mustBe true
        checkData(s"${outPath}/x=b/y=3", "b33.000000000000000000") mustBe true
        checkData(s"${outPath}/x=c/y=1", "c11.000000000000000000") mustBe true
        checkData(s"${outPath}/x=c/y=2", "c22.000000000000000000") mustBe true
        checkData(s"${outPath}/x=c/y=3", "c33.000000000000000000") mustBe true
      }
    }

    def extentionCheck(path: String, extention: String) =
      Path(path).jfile.listFiles.map(_.toString.endsWith(extention)).forall(_ == true)
    "add Extention" when {
      "single column" in {
        val outPath = "test/ptt/t3"
        import FileCtl._
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => FileCtlTest1(x.toString, cnt, cnt))).toDF
        df.partitionWriteToFileWithPartitionColumns(outPath, Seq("x"), partitionExtention = "ext")(_.mkString)
        extentionCheck(s"${outPath}/x=a", "ext") mustBe true
        checkData(s"${outPath}/x=a", "a11.000000000000000000") mustBe true
        checkData(s"${outPath}/x=a", "a22.000000000000000000") mustBe true
        checkData(s"${outPath}/x=a", "a33.000000000000000000") mustBe true
        extentionCheck(s"${outPath}/x=b", "ext") mustBe true
        checkData(s"${outPath}/x=b", "b11.000000000000000000") mustBe true
        checkData(s"${outPath}/x=b", "b22.000000000000000000") mustBe true
        checkData(s"${outPath}/x=b", "b33.000000000000000000") mustBe true
        extentionCheck(s"${outPath}/x=c", "ext") mustBe true
        checkData(s"${outPath}/x=c", "c11.000000000000000000") mustBe true
        checkData(s"${outPath}/x=c", "c22.000000000000000000") mustBe true
        checkData(s"${outPath}/x=c", "c33.000000000000000000") mustBe true
      }

      "multi column" in {
        val outPath = "test/ptt/t4"
        import FileCtl._
        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => FileCtlTest1(x.toString, cnt, cnt))).toDF
        df.partitionWriteToFileWithPartitionColumns(outPath, Seq("x", "y"), partitionExtention = "ext")(_.mkString)
        extentionCheck(s"${outPath}/x=a/y=1", "ext") mustBe true
        checkData(s"${outPath}/x=a/y=1", "a11.000000000000000000") mustBe true
        extentionCheck(s"${outPath}/x=a/y=2", "ext") mustBe true
        checkData(s"${outPath}/x=a/y=2", "a22.000000000000000000") mustBe true
        extentionCheck(s"${outPath}/x=a/y=3", "ext") mustBe true
        checkData(s"${outPath}/x=a/y=3", "a33.000000000000000000") mustBe true
        extentionCheck(s"${outPath}/x=b/y=1", "ext") mustBe true
        checkData(s"${outPath}/x=b/y=1", "b11.000000000000000000") mustBe true
        extentionCheck(s"${outPath}/x=b/y=2", "ext") mustBe true
        checkData(s"${outPath}/x=b/y=2", "b22.000000000000000000") mustBe true
        extentionCheck(s"${outPath}/x=b/y=3", "ext") mustBe true
        checkData(s"${outPath}/x=b/y=3", "b33.000000000000000000") mustBe true
        extentionCheck(s"${outPath}/x=c/y=1", "ext") mustBe true
        checkData(s"${outPath}/x=c/y=1", "c11.000000000000000000") mustBe true
        extentionCheck(s"${outPath}/x=c/y=2", "ext") mustBe true
        checkData(s"${outPath}/x=c/y=2", "c22.000000000000000000") mustBe true
        extentionCheck(s"${outPath}/x=c/y=3", "ext") mustBe true
        checkData(s"${outPath}/x=c/y=3", "c33.000000000000000000") mustBe true
      }
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package spark.common

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import scala.reflect.io.Directory
import spark.common.SparkContexts.context
import context.implicits._
import java.io.PrintWriter
import scala.util.Try
import scala.reflect.io.Path

case class FileCtlTest1(x: String, y: Int, z: BigDecimal)
class FileCtlTest extends WordSpec with MustMatchers with BeforeAndAfter {
  "FileCtl" should {
    "exists" in {
      FileCtl.exists("build.sbt") mustBe true
    }

    "not exists" in {
      FileCtl.exists("build.sxx") mustBe false
    }

    "exists wildCard" in {
      FileCtl.exists("*.sbt") mustBe true
      FileCtl.exists("[a-b]uild.sbt") mustBe true
    }

    "not exists wildCard" in {
      FileCtl.exists("*.sxx") mustBe false
      FileCtl.exists("[c-d]uild.sbt") mustBe false
    }

    "success deleteDirectory1" in {
      val d = (Directory("test") / "dummyDir")
      d.createDirectory(true, false)
      val f = d / "dummyFile"
      f.createFile(false)

      FileCtl.deleteDirectory(d.toFile.toString)
      d.exists mustBe false
    }

    "success deleteDirectory2" in {
      val d = (Directory("test") / "dummyDir")
      d.createDirectory(true, false)
      val f = d / "dummyFile"
      f.createFile(false)

      FileCtl.deleteDirectory(Directory("test").toString, "dummyDir")
      d.exists mustBe false
    }

    "with partition columns" when {
      "one Column" in {
        val outPath = "test/wpc1"
        FileCtl.deleteDirectory(outPath)

        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => FileCtlTest1(x.toString, cnt, cnt))).toDF
        FileCtl.loanPrintWriterCache { cache =>
          df.collect.foldLeft(cache) { (l, r) =>
            FileCtl.writeToFileWithPartitionColumns(outPath, partitionColumns = Seq("x"))(_.mkString(","))(l)(r)
          }
        }
        ('a' to 'c').flatMap(x => (1 to 3).map(cnt => s"${outPath}/x=${x}/0")).foreach { path =>
          withClue(path) { Path(path).exists mustBe true }
        }
      }

      "two Columns" in {
        val outPath = "test/wpc2"
        FileCtl.deleteDirectory(outPath)

        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => FileCtlTest1(x.toString, cnt, cnt))).toDF
        FileCtl.loanPrintWriterCache { cache =>
          df.collect.foldLeft(cache) { (l, r) =>
            FileCtl.writeToFileWithPartitionColumns(outPath, partitionColumns = Seq("x", "y"))(_.mkString(","))(l)(r)
          }
        }
        ('a' to 'c').flatMap(x => (1 to 3).map(cnt => s"${outPath}/x=${x}/y=${cnt}/0")).foreach { path =>
          withClue(path) { Path(path).exists mustBe true }
        }
      }
    }

    "with partition columns add Extention" when {
      "one Column" in {
        val outPath = "test/wpc1"
        FileCtl.deleteDirectory(outPath)

        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => FileCtlTest1(x.toString, cnt, cnt))).toDF
        FileCtl.loanPrintWriterCache { cache =>
          df.collect.foldLeft(cache) { (l, r) =>
            FileCtl.writeToFileWithPartitionColumns(outPath, partitionColumns = Seq("x"), partitionExtention = "ext")(_.mkString(","))(l)(r)
          }
        }
        ('a' to 'c').flatMap(x => (1 to 3).map(cnt => s"${outPath}/x=${x}/0.ext")).foreach { path =>
          withClue(path) { Path(path).exists mustBe true }
        }
      }

      "two Columns" in {
        val outPath = "test/wpc2"
        FileCtl.deleteDirectory(outPath)

        val df = ('a' to 'c').flatMap(x => (1 to 3).map(cnt => FileCtlTest1(x.toString, cnt, cnt))).toDF
        FileCtl.loanPrintWriterCache { cache =>
          df.collect.foldLeft(cache) { (l, r) =>
            FileCtl.writeToFileWithPartitionColumns(outPath, partitionColumns = Seq("x", "y"), partitionExtention = "ext")(_.mkString(","))(l)(r)
          }
        }
        ('a' to 'c').flatMap(x => (1 to 3).map(cnt => s"${outPath}/x=${x}/y=${cnt}/0.ext")).foreach { path =>
          withClue(path) { Path(path).exists mustBe true }
        }
      }
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package spark.common

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import spark.common.SparkContexts.context
import context.implicits._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.Row
import org.apache.spark.sql.SaveMode
import scala.util.Try
import org.joda.time.DateTime
import java.sql.Timestamp
import java.sql.Date
import org.joda.time.format.DateTimeFormatter
import org.joda.time.format.DateTimeFormat
import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils

class LargeInsertTest extends WordSpec with MustMatchers with BeforeAndAfter {
  import org.apache.spark.sql.types._

  val dbCtl = new DbCtl(DbCtl.dbInfo1)

  val tableColumns = (1 to 10).map(cnt => s"col${cnt} varchar(10)").mkString(",")

  Try { dbCtl.execSql("largeData", s"create table largeData($tableColumns) logging") }
  Try { dbCtl.execSql("largeDataNoLogging", s"create table largeDataNoLogging($tableColumns) nologging") }

  val structTypeAnyType = StructType((1 to 10).map(cnt => StructField(s"col${cnt}", StringType)))
  def colData(rec: Int) = (1 to 10).map(cnt => s"${cnt}_${rec}").toSeq
  val data = (1 to 10000).map(cnt => Row(colData(cnt): _*))
  val inDf = context.createDataFrame(SparkContexts.sc.makeRDD(data), structTypeAnyType).cache

  val pqCtl = new PqCtl("test/large")
  import pqCtl.implicits._
  //(1 to 100).par.foreach(cnt => inDf.writeParquet(s"large/$cnt"))
  val df = pqCtl.readParquet("large/*")

  "dummy" should {
    val tableName = "largeData"
    "insert recs" in {
      val dbCtl = new DbCtl(DbCtl.dbInfo1.copy(isDirectPathInsertMode = false))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
    }
  }

  "nonDirectInsert" should {
    val tableName = "largeData"
    "insert recs" in {
      val dbCtl = new DbCtl(DbCtl.dbInfo1.copy(isDirectPathInsertMode = false))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
    }

    "insert recs with commit size 1000" in {
      val dbCtl = new DbCtl(DbCtl.dbInfo1.copy(commitSize = Some(1000), isDirectPathInsertMode = false))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
    }

    "insert recs with commit size 10000" in {
      val dbCtl = new DbCtl(DbCtl.dbInfo1.copy(commitSize = Some(10000), isDirectPathInsertMode = false))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
    }

    "insert recs with commit size 100000" in {
      val dbCtl = new DbCtl(DbCtl.dbInfo1.copy(commitSize = Some(100000), isDirectPathInsertMode = false))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
    }
  }

  "directInsert" should {
    val tableName = "largeDataNoLogging"
    "insert recs" in {
      val dbCtl = new DbCtl(DbCtl.dbInfo1.copy(isDirectPathInsertMode = true))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
    }

    "insert recs with commit size 1000" in {
      val dbCtl = new DbCtl(DbCtl.dbInfo1.copy(commitSize = Some(1000), isDirectPathInsertMode = true))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
    }

    "insert recs with commit size 10000" in {
      val dbCtl = new DbCtl(DbCtl.dbInfo1.copy(commitSize = Some(10000), isDirectPathInsertMode = true))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
    }

    "insert recs with commit size 100000" in {
      val dbCtl = new DbCtl(DbCtl.dbInfo1.copy(commitSize = Some(100000), isDirectPathInsertMode = true))
      dbCtl.insertAccelerated(df, tableName, SaveMode.Overwrite)
    }
  }
}
', Error message: Source already set.
[03/24/2023 05:42:17] Warning: Error when loading symbols from the file 'package spark.common

import org.scalatest.WordSpec
import org.scalatest.MustMatchers
import org.scalatest.BeforeAndAfter
import scala.reflect.io.Directory
import SparkContexts.context.implicits._
import scala.util.Try

case class PqCtlTestData(a: String)
class PqCtlTest extends WordSpec with MustMatchers with BeforeAndAfter {
  val path = "test/pqtest"
  val name = "test"
  val pqCtl = new PqCtl(path)
  import pqCtl.implicits._
  Seq(PqCtlTestData("aaa")).toDF.writeParquet(name)

  "Strict Mode check" should {
    "be success. mode true" in {
      try {
        pqCtl.readParquet("testx", true)
        fail
      } catch {
        case t: Throwable => {
          t.getClass.getName mustBe "org.apache.spark.sql.AnalysisException"
          t.getMessage must startWith("Path does not exist:")
        }
      }
    }

    "be success. mode true. pattern2" in {
      try {
        pqCtl.readParquet("testx/*/*/aaa", true)
        fail
      } catch {
        case t: Throwable => {
          t.getClass.getName mustBe "org.apache.spark.sql.AnalysisException"
          t.getMessage must startWith("Path does not exist:")
        }
      }
    }

    "be success. mode false" in {
      pqCtl.readParquet("testx", false)
    }
  }

  "many read paths" should {
    "be normal end" when {
      Seq(PqCtlTestData("aaa")).toDF.writeParquet("manyPath1.pq")
      Seq(PqCtlTestData("bbb")).toDF.writeParquet("manyPath2.pq")
      "normal mode" in {
        val df = pqCtl.readParquet("manyPath1.pq , manyPath2.pq", false)
        val result = df.collect
        result(0).getAs[String]("a") mustBe "aaa"
        result(1).getAs[String]("a") mustBe "bbb"
      }

      "strict mode" in {
        val df = pqCtl.readParquet("manyPath1.pq , manyPath2.pq", true)
        val result = df.collect
        result(0).getAs[String]("a") mustBe "aaa"
        result(1).getAs[String]("a") mustBe "bbb"
      }
    }
  }
}', Error message: Source already set.
[03/24/2023 05:42:17] Info: Step 5/9 - Symbol Table Loading: COMPLETED
[03/24/2023 05:42:17] Debug: TaskParam assessmentModelPath = /tmp/Output-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/AssesmentModel/SparkSnowConvert
[03/24/2023 05:42:17] Debug: TaskParam sqlRegExExtractionDictionary = [DELETE, \b(delete)(.*?)\b(from)\b], [INSERT, \b(insert)\b(.*?)\b(into)\b], [SELECT, (?i)(?s)\b(select)\b(.*?)\b(from)\b], [UPDATE, \b(update)\b(.*?)\b(set)\b]
[03/24/2023 05:42:17] Debug: TaskParam projectId = Sources
[03/24/2023 05:42:17] Debug: TaskParam inputPath = /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources
[03/24/2023 05:42:17] Debug: TaskParam sessionId = 589b8506-ce73-40ee-9bc0-d938649a9163
[03/24/2023 05:42:17] Debug: TaskParam sparkUsagesInventoryStorage = Mobilize.SparkSnow.Assessment.SparkUsagesInventoryStorage
[03/24/2023 05:42:17] Debug: TaskParam importUsagesInventoryStorage = Mobilize.SparkCommon.Assessment.ImportUsagesInventoryStorage
[03/24/2023 05:42:17] Debug: TaskParam sqlExtractionReportStorage = Mobilize.SparkSnow.Assessment.SqlExtractionReportStorage
[03/24/2023 05:42:17] Debug: TaskParam packagesInventoryStorage = Mobilize.SparkCommon.Assessment.PackagesInventoryStorage
[03/24/2023 05:42:17] Debug: TaskParam symbolTable = Mobilize.Scala.SymbolTable.SclSymbolTable
[03/24/2023 05:42:17] Debug: TaskParam transformationVisitors = Value is not created.
[03/24/2023 05:42:17] Debug: TaskParam mappingDictionaries = Mobilize.SparkCommon.TransformationCore.MappingDictionaries
[03/24/2023 05:42:17] Debug: TaskParam conversionStatusData = Mobilize.SparkCommon.Assessment.ConversionStatus.ConversionStatusData
[03/24/2023 05:42:17] Debug: TaskParam snowConvertCoreVersion = 1.01.039
[03/24/2023 05:42:17] Debug: TaskParam SparkUsagesInventoryTask.Input = /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/input.wsp
[03/24/2023 05:42:17] Debug: TaskParam workingSet = Artinsoft.Common.Store.RepositoryWorkingSet`2[System.String,Artinsoft.Common.Store.IItemContainer]
[03/24/2023 05:42:17] Debug: TaskParam SparkUsagesInventoryTask.ItemMedatada = Artinsoft.Common.Store.ItemMetadata
[03/24/2023 05:42:17] Debug: TaskParam SparkUsagesInventoryTask.Enabled = True
[03/24/2023 05:42:17] Debug: TaskParam Repository = Artinsoft.Common.Store.Repository
[03/24/2023 05:42:17] Debug: TaskParam RepositoryDirectory = /tmp/Output-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/.mobilize/CommonEF
[03/24/2023 05:42:17] Info: Step 7/9 - Pre-Conversion Assessment: STARTED
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/MultiDbToMultiAny.scala'
Line number: '10'
Statement: 'def preExec(in: Unit)(implicit inArgs: InputArgs) : Map[String, DataFrame] = readDb'
Detail: 'Can't OpenScope for symbol named: 'preExec(scala.Unit,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.executor' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/DbOutputCommonFunctions.scala'
Line number: '10'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) = DbOutputCommonFunctions(df)'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.executor' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/DbOutputCommonFunctions.scala'
Line number: '14'
Statement: 'def apply(df: DataFrame)(implicit inArgs: InputArgs) = PqCommonColumnRemover(RowErrorRemover(df)).withColumn("VC_DISPOYMD", lit(inArgs.runningDateYMD))'
Detail: 'Can't OpenScope for symbol named: 'apply(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/AnyToPq.scala'
Line number: '10'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) = writeParquet(df)'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readParquet' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/MultiReadPq.scala'
Line number: '11'
Statement: 'def readParquet(implicit inArgs: InputArgs) = readPqNames.map( pqName =>(pqName, readParquetSingle(pqName))).toMap'
Detail: 'Can't OpenScope for symbol named: 'readParquet(d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.parser' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/ComponentFlowParser.scala'
Line number: '16'
Statement: 'def apply(target: String) = { parseAll(componentFlow, target) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/JefConverter.scala'
Line number: '22'
Statement: 'def readData(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).getLines.map{ line => val kv = line.split('\t') (kv(0), kv(1)) }.toMap'
Detail: 'Can't OpenScope for symbol named: 'readData(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'addControlCode' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/JefConverter.scala'
Line number: '29'
Statement: 'def addControlCode(origin: Map[String, String]) = origin ++ Map(tab -> "\t", kanjiOut -> "", kanjiIn9 -> "", kanjiIn12 -> "")'
Detail: 'Can't OpenScope for symbol named: 'addControlCode(scala.Map[scala.String,scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'isJefHalf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/JefConverter.scala'
Line number: '38'
Statement: 'def isJefHalf(domain: String, charEnc: String) = charEnc == "JEF" && !domain.startsWith("全角文字列")'
Detail: 'Can't OpenScope for symbol named: 'isJefHalf(scala.String,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'isJefFull' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/JefConverter.scala'
Line number: '39'
Statement: 'def isJefFull(domain: String, charEnc: String) = charEnc == "JEF" && domain.startsWith("全角文字列")'
Detail: 'Can't OpenScope for symbol named: 'isJefFull(scala.String,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'convJefToUtfHalf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/JefConverter.scala'
Line number: '41'
Statement: 'def convJefToUtfHalf(data: Array[Byte]) : String = { val conved = data.map{ byte =>(byte, Try(jefToUtfHalfData(f"$byte%02X")))} if (!conved.map(_._1).forall(_ == 0x00))       printErrorHalf(conved) conved.map(_._2.getOrElse("*")).mkString }'
Detail: 'Can't OpenScope for symbol named: 'convJefToUtfHalf(scala.Array[scala.Byte])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'convJefToUtfFull' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/JefConverter.scala'
Line number: '52'
Statement: 'def convJefToUtfFull(data: Array[Byte]) : String = { data.grouped(2).map{ byteArr => val jefCode = byteArr.map( x =>f"$x%02X").mkString jefToUtfFullKddiData.get(jefCode).orElse{ jefToUtfFullData.get(jefCode) }.getOrElse{    println(s"!!!![JEF CONV ERROR:FULL]${ byteArr.map( x =>f"$x%02X").mkString }") "■"    } }.mkString }'
Detail: 'Can't OpenScope for symbol named: 'convJefToUtfFull(scala.Array[scala.Byte])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'convUtfToJefHalf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/JefConverter.scala'
Line number: '64'
Statement: 'def convUtfToJefHalf(data: String) : Array[Byte] = data.map( x =>Try(utfToJefHalfData(x)).getOrElse("5C")).map( x =>Integer.parseInt(x.toString, 16)).map(_.toByte).toArray'
Detail: 'Can't OpenScope for symbol named: 'convUtfToJefHalf(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'convUtfToJefFull' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/JefConverter.scala'
Line number: '67'
Statement: 'def convUtfToJefFull(data: String) : Array[Byte] = { data.flatMap( x =>Try(utfToJefFullData(x)).getOrElse("A2A3")).grouped(2).map( x =>Integer.parseInt(x.toString, 16)).map(_.toByte).toArray }'
Detail: 'Can't OpenScope for symbol named: 'convUtfToJefFull(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.executor' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/Nothing_.scala'
Line number: '10'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) : DataFrame = df'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'parse' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/ConfParser.scala'
Line number: '23'
Statement: 'def parse(inFilePath: String) = parseAppConf(inFilePath)'
Detail: 'Can't OpenScope for symbol named: 'parse(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readConf[A]' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/ConfParser.scala'
Line number: '25'
Statement: 'def readConf [A](inFilePath: String)(proc: Array[String] => A) = { val fileEnc = "MS932"  val itemConfPath = s"itemConf/${ File(inFilePath).name }" Option(getClass.getClassLoader.getResourceAsStream(itemConfPath)).map( is =>Source.fromInputStream(is, fileEnc)).getOrElse{ Source.fromFile(inFilePath, fileEnc) }.getLines.drop(1).map( line =>proc(line.split('\t'))) }'
Detail: 'Can't OpenScope for symbol named: 'readConf[A](scala.String,lambda[Array[String],A])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'checkItem' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/ConfParser.scala'
Line number: '35'
Statement: 'def checkItem(items: Array[String])(abailables: Seq[String], targetIdx: Int, comment: String) = if (!abailables.contains(items(targetIdx)))    {    throw new IllegalArgumentException ( s"not available item:${ items(targetIdx) }(usage: ${ abailables.mkString(" or ") }) in $comment")    }'
Detail: 'Can't OpenScope for symbol named: 'checkItem(scala.Array[scala.String],scala.Seq[scala.String],scala.Int,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'parseAppConf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/ConfParser.scala'
Line number: '41'
Statement: 'def parseAppConf(inFilePath: String) = { val appConfs = readConf(inFilePath){ items =>appErrorCheck(items) if (items.size == 10)       {       AppConf(items(0), items(1), items(2), items(3), items(4) == "true", items(5) == "true", items(6) == "true", items(7), items(8), items(9))       } else       {       AppConf(items(0), items(1), items(2), items(3), items(4) == "true", items(5) == "true", items(6) == "true", items(7), items(8), items(9), items(10))       } }  val basePath = Path(inFilePath).toAbsolute.parent  val namePrefix = Path(inFilePath).name.split('_')(0) appConfs.map( appConf =>Conf(appConf, parseItemConf(basePath, namePrefix, appConf.AppId))) }'
Detail: 'Can't OpenScope for symbol named: 'parseAppConf(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'getAvailableBoolean' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/ConfParser.scala'
Line number: '55'
Statement: 'def getAvailableBoolean(items: Array[String]) = { if (items(3) == "fixed")       {       availableBoolean       } else       {       availableBooleanWithBlank       } }'
Detail: 'Can't OpenScope for symbol named: 'getAvailableBoolean(scala.Array[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'appErrorCheck' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/ConfParser.scala'
Line number: '59'
Statement: 'def appErrorCheck(items: Array[String]) = { val checker = checkItem(items)_  def comment(name: String) = s"AppConf[appId:${ items(0) } name:${ name }]" checker(availableFileFormat, 3, comment("fileFormat")) checker(getAvailableBoolean(items), 4, comment("newline")) checker(availableBoolean, 5, comment("header")) checker(availableBoolean, 6, comment("footer")) checker(availableStoreType, 7, comment("storeType")) }'
Detail: 'Can't OpenScope for symbol named: 'appErrorCheck(scala.Array[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'parseItemConf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/ConfParser.scala'
Line number: '69'
Statement: 'def parseItemConf(basePath: Directory, prefix: String, appId: String) = { val itemConfPath = basePath / s"${ prefix }_items_${ appId }.conf" readConf(itemConfPath.toString){ items =>itemErrorCheck(items, appId) if (items.size == 5)       {       ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true")       } else       {       ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true", items(5))       } } }'
Detail: 'Can't OpenScope for symbol named: 'parseItemConf(Directory,scala.String,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'itemErrorCheck' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/ConfParser.scala'
Line number: '81'
Statement: 'def itemErrorCheck(items: Array[String], appId: String) = { val checker = checkItem(items)_  def comment(name: String) = s"ItemConf[appId:${ appId } item:${ items(0) } name:${ name }]" checkItem(items)(availableBoolean, 4, comment("extractTarget")) }'
Detail: 'Can't OpenScope for symbol named: 'itemErrorCheck(scala.Array[scala.String],scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'd2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/HiRDB_readTest.scala'
Line number: '29'
Statement: 'def d2s(dateMill: Long) = new DateTime (dateMill).toString("yyyy-MM-dd")'
Detail: 'Can't OpenScope for symbol named: 'd2s(scala.Long)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'd2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/HiRDB_readTest.scala'
Line number: '30'
Statement: 'def d2s(date: Date) = new DateTime (date).toString("yyyy-MM-dd")'
Detail: 'Can't OpenScope for symbol named: 'd2s(_Unresolved.Date)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'd2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/HiRDB_readTest.scala'
Line number: '31'
Statement: 'def d2s(date: Timestamp) = new DateTime (date).toString("yyyy-MM-dd hh:mm:ss")'
Detail: 'Can't OpenScope for symbol named: 'd2s(_Unresolved.Timestamp)''
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'dateAddHyphen' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/HiRDB_readTest.scala'
Line number: '122'
Statement: 'def dateAddHyphen(str: String) = s"${ str.take(4) }-${ str.drop(4).take(2) }-${ str.drop(6) }"'
Detail: 'Can't OpenScope for symbol named: 'dateAddHyphen(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/HiRDB_readTest.scala'
Line number: '127'
Statement: 'def readDbWhere(inArgs: InputArgs) = Array(s""""DATE" = '${ dateAddHyphen(inArgs.runningDates(0)) }'""")'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/HiRDB_readTest.scala'
Line number: '138'
Statement: 'def readDbWhere(inArgs: InputArgs) = Array(s""""TMSTMP" = '${ dateAddHyphen(inArgs.runningDates(0)) } 00:00:00'""")'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'toOptions' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '43'
Statement: 'def toOptions(tableName: String) = { val addFetchSize = fetchSize.map( fs =>baseMap + (JDBCOptions.JDBC_BATCH_FETCH_SIZE -> fs.toString)).getOrElse(baseMap) new JDBCOptions (addFetchSize + (JDBCOptions.JDBC_TABLE_NAME -> tableName)) }'
Detail: 'Can't OpenScope for symbol named: 'toOptions(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'toOptionsInWrite' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '48'
Statement: 'def toOptionsInWrite(tableName: String) = { val addFetchSize = fetchSize.map( fs =>baseMap + (JDBCOptions.JDBC_BATCH_FETCH_SIZE -> fs.toString)).getOrElse(baseMap) new JdbcOptionsInWrite (addFetchSize + (JDBCOptions.JDBC_TABLE_NAME -> tableName)) }'
Detail: 'Can't OpenScope for symbol named: 'toOptionsInWrite(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'makeWhere10' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '71'
Statement: 'def makeWhere10(columnName: String) = ((0 to 9).map( cnt =>f"substr(${ columnName },-1,1) = '$cnt'") :+ s" substr(${ columnName },-1,1) not in ('0','1','2','3','4','5','6','7','8','9') ").toArray'
Detail: 'Can't OpenScope for symbol named: 'makeWhere10(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'localImport' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '74'
Statement: 'def localImport(tableName: String, cnt: Int = 100) = { val orgTb = new DbCtl (DbCtl.dbInfo2).readTable(tableName)  val posgre = new DbCtl () import posgre.implicits._ context.createDataFrame(sc.makeRDD(orgTb.take(cnt)), orgTb.schema).writeTable(tableName, SaveMode.Overwrite) posgre.readTable(tableName).show }'
Detail: 'Can't OpenScope for symbol named: 'localImport(scala.String,scala.Int)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'canHandle' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '91'
Statement: 'def canHandle(url: String) : Boolean = url.startsWith("jdbc:oracle") || url.contains("oracle")'
Detail: 'Can't OpenScope for symbol named: 'canHandle(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'getCatalystType' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '93'
Statement: 'def getCatalystType(sqlType: Int, typeName: String, size: Int, md: MetadataBuilder) : Option[DataType] = { if (sqlType == Types.NUMERIC && size == 0)       {       Option(DecimalType(DecimalType.MAX_PRECISION, 10))       } else       {       None       } }'
Detail: 'Can't OpenScope for symbol named: 'getCatalystType(scala.Int,scala.String,scala.Int,MetadataBuilder)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'canHandle' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '110'
Statement: 'def canHandle(url: String) : Boolean = url.startsWith("jdbc:derby") || url.contains("derby")'
Detail: 'Can't OpenScope for symbol named: 'canHandle(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'getTableFullName' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '132'
Statement: 'def getTableFullName(tableName: String) = { val sql = "SELECT TABLE_OWNER FROM ALL_SYNONYMS WHERE TABLE_NAME = ? AND (OWNER ='PUBLIC' OR OWNER = ?) ORDER BY DECODE(OWNER,'PUBLIC',1,0)"  val rs = prepExecSql(tableName, sql){ prs =>prs.setString(1, tableName.toUpperCase()) prs.setString(2, dbInfo.user.toUpperCase()) } if (rs.next())       s"${ rs.getString(1) }.${ tableName }" else       tableName }'
Detail: 'Can't OpenScope for symbol named: 'getTableFullName(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'clearTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '141'
Statement: 'def clearTable(tableName: String) = { elapse("clearTable"){    val targetTable = if (tableName.contains("."))          tableName else          Try(getTableFullName(tableName)).getOrElse(tableName) Try(truncateTable(targetTable)).recover{       case e => errorLog(s"FAILED TO TRUNCATE ${ targetTable }", e);deleteTable(tableName)       }.get    } }'
Detail: 'Can't OpenScope for symbol named: 'clearTable(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'execSql' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '148'
Statement: 'def execSql(tableName: String, sql: String) = { println(tableName, sql) JdbcUtils.createConnectionFactory(dbInfo.toOptions(tableName))().prepareStatement(sql).executeUpdate }'
Detail: 'Can't OpenScope for symbol named: 'execSql(scala.String,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'prepExecSql' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '152'
Statement: 'def prepExecSql(tableName: String, sql: String)(prsFunc: PreparedStatement => Unit) = { val prep = JdbcUtils.createConnectionFactory(dbInfo.toOptions(tableName))().prepareStatement(sql) prsFunc(prep) prep.executeQuery }'
Detail: 'Can't OpenScope for symbol named: 'prepExecSql(scala.String,scala.String,lambda[PreparedStatement,Unit])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'truncateTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '157'
Statement: 'def truncateTable(tableName: String) = execSql(tableName, s"TRUNCATE TABLE $tableName")'
Detail: 'Can't OpenScope for symbol named: 'truncateTable(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'dropTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '158'
Statement: 'def dropTable(tableName: String) = JdbcUtils.dropTable(JdbcUtils.createConnectionFactory(dbInfo.toOptions)(), tableName, dbInfo.toOptions)'
Detail: 'Can't OpenScope for symbol named: 'dropTable(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'deleteTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '160'
Statement: 'def deleteTable(tableName: String) = execSql(tableName, s"DELETE FROM $tableName")'
Detail: 'Can't OpenScope for symbol named: 'deleteTable(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'columnTypes' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '161'
Statement: 'def columnTypes(conn: Connection, tableName: String) = { val result = conn.getMetaData.getColumns(null, null, tableName.toUpperCase, "%") new Iterator[(String, Int)] {    def hasNext = result.next  def next() = (result.getString("COLUMN_NAME"), result.getString("DATA_TYPE").toInt)    }.toMap }'
Detail: 'Can't OpenScope for symbol named: 'columnTypes(Connection,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'changeColToDfTypes' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '169'
Statement: 'def changeColToDfTypes(colType: Int, dfTypes: DataType) = dfTypes match {    case DateType => DATE    case TimestampType => TIMESTAMP    case IntegerType => INTEGER    case LongType => DOUBLE    case DecimalType() => DECIMAL    case _ => colType }'
Detail: 'Can't OpenScope for symbol named: 'changeColToDfTypes(scala.Int,DataType)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '178'
Statement: 'def readTable(tableName: String) = { context.read.jdbc(dbInfo.url, tableName, props) }'
Detail: 'Can't OpenScope for symbol named: 'readTable(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '182'
Statement: 'def readTable(tableName: String, where: Array[String]) = { context.read.jdbc(dbInfo.url, tableName, where, props) }'
Detail: 'Can't OpenScope for symbol named: 'readTable(scala.String,scala.Array[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '186'
Statement: 'def readTable(tableName: String, requiredColumns: Array[String], where: Array[String]) = { JdbcCtl.readTable(this, tableName, requiredColumns, where) }'
Detail: 'Can't OpenScope for symbol named: 'readTable(scala.String,scala.Array[scala.String],scala.Array[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'loanConnection' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '190'
Statement: 'def loanConnection(tableName: String, sqlStr: String, executeBatch: Boolean = true)(f: PreparedStatement => Unit) {    val jdbcConn = JdbcUtils.createConnectionFactory(dbInfo.toOptions(tableName))()  val prs = jdbcConn.prepareStatement(sqlStr) try       {f(prs) if (executeBatch)             prs.executeBatch}    finally       {       Some(prs).foreach(_.close) Some(jdbcConn).foreach(_.close)       } }'
Detail: 'Can't OpenScope for symbol named: 'loanConnection(scala.String,scala.String,scala.Boolean,lambda[PreparedStatement,Unit])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'setValues' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '221'
Statement: 'def setValues(columnTypes: Map[String, Int], structType: StructType, cols: Array[String], prs: PreparedStatement)(row: Row) = { def setPreparedValue(preps: PreparedStatement, col: String, idx: Int) = {    structType(col).dataType match {          case _if row.isNullAt(row.fieldIndex(col)) => preps.setNull(idx + 1, changeColToDfTypes(columnTypes(col), structType(col).dataType))          case StringType => preps.setString(idx + 1, row.getAs[String](col))          case DateType => preps.setDate(idx + 1, row.getAs[Date](col))          case TimestampType => preps.setTimestamp(idx + 1, row.getAs[Timestamp](col))          case IntegerType => preps.setInt(idx + 1, row.getAs[Int](col))          case LongType => preps.setLong(idx + 1, row.getAs[Long](col))          case DecimalType() => preps.setBigDecimal(idx + 1, row.getAs[java.math.BigDecimal](col))       } preps    } cols.zipWithIndex.foldLeft(prs){    case (preps, (col, idx)) => setPreparedValue(preps, col, idx)    } prs.addBatch }'
Detail: 'Can't OpenScope for symbol named: 'setValues(scala.Map[scala.String,scala.Int],StructType,scala.Array[scala.String],PreparedStatement,Row)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'insertAccelerated' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '238'
Statement: 'def insertAccelerated(df: DataFrame, tableName: String, mode: SaveMode = SaveMode.Append, hint: String = "") = { val structType = df.schema  val fieldNames = structType.fieldNames  val columnsStr = fieldNames.mkString(",")  val bindStr = fieldNames.map(_ =>"?").mkString(",")  def nonDirectInsert = {    val insertStr = s"""       insert /*+ ${ hint } */ into ${ tableName }(${ columnsStr }) values(${ bindStr })       """ if (mode == SaveMode.Overwrite)          clearTable(tableName) df.foreachPartition(recordProcessor(tableName, insertStr, structType, fieldNames))    }  def directInsert = {    val insertStr = s"""       insert /*+ APPEND_VALUES ${ hint } */ into ${ tableName }(${ columnsStr }) values(${ bindStr })       """ if (mode == SaveMode.Overwrite)          clearTable(tableName) df.coalesce(1).foreachPartition(recordProcessor(tableName, insertStr, structType, fieldNames))    }  val dpi = sys.props.get(DbCtl.envName.DPI_MODE).map(DbCtl.checkTrueOrOn).getOrElse(dbInfo.isDirectPathInsertMode) if (dpi)       directInsert else       nonDirectInsert }'
Detail: 'Can't OpenScope for symbol named: 'insertAccelerated(DataFrame,scala.String,SaveMode,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'insertNotExists' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '267'
Statement: 'def insertNotExists(df: DataFrame, tableName: String, inKeys: Seq[String], mode: SaveMode = SaveMode.Append, hint: String = "") = { val keysStr = inKeys.mkString(",")  val structType = df.schema  val fieldNames = structType.fieldNames  val columnsStr = fieldNames.mkString(",")  val bindStr = fieldNames.map(_ =>"?").mkString(",")  val insertStr = s"""       insert /*+ ignore_row_on_dupkey_index(${ tableName }(${ keysStr })) ${ hint } */ into ${ tableName }(${ columnsStr }) values(${ bindStr })       """ if (mode == SaveMode.Overwrite)       clearTable(tableName) df.foreachPartition(recordProcessor(tableName, insertStr, structType, fieldNames)) }'
Detail: 'Can't OpenScope for symbol named: 'insertNotExists(DataFrame,scala.String,scala.Seq[scala.String],SaveMode,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'deleteRecords' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '280'
Statement: 'def deleteRecords(df: DataFrame, tableName: String, inKeys: Set[String], hint: String = "") = { val keysArray = inKeys.toArray  val keysStr = keysArray.map( k =>s"${ k.toLowerCase } = ?").mkString(" and ")  val deleteStr = s"""       delete /*+ ${ hint } */ from $tableName where $keysStr       """  val structType = df.schema df.foreachPartition(recordProcessor(tableName, deleteStr, structType, keysArray)) }'
Detail: 'Can't OpenScope for symbol named: 'deleteRecords(DataFrame,scala.String,scala.Set[scala.String],scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'dropArrayData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '294'
Statement: 'def dropArrayData(target: Seq[String], dropList: Seq[String]) = dropList.foldLeft(target){(l, r) =>l.filterNot(_ == r) }'
Detail: 'Can't OpenScope for symbol named: 'dropArrayData(scala.Seq[scala.String],scala.Seq[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'updateRecordsBase' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '298'
Statement: 'def updateRecordsBase(mode: DbCtl.UpdateMode)(df: DataFrame, tableName: String, inKeys: Set[String], ignoreColumnsForUpdate: Set[String] = Set.empty[String], hint: String = "") = { val keysArray = inKeys.toArray  val cols = df.columns  val colsWithoutKey = dropArrayData(df.columns, keysArray)  val colsWithoutKeyAndIgnore = dropArrayData(colsWithoutKey, ignoreColumnsForUpdate.toSeq)  val colsWithoutKeyStr = colsWithoutKeyAndIgnore.map( k =>s"${ k.toLowerCase } = ?").mkString(",")  val keysStr = keysArray.map( k =>s"${ k.toLowerCase } = ?").mkString(" and ")  val updateStr = s"""       update /*+ ${ hint } */ ${ tableName }  set $colsWithoutKeyStr where $keysStr       """  val usingStr = cols.map( x =>s"? $x").mkString(",")  val onStr = keysArray.map( x =>s"a.${ x } = b.${ x }").mkString(" and ")  val updateSetStr = colsWithoutKeyAndIgnore.map( x =>s"a.${ x } = b.${ x }").mkString(",")  val insertStrLeft = cols.map( x =>s"a.${ x }").mkString(",")  val insertStrRight = cols.map( x =>s"b.${ x }").mkString(",")  val mergeStr = s"""       merge /*+ ${ hint } */ into ${ tableName } a         using (select ${ usingStr } from dual) b         on (${ onStr })       when matched then         update set ${ updateSetStr }       when not matched then         insert (${ insertStrLeft }) values (${ insertStrRight })       """  val structType = df.schema mode match {       case DbCtl.UPSERT => df.foreachPartition(recordProcessor(tableName, mergeStr, structType, cols))       case DbCtl.UPDATE => df.foreachPartition(recordProcessor(tableName, updateStr, structType, colsWithoutKeyAndIgnore ++: keysArray))    } }'
Detail: 'Can't OpenScope for symbol named: 'updateRecordsBase(spark.common.DbCtl.UpdateMode,DataFrame,scala.String,scala.Set[scala.String],scala.Set[scala.String],scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'writeTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '335'
Statement: 'def writeTable(tableName: String, mode: SaveMode = SaveMode.Append) = mode match {    case SaveMode.Overwrite => {    clearTable(tableName) JdbcUtils.saveTable(df, None, true, dbInfo.toOptionsInWrite(tableName))    }    case SaveMode.Append => {    JdbcUtils.saveTable(df, None, true, dbInfo.toOptionsInWrite(tableName))    }    case _ => df.write.mode(mode).jdbc(dbInfo.url, tableName, props) }'
Detail: 'Can't OpenScope for symbol named: 'writeTable(scala.String,SaveMode)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'writeTableStandard' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '349'
Statement: 'def writeTableStandard(tableName: String, mode: SaveMode = SaveMode.Append) = mode match {    case SaveMode.Overwrite => {    clearTable(tableName) df.write.mode(mode).jdbc(dbInfo.url, tableName, props)    }    case _ => df.write.mode(mode).jdbc(dbInfo.url, tableName, props) }'
Detail: 'Can't OpenScope for symbol named: 'writeTableStandard(scala.String,SaveMode)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'autoCreateTable' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '360'
Statement: 'def autoCreateTable(tableName: String) {    Try(df.limit(1).write.mode(SaveMode.Overwrite).jdbc(dbInfo.url, tableName, props)) clearTable(tableName) }'
Detail: 'Can't OpenScope for symbol named: 'autoCreateTable(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readParquetAndWriteDb' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '368'
Statement: 'def readParquetAndWriteDb(readParquetBasePath: String, readParquetPath: String, writeTableName: String, saveMode: SaveMode = SaveMode.Append)(proc: DataFrame => DataFrame = df =>df) = { proc(new PqCtl (readParquetBasePath).readParquet(readParquetPath)).writeTable(writeTableName, saveMode) }'
Detail: 'Can't OpenScope for symbol named: 'readParquetAndWriteDb(scala.String,scala.String,scala.String,SaveMode,lambda[DataFrame,DataFrame])''
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.app.test.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/MakeDf.scala'
Line number: '19'
Statement: 'def apply(confPath: String) = new Plane (confPath)'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'dc' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.app.test.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/MakeDf.scala'
Line number: '20'
Statement: 'def dc(confPath: String) = new DomainConverter (confPath)'
Detail: 'Can't OpenScope for symbol named: 'dc(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'makeInputDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.app.test.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/MakeDf.scala'
Line number: '22'
Statement: 'def makeInputDf(rows: Seq[Row], names: Seq[String], domains: Seq[String]) = { val rdd = SparkContexts.sc.makeRDD(rows)  val ziped = names.zip(domains)  val (nameList, domainList) = ziped.filter{    case (names, domain) => !(domain.startsWith(Converter.NOT_USE_PREFIX) || domain.startsWith(Converter.REC_DIV_PREFIX))    }.unzip SparkContexts.context.createDataFrame(rdd, Converter.makeSchema(nameList)) }'
Detail: 'Can't OpenScope for symbol named: 'makeInputDf(Seq[Row],scala.Seq[scala.String],scala.Seq[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'parseItemConf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.app.test.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/MakeDf.scala'
Line number: '29'
Statement: 'def parseItemConf(confPath: String) = { val conf = ConfParser.readConf(confPath){ items =>if (items.size == 5)       {       ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true")       } else       {       ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true", items(5))       } }.toSeq (conf, conf.map(_.itemId), conf.map(_.cnvType)) }'
Detail: 'Can't OpenScope for symbol named: 'parseItemConf(scala.String)''
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'writeFixedFile' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.app.test.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/MakeDf.scala'
Line number: '68'
Statement: 'def writeFixedFile(writePath: String, append: Boolean = false, header: Boolean = false, footer: Boolean = false, newLine: Boolean = true, lineSeparator: String = "\n") = { System.setProperty("line.separator", lineSeparator) FileCtl.writeToFile(writePath, append){ pw =>df.collect.map(_.mkString).foreach{ x =>if (header)       pw.println(" " * x.mkString.length) if (newLine)       pw.println(x.mkString) else       pw.print(x.mkString) if (footer)       pw.println(" " * x.mkString.length) } } }'
Detail: 'Can't OpenScope for symbol named: 'writeFixedFile(scala.String,scala.Boolean,scala.Boolean,scala.Boolean,scala.Boolean,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/AnyToVal.scala'
Line number: '11'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) : T = outputValue(df.collect)'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readFile' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/ReadFile.scala'
Line number: '11'
Statement: 'def readFile(implicit inArgs: InputArgs) = { new FileConv (componentId, fileInputInfo, itemConfId).makeDf }'
Detail: 'Can't OpenScope for symbol named: 'readFile(d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readAppDefMd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.parser' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/D2kParser.scala'
Line number: '6'
Statement: 'def readAppDefMd(baseUrl: String, branch: String, appGroup: String, appId: String, fileName: String) = { val appBaseUrl = s"${ baseUrl }/raw/${ branch }/apps/${ appGroup }/${ appId }"  val url = s"${ appBaseUrl }/${ fileName }" Source.fromURL(s"${ url }?private_token=${ sys.env("GITLAB_TOKEN") }").getLines.mkString("\n") }'
Detail: 'Can't OpenScope for symbol named: 'readAppDefMd(scala.String,scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readItemDefMd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.parser' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/D2kParser.scala'
Line number: '12'
Statement: 'def readItemDefMd(baseUrl: String, branch: String, filePath: String) = { val itemsBaseUrl = s"${ baseUrl }/raw/${ branch }/apps/common/items"  val url = s"${ itemsBaseUrl }/${ filePath }" Source.fromURL(s"${ url }?private_token=${ sys.env("GITLAB_TOKEN") }").getLines.mkString("\n") }'
Detail: 'Can't OpenScope for symbol named: 'readItemDefMd(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readAppDefMd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.parser' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/D2kParser.scala'
Line number: '18'
Statement: 'def readAppDefMd(basePath: String, appGroup: String, appId: String, fileName: String) = { val appBasePath = s"${ basePath }/apps/${ appGroup }/${ appId }"  val path = s"${ appBasePath }/${ fileName }" Source.fromFile(path).getLines.mkString("\n") }'
Detail: 'Can't OpenScope for symbol named: 'readAppDefMd(scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readAppDefMd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.parser' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/D2kParser.scala'
Line number: '24'
Statement: 'def readAppDefMd(basePath: String) = { Source.fromFile(basePath).getLines.mkString("\n") }'
Detail: 'Can't OpenScope for symbol named: 'readAppDefMd(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readItemDefMd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.parser' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/D2kParser.scala'
Line number: '28'
Statement: 'def readItemDefMd(basePath: String) = { Source.fromFile(basePath).getLines.mkString("\n") }'
Detail: 'Can't OpenScope for symbol named: 'readItemDefMd(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'makeSchema' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/Converter.scala'
Line number: '25'
Statement: 'def makeSchema(inNames: Seq[String]) = { val names = inNames :+ SYSTEM_COLUMN_NAME.ROW_ERROR :+ SYSTEM_COLUMN_NAME.ROW_ERROR_MESSAGE StructType(names.map{    case name => StructField(name, StringType, true)    }) }'
Detail: 'Can't OpenScope for symbol named: 'makeSchema(scala.Seq[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'makeSchemaWithRecordError' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/Converter.scala'
Line number: '30'
Statement: 'def makeSchemaWithRecordError(inNames: Seq[String]) = { val names = inNames :+ SYSTEM_COLUMN_NAME.ROW_ERROR :+ SYSTEM_COLUMN_NAME.ROW_ERROR_MESSAGE :+ SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR StructType(names.map{    case name => StructField(name, StringType, true)    }) }'
Detail: 'Can't OpenScope for symbol named: 'makeSchemaWithRecordError(scala.Seq[scala.String])''
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'domainConvert' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/Converter.scala'
Line number: '52'
Statement: 'def domainConvert(dataAndDomainsAndNames: Seq[((Array[Byte], String), String)], charEnc: String) = { val (rowData, errMessage) = dataAndDomainsAndNames.foldLeft((Seq.empty[String], Seq.empty[ErrMessage])){(l, r) => val (convCols, errMessages) = l  val ((data, domain), name) = r if (domain == NOT_USE_PREFIX)       {       l       } else       {       DomainProcessor.execArrayByte(domain, data, charEnc) match {             case Right(d) => (convCols :+ d, errMessages)             case Left(m) => (convCols :+ null, errMessages :+ ErrMessage( name, domain, new String (data, if (charEnc == "JEF")                "ISO-8859-1" else                charEnc), m))          }       } } makeErrorMessage(errMessage, rowData) }'
Detail: 'Can't OpenScope for symbol named: 'domainConvert(Seq[Tuple3[Tuple2[Array[Byte],String],String]],scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'makeErrorMessage' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/Converter.scala'
Line number: '69'
Statement: 'def makeErrorMessage(errMessage: Seq[ErrMessage], rowData: Seq[String]) = if (errMessage.isEmpty)    {    rowData :+ "false" :+ ""    } else    {    rowData :+ "true" :+ errMessage.mkString("|")    }'
Detail: 'Can't OpenScope for symbol named: 'makeErrorMessage(scala.Seq[d2k.common.fileConv.ErrMessage],scala.Seq[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'removeHeaderAndFooter[A]' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/Converter.scala'
Line number: '76'
Statement: 'def removeHeaderAndFooter [A](data: Seq[A], hasHeader: Boolean, hasFooter: Boolean) = ((hasHeader, hasFooter) match {    case (true, true) => data.drop(1).dropRight(1)    case (true, false) => data.drop(1)    case (false, true) => data.dropRight(1)    case _ => data })'
Detail: 'Can't OpenScope for symbol named: 'removeHeaderAndFooter[A](Seq[A],scala.Boolean,scala.Boolean)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'removeHeaderAndFooter' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/Converter.scala'
Line number: '83'
Statement: 'def removeHeaderAndFooter(df: DataFrame, hasHeader: Boolean, hasFooter: Boolean, colNames: Seq[String], domainNames: Seq[String]) = { if (hasHeader || hasFooter)       {       val dataDivIdx = domainNames.indexWhere{ elem =>elem.startsWith(REC_DIV_PREFIX)}  val dataDivColName = colNames(dataDivIdx) df.filter(col(dataDivColName) === REC_DIV_EXTRACT).drop(dataDivColName)       } else       {       df       } }'
Detail: 'Can't OpenScope for symbol named: 'removeHeaderAndFooter(DataFrame,scala.Boolean,scala.Boolean,scala.Seq[scala.String],scala.Seq[scala.String])''
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'writeDb' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/WriteDb.scala'
Line number: '79'
Statement: 'def writeDb(dforg: DataFrame)(implicit inArgs: InputArgs) = { val df = convNa(dforg)  val tblName = inArgs.tableNameMapper.get(componentId).getOrElse(writeTableName)  val dbCtl = new DbCtl (writeDbInfo)  def modUpdateColumn(df: DataFrame) = df.withColumn("dt_d2kupddttm", lit(inArgs.sysSQLDate)).withColumn("id_d2kupdusr", lit(componentId))  def checkKeys = if (writeDbUpdateKeys.isEmpty)       throw new IllegalArgumentException ("writeDbUpdateKeys is empty") (writeDbMode, writeDbWithCommonColumn) match {          case (Insert, true) => {          dbCtl.insertAccelerated(DbCommonColumnAppender(df, componentId), tblName, writeDbSaveMode, writeDbHint)          }          case (Insert, false) => {          dbCtl.insertAccelerated(df, tblName, writeDbSaveMode, writeDbHint)          }          case (InsertAcc, true) => {          dbCtl.insertAccelerated(DbCommonColumnAppender(df, componentId), tblName, writeDbSaveMode, writeDbHint)          }          case (InsertAcc, false) => {          dbCtl.insertAccelerated(df, tblName, writeDbSaveMode, writeDbHint)          }          case (InsertNotExists(keys@_*), true) => {          dbCtl.insertNotExists(DbCommonColumnAppender(df, componentId), tblName, keys, writeDbSaveMode, writeDbHint)          }          case (InsertNotExists(keys@_*), false) => {          dbCtl.insertNotExists(df, tblName, keys, writeDbSaveMode, writeDbHint)          }          case (Update, true) => {          checkKeys dbCtl.updateRecords(modUpdateColumn(df), tblName, writeDbUpdateKeys, writeDbUpdateIgnoreColumns, writeDbHint)          }          case (Update, false) => {          checkKeys dbCtl.updateRecords(df, tblName, writeDbUpdateKeys, writeDbUpdateIgnoreColumns, writeDbHint)          }          case (Upsert, true) => {          checkKeys dbCtl.upsertRecords(DbCommonColumnAppender(df, componentId), tblName,  writeDbUpdateKeys, Set("dt_d2kmkdttm", "id_d2kmkusr", "nm_d2kupdtms", "fg_d2kdelflg") ++ writeDbUpdateIgnoreColumns, writeDbHint)          }          case (Upsert, false) => {          checkKeys dbCtl.upsertRecords(df, tblName, writeDbUpdateKeys, writeDbUpdateIgnoreColumns, writeDbHint)          }          case (DeleteLogical, true) => {          checkKeys  val deleteFlagName = "fg_d2kdelflg"  val deleteTarget = df.withColumn(deleteFlagName, lit("1")).select(deleteFlagName, writeDbUpdateKeys.toSeq :_*) dbCtl.updateRecords( modUpdateColumn(deleteTarget),  tblName, writeDbUpdateKeys, Set("dt_d2kmkdttm", "id_d2kmkusr", "nm_d2kupdtms"), writeDbHint)          }          case (DeleteLogical, false) => throw new IllegalArgumentException ("DeleteLogical and writeDbWithCommonColumn == false can not used be togather")          case (DeletePhysical, _) => {          checkKeys dbCtl.deleteRecords(df, tblName, writeDbUpdateKeys, writeDbHint)          }       } dforg.sqlContext.emptyDataFrame }'
Detail: 'Can't OpenScope for symbol named: 'writeDb(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'makeSliceLen' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '19'
Statement: 'def makeSliceLen(len: Seq[Int]) = len.foldLeft((0, List.empty[(Int, Int)])){(l, r) =>(l._1 + r, l._2 :+ (l._1, l._1 + r))}'
Detail: 'Can't OpenScope for symbol named: 'makeSliceLen(scala.Seq[scala.Int])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'cnvFromFixed' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '21'
Statement: 'def cnvFromFixed(names: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)])(inData: Array[Byte]) = { val dataAndDomainsAndNames = sliceLen.map{    case (start, end) => inData.slice(start, end)    }.zip(domains).zip(names)  val result = Converter.domainConvert(dataAndDomainsAndNames, charEnc) Row(result :_*) }'
Detail: 'Can't OpenScope for symbol named: 'cnvFromFixed(scala.Seq[scala.String],scala.Seq[scala.String],scala.List[scala.Tuple2[scala.Int,scala.Int]],scala.Array[scala.Byte])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'cnvFromFixedWithIndex' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '27'
Statement: 'def cnvFromFixedWithIndex(names: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)])(inData: (Array[Byte], Long)) = { val dataAndDomainsAndNames = sliceLen.map{    case (start, end)if start == -1 && end == -1 => inData._2.toString.getBytes    case (start, end) => inData._1.slice(start, end)    }.zip(domains).zip(names)  val result = Converter.domainConvert(dataAndDomainsAndNames, charEnc) Row(result :_*) }'
Detail: 'Can't OpenScope for symbol named: 'cnvFromFixedWithIndex(scala.Seq[scala.String],scala.Seq[scala.String],scala.List[scala.Tuple2[scala.Int,scala.Int]],Tuple2[Array[Byte],Long])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'cnvFromFixedAddAllData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '36'
Statement: 'def cnvFromFixedAddAllData(names: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)])(inData: Array[Byte]) = { val dataAndDomainsAndNames = sliceLen.map{    case (start, end) => inData.slice(start, end)    }.zip(domains).zip(names)  val result = Seq(new String (inData, charEnc)) ++ Converter.domainConvert(dataAndDomainsAndNames, charEnc) Row(result :_*) }'
Detail: 'Can't OpenScope for symbol named: 'cnvFromFixedAddAllData(scala.Seq[scala.String],scala.Seq[scala.String],scala.List[scala.Tuple2[scala.Int,scala.Int]],scala.Array[scala.Byte])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'addLineBreak' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '42'
Statement: 'def addLineBreak(totalLen: Int, lineBreak: Boolean) = (lineBreak, newLineCode) match {    case (false, _) => totalLen    case (true, CR) => totalLen + 1    case (true, LF) => totalLen + 1    case (true, CRLF) => totalLen + 2 }'
Detail: 'Can't OpenScope for symbol named: 'addLineBreak(scala.Int,scala.Boolean)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'addIndexColumn' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '49'
Statement: 'def addIndexColumn(names: Seq[String], nameList: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)]) = (names :+ SYSTEM_COLUMN_NAME.RECORD_INDEX,  nameList :+ SYSTEM_COLUMN_NAME.RECORD_INDEX,  domains :+ "文字列",  sliceLen :+ (-1, -1))'
Detail: 'Can't OpenScope for symbol named: 'addIndexColumn(scala.Seq[scala.String],scala.Seq[scala.String],scala.Seq[scala.String],scala.List[scala.Tuple2[scala.Int,scala.Int]])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'arrToMap' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '55'
Statement: 'def arrToMap(targetNames: Seq[String], names: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)])(inData: Array[Byte]) : Map[String, String] = { val dataAndDomainsAndNames = sliceLen.map{    case (start, end) => inData.slice(start, end)    }.zip(domains).zip(names)  val targetData = targetNames.flatMap{ target =>dataAndDomainsAndNames.filter(_._2 == target) }  val converted = Converter.domainConvert(targetData, charEnc) targetData.map(_._2).zip(converted).toMap }'
Detail: 'Can't OpenScope for symbol named: 'arrToMap(scala.Seq[scala.String],scala.Seq[scala.String],scala.Seq[scala.String],scala.List[scala.Tuple2[scala.Int,scala.Int]],scala.Array[scala.Byte])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'addArr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '64'
Statement: 'def addArr(row: Row, arr: Array[Byte]) = { val rowValue = Row(row.toSeq :_*) withBinaryRecord.map(_ =>Row.merge(rowValue, Row(arr.clone))).getOrElse(rowValue) }'
Detail: 'Can't OpenScope for symbol named: 'addArr(Row,scala.Array[scala.Byte])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'makeInputDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '72'
Statement: 'def makeInputDf(len: Seq[Int], names: Seq[String], domains: Seq[String], filePath: String) = { val (totalLen_, sliceLen) = makeSliceLen(len)  val totalLen = addLineBreak(totalLen_, lineBreak)  val ziped = names.zip(domains)  val (nameList, domainList) = ziped.filter{    case (names, domain) => !(domain.startsWith(Converter.NOT_USE_PREFIX))    }.unzip  val rdd = Option(preFilter).map{ pf =>sc.binaryRecords(filePath, totalLen).flatMap{ arr =>if (pf._2(arrToMap(pf._1, names, domains, sliceLen)(arr)))       Some(arr) else       None } }.getOrElse(sc.binaryRecords(filePath, totalLen))  val df = if (withIndex)       {       if (charEnc == "JEF")             {             throw new IllegalArgumentException ("JEF CharEnc is not supportted")             }  val rddWithIdx = rdd.zipWithIndex  val (namesWithIdx, nameListWithIdx, domainsWithIdx, sliceLenWithIdx) = addIndexColumn(names, nameList, domains, sliceLen) context.createDataFrame(rddWithIdx.map{          case (arr, long) => addArr(cnvFromFixedWithIndex(namesWithIdx, domainsWithIdx, sliceLenWithIdx)(arr, long), arr)          }, addStructType(Converter.makeSchema(nameListWithIdx)))       } else       {       context.createDataFrame(rdd.map( arr =>addArr(cnvFromFixed(names, domains, sliceLen)(arr), arr)), addStructType(Converter.makeSchema(nameList)))       } Converter.removeHeaderAndFooter(df, hasHeader, hasFooter, names, domains) }'
Detail: 'Can't OpenScope for symbol named: 'makeInputDf(scala.Seq[scala.Int],scala.Seq[scala.String],scala.Seq[scala.String],scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'recordErrorCheckAndMakeInputDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '100'
Statement: 'def recordErrorCheckAndMakeInputDf(len: Seq[Int], names: Seq[String], domains: Seq[String], filePath: String) = { if (charEnc == "JEF")       {       throw new IllegalArgumentException ("JEF CharEnc is not supportted")       } if (!lineBreak)       {       throw new IllegalArgumentException ("must be lineBreak = true when recordLengthCheck = true")       }  val (totalLen_, sliceLen) = makeSliceLen(len)  val totalLen = addLineBreak(totalLen_, lineBreak)  val ziped = names.zip(domains)  val (nameList, domainList) = ziped.filter{    case (names, domain) => !(domain.startsWith(Converter.NOT_USE_PREFIX))    }.unzip  val splitByLineBreak = {    sc.binaryFiles(filePath).flatMap{       case (_, pds) => {       var buff = new Array[Byte] (1)  var resultArr = ArrayBuffer[Array[Byte]]()  var tmpArr = ArrayBuffer[Byte]()  val br = new BufferedInputStream (pds.open()) while (br.read(buff) != -1)             {             val byte = buff(0) if (byte == '\n' || byte == '\r')                   {                   resultArr += tmpArr.toArray tmpArr = ArrayBuffer[Byte]()                   } else                   {                   tmpArr += byte                   }             } Option(preFilter).map{ pf =>resultArr.flatMap{ arr =>if (pf._2(arrToMap(pf._1, names, domains, sliceLen)(arr)))             Some(arr) else             None } }.getOrElse(resultArr)       }       }    }  val df = if (withIndex)       {       val (namesWithIdx, nameListWithIdx, domainsWithIdx, sliceLenWithIdx) = addIndexColumn(names, nameList, domains, sliceLen)  val r = splitByLineBreak.zipWithIndex.map{          case (arr, idx) => val row = cnvFromFixedWithIndex(namesWithIdx, domainsWithIdx, sliceLenWithIdx)(arr, idx) addArr(Row(row.toSeq :+ (arr.size != totalLen_).toString :_*), arr)          } context.createDataFrame(r, addStructType(Converter.makeSchemaWithRecordError(nameListWithIdx)))       } else       {       val r = splitByLineBreak.map{ arr => val row = cnvFromFixed(names, domains, sliceLen)(arr) addArr(Row(row.toSeq :+ (arr.size != totalLen_).toString :_*), arr) } context.createDataFrame(r, addStructType(Converter.makeSchemaWithRecordError(nameList)))       } Converter.removeHeaderAndFooter(df, hasHeader, hasFooter, names, domains) }'
Detail: 'Can't OpenScope for symbol named: 'recordErrorCheckAndMakeInputDf(scala.Seq[scala.Int],scala.Seq[scala.String],scala.Seq[scala.String],scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/MultiDbToMapDf.scala'
Line number: '10'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'comm試算' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.component.sh' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/component/sh/CommissionBaseChannelSelector.scala'
Line number: '11'
Statement: 'def comm試算(uniqueKeys: String*) = new trComm試算 {    override val groupingKeys = uniqueKeys }'
Detail: 'Can't OpenScope for symbol named: 'comm試算(_Seq*[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'comm実績_月次手数料' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.component.sh' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/component/sh/CommissionBaseChannelSelector.scala'
Line number: '17'
Statement: 'def comm実績_月次手数料(uniqueKeys: String*) = new trComm実績_月次手数料 {    override val groupingKeys = uniqueKeys }'
Detail: 'Can't OpenScope for symbol named: 'comm実績_月次手数料(_Seq*[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'comm実績_割賦充当' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.component.sh' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/component/sh/CommissionBaseChannelSelector.scala'
Line number: '23'
Statement: 'def comm実績_割賦充当(uniqueKeys: String*) = new trComm実績_割賦充当 {    override val groupingKeys = uniqueKeys }'
Detail: 'Can't OpenScope for symbol named: 'comm実績_割賦充当(_Seq*[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'comm実績_直営店' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.component.sh' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/component/sh/CommissionBaseChannelSelector.scala'
Line number: '29'
Statement: 'def comm実績_直営店(uniqueKeys: String*) = new trComm実績_直営店 {    override val groupingKeys = uniqueKeys }'
Detail: 'Can't OpenScope for symbol named: 'comm実績_直営店(_Seq*[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'comm毎月割一時金' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.component.sh' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/component/sh/CommissionBaseChannelSelector.scala'
Line number: '35'
Statement: 'def comm毎月割一時金(uniqueKeys: String*) = new trComm毎月割一時金 {    override val groupingKeys = uniqueKeys }'
Detail: 'Can't OpenScope for symbol named: 'comm毎月割一時金(_Seq*[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'writeFilePath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/WriteFile.scala'
Line number: '46'
Statement: 'def writeFilePath(implicit inArgs: InputArgs) : String = inArgs.baseOutputFilePath'
Detail: 'Can't OpenScope for symbol named: 'writeFilePath(d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'writeFile' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/WriteFile.scala'
Line number: '79'
Statement: 'def writeFile(df: DataFrame)(implicit inArgs: InputArgs) = { val writeFilePathAndName = s"${ writeFilePath }/${ writeFileName }"  val vari = new VariableFile (writeFilePathAndName, writeFileVariableWrapDoubleQuote, writeFileVariableEscapeChar, writeCharEncoding, writeFilePartitionColumns, writeFilePartitionExtention)  val fixed = new FixedFile (writeFilePathAndName, writeFilePartitionColumns, writeFilePartitionExtention)  val writer = writeFileMode match {       case Csv => vari.writeSingle(",")       case Csv(wrapTargetCols@_*) => vari.writeSingleCsvWithDoubleQuote(wrapTargetCols.toSet)       case Tsv => vari.writeSingle("\t")       case Fixed(itemLengths@_*) => fixed.writeSingle_MS932(itemLengths)       case Fixed => new FixedFileWithConfFile (writeFilePathAndName).writeFile(writeFileFunc)       case partition.Csv => vari.writePartition(",")       case partition.Csv(wrapTargetCols@_*) => vari.writePartitionCsvWithDoubleQuote(wrapTargetCols.toSet)       case partition.Tsv => vari.writePartition("\t")       case partition.Fixed(itemLengths@_*) => fixed.writePartition_MS932(itemLengths)       case hdfs.Csv => vari.writeHdfs(",")       case hdfs.Tsv => vari.writeHdfs("\t")       case hdfs.Fixed(itemLengths@_*) => fixed.writeHdfs_MS932(itemLengths)       case sequence.Csv => vari.writeSequence(",")       case sequence.Csv(wrapTargetCols@_*) => vari.writeSequenceCsvWithDoubleQuote(wrapTargetCols.toSet)       case sequence.Tsv => vari.writeSequence("\t")       case sequence.Fixed(itemLengths@_*) => fixed.writeSequence_MS932(itemLengths)       case _ => throw new IllegalArgumentException (s"${ writeFileMode } is unusable")    } writer(df) df.sqlContext.emptyDataFrame }'
Detail: 'Can't OpenScope for symbol named: 'writeFile(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/template/DfToFileTest.scala'
Line number: '40'
Statement: 'def exec(implicit inArgs: InputArgs) = targetComponent.run(df)(inArgs)'
Detail: 'Can't OpenScope for symbol named: 'exec(d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'rpad' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/template/DfToFileTest.scala'
Line number: '44'
Statement: 'def rpad(target: String, len: Int, pad: String = " ") = { val str = if (target == null)       {       ""       } else       {       target       }  val strSize = str.getBytes("MS932").size  val padSize = len - strSize s"${ str }${ pad * padSize }" }'
Detail: 'Can't OpenScope for symbol named: 'rpad(scala.String,scala.Int,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'org.apache.spark.sql' package.
The node and its children will be skipped.
Location: 'src/main/scala/org/apache/spark/sql/JdbcCtl.scala'
Line number: '21'
Statement: 'def readTable(dbCtl: DbCtl, tableName: String, requiredColumns: Array[String], where: Array[String] = Array("1 = 1"), whereFilter: Array[Filter] = Array.empty[Filter]) = { val partitions: Array[Partition] = where.zipWithIndex.map{    case (w, idx) => JDBCPartition(w, idx)    }  val structTypes = JDBCRDD.resolveTable(dbCtl.dbInfo.toOptions(tableName))  val reqStructTypes = pruneSchema(structTypes, requiredColumns)  val rdd = JDBCRDD.scanTable( SparkContexts.sc, structTypes, requiredColumns, whereFilter, partitions, dbCtl.dbInfo.toOptions(tableName)).map{ r => val values = internalRowToRow(r, reqStructTypes).zipWithIndex.map{    case (strType, idx) => strType match {       case _if r.isNullAt(idx) == true => null       case StringType => r.getString(idx)       case TimestampType => new Timestamp (r.getLong(idx) / 1000)       case x:DecimalType => r.getDecimal(idx, x.precision, x.scale).toBigDecimal    }    } Row(values :_*) } SparkContexts.context.createDataFrame(rdd, reqStructTypes) }'
Detail: 'Can't OpenScope for symbol named: 'readTable(DbCtl,scala.String,scala.Array[scala.String],scala.Array[scala.String],Array[Filter])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'pruneSchema' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'org.apache.spark.sql' package.
The node and its children will be skipped.
Location: 'src/main/scala/org/apache/spark/sql/JdbcCtl.scala'
Line number: '45'
Statement: 'def pruneSchema(schema: StructType, columns: Array[String]) = { val fieldMap = Map(schema.fields.map( x =>x.name -> x) :_*) new StructType (columns.map( c =>fieldMap(c))) }'
Detail: 'Can't OpenScope for symbol named: 'pruneSchema(org.apache.spark.sql.types.StructType,scala.Array[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'internalRowToRow' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'org.apache.spark.sql' package.
The node and its children will be skipped.
Location: 'src/main/scala/org/apache/spark/sql/JdbcCtl.scala'
Line number: '50'
Statement: 'def internalRowToRow(iRow: InternalRow, schema: StructType) = { schema.map(_.dataType) }'
Detail: 'Can't OpenScope for symbol named: 'internalRowToRow(InternalRow,org.apache.spark.sql.types.StructType)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'takek' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '32'
Statement: 'def takek(inStr: Column, len: Int) = udf_takek(inStr, lit(len))'
Detail: 'Can't OpenScope for symbol named: 'takek(Column,scala.Int)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'rpadk' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '51'
Statement: 'def rpadk(str: Column, len: Int, pad: String) = udf_rpadk(str, lit(len), lit(pad))'
Detail: 'Can't OpenScope for symbol named: 'rpadk(Column,scala.Int,scala.String)''
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'dateCalc' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '96'
Statement: 'def dateCalc(inDt: DateTime, month: Int, day: Int) = { val calcDay = (dt: DateTime) =>if (day != 0)       dt.plusDays(day) else       dt  val calcMonth = (dt: DateTime) =>if (month != 0)       dt.plusMonths(month) else       dt Option(inDt).map((calcMonth andThen calcDay)(_)).getOrElse(inDt) }'
Detail: 'Can't OpenScope for symbol named: 'dateCalc(DateTime,scala.Int,scala.Int)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'calcAndDiff' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '157'
Statement: 'def calcAndDiff(base: String, beginDaysDiff: Int, endDaysDiff: Int = 0)(targets: Column*) : Column = { targets.foldLeft(lit(false)){    case (result, target) => {    result || calcAndDiff(target, lit(base), lit(beginDaysDiff), lit(endDaysDiff))    }    } }'
Detail: 'Can't OpenScope for symbol named: 'calcAndDiff(scala.String,scala.Int,scala.Int,_Seq*[Column])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'getDateRangeStatus' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '236'
Statement: 'def getDateRangeStatus(beginDate: String, endDate: String, runningDateYMD: String) = { runningDateYMD match {       case targetif target < nullToMinDate(beginDate) => STATUS_BEFORE       case targetif target >= nullToMaxDate(endDate) => STATUS_END       case _ => STATUS_ON    } }'
Detail: 'Can't OpenScope for symbol named: 'getDateRangeStatus(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'isBlankDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '244'
Statement: 'def isBlankDate(target: String) = { target match {       case null => true       case "" => true       case "00010101" => true       case _ => false    } }'
Detail: 'Can't OpenScope for symbol named: 'isBlankDate(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'removeHyphen' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '253'
Statement: 'def removeHyphen(target: String) = { target match {       case null => null       case "" => ""       case _ => target.replaceAll("-", "")    } }'
Detail: 'Can't OpenScope for symbol named: 'removeHyphen(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'blankToMaxDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '261'
Statement: 'def blankToMaxDate(target: String) = if (isBlankDate(target))    "99991231" else    target'
Detail: 'Can't OpenScope for symbol named: 'blankToMaxDate(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'nullToMinDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '262'
Statement: 'def nullToMinDate(target: String) = if (target == null)    "00010101" else    target'
Detail: 'Can't OpenScope for symbol named: 'nullToMinDate(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'nullToMaxDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '263'
Statement: 'def nullToMaxDate(target: String) = if (target == null)    "99991231" else    target'
Detail: 'Can't OpenScope for symbol named: 'nullToMaxDate(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'strToDt' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '308'
Statement: 'def strToDt(dateStr: String) = LocalDate.parse(dateStr, DateTimeFormatter.ofPattern("yyyyMMdd"))'
Detail: 'Can't OpenScope for symbol named: 'strToDt(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/MultiPqToMultiAny.scala'
Line number: '10'
Statement: 'def preExec(in: Unit)(implicit inArgs: InputArgs) : Map[String, DataFrame] = readParquet'
Detail: 'Can't OpenScope for symbol named: 'preExec(scala.Unit,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'makeBdField' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '39'
Statement: 'def makeBdField(len: String, name: String) = { val (p, s) = if (len.trim.isEmpty)       {       (10, 0)       } else       {       val ps = len.trim.split(',') if (ps.size == 2)             {             (ps(0).toInt, ps(1).toInt)             } else             {             (len.toInt, 0)             }       } StructField(name, DecimalType(p, s), true) }'
Detail: 'Can't OpenScope for symbol named: 'makeBdField(scala.String,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'itemConfToTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '53'
Statement: 'def itemConfToTable(confPath: String) = { val confs = parseItemConf(confPath)  val tableInfos = confs.map{ c => val data = DomainProcessor.exec(c.cnvType, "x" * c.length.toInt).right.get.mkString("\"", "", "\"")  val ml = Seq(toLength(c.itemName), toLength(c.itemId), toLength(c.length), toLength(c.cnvType), toLength(data)).max  val toMl = toMaxLength(ml)_ TableInfo(toMl(c.itemName), toMl(c.itemId), toMl(c.length), toMl(c.cnvType), toMl(data), ml) } tableInfoToTableStr(tableInfos) }'
Detail: 'Can't OpenScope for symbol named: 'itemConfToTable(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'itemConfMdToTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '64'
Statement: 'def itemConfMdToTable(url: String) = { val md = Source.fromURL(s"${ url }?private_token=${ sys.env("GITLAB_TOKEN") }").getLines.toList.drop(6)  val tableInfos = md.map{ line => val items = line.split('|').drop(1).map(_.trim)  val (id, name, domain, length) = (items(0), items(1), items(2), items(3))  val data = DomainProcessor.exec(domain, "x" * length.toInt).right.get.mkString("\"", "", "\"")  val ml = Seq(toLength(name), toLength(id), toLength(length), toLength(domain), toLength(data)).max  val toMl = toMaxLength(ml)_ TableInfo(toMl(name), toMl(id), toMl(length), toMl(domain), toMl(data), ml) } tableInfoToTableStr(tableInfos) }'
Detail: 'Can't OpenScope for symbol named: 'itemConfMdToTable(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'itemsMdToTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '77'
Statement: 'def itemsMdToTable(url: String) = { def tableInfos(md: BufferedSource) = md.getLines.toList.drop(7).filter(!_.isEmpty).map{ line => val items = line.split('|').drop(1).map(_.trim)  val (id, name, domain, length) = (items(0), items(1), items(2), items(3))  val data = (domain.toLowerCase match {       case "string" | "varchar2" | "char" => "x" * length.toInt       case "timestamp" | "日付時刻" => "00010101000000"       case "date" => "00010101"       case "bigdecimal" | "number" | "decimal" => "0"       case _ => DomainProcessor.exec(domain, "x" * length.toInt).right.get    }).mkString("\"", "", "\"")  val ml = Seq(toLength(name), toLength(id), toLength(length), toLength(domain), toLength(data)).max  val toMl = toMaxLength(ml)_ TableInfo(toMl(name), toMl(id), toMl(length), toMl(domain), toMl(data), ml) } Try{Source.fromURL(s"${ url }?private_token=${ sys.env("GITLAB_TOKEN") }")}.map( md =>tableInfoToTableStr(tableInfos(md))).toOption.orElse{    System.err.println(s"Specific is not found[$url]") None    } }'
Detail: 'Can't OpenScope for symbol named: 'itemsMdToTable(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'makeSchema' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '99'
Statement: 'def makeSchema(colNames: Seq[String], domains: Seq[String], colLengths: Seq[String]) = StructType(colNames.zip(colLengths).zip(domains.map(_.toLowerCase)).map{ case ((name, _), "string") => StructField(name, StringType, true) case ((name, _), "varchar2") => StructField(name, StringType, true) case ((name, _), "char") => StructField(name, StringType, true) case ((name, _), "date") => StructField(name, DateType, true) case ((name, _), "timestamp") => StructField(name, TimestampType, true) case ((name, _), "日付時刻") => StructField(name, TimestampType, true) case ((name, _), "int") => StructField(name, IntegerType, true) case ((name, _), "integer") => StructField(name, IntegerType, true) case ((name, len), "bigdecimal") => makeBdField(len, name) case ((name, len), "numeric") => makeBdField(len, name) case ((name, len), "decimal") => makeBdField(len, name) case ((name, len), "number") => makeBdField(len, name) case ((name, _), _) => StructField(name, StringType, true) })'
Detail: 'Can't OpenScope for symbol named: 'makeSchema(scala.Seq[scala.String],scala.Seq[scala.String],scala.Seq[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'toLength' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '116'
Statement: 'def toLength(str: String) = str.getBytes("MS932").length'
Detail: 'Can't OpenScope for symbol named: 'toLength(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'toMaxLength' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '117'
Statement: 'def toMaxLength(maxLen: Int)(str: String) = { val addLen = maxLen - str.getBytes("MS932").length str + (" " * addLen) }'
Detail: 'Can't OpenScope for symbol named: 'toMaxLength(scala.Int,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'tableColToList' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '121'
Statement: 'def tableColToList(cols: String, defaultValue: String = "") = cols.split('|').map{ col => val value = col.trim if (value.isEmpty)    defaultValue else    value }.toList'
Detail: 'Can't OpenScope for symbol named: 'tableColToList(scala.String,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'removeDq' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '126'
Statement: 'def removeDq(target: String) = "^\"(.*)\"$".r.findFirstMatchIn(target).map(_.group(1).replace("\"\"", "\"")).getOrElse(target)'
Detail: 'Can't OpenScope for symbol named: 'removeDq(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'toTableCol' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '128'
Statement: 'def toTableCol(strSeq: Seq[String]) = strSeq.mkString("| ", " | ", " |")'
Detail: 'Can't OpenScope for symbol named: 'toTableCol(scala.Seq[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'parseItemConf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '139'
Statement: 'def parseItemConf(confPath: String) = { ConfParser.readConf(confPath){ items =>if (items.size == 5)       {       ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true")       } else       {       ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true", items(5))       } }.toSeq }'
Detail: 'Can't OpenScope for symbol named: 'parseItemConf(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'readMdTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '154'
Statement: 'def readMdTable(path: String) : MdInfo = { println(s"read:${ readMdPath }/${ path }") MdInfo(Source.fromFile(s"${ readMdPath }/${ path }").getLines.mkString("\n")) }'
Detail: 'Can't OpenScope for symbol named: 'readMdTable(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'toCsv' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '160'
Statement: 'def toCsv(writeName: String, wrapDoubleQuote: Boolean = false, hasHeader: Boolean = false, lineSeparator: String = "\n") = toVariable(",")(writeName, wrapDoubleQuote, hasHeader, lineSeparator)'
Detail: 'Can't OpenScope for symbol named: 'toCsv(scala.String,scala.Boolean,scala.Boolean,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'toTsv' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '164'
Statement: 'def toTsv(writeName: String, wrapDoubleQuote: Boolean = false, hasHeader: Boolean = false, lineSeparator: String = "\n") = toVariable("\t")(writeName, wrapDoubleQuote, hasHeader, lineSeparator)'
Detail: 'Can't OpenScope for symbol named: 'toTsv(scala.String,scala.Boolean,scala.Boolean,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'toVsv' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '168'
Statement: 'def toVsv(writeName: String, wrapDoubleQuote: Boolean = false, hasHeader: Boolean = false, lineSeparator: String = "\n") = toVariable("|")(writeName, wrapDoubleQuote, hasHeader, lineSeparator)'
Detail: 'Can't OpenScope for symbol named: 'toVsv(scala.String,scala.Boolean,scala.Boolean,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'toSsv' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '172'
Statement: 'def toSsv(writeName: String, wrapDoubleQuote: Boolean = false, hasHeader: Boolean = false, lineSeparator: String = "\n") = toVariable(" ")(writeName, wrapDoubleQuote, hasHeader, lineSeparator)'
Detail: 'Can't OpenScope for symbol named: 'toSsv(scala.String,scala.Boolean,scala.Boolean,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'toVariable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '176'
Statement: 'def toVariable(separator: String)(writeName: String, wrapDoubleQuote: Boolean, hasHeader: Boolean, lineSeparator: String) = { val allData = data.stripMargin.split("\n").drop(1)  val itemNames = tableColToList(allData(0))  val writeData = allData.drop(5).map(tableColToList(_).map( x   =>if (wrapDoubleQuote)       x else       removeDq(x)).mkString(separator)) System.setProperty("line.separator", lineSeparator) FileCtl.writeToFile(s"$outputPath/$writeName", false){ pw =>if (hasHeader)       pw.println(itemNames.mkString(separator)) writeData.foreach(pw.println) } }'
Detail: 'Can't OpenScope for symbol named: 'toVariable(scala.String,scala.String,scala.Boolean,scala.Boolean,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'toFixed' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '189'
Statement: 'def toFixed(writeName: String, hasHeader: Boolean = false, hasFooter: Boolean = false, lineBreak: Boolean = true, lineSeparator: String = "\n", charEnc: String = "MS932") = writeBinData(writeName, lineSeparator){(data: String, len: String, dataType: String) =>dataType match {    case xif x.endsWith("_PD") => pack(BigDecimal(data).toBigInt, BigDecimal(len).toInt)    case xif x.endsWith("_ZD") => zone(BigDecimal(data).toBigInt, BigDecimal(len).toInt)    case "数字" => s"%0${ len }d".format(data.toInt).getBytes(charEnc)    case "数字_SIGNED" => s"%+0${ len }d".format(data.toInt).getBytes(charEnc)    case "未使用" => (" " * BigDecimal(len).toInt).getBytes(charEnc)    case _ => data.padTo(len.toInt, ' ').getBytes(charEnc) } }'
Detail: 'Can't OpenScope for symbol named: 'toFixed(scala.String,scala.Boolean,scala.Boolean,scala.Boolean,scala.String,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'toJef' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '202'
Statement: 'def toJef(writeName: String, lineSeparator: String = "\n") = writeBinData(writeName, lineSeparator){(data: String, len: String, dataType: String) =>dataType match {    case xif x.endsWith("_PD") => pack(BigDecimal(data).toBigInt, BigDecimal(len).toInt)    case xif x.endsWith("_ZD") => zone(BigDecimal(data).toBigInt, BigDecimal(len).toInt)    case "全角文字列" => JefConvert(data).toJefFull    case "未使用" => JefConvert(" " * BigDecimal(len).toInt).toJefHalf    case _ => JefConvert(data).toJefHalf } }'
Detail: 'Can't OpenScope for symbol named: 'toJef(scala.String,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'writeBinData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '213'
Statement: 'def writeBinData(writeName: String, lineSeparator: String)(func: (String, String, String) => Array[Byte]) = { val allData = data.stripMargin.split("\n")  val itemNames = tableColToList(allData(1))  val itemTypes = tableColToList(allData(5))  val itemLengths = tableColToList(allData(4))  val outData = allData.drop(6).map{ tableColToList(_).zip(itemLengths).zip(itemTypes).map{    case ((data, len), types) => func(removeDq(data), len.split(',').head.trim, types)    }.foldLeft(Array[Byte]())(_ ++: _) } writeBytes(s"$outputPath/$writeName")(outData) }'
Detail: 'Can't OpenScope for symbol named: 'writeBinData(scala.String,scala.String,lambda[String,String,String,Array[Byte]])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'zone' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '226'
Statement: 'def zone(target: BigInt, zonedByteLen: Int) = { val sign = if (target < 0)       {       'd'       } else       {       'f'       }  val targetStr = String.valueOf(target.abs)  val pad = "0" * (zonedByteLen - targetStr.length())  val targetBytes = (pad + targetStr).init.map{ char =>Integer.parseInt(s"f${ char }", 16).toByte} (targetBytes :+ Integer.parseInt(s"f${ sign }${ targetStr.last }", 16).toByte).toArray }'
Detail: 'Can't OpenScope for symbol named: 'zone(BigInt,scala.Int)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'pack' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '234'
Statement: 'def pack(target: BigInt, packedByteLen: Int) = { val sign = if (target < 0)       {       'd'       } else       {       'f'       }  val targetStr = (String.valueOf(target.abs)) + sign  val pad = "0" * ((packedByteLen * 2) - targetStr.length())  val targetBytes = (pad + targetStr).grouped(2).map{ hexChar =>Integer.parseInt(hexChar, 16).toByte} targetBytes.toArray }'
Detail: 'Can't OpenScope for symbol named: 'pack(BigInt,scala.Int)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'writeBytes' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '242'
Statement: 'def writeBytes(path: String, lineBreak: Boolean = false, newLineCode: String = "\n")(contents: Array[Array[Byte]]) = { val out = new FileOutputStream (path) contents.foreach{ arr =>out.write(arr) if (lineBreak)       out.write(newLineCode.toCharArray.map(_.toByte)) } out.close }'
Detail: 'Can't OpenScope for symbol named: 'writeBytes(scala.String,scala.Boolean,scala.String,scala.Array[scala.Array[scala.Byte]])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'toPq' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '280'
Statement: 'def toPq(name: String) : Unit = toPq(new PqCtl (outputPath), name)'
Detail: 'Can't OpenScope for symbol named: 'toPq(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'toPq' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '282'
Statement: 'def toPq(pqCtl: PqCtl, name: String) = { import pqCtl.implicits._ toDf.writeParquet(name) }'
Detail: 'Can't OpenScope for symbol named: 'toPq(PqCtl,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'toDb' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '287'
Statement: 'def toDb(tableName: String, dbInfo: DbInfo = DbConnectionInfo.bat1) : Unit = toDb(new DbCtl (dbInfo), tableName)'
Detail: 'Can't OpenScope for symbol named: 'toDb(scala.String,DbInfo)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'toDb' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '289'
Statement: 'def toDb(dbCtl: DbCtl, tableName: String) = { import dbCtl.implicits._ Try{dbCtl.dropTable(tableName)} toDf.writeTableStandard(tableName, SaveMode.Append) }'
Detail: 'Can't OpenScope for symbol named: 'toDb(DbCtl,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'checkCsv' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '295'
Statement: 'def checkCsv(filePath: String, hasHeader: Boolean = false) = checkVariable(",")(filePath, hasHeader)'
Detail: 'Can't OpenScope for symbol named: 'checkCsv(scala.String,scala.Boolean)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'checkTsv' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '296'
Statement: 'def checkTsv(filePath: String, hasHeader: Boolean = false) = checkVariable("\t")(filePath, hasHeader)'
Detail: 'Can't OpenScope for symbol named: 'checkTsv(scala.String,scala.Boolean)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'checkVariable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '298'
Statement: 'def checkVariable(separator: String)(filePath: String, hasHeader: Boolean = false) = { val fileList = Source.fromFile(filePath).getLines.toList  val target = (if (hasHeader)       fileList.drop(1) else       fileList)  val expect = toDf.collect target.zip(expect.zipWithIndex).map{    case (t, (e, idx)) => val splitted = t.split(separator).map(removeDq)  val fieldNames = e.schema.fieldNames (0 until splitted.length).foreach{ pos =>withClue((s"LineNo:${ idx + 7 }", fieldNames(pos))){ splitted(pos).toString mustBe e(pos).toString } }    } }'
Detail: 'Can't OpenScope for symbol named: 'checkVariable(scala.String,scala.String,scala.Boolean)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'checkFixed' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '314'
Statement: 'def checkFixed(filePath: String, hasHeader: Boolean = false, hasFooter: Boolean = false) = { val fileList = Source.fromFile(filePath).getLines.toList  val headerChecked = if (hasHeader)       fileList.drop(1) else       fileList  val target = (hasHeader, hasFooter) match {       case (true, true) => fileList.drop(1).dropRight(1)       case (true, false) => fileList.drop(1)       case (false, true) => fileList.dropRight(1)       case (false, false) => fileList    }  val dataTable = data.stripMargin.split("\n").drop(1)  val colLengths = tableColToList(dataTable(3)).map(_.toInt)  val expect = toDf.collect target.zip(expect.zipWithIndex).map{    case (t, (e, idx)) => val splitted = colLengths.foldLeft((t.getBytes("MS932"), Seq.empty[String])){(l, r) =>(l._1.drop(r), l._2 :+ new String (l._1.take(r), "MS932")) }._2  val fieldNames = e.schema.fieldNames (0 until splitted.length).foreach{ pos =>withClue((s"LineNo:${ idx + 7 }", fieldNames(pos))){ splitted(pos).toString mustBe e(pos).toString } }    } }'
Detail: 'Can't OpenScope for symbol named: 'checkFixed(scala.String,scala.Boolean,scala.Boolean)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'sortDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '342'
Statement: 'def sortDf(target: DataFrame, expect: DataFrame, sortKeys: Seq[String]) = { target.show  val result = if (!sortKeys.isEmpty)       {       (target.sort(sortKeys.head, sortKeys.tail :_*), expect.sort(sortKeys.head, sortKeys.tail :_*))       } else       {       (target, expect)       }  val expectCollect = result._2.collect result._1.show(expectCollect.size, false) testingRows(result._1.collect, expect.rdd.zipWithIndex.sortBy( x =>sortKeys.map( k =>x._1.getAs[String](k)).mkString).collect) }'
Detail: 'Can't OpenScope for symbol named: 'sortDf(DataFrame,DataFrame,scala.Seq[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'checkPq' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '352'
Statement: 'def checkPq(name: String) : Unit = checkPq(new PqCtl (outputPath), name, Seq.empty[String])'
Detail: 'Can't OpenScope for symbol named: 'checkPq(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'checkPq' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '354'
Statement: 'def checkPq(name: String, sortKeys: Seq[String]) : Unit = checkPq(new PqCtl (outputPath), name, sortKeys)'
Detail: 'Can't OpenScope for symbol named: 'checkPq(scala.String,scala.Seq[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'checkPq' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '356'
Statement: 'def checkPq(pqCtl: PqCtl, name: String, sortKeys: Seq[String] = Seq.empty[String]) = sortDf(pqCtl.readParquet(name), toDf, sortKeys)'
Detail: 'Can't OpenScope for symbol named: 'checkPq(PqCtl,scala.String,scala.Seq[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'checkDb' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '358'
Statement: 'def checkDb(tableName: String, sortKeys: Seq[String] = Seq.empty[String], dbInfo: DbInfo = DbConnectionInfo.bat1) : Unit = checkDb(new DbCtl (dbInfo), tableName, sortKeys)'
Detail: 'Can't OpenScope for symbol named: 'checkDb(scala.String,scala.Seq[scala.String],DbInfo)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'checkDb' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '361'
Statement: 'def checkDb(dbCtl: DbCtl, tableName: String, sortKeys: Seq[String]) = sortDf(dbCtl.readTable(tableName), toDf, sortKeys)'
Detail: 'Can't OpenScope for symbol named: 'checkDb(DbCtl,scala.String,scala.Seq[scala.String])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'checkDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '363'
Statement: 'def checkDf(df: DataFrame, lineOffset: Int = 0) = testingRows(df.collect, toDf.rdd.zipWithIndex.collect, lineOffset)'
Detail: 'Can't OpenScope for symbol named: 'checkDf(DataFrame,scala.Int)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'testingRows' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '365'
Statement: 'def testingRows(target: Array[Row], expect: Array[(Row, Long)], lineOffset: Int = 0) = { val fieldNames = expect.headOption.map(_._1.schema.fieldNames).getOrElse(Array.empty[String])  val expectTypes = expect.headOption.map(_._1.schema.fields.map(_.dataType.toString)).getOrElse(Array.empty[String])  val targetTypes = target.headOption.map(_.schema.fields.map( x =>(x.name, x.dataType.toString)).toMap).getOrElse(Map.empty[String, String])  val systemItems = Seq("DT_D2KMKDTTM", "ID_D2KMKUSR", "DT_D2KUPDDTTM", "ID_D2KUPDUSR", "NM_D2KUPDTMS", "FG_D2KDELFLG") if (target.isEmpty)       logger.warn("Target Data is Empty") target.zip(expect).map{    case (t, (e, idx)) => (0 until e.length).foreach{ pos => val name = fieldNames(pos) if (!systemItems.contains(name))       {       withClue((s"LineNo:${ idx + 7 + lineOffset }", name)){ expectTypes(pos) match {             case "DateType" => t.getAs[Any](name).toString.replaceAll("-", "").take(8) mustBe e.getAs[Any](name).toString.replaceAll("-", "").take(8)             case "TimestampType" => t.getAs[Timestamp](name).toString.replaceAll("[-:\\s\\.]", "").take(14) mustBe e.getAs[Timestamp](name).toString.replaceAll("[-:\\s\\.]", "").take(14)             case "IntegerType" => val targetData = if (targetTypes(name).startsWith("Integer"))                {                t.getAs[Integer](name)                } else                {                t.getAs[java.math.BigDecimal](name)                } Option(targetData).map(_.toString).getOrElse("null") mustBe Option(e.getAs[Integer](name)).map(_.toString).getOrElse("null")             case typif typ.startsWith("DecimalType") => {             val targetData = if (targetTypes(name).startsWith("Integer"))                   {                   t.getAs[Integer](name)                   } else                   {                   t.getAs[java.math.BigDecimal](name)                   } Option(targetData).map(_.toString).getOrElse("null") mustBe Option(e.getAs[java.math.BigDecimal](name)).map(_.toString).getOrElse("null")             }             case _ => {             val targetVal = t.getAs[String](name) (if (targetVal == null)                   "" else                   targetVal) mustBe e.getAs[String](name)             }          } }       } }    } }'
Detail: 'Can't OpenScope for symbol named: 'testingRows(Array[Row],Array[Tuple2[Row,Long]],scala.Int)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/PqJoinToPq.scala'
Line number: '10'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = PqJoinToPq.this.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'writeSingle_MS932' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.file.output' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFile.scala'
Line number: '33'
Statement: 'def writeSingle_MS932(itemLengths: Seq[Int]) = (df: DataFrame) =>{ if (writeFilePartitionColumns.isEmpty)       {       Directory(fileName).createDirectory(true, false) Files.deleteIfExists(FileSystems.getDefault.getPath(fileName)) FileCtl.writeToFile(fileName){ pw => val collected = df.collect elapse(s"fileWrite:${ fileName }"){ collected.foreach{ row =>pw.println(mkOutputStr(itemLengths)(row))} } }       } else       {       FileCtl.deleteDirectory(fileName) elapse(s"fileWrite:${ fileName }"){ FileCtl.loanPrintWriterCache{ cache =>df.collect.foldLeft(cache){(l, r) =>FileCtl.writeToFileWithPartitionColumns( fileName, partitionColumns = writeFilePartitionColumns, partitionExtention = writeFilePartitionExtention)( mkOutputStr(itemLengths))(l)(r) } } }       } }'
Detail: 'Can't OpenScope for symbol named: 'writeSingle_MS932(scala.Seq[scala.Int])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'writeHdfs_MS932' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.file.output' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFile.scala'
Line number: '57'
Statement: 'def writeHdfs_MS932(itemLengths: Seq[Int]) = (df: DataFrame) =>{ val partCheckeddDf = if (writeFilePartitionColumns.isEmpty)       {       val paddedDf = df.na.fill("") ~> paddingSpace(itemLengths)  val sch = StructType(Seq(StructField("str", StringType, true)))  val rows = paddedDf.rdd.map( r =>Row(r.toSeq.mkString(""))) SparkContexts.context.createDataFrame(rows, sch).write       } else       {       val targetSchemas = writeFilePartitionColumns.map{ n => val sc = df.schema(n) sc.copy(dataType = StringType) }  val sch = StructType(targetSchemas ++ Seq(StructField("value", StringType, true)))  val fieldNames = LinkedHashSet(df.schema.map(_.name) :_*)  val rows = df.rdd.map{ row => val keyValues = writeFilePartitionColumns.map( n =>row.get(row.fieldIndex(n)).toString)  val fixedValues = (fieldNames -- writeFilePartitionColumns).zip(itemLengths).foldLeft(ListBuffer.empty[String]){(l, r) =>l.append(paddingMS932(row.get(row.fieldIndex(r._1)), r._2)) l }.mkString("") Row((keyValues :+ fixedValues) :_*) } SparkContexts.context.createDataFrame(rows, sch).write.partitionBy(writeFilePartitionColumns :_*)       } partCheckeddDf.mode(SaveMode.Overwrite).text(fileName) }'
Detail: 'Can't OpenScope for symbol named: 'writeHdfs_MS932(scala.Seq[scala.Int])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'paddingSpace' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.file.output' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFile.scala'
Line number: '100'
Statement: 'def paddingSpace(targetLengths: Seq[Int]) = (df: DataFrame) =>{ val items = df.schema.map(_.name).zip(targetLengths)  val padSpace = items.map{    case (n, l) => (n, paddingMS932Udf(df(n), lit(l)))    } df ~> editColumnsAndSelect(padSpace.e) }'
Detail: 'Can't OpenScope for symbol named: 'paddingSpace(scala.Seq[scala.Int])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'writePartition_MS932' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.file.output' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFile.scala'
Line number: '106'
Statement: 'def writePartition_MS932(itemLengths: Seq[Int]) = (df: DataFrame) =>{ import DfCtl.implicits._ if (writeFilePartitionColumns.isEmpty)       {       df.partitionWriteFile( fileName, true, partitionExtention = writeFilePartitionExtention)(mkOutputStr(itemLengths))       } else       {       FileCtl.deleteDirectory(fileName) df.partitionWriteToFileWithPartitionColumns( fileName, writeFilePartitionColumns, true, partitionExtention = writeFilePartitionExtention)(mkOutputStr(itemLengths))       } }'
Detail: 'Can't OpenScope for symbol named: 'writePartition_MS932(scala.Seq[scala.Int])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'writeSequence_MS932' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.file.output' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFile.scala'
Line number: '118'
Statement: 'def writeSequence_MS932(itemLengths: Seq[Int]) = (df: DataFrame) =>{ FileCtl.deleteDirectory(fileName) df.rdd.map( row =>(NullWritable.get, mkOutputBinary(itemLengths)(row))).saveAsSequenceFile(fileName, Some(classOf[org.apache.hadoop.io.compress.SnappyCodec])) }'
Detail: 'Can't OpenScope for symbol named: 'writeSequence_MS932(scala.Seq[scala.Int])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'mkOutputStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.file.output' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFile.scala'
Line number: '124'
Statement: 'def mkOutputStr(itemLengths: Seq[Int])(row: Row) = itemLengths.zipWithIndex.foldLeft(new StringBuffer)((l, r)   =>l.append(new String (mkArrByte(row, r), charSet))).toString'
Detail: 'Can't OpenScope for symbol named: 'mkOutputStr(scala.Seq[scala.Int],Row)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'mkOutputBinary' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.file.output' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFile.scala'
Line number: '128'
Statement: 'def mkOutputBinary(itemLengths: Seq[Int])(row: Row) = itemLengths.zipWithIndex.foldLeft(Array.empty[Byte])((l, r) =>l ++ mkArrByte(row, r))'
Detail: 'Can't OpenScope for symbol named: 'mkOutputBinary(scala.Seq[scala.Int],Row)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'mkArrByte' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.file.output' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFile.scala'
Line number: '131'
Statement: 'def mkArrByte(row: Row, itemInfo: (Int, Int)) = { val (len, idx) = itemInfo Option(row.get(idx)).map{ x => val target = x.toString.getBytes(charSet) if (len <= target.size)       {       target.take(len)       } else       {       target ++ Array.fill(len - target.size)(pad)       } }.getOrElse(Array.fill(len)(pad)) }'
Detail: 'Can't OpenScope for symbol named: 'mkArrByte(Row,scala.Tuple2[scala.Int,scala.Int])''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.dic' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/dic/GenerateDictionary.scala'
Line number: '13'
Statement: 'def apply(baseUrl: String) = new GenerateDictionary (baseUrl)'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'generate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.dic' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/dic/GenerateDictionary.scala'
Line number: '20'
Statement: 'def generate(branch: String) = { val dbResult = generateDictionary(baseUrl, branch, "db")  val pqResult = generateDictionary(baseUrl, branch, "pq")  val result = (dbResult ++ pqResult).groupBy( d =>(d.id, d.dic)).map{    case (k, d) => d.head    }.toList  val outputPath = "data/dicGen"  val outputFile = s"${ outputPath }/d2k_appdef.txt" Directory(outputPath).createDirectory(true, false) FileCtl.writeToFile(outputFile, false, "MS932"){ w =>result.sortBy(_.id).foreach(w.println) } println(s"[Finish Dictionary Generate] ${ outputFile }") }'
Detail: 'Can't OpenScope for symbol named: 'generate(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'generateDictionary' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.dic' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/dic/GenerateDictionary.scala'
Line number: '34'
Statement: 'def generateDictionary(baseUrl: String, branch: String, category: String) = { val itemsUrl = s"${ baseUrl }/raw/${ branch }/apps/common/items/${ category }"  val url = s"${ itemsUrl }/README.md"  val dicConv = dicConvertPattern(baseUrl, "master")  val md = Source.fromURL(s"${ url }?private_token=${ sys.env("GITLAB_TOKEN") }").getLines.toList  val grpList = md.filter(!_.isEmpty).dropWhile( line =>!line.contains("## 業務グループ一覧")).drop(3).map(_.split('|')(1).trim.split('(')(1).dropRight(1))  val files = fileList(itemsUrl, grpList.head) (for {       g <- grpList  f <- fileList(itemsUrl, g)    } yield {    makeDic(dicConv, baseUrl, branch, category, g.split('/')(0), f)    }).flatten }'
Detail: 'Can't OpenScope for symbol named: 'generateDictionary(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'fileList' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.dic' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/dic/GenerateDictionary.scala'
Line number: '52'
Statement: 'def fileList(baseUrl: String, grpReadme: String) = { val grpUrl = s"${ baseUrl }/${ grpReadme }"  val md = Source.fromURL(s"${ grpUrl }?private_token=${ sys.env("GITLAB_TOKEN") }").getLines.toList md.filter(!_.isEmpty).dropWhile( line =>!line.contains("## ")).drop(3).map(_.split('|')(1).trim.split('(')(1).dropRight(1)) }'
Detail: 'Can't OpenScope for symbol named: 'fileList(scala.String,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'makeDic' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.dic' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/dic/GenerateDictionary.scala'
Line number: '59'
Statement: 'def makeDic(dicConv: Map[String, String], baseUrl: String, branch: String, category: String, grpName: String, fileName: String) = { val dicKeys = dicConv.keys.toList  val itemdef = ItemDefParser(baseUrl, branch, s"${ category }/${ grpName }/${ fileName }").get itemdef.details.map{ items => val target = dicConv.keys.filter( k =>itemdef.id.toLowerCase.startsWith(k)).head  val conved = itemdef.id.toLowerCase.replaceAllLiterally(target, dicConv(target)) DicData(conved, itemdef.name, s"${ items.id }[${ items.name }]") } }'
Detail: 'Can't OpenScope for symbol named: 'makeDic(scala.Map[scala.String,scala.String],scala.String,scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'dicConvertPattern' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.dic' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/dic/GenerateDictionary.scala'
Line number: '69'
Statement: 'def dicConvertPattern(baseUrl: String, branch: String) = { val dicConvUrl = s"${ baseUrl }/raw/${ branch }/guide/dicgen/dicPattern.md"  val md = Source.fromURL(s"${ dicConvUrl }?private_token=${ sys.env("GITLAB_TOKEN") }").getLines.toList md.filter(!_.isEmpty).dropWhile( line =>!line.contains("## 項目変換パターン")).drop(3).map{ x => val splitted = x.split('|') (splitted(1).trim -> splitted(2).trim) }.toMap }'
Detail: 'Can't OpenScope for symbol named: 'dicConvertPattern(scala.String,scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'mkFilePath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.file.output' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/file/output/FixedFileTest.scala'
Line number: '32'
Statement: 'def mkFilePath(fileName: String) = s"${ outputDir }/${ fileName }"'
Detail: 'Can't OpenScope for symbol named: 'mkFilePath(scala.String)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/PqToXxx.scala'
Line number: '9'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/PqToXxx.scala'
Line number: '13'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/PqToXxx.scala'
Line number: '17'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/PqToXxx.scala'
Line number: '21'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/PqToXxx.scala'
Line number: '25'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'writePqPath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/WritePq.scala'
Line number: '11'
Statement: 'def writePqPath(implicit inArgs: InputArgs) : String = inArgs.baseOutputFilePath'
Detail: 'Can't OpenScope for symbol named: 'writePqPath(d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:17] Error: An error ocurred at 'OpenScope for node with name 'writeParquet' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/WritePq.scala'
Line number: '15'
Statement: 'def writeParquet(df: DataFrame)(implicit inArgs: InputArgs) = { val pqCtl = new PqCtl (writePqPath) import pqCtl.implicits._ if (writePqPartitionColumns.isEmpty)       {       df.writeParquet(writePqName)       } else       {       df.writeParquetWithPartitionBy(writePqName, writePqPartitionColumns :_*)       } df.sqlContext.emptyDataFrame }'
Detail: 'Can't OpenScope for symbol named: 'writeParquet(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:17] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/OneInToOneOutForDf.scala'
Line number: '10'
Statement: 'preExec(in: IN)(implicit inArgs: InputArgs): DataFrame'
Detail: 'Can't OpenScope for symbol named: 'preExec(d2k.common.df.flow.OneInToOneOutForDf[IN,OUT].IN,d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/OneInToOneOutForDf.scala'
Line number: '12'
Statement: 'exec(df: DataFrame)(implicit inArgs: InputArgs): DataFrame'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/OneInToOneOutForDf.scala'
Line number: '14'
Statement: 'postExec(df: DataFrame)(implicit inArgs: InputArgs): OUT'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'run' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.flow' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/OneInToOneOutForDf.scala'
Line number: '16'
Statement: 'def run(in: IN)(implicit inArgs: InputArgs) : OUT = { val input = try       {preExec(in)}    catch {       case t:Throwable => platformError(t);throw t    } if (inArgs.isDebug)       {       println(s"${ inArgs.applicationId }[input]") input.show(false)       }  val output = try       {exec(input)}    catch {       case t:Throwable => appError(t);throw t    } if (inArgs.isDebug)       {       println(s"${ inArgs.applicationId }[output]") output.show(false)       } try       {postExec(output)}    catch {       case t:Throwable => platformError(t);throw t    } }'
Detail: 'Can't OpenScope for symbol named: 'run(d2k.common.df.flow.OneInToOneOutForDf[IN,OUT].IN,d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'debug' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.flow' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/OneInToOneOutForDf.scala'
Line number: '46'
Statement: 'def debug(in: IN)(implicit inArgs: InputArgs) : OUT = run(in)(inArgs.copy(isDebug = true))'
Detail: 'Can't OpenScope for symbol named: 'debug(d2k.common.df.flow.OneInToOneOutForDf[IN,OUT].IN,d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.executor' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/ConvNa.scala'
Line number: '11'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) : DataFrame = ConvNaTs(ConvNaDate(df, dateColumns), tsColumns)'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.executor' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/ConvNa.scala'
Line number: '17'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) : DataFrame = ConvNaDate(df, dateColumns)'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.executor' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/ConvNa.scala'
Line number: '22'
Statement: 'def apply(df: DataFrame, dateColumnNames: Seq[String])(implicit inArgs: InputArgs) = df.na.fill(dateInit, dateColumnNames).na.replace(dateColumnNames, Map("" -> dateInit))'
Detail: 'Can't OpenScope for symbol named: 'apply(DataFrame,scala.Seq[scala.String],d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.executor' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/ConvNa.scala'
Line number: '28'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) : DataFrame = ConvNaTs(df, tsColumns)'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.executor' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/ConvNa.scala'
Line number: '33'
Statement: 'def apply(df: DataFrame, tsColumnNames: Seq[String])(implicit inArgs: InputArgs) = df.na.fill(tsInit, tsColumnNames).na.replace(tsColumnNames, Map("" -> tsInit))'
Detail: 'Can't OpenScope for symbol named: 'apply(DataFrame,scala.Seq[scala.String],d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/DfJoinVariableToAny.scala'
Line number: '22'
Statement: 'def preExec(left: DataFrame)(implicit inArgs: InputArgs) : DataFrame = { val orgDf = left.columns.foldLeft(left)((df, name) =>df.withColumnRenamed(name, s"$prefixName#$name")) joins.foldLeft(orgDf){(odf, vj) => val (joinDf, uniqId) = vj.inputInfo match {       case x:PqInputInfoBase => (new PqCtl (x.inputDir(componentId)).readParquet(x.pqName), x.pqName)       case x:FileInputInfoBase => {       val fileDf = new FileConv (componentId, x, x.envName, true).makeDf  val droppedRowErr = if (x.dropRowError)             fileDf.drop("ROW_ERR").drop("ROW_ERR_MESSAGE") else             fileDf (droppedRowErr, x.itemConfId)       }    }  val joinedPrefixName = if (vj.prefixName.isEmpty)       uniqId else       vj.prefixName  val addNameDf = joinDf.columns.foldLeft(joinDf){(df, name) =>df.withColumnRenamed(name, s"${ joinedPrefixName }#${ name }") }  val joinedDf = odf.join(addNameDf, vj.joinExprs, "left_outer") vj.dropCols.foldLeft(joinedDf)((l, r) =>l.drop(r)) } }'
Detail: 'Can't OpenScope for symbol named: 'preExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'elapse' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Logging.scala'
Line number: '13'
Statement: 'def elapse(message: String)(func: =>Unit) = { logger.info(s" Start[${ message }]")  val startTime = System.currentTimeMillis func  val endTime = System.currentTimeMillis  val elapse = BigDecimal(endTime - startTime) / 1000 logger.info(f"finish[${ message }] elapse:${ elapse }%,.3fs") }'
Detail: 'Can't OpenScope for symbol named: 'elapse(scala.String,=>Unit)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'toCols' of type 'Mobilize.Scala.AST.SclFunDcl' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '13'
Statement: 'trait Editors {    val colName: String     def toCols(colNames: Set[String], cols: Seq[Column]): (Set[String], Seq[Column]) }'
Detail: 'Can't OpenScope for symbol named: 'toCols(scala.Set[scala.String],Seq[Column])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'toCols' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '18'
Statement: 'def toCols(colNames: Set[String], cols: Seq[Column]) = (colNames - colName, cols :+ (editor as colName))'
Detail: 'Can't OpenScope for symbol named: 'toCols(scala.Set[scala.String],Seq[Column])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'toCols' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '24'
Statement: 'def toCols(colNames: Set[String], cols: Seq[Column]) = Option(editor).map( e =>(colNames - colNameFrom, cols :+ (editor as colNameTo))).getOrElse((colNames - colNameFrom, cols :+ (col(colNameFrom) as colNameTo)))'
Detail: 'Can't OpenScope for symbol named: 'toCols(scala.Set[scala.String],Seq[Column])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'toCols' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '31'
Statement: 'def toCols(colNames: Set[String], cols: Seq[Column]) = (colNames - colName, cols)'
Detail: 'Can't OpenScope for symbol named: 'toCols(scala.Set[scala.String],Seq[Column])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'toCols' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '37'
Statement: 'def toCols(colNames: Set[String], cols: Seq[Column]) = (colNames - colName, cols :+ (col(inColName).cast(castType)))'
Detail: 'Can't OpenScope for symbol named: 'toCols(scala.Set[scala.String],Seq[Column])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'toCols' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '43'
Statement: 'def toCols(colNames: Set[String], cols: Seq[Column]) = { val regxMatchNames = colNames.flatMap(inRegex.r.findFirstIn) (colNames -- regxMatchNames, cols ++ regxMatchNames.map( name =>col(name).cast(castType))) }'
Detail: 'Can't OpenScope for symbol named: 'toCols(scala.Set[scala.String],Seq[Column])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'toCols' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '51'
Statement: 'def toCols(colNames: Set[String], cols: Seq[Column]) = (colNames - colName, cols :+ func(col(inColName)))'
Detail: 'Can't OpenScope for symbol named: 'toCols(scala.Set[scala.String],Seq[Column])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'toCols' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '57'
Statement: 'def toCols(colNames: Set[String], cols: Seq[Column]) = { val regxMatchNames = colNames.flatMap(inRegex.r.findFirstIn) (colNames -- regxMatchNames, cols ++ regxMatchNames.map( name =>func(col(name)))) }'
Detail: 'Can't OpenScope for symbol named: 'toCols(scala.Set[scala.String],Seq[Column])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'editColumns' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '63'
Statement: 'def editColumns(editors: Seq[Editors]) = (df: DataFrame) =>{ val (colNames, cols) = editors.foldLeft((df.schema.fieldNames.toSet, Seq.empty[Column])){    case ((colNames, cols), target) => target.toCols(colNames, cols)    }  val schemaNames = colNames.map( d =>col(d)) df.select((schemaNames.toSeq ++ cols) :_*) }'
Detail: 'Can't OpenScope for symbol named: 'editColumns(scala.Seq[spark.common.DfCtl.Editors])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'editColumnsAndSelect' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '71'
Statement: 'def editColumnsAndSelect(editors: Seq[Editors]) = (df: DataFrame) =>{ val (colNames, cols) = editors.foldLeft((df.schema.fieldNames.toSet, Seq.empty[Column])){    case ((colNames, cols), target) => target.toCols(colNames, cols)    } df.select(cols :_*) }'
Detail: 'Can't OpenScope for symbol named: 'editColumnsAndSelect(scala.Seq[spark.common.DfCtl.Editors])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'applyAll' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '78'
Statement: 'def applyAll(colNames: Seq[String], applyCode: Column => Column) = (df: DataFrame) =>{ val schemaNames = colNames.map( d =>applyCode(col(d))) df.select(schemaNames :_*) }'
Detail: 'Can't OpenScope for symbol named: 'applyAll(scala.Seq[scala.String],lambda[Column,Column])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'selectMaxValue' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '83'
Statement: 'def selectMaxValue(targetCols: Seq[String], orderCols: Seq[Column]) = (df: DataFrame) =>{ val win = Window.partitionBy(targetCols.map(col) :_*).orderBy(orderCols :_*) df.withColumn("rank", row_number.over(win)).filter("rank = 1").drop("rank") }'
Detail: 'Can't OpenScope for symbol named: 'selectMaxValue(scala.Seq[scala.String],Seq[Column])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'groupingAgg' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '88'
Statement: 'def groupingAgg(groupingColumns: Seq[String], aggColumns: Seq[Column]) = (df: DataFrame) =>{ df.groupBy(groupingColumns.map(col) :_*).agg(aggColumns.head, aggColumns.tail :_*) }'
Detail: 'Can't OpenScope for symbol named: 'groupingAgg(scala.Seq[scala.String],Seq[Column])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'groupingSum' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '92'
Statement: 'def groupingSum(groupingColumns: Seq[String], sumColumns: Seq[String]) = (df: DataFrame) =>{ val cols = sumColumns.map( col =>sum(col) as col) groupingAgg(groupingColumns, cols)(df) }'
Detail: 'Can't OpenScope for symbol named: 'groupingSum(scala.Seq[scala.String],scala.Seq[scala.String])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'addColumnPrefix' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '97'
Statement: 'def addColumnPrefix(name: String) = (df: DataFrame) =>{ val cols = df.schema.map( x =>df(x.name) as s"${ name }_${ x.name }") df.select(cols :_*) }'
Detail: 'Can't OpenScope for symbol named: 'addColumnPrefix(scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'dropColumnPrefix' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '102'
Statement: 'def dropColumnPrefix(name: String) = (df: DataFrame) =>{ val cols = df.schema.map(_.name).filter(!_.startsWith(s"${ name }_")).map(col) df.select(cols :_*) }'
Detail: 'Can't OpenScope for symbol named: 'dropColumnPrefix(scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'pickMaxValueRow' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '112'
Statement: 'def pickMaxValueRow(pks: String*)(maxValueTarget: String*) = df.sort((pks.map( x =>col(x)) ++ maxValueTarget.map( x =>col(x).desc)) :_*).dropDuplicates(pks)'
Detail: 'Can't OpenScope for symbol named: 'pickMaxValueRow(_Seq*[scala.String],_Seq*[scala.String])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'partitionWriteFile' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '115'
Statement: 'def partitionWriteFile(filePath: String, overwrite: Boolean = true, charEnc: String = "MS932", partitionExtention: String = "")(func: Row => String) {    if (overwrite)       FileCtl.deleteDirectory(filePath) FileCtl.createDirectory(filePath) df.rdd.mapPartitionsWithIndex{(idx, iterRow) => val fullPath = FileCtl.addExtention(s"${ filePath }/${ idx }", partitionExtention) FileCtl.writeToFile(fullPath, true, charEnc){ pw =>elapse(s"fileWrite:${ fullPath }"){ iterRow.foreach( row =>pw.println(func(row))) } } Seq.empty[Row].toIterator }.foreach(_ =>()) }'
Detail: 'Can't OpenScope for symbol named: 'partitionWriteFile(scala.String,scala.Boolean,scala.String,scala.String,lambda[Row,String])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'partitionWriteToFileWithPartitionColumns' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '132'
Statement: 'def partitionWriteToFileWithPartitionColumns(filePath: String, partitionColumns: Seq[String], overwrite: Boolean = true, charEnc: String = "MS932", partitionExtention: String = "")(func: Row => String) {    if (overwrite)       FileCtl.deleteDirectory(filePath) FileCtl.createDirectory(filePath) df.rdd.mapPartitionsWithIndex{(idx, iterRow) => val fullPath = FileCtl.addExtention(s"${ filePath }/${ idx }", partitionExtention) elapse(s"fileWrite:${ fullPath }"){ FileCtl.loanPrintWriterCache{ cache =>iterRow.foldLeft(cache){(l, r) =>FileCtl.writeToFileWithPartitionColumns( filePath, idx, charEnc, partitionColumns, partitionExtention)(func)(l)(r) } } } Seq.empty[Row].toIterator }.foreach(_ =>()) }'
Detail: 'Can't OpenScope for symbol named: 'partitionWriteToFileWithPartitionColumns(scala.String,scala.Seq[scala.String],scala.Boolean,scala.String,scala.String,lambda[Row,String])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/AnyToPq_Db.scala'
Line number: '14'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) = { writeParquet(df)  val pqCtl = new PqCtl (writePqPath) writeDb(pqCtl.readParquet(writePqName)) }'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'checkData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/spark/common/DfCtlTest.scala'
Line number: '367'
Statement: 'def checkData(path: String, target: String) = { val basePath = Path(path)  val files = basePath.jfile.listFiles  val recs = files.flatMap( name =>Source.fromFile(name).getLines) recs.contains(target) }'
Detail: 'Can't OpenScope for symbol named: 'checkData(scala.String,scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'extentionCheck' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/spark/common/DfCtlTest.scala'
Line number: '408'
Statement: 'def extentionCheck(path: String, extention: String) = Path(path).jfile.listFiles.map(_.toString.endsWith(extention)).forall(_ == true)'
Detail: 'Can't OpenScope for symbol named: 'extentionCheck(scala.String,scala.String)''
[03/24/2023 05:42:18] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:18] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:18] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'lastUpdateTime' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/InputArgs.scala'
Line number: '48'
Statement: 'def lastUpdateTime(tableName: String, readDbInfo: DbInfo = DbConnectionInfo.bat1) = { val options = new JDBCOptions (Map( JDBCOptions.JDBC_URL -> readDbInfo.url,  JDBCOptions.JDBC_TABLE_NAME -> tableName,  "user" -> readDbInfo.user,  "password" -> readDbInfo.password,  "charSet" -> readDbInfo.charSet))  val ps = JdbcUtils.createConnectionFactory(options)().prepareStatement("select DT_FROMUPDYMDTM, DT_TOUPDYMDTM from MOP012 where ID_TBLID = ?") ps.setString(1, tableName)  val rs = ps.executeQuery  val result = try       {Iterator.continually((rs.next, rs)).takeWhile(_._1).map{          case (_, rec) => LastUpdateTime(rec.getTimestamp("DT_FROMUPDYMDTM"), rec.getTimestamp("DT_TOUPDYMDTM"))          }.toSeq}    finally       {       rs.close       } result.headOption.getOrElse(throw new IllegalArgumentException (s"tableName is not defined[$tableName]")) }'
Detail: 'Can't OpenScope for symbol named: 'lastUpdateTime(scala.String,DbInfo)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/InputArgs.scala'
Line number: '74'
Statement: 'def apply(d2kBasePath: String, productionMode: String, confPath: String, dataPath: String, projectId: String, processId: String, applicationId: String, runningDateFileFullPath: String) = { val confBasePath = s"$d2kBasePath/$productionMode/$confPath"  val fileConvInputFile = s"$confBasePath/import/${ projectId }_app.conf"  val fileConvOutputFile = s"$confBasePath/export/${ projectId }_app.conf"  val baseInputFilePath = s"$d2kBasePath/$productionMode/$dataPath/output"  val baseOutputFilePath = s"$d2kBasePath/$productionMode/$dataPath/output"  val baseErrProofFilePath = s"$d2kBasePath/$productionMode/$dataPath/error"  val runningDates = Source.fromFile(runningDateFileFullPath).getLines.toList(1).split(" ")  val dateFormat = DateTimeFormat.forPattern("yyyyMMdd")  val runningSQLDate = new Date (dateFormat.withZoneUTC.parseDateTime(runningDates(0)).getMillis) new InputArgs (d2kBasePath, productionMode, confPath, dataPath,  projectId, processId, applicationId,  runningDateFileFullPath, confBasePath, fileConvInputFile, fileConvOutputFile,  baseInputFilePath, baseOutputFilePath, baseErrProofFilePath,  runningDates, runningSQLDate) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/InputArgs.scala'
Line number: '95'
Statement: 'def apply(projectId: String, processId: String, applicationId: String, runningDateFileFullPath: String) : InputArgs = { apply("/D2Khome", "HN", "APL/conf/spark", "sparkWK/Parquet", projectId, processId, applicationId, runningDateFileFullPath) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'execUt' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.app.test.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/MarkdownTester.scala'
Line number: '14'
Statement: 'def execUt(componentInstanceName: String)(targets: (DataFrame => DataFrame)*) = { val classNames = targets.head.getClass.getName.split('$')  val appName = classNames.head.split('.').last  val makeRes = MakeResource("test/dev/data/output", s"${ appName }Test/ut/${ componentInstanceName }") s"be success ${ componentInstanceName }" when {    targets.foreach{ func => val funcName = func.getClass.getName.split('$').dropRight(1).takeRight(1).head funcName in {       val df = makeRes.readMdTable(s"${ funcName }_data.md").toDf if (showData)             println(s"[Input Data:${ componentInstanceName }:${ funcName }]");df.show(false)  val expect = makeRes.readMdTable(s"${ funcName }_expect.md") if (showData)             println(s"[Expect Data:${ componentInstanceName }:${ funcName }]");expect.toDf.show(false)  val result = func(df) if (showData)             println(s"[Result Data:${ componentInstanceName }:${ funcName }]");result.show(false) withClue("Record Size Check"){ result.count mustBe expect.toDf.count } expect.checkDf(result)       } }    } }'
Detail: 'Can't OpenScope for symbol named: 'execUt(scala.String,_Seq*[Tuple2[lambda[DataFrame,DataFrame]]])''
[03/24/2023 05:42:18] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:18] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/OneInToOneOut.scala'
Line number: '6'
Statement: 'preExec(in: IN)(implicit inArgs: InputArgs): PREOUT'
Detail: 'Can't OpenScope for symbol named: 'preExec(d2k.common.df.flow.base.OneInToOneOut[IN,PREOUT,MID,POSTIN,OUT].IN,d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/OneInToOneOut.scala'
Line number: '8'
Statement: 'exec(df: MID)(implicit inArgs: InputArgs): MID'
Detail: 'Can't OpenScope for symbol named: 'exec(d2k.common.df.flow.base.OneInToOneOut[IN,PREOUT,MID,POSTIN,OUT].MID,d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/OneInToOneOut.scala'
Line number: '10'
Statement: 'postExec(df: POSTIN)(implicit inArgs: InputArgs): OUT'
Detail: 'Can't OpenScope for symbol named: 'postExec(d2k.common.df.flow.base.OneInToOneOut[IN,PREOUT,MID,POSTIN,OUT].POSTIN,d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'run' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/OneInToOneOut.scala'
Line number: '12'
Statement: 'run(in: IN)(implicit inArgs: InputArgs): OUT'
Detail: 'Can't OpenScope for symbol named: 'run(d2k.common.df.flow.base.OneInToOneOut[IN,PREOUT,MID,POSTIN,OUT].IN,d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.parser' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/ItemDefParser.scala'
Line number: '9'
Statement: 'def apply(s: String) = { val splitted = s.split('|') new TableItem5 (splitted(1), splitted(2), splitted(3), splitted(4), splitted(5)) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.parser' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/ItemDefParser.scala'
Line number: '17'
Statement: 'def apply(baseUrl: String, branch: String, filePath: String) = { val parsed = parseAll(itemDef, readItemDefMd(baseUrl, branch, filePath)) println(parsed) parsed }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.parser' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/ItemDefParser.scala'
Line number: '23'
Statement: 'def apply(basePath: String) = { parseAll(itemDef, readItemDefMd(basePath)) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'writeFilePath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.mixIn' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/mixIn/OraLoader.scala'
Line number: '11'
Statement: 'def writeFilePath(implicit inArgs: InputArgs) = sys.env("DB_LOADING_FILE_PATH")'
Detail: 'Can't OpenScope for symbol named: 'writeFilePath(d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfUnionToDf.scala'
Line number: '10'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'makeDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/TextConverter.scala'
Line number: '19'
Statement: 'def makeDf(options: Map[String, String])(names: Seq[String], domains: Seq[String], path: Set[String]) = { val rdd = SparkContexts.context.read.options(options).csv(path.toSeq :_*).rdd.map{ row => val dataAndDomainsAndNames = row.toSeq.map( s =>Option(s).map(_.toString).getOrElse("")).zip(domains).zip(names) Row(Converter.domainConvert(dataAndDomainsAndNames) :_*) }  val ziped = names.zip(domains)  val (nameList, domainList) = ziped.filter{    case (names, domain) => !(domain.startsWith(Converter.NOT_USE_PREFIX) || domain.startsWith(Converter.REC_DIV_PREFIX))    }.unzip context.createDataFrame(rdd, Converter.makeSchema(nameList)) }'
Detail: 'Can't OpenScope for symbol named: 'makeDf(scala.Map[scala.String,scala.String],scala.Seq[scala.String],scala.Seq[scala.String],scala.Set[scala.String])''
[03/24/2023 05:42:18] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:18] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:18] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:18] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:18] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'dateTest' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/fileConv/DomainProcessorTest.scala'
Line number: '728'
Statement: 'def dateTest(domain_name: String, empty: String, max: String, max_over: String, min: String, min_under: String, not_digit: String, invalid: String, INVALID_DATE_MSG: String = null) {    val max_exp = max.replaceAll("[-:/]", "")  val min_exp = min.replaceAll("[-:/]", "")  val actual_empty = execRight(domain_name, empty) actual_empty mustBe min_exp  val nullString = new String (Array[Byte](0x00))  val actual_null = execRight(domain_name, empty.replaceAll(" ", nullString)) actual_null mustBe min_exp  val actual_max = execRight(domain_name, max) actual_max mustBe max_exp  val actual_min = execRight(domain_name, min) actual_min mustBe min_exp if (min_under != null)       {       val actual_min_under = execRight(domain_name, min_under) actual_min_under mustBe min_exp       } }'
Detail: 'Can't OpenScope for symbol named: 'dateTest(scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'timestampTest' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/fileConv/DomainProcessorTest.scala'
Line number: '762'
Statement: 'def timestampTest(domain_name: String, empty: String, max: String, max_over: String, min: String, min_under: String, not_digit: String, invalid: String, max_exp: String, min_exp: String) {    val actual_empty = execRight(domain_name, empty) actual_empty mustBe min_exp  val nullString = new String (Array[Byte](0x00))  val actual_null = execRight(domain_name, empty.replaceAll(" ", nullString)) actual_null mustBe min_exp  val actual_max = execRight(domain_name, max) actual_max mustBe max_exp  val actual_min = execRight(domain_name, min) actual_min mustBe min_exp if (min_under != null)       {       val actual_min_under = execRight(domain_name, min_under) actual_min_under mustBe min_exp       } }'
Detail: 'Can't OpenScope for symbol named: 'timestampTest(scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'execLeft' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/fileConv/DomainProcessorTest.scala'
Line number: '784'
Statement: 'def execLeft(domain_name: String, data: String) = checkLeft(DomainProcessor.exec(domain_name, data))'
Detail: 'Can't OpenScope for symbol named: 'execLeft(scala.String,scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'execLeft2' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/fileConv/DomainProcessorTest.scala'
Line number: '787'
Statement: 'def execLeft2(domain_name: String, data: Array[Int], charEnc: String = "MS932") = checkLeft(DomainProcessor.execArrayByte(domain_name, data.map(_.toByte), charEnc))'
Detail: 'Can't OpenScope for symbol named: 'execLeft2(scala.String,scala.Array[scala.Int],scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'execRight' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/fileConv/DomainProcessorTest.scala'
Line number: '797'
Statement: 'def execRight(domain_name: String, data: String) = checkRight(DomainProcessor.exec(domain_name, data))'
Detail: 'Can't OpenScope for symbol named: 'execRight(scala.String,scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'execRight2' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/fileConv/DomainProcessorTest.scala'
Line number: '800'
Statement: 'def execRight2(domain_name: String, data: Array[Int], charEnc: String = "MS932") = checkRight(DomainProcessor.execArrayByte(domain_name, data.map(_.toByte), charEnc))'
Detail: 'Can't OpenScope for symbol named: 'execRight2(scala.String,scala.Array[scala.Int],scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfJoinPqToDf.scala'
Line number: '10'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'writeToFile' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '14'
Statement: 'def writeToFile(fileName: String, append: Boolean = false, charEnc: String = "MS932")(func: PrintWriter => Unit) {    val outFile = new PrintWriter (new OutputStreamWriter (new FileOutputStream (fileName, append), charEnc)) func(outFile) outFile.close() }'
Detail: 'Can't OpenScope for symbol named: 'writeToFile(scala.String,scala.Boolean,scala.String,lambda[PrintWriter,Unit])''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'addExtention' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '27'
Statement: 'def addExtention(path: String, ext: String) = if (ext.isEmpty)    path else    s"${ path }.${ ext }"'
Detail: 'Can't OpenScope for symbol named: 'addExtention(scala.String,scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'writeToFileWithPartitionColumns' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '29'
Statement: 'def writeToFileWithPartitionColumns(fileName: String, partitionIndex: Int = 0, charEnc: String = "MS932", partitionColumns: Seq[String] = Seq.empty[String], partitionExtention: String = "")(func: Row => String)(pwCache: Map[String, PrintWriter])(row: Row) = { val outPath = partitionColumns.map{ col =>s"${ col }=${ row.getAs[String](col) }" }.mkString(s"${ fileName }/", "/", "") FileCtl.createDirectory(outPath)  val outFile = pwCache.getOrElse(outPath,  new PrintWriter (new OutputStreamWriter (new FileOutputStream ( addExtention(s"${ outPath }/${ partitionIndex }", partitionExtention), true), charEnc))) outFile.println(func(row)) if (pwCache.isDefinedAt(outPath))       pwCache else       pwCache.updated(outPath, outFile) }'
Detail: 'Can't OpenScope for symbol named: 'writeToFileWithPartitionColumns(scala.String,scala.Int,scala.String,scala.Seq[scala.String],scala.String,lambda[Row,String],Map[String,PrintWriter],Row)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'loadEnv' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '46'
Statement: 'def loadEnv(filePath: String) : Properties = { val env = new Properties () env.load(new FileInputStream (filePath)) env }'
Detail: 'Can't OpenScope for symbol named: 'loadEnv(scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'exists' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '52'
Statement: 'def exists(filePath: String) : Boolean = { val conf = SparkContexts.sc.hadoopConfiguration  val fs = FileSystem.get(conf) Option(fs.globStatus(new Path (filePath))).map(!_.isEmpty).getOrElse(false) }'
Detail: 'Can't OpenScope for symbol named: 'exists(scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'createDirectory' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '58'
Statement: 'def createDirectory(fullPath: String) {    Directory(fullPath).createDirectory(true, false) }'
Detail: 'Can't OpenScope for symbol named: 'createDirectory(scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'createDirectory' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '62'
Statement: 'def createDirectory(dirPath: String, filePath: String) {    (Directory(dirPath) / filePath).createDirectory(true, false) }'
Detail: 'Can't OpenScope for symbol named: 'createDirectory(scala.String,scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'deleteDirectory' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '66'
Statement: 'def deleteDirectory(fullPath: String) {    Directory(fullPath).deleteRecursively }'
Detail: 'Can't OpenScope for symbol named: 'deleteDirectory(scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'deleteDirectory' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '70'
Statement: 'def deleteDirectory(dirPath: String, filePath: String) {    (Directory(dirPath) / filePath).deleteRecursively }'
Detail: 'Can't OpenScope for symbol named: 'deleteDirectory(scala.String,scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'toMarkdown' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.app.test.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/ExcelConverter.scala'
Line number: '9'
Statement: 'def toMarkdown(excelPath: String, sheet: String, mdFile: String) = { val data = to2DArray(excelPath, sheet)  val colSize = data(0).filter(!_.isEmpty).size  val x: Seq[List[String]] = Seq( data(0).toList,  data(0).map(_ =>"----").toList,  data(0).toList,  data(1).map{    case xif x.endsWith("_ZD") => "ZD"    case xif x.endsWith("_PD") => "PD"    case "全角文字列" => "全角文字列"    case _ => "半角文字列"    }.toList.take(colSize),  data(2).toList.take(colSize))  val x2 = x ++ data.toList.drop(3).map(_.toList)  val x3 = x2.map( a =>a.take(colSize).mkString("|", "|", "|")) System.setProperty("line.separator", "\n") FileCtl.writeToFile(mdFile, charEnc = "UTF-8"){ pw =>pw.println(s"# ${ sheet }") x3.foreach(pw.println) } }'
Detail: 'Can't OpenScope for symbol named: 'toMarkdown(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'to2DArray' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.app.test.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/ExcelConverter.scala'
Line number: '33'
Statement: 'def to2DArray(path: String, sheetName: String) = { val sheet = getTargetSheet(path, sheetName)  val rowCnt = sheet.getLastRowNum() + 1 (0 to rowCnt).flatMap{ i =>Option(sheet.getRow(i)).map{ row =>(0 to row.getLastCellNum).flatMap{ cellCnt =>Option(row.getCell(cellCnt)).map( cell =>getStrVal(cell)) } } } }'
Detail: 'Can't OpenScope for symbol named: 'to2DArray(scala.String,scala.String)''
[03/24/2023 05:42:18] Error: An error ocurred at 'OpenScope for node with name 'getTargetSheet' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.app.test.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/ExcelConverter.scala'
Line number: '60'
Statement: 'def getTargetSheet(path: String, sheetName: String) = { val input = new File (path)  val book = WorkbookFactory.create(input) book.getSheet(sheetName) }'
Detail: 'Can't OpenScope for symbol named: 'getTargetSheet(scala.String,scala.String)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/AnyToDb.scala'
Line number: '10'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) = writeDb(df)'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DbToXxx.scala'
Line number: '9'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DbToXxx.scala'
Line number: '13'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DbToXxx.scala'
Line number: '17'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DbToXxx.scala'
Line number: '21'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DbToXxx.scala'
Line number: '25'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/CommonServices.scala'
Line number: '9'
Statement: 'def apply(df: DataFrame, componentId: String)(implicit inArgs: InputArgs) = { val addCommonItems = df.withColumn("DT_D2KMKDTTM", lit(inArgs.sysSQLDate)).withColumn("ID_D2KMKUSR", lit(componentId)).withColumn("DT_D2KUPDDTTM", lit(inArgs.sysSQLDate)).withColumn("ID_D2KUPDUSR", lit(componentId)).withColumn("NM_D2KUPDTMS", lit("0")).withColumn("FG_D2KDELFLG", lit("0"))  val comonColumnNames = Array("DT_D2KMKDTTM", "ID_D2KMKUSR", "DT_D2KUPDDTTM", "ID_D2KUPDUSR", "NM_D2KUPDTMS", "FG_D2KDELFLG")  val otherColumns = addCommonItems.columns  val dropCommonColumns = comonColumnNames.foldLeft(otherColumns){(l, r) =>l.filter(_ != r)}  val moveToFrontColumns = comonColumnNames ++ dropCommonColumns addCommonItems.select(moveToFrontColumns.head, moveToFrontColumns.drop(1).toSeq :_*) }'
Detail: 'Can't OpenScope for symbol named: 'apply(DataFrame,scala.String,d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'inputDir' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/InputInfo.scala'
Line number: '53'
Statement: 'def inputDir(componentId: String) : String = Try{sys.env(s"PQ_INPUT_PATH_${ componentId }")}. getOrElse(Try{sys.env(s"PQ_INPUT_PATH_${ envName }")}.getOrElse(sys.env(s"PQ_INPUT_PATH_${ ENV_NAME_DEFAULT }")))'
Detail: 'Can't OpenScope for symbol named: 'inputDir(scala.String)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'inputDir' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/InputInfo.scala'
Line number: '64'
Statement: 'def inputDir(componentId: String) : String = Try{sys.env(s"FILE_INPUT_PATH_${ componentId }")}. getOrElse(Try{sys.env(s"FILE_INPUT_PATH_${ envName }")}.getOrElse(sys.env(s"FILE_INPUT_PATH_${ ENV_NAME_DEFAULT }")))'
Detail: 'Can't OpenScope for symbol named: 'inputDir(scala.String)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'searchResource' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/GeneratingApplicationRouteFinder.scala'
Line number: '27'
Statement: 'def searchResource(resourceName: String) = appDefList.filter{ case (_, appdef) => val existCheck = appdef.outputList.filter(_.id == resourceName) !existCheck.isEmpty }'
Detail: 'Can't OpenScope for symbol named: 'searchResource(scala.String)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'readDb' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/MultiReadDb.scala'
Line number: '11'
Statement: 'def readDb(implicit inArgs: InputArgs) = readTableNames.map( tblnm =>(tblnm, readDbSingle(tblnm))).toMap'
Detail: 'Can't OpenScope for symbol named: 'readDb(d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'd2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/spark/common/DbCtlTest.scala'
Line number: '23'
Statement: 'def d2s(dateMill: Long) = new DateTime (dateMill).toString("yyyy-MM-dd")'
Detail: 'Can't OpenScope for symbol named: 'd2s(scala.Long)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'date2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/spark/common/DbCtlTest.scala'
Line number: '24'
Statement: 'def date2s(date: Date) = new DateTime (date).toString("yyyy-MM-dd")'
Detail: 'Can't OpenScope for symbol named: 'date2s(_Unresolved.Date)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'd2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/spark/common/DbCtlTest.scala'
Line number: '25'
Statement: 'def d2s(date: Timestamp) = new DateTime (date).toString("yyyy-MM-dd")'
Detail: 'Can't OpenScope for symbol named: 'd2s(_Unresolved.Timestamp)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'testDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/spark/common/DbCtlTest.scala'
Line number: '546'
Statement: 'def testDate(day: Int) = DateTime.parse(s"2016-1-${ day }").getMillis'
Detail: 'Can't OpenScope for symbol named: 'testDate(scala.Int)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'day' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/spark/common/DbCtlTest.scala'
Line number: '566'
Statement: 'def day(date: Timestamp) = new DateTime (date.getTime).getDayOfMonth'
Detail: 'Can't OpenScope for symbol named: 'day(_Unresolved.Timestamp)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.executor' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/PqCommonColumnRemover.scala'
Line number: '10'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) : DataFrame = PqCommonColumnRemover(df)'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.executor' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/PqCommonColumnRemover.scala'
Line number: '14'
Statement: 'def apply(df: DataFrame)(implicit inArgs: InputArgs) = df.drop(Converter.SYSTEM_COLUMN_NAME.ROW_ERROR).drop(Converter.SYSTEM_COLUMN_NAME.ROW_ERROR_MESSAGE)'
Detail: 'Can't OpenScope for symbol named: 'apply(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'timestamp_yyyymmdd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/InputArgsTest.scala'
Line number: '63'
Statement: 'def timestamp_yyyymmdd(yyyy: Int, mm: Int, dd: Int) = Timestamp.valueOf(LocalDateTime.of(yyyy, mm, dd, 0, 0, 0))'
Detail: 'Can't OpenScope for symbol named: 'timestamp_yyyymmdd(scala.Int,scala.Int,scala.Int)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/InputArgsTest.scala'
Line number: '140'
Statement: 'def exec(implicit inArgs: InputArgs) = { dbTodb.run(Unit) }'
Detail: 'Can't OpenScope for symbol named: 'exec(d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/InputArgsTest.scala'
Line number: '147'
Statement: 'def readDbWhere(inArgs: InputArgs) = { val ut = inArgs.lastUpdateTime(writeTableName) Array(s"TMSTMP >= '${ ut.from }' and TMSTMP <= '${ ut.to }'") }'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/InputArgsTest.scala'
Line number: '169'
Statement: 'def exec(implicit inArgs: InputArgs) = { dbTodb.run(Unit) }'
Detail: 'Can't OpenScope for symbol named: 'exec(d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/InputArgsTest.scala'
Line number: '176'
Statement: 'def readDbWhere(inArgs: InputArgs) = Array(s"TMSTMP >= '${ inArgs.lastUpdateTime("xxx") }'")'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'readData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/KanaConverter.scala'
Line number: '8'
Statement: 'def readData(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(s"kanaConv/$fileName"), charEnc).getLines'
Detail: 'Can't OpenScope for symbol named: 'readData(scala.String)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/KanaConverter.scala'
Line number: '18'
Statement: 'def apply(inStr: String) = { if (inStr != null)       {       inStr.map( c =>cnvMap.getOrElse(c.toString, zenkakuCnv(c.toString))).mkString       } else       {       inStr       } }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'select' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/KanaConverter.scala'
Line number: '19'
Statement: 'def select(inStr: String) = { if (inStr != null)       {       inStr.map( c =>cnvMapSelect.getOrElse(c.toString, zenkakuCnv(c.toString))).mkString       } else       {       inStr       } }'
Detail: 'Can't OpenScope for symbol named: 'select(scala.String)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/TwoInToOneOutForDf.scala'
Line number: '10'
Statement: 'preExec(in1: IN1, in2: IN2)(implicit inArgs: InputArgs): DataFrame'
Detail: 'Can't OpenScope for symbol named: 'preExec(d2k.common.df.flow.TwoInToOneOutForDf[IN1,IN2,OUT].IN1,d2k.common.df.flow.TwoInToOneOutForDf[IN1,IN2,OUT].IN2,d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/TwoInToOneOutForDf.scala'
Line number: '12'
Statement: 'exec(df: DataFrame)(implicit inArgs: InputArgs): DataFrame'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/TwoInToOneOutForDf.scala'
Line number: '14'
Statement: 'postExec(df: DataFrame)(implicit inArgs: InputArgs): OUT'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'run' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.flow' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/TwoInToOneOutForDf.scala'
Line number: '16'
Statement: 'def run(in1: IN1, in2: IN2)(implicit inArgs: InputArgs) : OUT = { val input = try       {preExec(in1, in2)}    catch {       case t:Throwable => platformError(t);throw t    } if (inArgs.isDebug)       {       println(s"${ inArgs.applicationId }[input]") input.show(false)       }  val output = try       {exec(input)}    catch {       case t:Throwable => appError(t);throw t    } if (inArgs.isDebug)       {       println(s"${ inArgs.applicationId }[output]") output.show(false)       } try       {postExec(output)}    catch {       case t:Throwable => platformError(t);throw t    } }'
Detail: 'Can't OpenScope for symbol named: 'run(d2k.common.df.flow.TwoInToOneOutForDf[IN1,IN2,OUT].IN1,d2k.common.df.flow.TwoInToOneOutForDf[IN1,IN2,OUT].IN2,d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: An error ocurred at 'OpenScope for node with name 'debug' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.flow' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/TwoInToOneOutForDf.scala'
Line number: '46'
Statement: 'def debug(in1: IN1, in2: IN2)(implicit inArgs: InputArgs) : OUT = run(in1, in2)(inArgs.copy(isDebug = true))'
Detail: 'Can't OpenScope for symbol named: 'debug(d2k.common.df.flow.TwoInToOneOutForDf[IN1,IN2,OUT].IN1,d2k.common.df.flow.TwoInToOneOutForDf[IN1,IN2,OUT].IN2,d2k.common.InputArgs)''
[03/24/2023 05:42:19] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:19] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.sh' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/sh/CommissionBaseChannelSelectorTmpl.scala'
Line number: '33'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = if (groupingKeys.contains(UniqueKey))    {    (df ~> c03_DfToDf.run, broadcast(c01_DbToDf(info).run(Unit))) ~> c02_DfJoinToDf.run ~> c04_DfToDf.run    } else    {    (df, broadcast(c01_DbToDf(info).run(Unit))) ~> c02_DfJoinToDf.run    }'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'c01_DbToDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.sh' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/sh/CommissionBaseChannelSelectorTmpl.scala'
Line number: '40'
Statement: 'def c01_DbToDf(info: CommissionBaseChannelSelectorInfo) = new DbToDf with Executor {    val componentId = "MAA300"     override val columns = Array("DV_DISCRDIV", "CD_CHNLCD", "DV_OUTOBJDIV", "DV_TRICALCOBJDIV")     override val readDbWhere = Array(s"DV_DISPODIV = '${ info.DV_DISPODIV }'")     def invoke(df: DataFrame)(implicit inArgs: InputArgs) = df ~> f01     def f01(implicit inArgs: InputArgs) = (_ : DataFrame).na.fill(" ") }'
Detail: 'Can't OpenScope for symbol named: 'c01_DbToDf(d2k.common.df.template.sh.CommissionBaseChannelSelectorInfo)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.sh' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/sh/CommissionBaseChannelSelectorTmpl.scala'
Line number: '61'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) = df ~> f01 ~> f02 ~> f03 ~> f04'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.sh' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/sh/CommissionBaseChannelSelectorTmpl.scala'
Line number: '78'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) = df ~> f01'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.sh' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/sh/CommissionBaseChannelSelectorTmpl.scala'
Line number: '91'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) = df ~> f01'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/OneInToMapOut.scala'
Line number: '7'
Statement: 'preExec(in: IN)(implicit inArgs: InputArgs): Map[String, MID]'
Detail: 'Can't OpenScope for symbol named: 'preExec(d2k.common.df.flow.base.OneInToMapOut[IN,MID,OUT].IN,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/OneInToMapOut.scala'
Line number: '9'
Statement: 'exec(df: MID)(implicit inArgs: InputArgs): MID'
Detail: 'Can't OpenScope for symbol named: 'exec(d2k.common.df.flow.base.OneInToMapOut[IN,MID,OUT].MID,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/OneInToMapOut.scala'
Line number: '11'
Statement: 'postExec(df: Map[String, MID])(implicit inArgs: InputArgs): OUT'
Detail: 'Can't OpenScope for symbol named: 'postExec(scala.Map[scala.String,d2k.common.df.flow.base.OneInToMapOut[IN,MID,OUT].MID],d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'run' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/OneInToMapOut.scala'
Line number: '13'
Statement: 'run(in: IN)(implicit inArgs: InputArgs): OUT'
Detail: 'Can't OpenScope for symbol named: 'run(d2k.common.df.flow.base.OneInToMapOut[IN,MID,OUT].IN,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'readDb' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/SingleReadDb.scala'
Line number: '11'
Statement: 'def readDb(implicit inArgs: InputArgs) = readDbSingle(readTableName)'
Detail: 'Can't OpenScope for symbol named: 'readDb(d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.sql' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/sql/SqlDefParser.scala'
Line number: '12'
Statement: 'def apply(baseUrl: String, branch: String, appGroup: String, appId: String) = { val parsed = parseAll(sqlDef, readAppDefMd(baseUrl, branch, appGroup, appId, "README.md")) println(parsed) parsed }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.sql' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/sql/SqlDefParser.scala'
Line number: '18'
Statement: 'def apply(baseUrl: String, appGroup: String, appId: String) = { val parsed = parseAll(sqlDef, readAppDefMd(baseUrl, appGroup, appId, "README.md")) println(parsed) parsed }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.executor' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/RowErrorRemover.scala'
Line number: '9'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) : DataFrame = RowErrorRemover(df)'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.executor' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/RowErrorRemover.scala'
Line number: '13'
Statement: 'def apply(df: DataFrame)(implicit inArgs: InputArgs) = df.filter(col("ROW_ERR") === lit("false"))'
Detail: 'Can't OpenScope for symbol named: 'apply(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'confs' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.file.output' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFileWithConfFile.scala'
Line number: '12'
Statement: 'def confs(confPath: String) = { Source.fromFile(confPath).getLines.map{ line => val data = line.split("\t") (data(0), data(1)) }.toMap }'
Detail: 'Can't OpenScope for symbol named: 'confs(scala.String)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'rpad' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.file.output' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFileWithConfFile.scala'
Line number: '40'
Statement: 'def rpad(target: String, len: Int, pad: String = " ") = { val str = if (target == null)       {       ""       } else       {       target       }  val strSize = str.getBytes("MS932").size  val padSize = len - strSize s"${ str }${ pad * padSize }" }'
Detail: 'Can't OpenScope for symbol named: 'rpad(scala.String,scala.Int,scala.String)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/DfToAny.scala'
Line number: '9'
Statement: 'def preExec(in: DataFrame)(implicit inArgs: InputArgs) : DataFrame = in'
Detail: 'Can't OpenScope for symbol named: 'preExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/AnyToFile.scala'
Line number: '10'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) = writeFile(df)'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfJoinVariableToDf.scala'
Line number: '10'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/DbToAny.scala'
Line number: '10'
Statement: 'def preExec(in: Unit)(implicit inArgs: InputArgs) : DataFrame = readDb'
Detail: 'Can't OpenScope for symbol named: 'preExec(scala.Unit,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'timestamp_yyyyMMdd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeDate.scala'
Line number: '8'
Statement: 'def timestamp_yyyyMMdd(str: String) = Timestamp.valueOf(LocalDateTime.of(str.take(4).toInt, str.drop(4).take(2).toInt, str.drop(6).take(2).toInt, 0, 0, 0))'
Detail: 'Can't OpenScope for symbol named: 'timestamp_yyyyMMdd(scala.String)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'timestamp_yyyyMMddhhmmss' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeDate.scala'
Line number: '11'
Statement: 'def timestamp_yyyyMMddhhmmss(str: String) = Timestamp.valueOf(LocalDateTime.of( str.take(4).toInt, str.drop(4).take(2).toInt, str.drop(6).take(2).toInt,  str.drop(8).take(2).toInt, str.drop(10).take(2).toInt, str.drop(12).take(2).toInt))'
Detail: 'Can't OpenScope for symbol named: 'timestamp_yyyyMMddhhmmss(scala.String)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'timestamp_yyyyMMddhhmmssSSS' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeDate.scala'
Line number: '16'
Statement: 'def timestamp_yyyyMMddhhmmssSSS(str: String) = Timestamp.valueOf(LocalDateTime.of( str.take(4).toInt, str.drop(4).take(2).toInt, str.drop(6).take(2).toInt,  str.drop(8).take(2).toInt, str.drop(10).take(2).toInt, str.drop(12).take(2).toInt, str.drop(14).toInt * 1000000))'
Detail: 'Can't OpenScope for symbol named: 'timestamp_yyyyMMddhhmmssSSS(scala.String)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'date_yyyyMMdd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeDate.scala'
Line number: '21'
Statement: 'def date_yyyyMMdd(str: String) = new Date (timestamp_yyyyMMdd(str).getTime)'
Detail: 'Can't OpenScope for symbol named: 'date_yyyyMMdd(scala.String)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'timestamp_yyyymmdd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeDate.scala'
Line number: '37'
Statement: 'def timestamp_yyyymmdd(yyyy: Int, mm: Int, dd: Int) = Timestamp.valueOf(LocalDateTime.of(yyyy, mm, dd, 0, 0, 0))'
Detail: 'Can't OpenScope for symbol named: 'timestamp_yyyymmdd(scala.Int,scala.Int,scala.Int)''
[03/24/2023 05:42:20] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:20] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:20] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/ReadDbTest.scala'
Line number: '77'
Statement: 'def readDbWhere(inArgs: InputArgs) = Array("NUM5 = '2000'")'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/ReadDbTest.scala'
Line number: '117'
Statement: 'def readDbWhere(inArgs: InputArgs) = Array("NUM5 = '2000'")'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/ReadDbTest.scala'
Line number: '210'
Statement: 'def readDbWhere(inArgs: InputArgs) = Array("NUM5 = '2000'")'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/ReadDbTest.scala'
Line number: '254'
Statement: 'def readDbWhere(inArgs: InputArgs) = Array("NUM5 = '2000'")'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/TwoAnyToPq.scala'
Line number: '10'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) = writeParquet(df)'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'execAssertEquals' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '255'
Statement: 'def execAssertEquals(input: String, expected: String, targetUdf: UserDefinedFunction) {    val df = context.createDataFrame(Seq(Test(input)))  val result = df.withColumn("result", targetUdf(df("str"))).collect result(0).getAs[String]("result") mustBe expected }'
Detail: 'Can't OpenScope for symbol named: 'execAssertEquals(scala.String,scala.String,UserDefinedFunction)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'execGetStatus' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '447'
Statement: 'def execGetStatus(df: DataFrame, expected: String) = assertEquals(df.withColumn("result", getStatus(df("DT_BEGIN"), df("DT_END"), lit(MANG_DT_STR_TODAY))), expected)'
Detail: 'Can't OpenScope for symbol named: 'execGetStatus(DataFrame,scala.String)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'execGetStatusWithDeleteDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '451'
Statement: 'def execGetStatusWithDeleteDate(df: DataFrame, expected: String) = assertEquals(df.withColumn("result", getStatusWithDeleteDate(df("DT_BEGIN"), df("DT_END"), df("DT_DELETE"), lit(MANG_DT_STR_TODAY))), expected)'
Detail: 'Can't OpenScope for symbol named: 'execGetStatusWithDeleteDate(DataFrame,scala.String)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'execGetStatusWithBlankReplace' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '455'
Statement: 'def execGetStatusWithBlankReplace(df: DataFrame, expected: String) = assertEquals(df.withColumn("result", getStatusWithBlankReplace(df("DT_BEGIN"), df("DT_END"), lit(MANG_DT_STR_TODAY))), expected)'
Detail: 'Can't OpenScope for symbol named: 'execGetStatusWithBlankReplace(DataFrame,scala.String)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'execGetStatusWithBlankReplaceAndDeleteDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '459'
Statement: 'def execGetStatusWithBlankReplaceAndDeleteDate(df: DataFrame, expected: String) = assertEquals(df.withColumn("result", getStatusWithBlankReplaceAndDeleteDate(df("DT_BEGIN"), df("DT_END"), df("DT_DELETE"), lit(MANG_DT_STR_TODAY))), expected)'
Detail: 'Can't OpenScope for symbol named: 'execGetStatusWithBlankReplaceAndDeleteDate(DataFrame,scala.String)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'assertEquals' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '462'
Statement: 'def assertEquals(df: DataFrame, expected: String) {    val actual = df.collect()(0).getAs[String]("result") actual mustBe expected }'
Detail: 'Can't OpenScope for symbol named: 'assertEquals(DataFrame,scala.String)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'makeStringDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '466'
Statement: 'def makeStringDf(DT_BEGIN: String, DT_END: String, DT_DELETE: String) = context.createDataFrame(Seq(DateRangeTestStringType(DT_BEGIN, DT_END, DT_DELETE)))'
Detail: 'Can't OpenScope for symbol named: 'makeStringDf(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'makeDateDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '469'
Statement: 'def makeDateDf(DT_BEGIN: java.sql.Date, DT_END: java.sql.Date, DT_DELETE: java.sql.Date) = context.createDataFrame(Seq(DateRangeTestDateType(DT_BEGIN, DT_END, DT_DELETE)))'
Detail: 'Can't OpenScope for symbol named: 'makeDateDf(_Unresolved.Date,_Unresolved.Date,_Unresolved.Date)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'testCutLimitStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '764'
Statement: 'def testCutLimitStr(input: String, cutLen: Int, exp: String) = { import SparkContexts.context.implicits._  val df = SparkContexts.sc.makeRDD( Seq(Test(input))).toDF  val result = df.withColumn("result", Udfs.cutLimitStr(col("str"), lit(cutLen))).collect result(0).getAs[String]("result") mustBe exp }'
Detail: 'Can't OpenScope for symbol named: 'testCutLimitStr(scala.String,scala.Int,scala.String)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'testCalcSchoolAge' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '783'
Statement: 'def testCalcSchoolAge(birthY: Int, birthM: Int, birthD: Int, runningYMD: String, exp: Int) = { import SparkContexts.context.implicits._  val cal = Calendar.getInstance cal.set(Calendar.YEAR, birthY) cal.set(Calendar.MONTH, birthM) cal.set(Calendar.DATE, birthD)  val df = SparkContexts.sc.makeRDD( Seq(DateTest(runningYMD, null, new java.sql.Timestamp (cal.getTime.getTime)))).toDF  val result = df.withColumn("result", Udfs.calcSchoolAge(df("sqlTimestamp"), df("str"))).collect result(0).getAs[Integer]("result") mustBe exp }'
Detail: 'Can't OpenScope for symbol named: 'testCalcSchoolAge(scala.Int,scala.Int,scala.Int,scala.String,scala.Int)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'testCalcAge' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '824'
Statement: 'def testCalcAge(birthYMD: String, runningYMD: String, exp: Int) = { var simpleDateFormat = new SimpleDateFormat ("yyyyMMdd")  var date = simpleDateFormat.parse(birthYMD)  val df = SparkContexts.sc.makeRDD( Seq(DateTest(runningYMD, null, new java.sql.Timestamp (date.getTime)))).toDF  val result = df.withColumn("result", Udfs.calcAge(df("sqlTimestamp"), df("str"))).collect result(0).getAs[Integer]("result") mustBe exp }'
Detail: 'Can't OpenScope for symbol named: 'testCalcAge(scala.String,scala.String,scala.Int)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/TwoDfUnionToAny.scala'
Line number: '9'
Statement: 'def preExec(left: DataFrame, right: DataFrame)(implicit inArgs: InputArgs) : DataFrame = left.union(right)'
Detail: 'Can't OpenScope for symbol named: 'preExec(DataFrame,DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/Executor.scala'
Line number: '7'
Statement: 'invoke(df: DataFrame)(implicit inArgs: InputArgs): DataFrame'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/PqToAny.scala'
Line number: '10'
Statement: 'def preExec(in: Unit)(implicit inArgs: InputArgs) : DataFrame = readParquet'
Detail: 'Can't OpenScope for symbol named: 'preExec(scala.Unit,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfJoinToDf.scala'
Line number: '10'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/SparkApp.scala'
Line number: '18'
Statement: 'exec(implicit inArgs: InputArgs): DataFrame'
Detail: 'Can't OpenScope for symbol named: 'exec(d2k.common.InputArgs)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'runner' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/SparkApp.scala'
Line number: '20'
Statement: 'def runner(args: Array[String], isDebug: Boolean = false) {    println(s"${ new DateTime toString (DATE_FORMAT) } INFO START")  val inputArgs = if (args.length == 8)       {       InputArgs(args(0), args(1), args(2), args(3), args(4), args(5), args(6), args(7))       } else       {       InputArgs(args(0), args(1), args(2), args(3))       } if (isDebug)       {       println(inputArgs)       } try       exec(inputArgs.copy(isDebug = isDebug)).sparkSession.stop    catch {       case e:Throwable => println(s"${ new DateTime toString (DATE_FORMAT) } ERROR ${ e.toString() }");throw e    } println(s"${ new DateTime toString (DATE_FORMAT) } INFO FINISHED") }'
Detail: 'Can't OpenScope for symbol named: 'runner(scala.Array[scala.String],scala.Boolean)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'main' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/SparkApp.scala'
Line number: '34'
Statement: 'def main(args: Array[String]) {    runner(args) }'
Detail: 'Can't OpenScope for symbol named: 'main(scala.Array[scala.String])''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'debug' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/SparkApp.scala'
Line number: '38'
Statement: 'def debug(args: Array[String]) {    runner(args, true) }'
Detail: 'Can't OpenScope for symbol named: 'debug(scala.Array[scala.String])''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'errorLog' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/Logging.scala'
Line number: '8'
Statement: 'def errorLog(message: String, t: Throwable) = logger.error(message, t)'
Detail: 'Can't OpenScope for symbol named: 'errorLog(scala.String,Throwable)''
[03/24/2023 05:42:20] Error: An error ocurred at 'OpenScope for node with name 'elapse' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/Logging.scala'
Line number: '12'
Statement: 'def elapse(message: String)(func: =>Unit) = { logger.info(s" Start[${ message }]")  val startTime = System.currentTimeMillis func  val endTime = System.currentTimeMillis  val elapse = BigDecimal(endTime - startTime) / 1000 logger.info(f"finish[${ message }] elapse:${ elapse }%,.3fs") }'
Detail: 'Can't OpenScope for symbol named: 'elapse(scala.String,=>Unit)''
[03/24/2023 05:42:20] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/TwoPqJoinToAny.scala'
Line number: '16'
Statement: 'def preExec(in1: Unit, in2: Unit)(implicit inArgs: InputArgs) : DataFrame = { val left = readParquetSingle(leftPqName)  val right = readParquetSingle(rightPqName)  val joined = left.join(right, joinExprs(left, right), joinType) joined.select(select(left, right).toArray :_*) }'
Detail: 'Can't OpenScope for symbol named: 'preExec(scala.Unit,scala.Unit,d2k.common.InputArgs)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.src' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/src/FlowLogicGenerator.scala'
Line number: '26'
Statement: 'def apply(target: Seq[(String, String)]) = { val flowMap = target.foldLeft(Map.empty[String, Seq[String]]){(l, r) => val (a, b) = r l.updated(b, l.get(b).getOrElse(Seq.empty[String]) :+ a) }  def conv(s: String) : Tree = {    val flowId = flowMap.get(s).getOrElse(Seq.empty[String]) flowId.size match {          case 0 => Top(s)          case 1 => Node(s, conv(flowId.head))          case 2 => Join(s, conv(flowId(0)), conv(flowId(1)))       }    } flowMap("CfEnd").map( e =>conv(e).toFlow).mkString("\n\n") }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.Seq[scala.Tuple2[scala.String,scala.String]])''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/TwoAnyToDb.scala'
Line number: '10'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) = writeDb(df)'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'mkDbInfo' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/ReadDb.scala'
Line number: '9'
Statement: 'def mkDbInfo(envLabel: String) = DbInfo(sys.env(s"DB_URL_$envLabel"), sys.env(s"DB_USER_$envLabel"), sys.env(s"DB_PASSWORD_$envLabel"))'
Detail: 'Can't OpenScope for symbol named: 'mkDbInfo(scala.String)''
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/ReadDb.scala'
Line number: '31'
Statement: 'def readDbWhere(inArgs: InputArgs) : Array[String] = Array.empty[String]'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'readDbSingle' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/ReadDb.scala'
Line number: '38'
Statement: 'def readDbSingle(tableName: String)(implicit inArgs: InputArgs) = { val tblName = inArgs.tableNameMapper.get(componentId).getOrElse(tableName)  val dbCtl = new DbCtl (readDbInfo)  val readDbWhereWithArgs = readDbWhere(inArgs) (readDbWhere.isEmpty, readDbWhereWithArgs.isEmpty) match {       case (true, true) => selectReadTable(dbCtl, tblName)       case (false, true) => selectReadTable(dbCtl, tblName, readDbWhere)       case (true, false) => selectReadTable(dbCtl, tblName, readDbWhereWithArgs)       case (false, false) => throw new IllegalArgumentException ("Can not defined both readDbWhere and readDbWhere(inArgs)")    } }'
Detail: 'Can't OpenScope for symbol named: 'readDbSingle(scala.String,d2k.common.InputArgs)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'selectReadTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/ReadDb.scala'
Line number: '50'
Statement: 'def selectReadTable(dbCtl: DbCtl, tableName: String, readDbWhere: Array[String] = Array.empty[String]) = { (columns.isEmpty, readDbWhere.isEmpty) match {       case (true, true) => dbCtl.readTable(tableName)       case (true, false) => dbCtl.readTable(tableName, readDbWhere)       case (false, true) => dbCtl.readTable(tableName, columns, Array("1 = 1"))       case (false, false) => dbCtl.readTable(tableName, columns, readDbWhere)    } }'
Detail: 'Can't OpenScope for symbol named: 'selectReadTable(DbCtl,scala.String,scala.Array[scala.String])''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'colData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/spark/common/LargeInsertTest.scala'
Line number: '30'
Statement: 'def colData(rec: Int) = (1 to 10).map( cnt =>s"${ cnt }_${ rec }").toSeq'
Detail: 'Can't OpenScope for symbol named: 'colData(scala.Int)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.parser' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/AppDefParser.scala'
Line number: '12'
Statement: 'def apply(baseUrl: String, branch: String, appGroup: String, appId: String) = { val parsed = parseAll(appDef, readAppDefMd(baseUrl, branch, appGroup, appId, "README.md")) println(parsed) parsed }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.parser' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/AppDefParser.scala'
Line number: '18'
Statement: 'def apply(baseUrl: String, appGroup: String, appId: String) = { val parsed = parseAll(appDef, readAppDefMd(baseUrl, appGroup, appId, "README.md")) println(parsed) parsed }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.parser' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/AppDefParser.scala'
Line number: '24'
Statement: 'def apply(path: String) = { parseAll(appDef, readAppDefMd(path)) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'getMD5Str' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MD5Utils.scala'
Line number: '8'
Statement: 'def getMD5Str(targetStr: String) : String = { val md5 = MessageDigest.getInstance("MD5")  val md5Data = md5.digest(targetStr.getBytes(charEnc)) md5Data.foldLeft(""){(l, r) => val i: Int = r.asInstanceOf[Int]  val result = if (i < 0)       {       i + 256       } else       {       i       } if (result < 16)       {       s"${ l }0${ Integer.toHexString(result) }"       } else       {       s"${ l }${ Integer.toHexString(result) }"       } } }'
Detail: 'Can't OpenScope for symbol named: 'getMD5Str(scala.String)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'getMD5Str' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MD5Utils.scala'
Line number: '26'
Statement: 'def getMD5Str(targetStr: String, md5WordStr: String) : String = getMD5Str(targetStr.trim + getMD5Str(md5WordStr))'
Detail: 'Can't OpenScope for symbol named: 'getMD5Str(scala.String,scala.String)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'getMD5Base64Str' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MD5Utils.scala'
Line number: '29'
Statement: 'def getMD5Base64Str(targetStr: String, md5WordStr: String, flag: Boolean = true) : String = { val bytes = MessageDigest.getInstance("MD5").digest((targetStr.trim + getMD5Str(md5WordStr)).getBytes(charEnc))  val base64 = new String (Base64.encodeBase64(bytes)) if (flag && base64.takeRight(2) == "==")       {       base64.dropRight(2)       } else       {       base64       } }'
Detail: 'Can't OpenScope for symbol named: 'getMD5Base64Str(scala.String,scala.String,scala.Boolean)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'check' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.component.sh' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/component/sh/CommissionBaseChannelSelectorTest.scala'
Line number: '26'
Statement: 'def check(key: String, DV_OUTOBJDIV: String, DV_TRICALCOBJDIV: String) = { (df: DataFrame) =>df.filter($"key" === key).collect.foreach{ row =>row.getAs[String]("DV_OUTOBJDIV") mustBe DV_OUTOBJDIV row.getAs[String]("DV_TRICALCOBJDIV") mustBe DV_TRICALCOBJDIV } df }'
Detail: 'Can't OpenScope for symbol named: 'check(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'checkPostCode' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/PostCodeNormalizer.scala'
Line number: '11'
Statement: 'def checkPostCode(sizeList: Seq[Int])(postCode: String) = Option(postCode).flatMap{ p =>if (p.forall(_.isDigit) && sizeList.contains(p.size))    Some(p) else    None }.getOrElse("")'
Detail: 'Can't OpenScope for symbol named: 'checkPostCode(scala.Seq[scala.Int],scala.String)''
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/PostCodeNormalizer.scala'
Line number: '18'
Statement: 'def apply(postCode: String) : String = Option(postCode).map{ pCode => val splitted = pCode.split("-") splitted.size match {    case EXIST_HYPHEN => val p = parent(splitted(PARENT_POSITION))  val c = child(splitted(CHILE_POSITION)) if (c.isEmpty)       p else       s"${ p }-${ c }"    case NO_HYPHEN => val target = splitted.head  val parentSize = PARENT_CORRECT_SIZE.head parent(target.take(parentSize)) + child(target.drop(parentSize)) } }.getOrElse("")'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'single' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/PostCodeNormalizer.scala'
Line number: '32'
Statement: 'def single(postCode: String) : String = apply(postCode)'
Detail: 'Can't OpenScope for symbol named: 'single(scala.String)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'd2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteDbTest.scala'
Line number: '25'
Statement: 'def d2s(dateMill: Long) = new DateTime (dateMill).toString("yyyy-MM-dd")'
Detail: 'Can't OpenScope for symbol named: 'd2s(scala.Long)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'd2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteDbTest.scala'
Line number: '26'
Statement: 'def d2s(date: Date) = new DateTime (date).toString("yyyy-MM-dd")'
Detail: 'Can't OpenScope for symbol named: 'd2s(_Unresolved.Date)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'd2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteDbTest.scala'
Line number: '27'
Statement: 'def d2s(date: Timestamp) = new DateTime (date).toString("yyyy-MM-dd hh:mm:ss")'
Detail: 'Can't OpenScope for symbol named: 'd2s(_Unresolved.Timestamp)''
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'splitIdAndName' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.rc' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/rc/RoughConceptGenerator.scala'
Line number: '16'
Statement: 'def splitIdAndName(s: String) = { val splitted = s.split('[') (splitted(0).trim, splitted(1).trim.dropRight(1)) }'
Detail: 'Can't OpenScope for symbol named: 'splitIdAndName(scala.String)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name '+=' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.rc' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/rc/RoughConceptGenerator.scala'
Line number: '79'
Statement: 'def +=(rcd: RCData) = this.copy( if (rcd.obj.isEmpty)    objs else    rcd.obj :: objs,  if (rcd.frame.isEmpty)    frames else    rcd.frame :: frames,  if (rcd.link.isEmpty)    links else    rcd.link :: links)'
Detail: 'Can't OpenScope for symbol named: '+=(d2k.appdefdoc.gen.rc.RCData)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'fileToStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.rc' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/rc/RoughConceptGenerator.scala'
Line number: '113'
Statement: 'def fileToStr(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString'
Detail: 'Can't OpenScope for symbol named: 'fileToStr(scala.String)''
[03/24/2023 05:42:21] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'objToMd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.rc' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/rc/RoughConceptGenerator.scala'
Line number: '151'
Statement: 'def objToMd(rcds: RCDataStore) = { s"""${ rcds.objs.mkString("\n") }        frame ${ appdef.appInfo.id } { ${ rcds.frames.mkString("\n") } }  ${ (rcds.links ++ links).mkString("\n") }  !include ps.puml""" }'
Detail: 'Can't OpenScope for symbol named: 'objToMd(d2k.appdefdoc.gen.rc.RCDataStore)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.executor' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/BinaryRecordConverter.scala'
Line number: '16'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) : DataFrame = BinaryRecordConverter(binaryRecordName, itemConfId, charEnc)(df)'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:21] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.executor' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/BinaryRecordConverter.scala'
Line number: '20'
Statement: 'def apply(binaryRecordName: String, itemConfId: String, charEnc: String)(df: DataFrame)(implicit inArgs: InputArgs) : DataFrame = { val itemConfs = ConfParser.parseItemConf(Path(inArgs.fileConvInputFile).toAbsolute.parent, inArgs.projectId, itemConfId).toList  val len = itemConfs.map(_.length.toInt)  val names = itemConfs.map(_.itemId)  val domains = itemConfs.map(_.cnvType)  def makeSliceLen(len: Seq[Int]) = len.foldLeft((0, List.empty[(Int, Int)])){(l, r) =>(l._1 + r, l._2 :+ (l._1, l._1 + r))}  val (totalLen_, sliceLen) = makeSliceLen(len)  val ziped = names.zip(domains)  val (nameList, domainList) = ziped.filter{    case (names, domain) => !(domain.startsWith(Converter.NOT_USE_PREFIX))    }.unzip  def cnvFromFixed(names: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)])(inData: Array[Byte]) = {    val dataAndDomainsAndNames = sliceLen.map{       case (start, end) => inData.slice(start, end)       }.zip(domains).zip(names)  val result = Converter.domainConvert(dataAndDomainsAndNames, charEnc) Row.fromSeq(result)    }  val droppedDf = df.drop("ROW_ERR").drop("ROW_ERR_MESSAGE")  val rdd = droppedDf.rdd.map{ orgRow => val row = cnvFromFixed(names, domains, sliceLen)(orgRow.getAs[Array[Byte]](binaryRecordName)) Row.merge(orgRow, row) }  val schema = Converter.makeSchema(nameList).foldLeft(droppedDf.schema){(l, r) =>l.add(r.name, r.dataType)} SparkContexts.context.createDataFrame(rdd, schema).drop(binaryRecordName) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String,DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readPqPath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteFileTest.scala'
Line number: '286'
Statement: 'def readPqPath(implicit inArgs: InputArgs) : String = s"test/dev/data/mypath"'
Detail: 'Can't OpenScope for symbol named: 'readPqPath(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writePqPath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteFileTest.scala'
Line number: '287'
Statement: 'def writePqPath(implicit inArgs: InputArgs) : String = s"test/dev/data/mypath"'
Detail: 'Can't OpenScope for symbol named: 'writePqPath(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeFilePath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteFileTest.scala'
Line number: '288'
Statement: 'def writeFilePath(implicit inArgs: InputArgs) : String = s"test/dev/data/mypath2"'
Detail: 'Can't OpenScope for symbol named: 'writeFilePath(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readPqPath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteFileTest.scala'
Line number: '306'
Statement: 'def readPqPath(implicit inArgs: InputArgs) : String = s"test/dev/data/mypath"'
Detail: 'Can't OpenScope for symbol named: 'readPqPath(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writePqPath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteFileTest.scala'
Line number: '307'
Statement: 'def writePqPath(implicit inArgs: InputArgs) : String = s"test/dev/data/mypath"'
Detail: 'Can't OpenScope for symbol named: 'writePqPath(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeFilePath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteFileTest.scala'
Line number: '308'
Statement: 'def writeFilePath(implicit inArgs: InputArgs) : String = s"test/dev/data/mypath"'
Detail: 'Can't OpenScope for symbol named: 'writeFilePath(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'toSchema' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/ReadPq.scala'
Line number: '9'
Statement: 'def toSchema(names: Seq[String]) = names.map{ name =>name.split("_").toList.headOption.map{ case "DT" => "date" case "NM" | "AM" => "decimal" case _ => "string" } }'
Detail: 'Can't OpenScope for symbol named: 'toSchema(scala.Seq[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readPqPath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/ReadPq.scala'
Line number: '19'
Statement: 'def readPqPath(implicit inArgs: InputArgs) : String = inArgs.baseInputFilePath'
Detail: 'Can't OpenScope for symbol named: 'readPqPath(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readParquetSingle' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/ReadPq.scala'
Line number: '32'
Statement: 'def readParquetSingle(pqName: String)(implicit inArgs: InputArgs) = { val pqCtl = new PqCtl (readPqPath) pqCtl.readParquet(pqName, readPqStrictCheckMode, readPqEmptySchema) }'
Detail: 'Can't OpenScope for symbol named: 'readParquetSingle(scala.String,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'write' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.test' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/test/GenerateTestCase.scala'
Line number: '11'
Statement: 'def write(outputBasePath: String = s"data/testGen") = { writeTestCase(outputBasePath) writeInputMd(outputBasePath) writeOutputMd(outputBasePath) }'
Detail: 'Can't OpenScope for symbol named: 'write(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeTestCase' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.test' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/test/GenerateTestCase.scala'
Line number: '16'
Statement: 'def writeTestCase(outputBasePath: String) = { val writePath = Path(s"${ outputBasePath }/${ groupId }") writePath.createDirectory(failIfExists = false)  val outPath = writePath / s"${ appId }Test.scala" outPath.toFile.writeAll(testCase) }'
Detail: 'Can't OpenScope for symbol named: 'writeTestCase(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeInputMd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.test' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/test/GenerateTestCase.scala'
Line number: '23'
Statement: 'def writeInputMd(outputBasePath: String) = { val writePath = Path(s"${ outputBasePath }/markdown/${ appId }/AT") writePath.createDirectory(failIfExists = false) inputMdData.foreach{    case (ioMd, tableData) => val outPath = writePath / s"${ ioMd.id }.md"  val outputData = s"# ${ ioMd.name }\n${ tableData }" outPath.toFile.writeAll(tableData)    } }'
Detail: 'Can't OpenScope for symbol named: 'writeInputMd(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeOutputMd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.test' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/test/GenerateTestCase.scala'
Line number: '34'
Statement: 'def writeOutputMd(outputBasePath: String) = { val writePath = Path(s"${ outputBasePath }/markdown/${ appId }/AT") writePath.createDirectory(failIfExists = false) outputMdData.foreach{    case (ioMd, tableData) => val outPath = writePath / s"${ ioMd.id }.md"  val outputData = s"# ${ ioMd.name }\n${ tableData }" outPath.toFile.writeAll(tableData)    } }'
Detail: 'Can't OpenScope for symbol named: 'writeOutputMd(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.test' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/test/GenerateTestCase.scala'
Line number: '47'
Statement: 'def apply(baseUrl: String) = new GenerateTestCase (baseUrl)'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'generate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.test' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/test/GenerateTestCase.scala'
Line number: '51'
Statement: 'def generate(branch: String, appGroup: String, appId: String) = { val appBaseUrl = s"${ baseUrl }/raw/${ branch }/apps/${ appGroup }/${ appId }"  val itemsBaseUrl = s"${ baseUrl }/raw/${ branch }/apps/common/items"  val url = s"${ appBaseUrl }/README.md"  val md = Source.fromURL(s"${ url }?private_token=${ sys.env("GITLAB_TOKEN") }").getLines.toList  val ioList = md.filter(!_.isEmpty).dropWhile( line =>!line.contains("## 03. 入出力データ一覧"))  val inputList = ioList.drop(2).takeWhile(!_.contains("### 03.02. 出力")).drop(2)  val outputList = ioList.dropWhile(!_.contains("### 03.02. 出力")).drop(1)  def strToIoMdInfo(str: String) = {    val ioInfoRegx = "\\|\\s*\\[(.*)]\\((.*)\\)\\s*\\|(.*)\\|(.*)\\|".r ioInfoRegx.findFirstMatchIn(str).map( g =>IoMdInfo(g.group(3).trim, g.group(1).trim, g.group(4).trim, g.group(2).trim))    }  val ioTypeToCnvMethodName = (ioType: String, appId: String) =>ioType.toLowerCase match {       case "pq" => s"""toPq("${ appId }")"""       case "db" => s"""toDb("${ appId }")"""       case "jef" => s"""toJef("${ appId }")"""       case "file(fixed)" => "toFixed(\"writePath\")"       case "file(csv)" => "toCsv(\"writePath\")"       case "file(tsv)" => "toTsv(\"writePath\")"    }  val ioTypeToCheckMethodName = (ioType: String, appId: String) =>ioType.toLowerCase match {       case "pq" => s"""checkPq("${ appId }.pq")"""       case "db" => s"""checkDb("${ appId }")"""       case "file(fixed)" => "checkFixed(\"writePath\")"       case "file(csv)" => "checkCsv(\"writePath\")"       case "file(tsv)" => "checkTsv(\"writePath\")"    }  val tableTemplate = "        //%%inputDataName%%\n        %%inputData%%.%%inputConvMethod%%"  def imiToTemplate(imi: IoMdInfo, ioTypeCnv: (String, String) => String) = {    val itemName = imi.path.split("/").takeRight(2).mkString("/") tableTemplate.replaceAllLiterally("%%inputDataName%%", imi.name).replaceAllLiterally("%%inputData%%", s"""x.readMdTable("${ imi.id }.md")""").replaceAllLiterally("%%inputConvMethod%%", ioTypeCnv(imi.ioType, imi.id))    }  def imiToMdData(imi: IoMdInfo) = {    println(s"read:${ imi.id }:${ imi.name }")  val itemName = imi.path.split("/").takeRight(3).mkString("/") s"# ${ imi.name }\n${ MakeResource.itemsMdToTable(s"${ itemsBaseUrl }/${ itemName }").getOrElse("") }\n"    }  val template = Option(getClass.getClassLoader.getResourceAsStream("genTemplates/testcaseAt.tmpl")).map( is =>Source.fromInputStream(is)).get.mkString  val inputInfos = inputList.flatMap(strToIoMdInfo).map( d =>imiToTemplate(d, ioTypeToCnvMethodName))  val inputMdData = inputList.flatMap(strToIoMdInfo).map( d =>(d, imiToMdData(d)))  val outputInfos = outputList.flatMap(strToIoMdInfo).map( d =>imiToTemplate(d, ioTypeToCheckMethodName))  val outputMdData = outputList.flatMap(strToIoMdInfo).map( d =>(d, imiToMdData(d)))  val testCaseStr = template.replaceAllLiterally("%%APP_NAME%%", appId).replaceAllLiterally("%%PROJECT_ID%%", appGroup).replaceAllLiterally("%%READ_DATA%%", inputInfos.mkString("\n\n")).replaceAllLiterally("%%CHECK_DATA%%", outputInfos.mkString("\n\n")) OutputData(appGroup, appId, testCaseStr, inputMdData, outputMdData) }'
Detail: 'Can't OpenScope for symbol named: 'generate(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/DfJoinMultiPqToAny.scala'
Line number: '19'
Statement: 'def preExec(left: DataFrame)(implicit inArgs: InputArgs) : DataFrame = { val inFilePath = inputFilePath.getOrElse(inArgs.baseInputFilePath)  val orgDf = left.columns.foldLeft(left)((df, name) =>df.withColumnRenamed(name, s"$prefixName#$name")) joinPqInfoList.foldLeft(orgDf){(odf, pqInfo) => val pqDf = new PqCtl (inFilePath).readParquet(pqInfo.name)  val pname = if (pqInfo.prefixName.isEmpty)       pqInfo.name else       pqInfo.prefixName  val addNameDf = pqDf.columns.foldLeft(pqDf){(df, name) =>df.withColumnRenamed(name, s"$pname#$name") }  val joinedDf = odf.join(addNameDf, pqInfo.joinExprs, pqInfo.joinType) pqInfo.dropCols.foldLeft(joinedDf)((l, r) =>l.drop(r)) } }'
Detail: 'Can't OpenScope for symbol named: 'preExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.component.cmn' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/component/cmn/PostCodeConverter.scala'
Line number: '25'
Statement: 'def apply()(implicit inArgs: InputArgs) = new PostCodeConverter'
Detail: 'Can't OpenScope for symbol named: 'apply(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'localGovernmentCode' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.component.cmn' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/component/cmn/PostCodeConverter.scala'
Line number: '32'
Statement: 'def localGovernmentCode(postCodeName1: String, postCodeName2: String = "")(outName1: String, outName2: String = "") = cnvLocalGovernmentCode(postCodeName1, postCodeName2)(outName1, outName2)'
Detail: 'Can't OpenScope for symbol named: 'localGovernmentCode(scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'cnvLocalGovernmentCode' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.component.cmn' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/component/cmn/PostCodeConverter.scala'
Line number: '35'
Statement: 'def cnvLocalGovernmentCode(postCodeName1: String, postCodeName2: String)(outName1: String, outName2: String) = (df: DataFrame) =>{ val convertUdf = udf{(inPostCode: String) => val postCode = Option(inPostCode).map(_.replaceAllLiterally("-", "").trim).getOrElse("")  def code3 = postMap.get(postCode)  def code5 = postMap.get(postCode).orElse{ postMap.get(postCode.take(3)) }  def code7 = postMap.get(postCode).orElse{ postMap.get(postCode.take(3) + "0000") }.orElse{ postMap.get(postCode.take(3)) } (postCode.size match {       case 3 => code3       case 5 => code5       case 7 => code7       case _ => None    }).getOrElse(("", "")) }  val postCodeCol = if (postCodeName2.isEmpty)       col(postCodeName1) else       concat(trim(col(postCodeName1)), col(postCodeName2))  val df2 = df ~> editColumns(Seq(("_POSTCODES_", convertUdf(postCodeCol)).e))  val outCol = if (outName2.isEmpty)       Seq((outName1, $"_POSTCODES_._1").e) else       Seq((outName1, $"_POSTCODES_._1").e, (outName2, $"_POSTCODES_._2").e) (df2 ~> editColumns(outCol)).drop("_POSTCODES_").na.fill("", Seq(outName1, outName2)).na.replace(outName1, Map("" -> "99")).na.replace(outName2, Map("" -> "999")) }'
Detail: 'Can't OpenScope for symbol named: 'cnvLocalGovernmentCode(scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/TwoInToOneOut.scala'
Line number: '6'
Statement: 'preExec(in1: IN1, in2: IN2)(implicit inArgs: InputArgs): PREOUT'
Detail: 'Can't OpenScope for symbol named: 'preExec(d2k.common.df.flow.base.TwoInToOneOut[IN1,IN2,PREOUT,MID,POSTIN,OUT].IN1,d2k.common.df.flow.base.TwoInToOneOut[IN1,IN2,PREOUT,MID,POSTIN,OUT].IN2,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/TwoInToOneOut.scala'
Line number: '8'
Statement: 'exec(df: MID)(implicit inArgs: InputArgs): MID'
Detail: 'Can't OpenScope for symbol named: 'exec(d2k.common.df.flow.base.TwoInToOneOut[IN1,IN2,PREOUT,MID,POSTIN,OUT].MID,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/TwoInToOneOut.scala'
Line number: '10'
Statement: 'postExec(df: POSTIN)(implicit inArgs: InputArgs): OUT'
Detail: 'Can't OpenScope for symbol named: 'postExec(d2k.common.df.flow.base.TwoInToOneOut[IN1,IN2,PREOUT,MID,POSTIN,OUT].POSTIN,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'run' of type 'Mobilize.Scala.AST.SclFunDcl' in 'd2k.common.df.flow.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/TwoInToOneOut.scala'
Line number: '12'
Statement: 'run(in1: IN1, in2: IN2)(implicit inArgs: InputArgs): OUT'
Detail: 'Can't OpenScope for symbol named: 'run(d2k.common.df.flow.base.TwoInToOneOut[IN1,IN2,PREOUT,MID,POSTIN,OUT].IN1,d2k.common.df.flow.base.TwoInToOneOut[IN1,IN2,PREOUT,MID,POSTIN,OUT].IN2,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.sql' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/sql/SqlLogicParser.scala'
Line number: '9'
Statement: 'def apply(baseUrl: String, branch: String, appGroup: String, appId: String) = { (parser_ andThen replaceComment_)(readAppDefMd(baseUrl, branch, appGroup, appId, "README.md")) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.sql' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/sql/SqlLogicParser.scala'
Line number: '13'
Statement: 'def apply(baseUrl: String, appGroup: String, appId: String) = { (parser_ andThen replaceComment_)(readAppDefMd(baseUrl, appGroup, appId, "README.md")) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'parser' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.sql' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/sql/SqlLogicParser.scala'
Line number: '17'
Statement: 'def parser(s: String) = s.split("```sql")(1).split("```")(0)'
Detail: 'Can't OpenScope for symbol named: 'parser(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'replaceComment' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.sql' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/sql/SqlLogicParser.scala'
Line number: '23'
Statement: 'def replaceComment(inStr: String) = { Seq( regxChar.findAllMatchIn(inStr).map( x =>(x.toString, x.group(1))),  regxItem.findAllMatchIn(inStr).map( x =>(x.toString, x.group(1))),  regxDecimal.findAllMatchIn(inStr).map( x =>(x.toString, x.group(1))),  regxParent.findAllMatchIn(inStr).map( x =>(x.toString, s"""${ x.group(1) }."""))).reduce(_ ++ _).toList.sortBy( x =>x._1.size).reverse.foldLeft(inStr){(l, r) =>l.replaceAllLiterally(r._1, r._2)} }'
Detail: 'Can't OpenScope for symbol named: 'replaceComment(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/MultiAnyToMapDf.scala'
Line number: '10'
Statement: 'def postExec(df: Map[String, DataFrame])(implicit inArgs: InputArgs) : Map[String, DataFrame] = df'
Detail: 'Can't OpenScope for symbol named: 'postExec(Map[String,DataFrame],d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfToXxx.scala'
Line number: '9'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfToXxx.scala'
Line number: '13'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfToXxx.scala'
Line number: '17'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfToXxx.scala'
Line number: '21'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfToXxx.scala'
Line number: '25'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '59'
Statement: 'def exec(domain: String, data: String) : Either[String, String] = dp{ domain match {    case "年月日" => convDate(data, "00010101", "99991231", FORMATTER_YYYYMMDD)    case "年月日_SL" => convDate(data.replaceAll("/", ""), "00010101", "99991231", FORMATTER_YYYYMMDD)    case "年月日_HY" => convDate(data.replaceAll("-", ""), "00010101", "99991231", FORMATTER_YYYYMMDD)    case "年月" => convDate(data, "000101", "999912", FORMATTER_YYYYMM)    case "年月_SL" => convDate(data.replaceAll("/", ""), "000101", "999912", FORMATTER_YYYYMM)    case "月日" => convDate(data, "0101", "1231", FORMATTER_MMDD)    case "年" => convDateParts(data, "0001", "9999")    case "月" => convDateParts(data, "01", "12")    case "日" => convDateParts(data, "01", "31")    case "年月日時分秒" => convDate(data, "00010101000000", "99991231235959", FORMATTER_YYYYMMDDHHMMSS)    case "年月日時分秒_HC" => convDate(data.replaceAll("[- :]", ""), "00010101000000", "99991231235959", FORMATTER_YYYYMMDDHHMMSS)    case "年月日時分秒_SC" => convDate(data.replaceAll("[/ :]", ""), "00010101000000", "99991231235959", FORMATTER_YYYYMMDDHHMMSS)    case "年月日時分秒_CO" => convDate(data.replaceAll("[ :]", ""), "00010101000000", "99991231235959", FORMATTER_YYYYMMDDHHMMSS)    case "年月日時分ミリ秒" => convTimeStamp(data, "00010101000000000", "99991231235959999", FORMATTER_YYYYMMDDHHMMSSMS, FORMAT_YYYYMMDDHHMMSSMS)    case "年月日時分ミリ秒/ミリ秒小数点付加" => addPeriod(convTimeStamp(data, "00010101000000000", "99991231235959999", FORMATTER_YYYYMMDDHHMMSSMS, FORMAT_YYYYMMDDHHMMSSMS), 14)    case "年月日時" => convDate(data, "0001010100", "9999123123", FORMATTER_YYYYMMDDHH)    case "年月日時分" => convDate(data, "000101010000", "999912312359", FORMATTER_YYYYMMDDHHMM)    case "年月日時分_SC" => convDate(data.replaceAll("[/ :]", ""), "000101010000", "999912312359", FORMATTER_YYYYMMDDHHMM)    case "時分秒" => convDate(data, "000000", "235959", FORMATTER_HHMMSS)    case "時分秒_CO" => convDate(data.replaceAll(":", ""), "000000", "235959", FORMATTER_HHMMSS)    case "時分ミリ秒" => convTimeStamp(data, "000000000", "235959999", FORMATTER_HHMMSSMS, FORMAT_HHMMSSMS)    case "時分ミリ秒/ミリ秒小数点付加" => addPeriod(convTimeStamp(data, "000000000", "235959999", FORMATTER_HHMMSSMS, FORMAT_HHMMSSMS), 6)    case "時分" => convDate(data, "0000", "2359", FORMATTER_HHMM)    case "時" => convDateParts(data, "00", "23")    case "分" => convDateParts(data, "00", "59")    case "秒" => convDateParts(data, "00", "59")    case "時間" => convTime(data, "000000", "995959")    case "数字" => convDigit(data)    case "数字_SIGNED" => convSignedDigit(data)    case "Byte配列" => Right(data)    case "文字列" => Right(data.trim)    case "文字列_trim_無し" => Right(data)    case "文字列_trim_半角" => Right(data.trim)    case "文字列_trim_全角" => Right(trimFull(data))    case "文字列_trim_全半角" => Right(trimFullAndHalf(data))    case "全角文字列" => Right(trimFull(data))    case "全角文字列_trim_無し" => Right(data)    case "全角文字列_trim_全角" => Right(trimFull(data))    case "レコード区分_NUMBER" => convDataDiv(data)    case "レコード区分_ALPHABET" => Right(data)    case "通信方式" => convCommMthd(data, "       ")    case "識別子" => Right(data.trim)    case _ => throw new RuntimeException (s"${ ERR_MSG_INVALID_DOMAIN }:${ domain }") } }'
Detail: 'Can't OpenScope for symbol named: 'exec(scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'trimFull' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '107'
Statement: 'def trimFull(data: String) = data.dropWhile(_ == '　').reverse.dropWhile(_ == '　').reverse'
Detail: 'Can't OpenScope for symbol named: 'trimFull(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'trimFullAndHalf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '108'
Statement: 'def trimFullAndHalf(data: String) = data.dropWhile( s =>s == '　' || s == ' ').reverse.dropWhile( s =>s == '　' || s == ' ').reverse'
Detail: 'Can't OpenScope for symbol named: 'trimFullAndHalf(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'execArrayByte' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '109'
Statement: 'def execArrayByte(domain: String, data: Array[Byte], charEnc: String) : Either[String, String] = data match {    case targetif domain == "Byte配列" => Right(new String (target, "ISO-8859-1"))    case targetif (isPd(domain) || isZd(domain)) && (isNull(target)) => exec(domain.dropRight(3), new String (data, charEnc))    case targetif (isPd(domain) || isZd(domain)) && (isEmpty(target)) => digitErrOrConvDate(domain.dropRight(3), new String (data, charEnc))    case targetif isPd(domain) && !isValidSign(target) => digitErrOrConvDate(domain.dropRight(3), new String (data, charEnc))    case targetif isPd(domain) => convDigitOrDatePd(domain.dropRight(3), target)    case targetif isZd(domain) && (!isValidSignZd(target)) => digitErrOrConvDate(domain.dropRight(3), new String (data, charEnc))    case targetif isZd(domain) => convZd(domain.dropRight(3), target)    case targetif JefConverter.isJefHalf(domain, charEnc) => exec(domain, JefConverter.convJefToUtfHalf(data))    case targetif JefConverter.isJefFull(domain, charEnc) => exec(domain, JefConverter.convJefToUtfFull(data))    case _ => exec(domain, new String (data, charEnc)) }'
Detail: 'Can't OpenScope for symbol named: 'execArrayByte(scala.String,scala.Array[scala.Byte],scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'convDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '137'
Statement: 'def convDate(data: String, min: String, max: String, format: DateTimeFormatter) = data match {    case targetif (isNull(target)) => Right(min)    case targetif (isEmpty(target)) => Right(min)    case targetif (!isDigit(target)) => Right(min)    case allNineRegex(_*) => Right(max)    case targetif (target.toLong < min.toLong) => Right(min)    case targetif (target.toLong > max.toLong) => Right(min)    case targetif (!isDate(target, format)) => Right(min)    case target => Right(target) }'
Detail: 'Can't OpenScope for symbol named: 'convDate(scala.String,scala.String,scala.String,DateTimeFormatter)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'convTimeStamp' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '150'
Statement: 'def convTimeStamp(data: String, min: String, max: String, formatter: DateTimeFormatter, format: String) = { val targetMinPad = data.padTo(format.size, '0')  val targetMaxPad = data.padTo(format.size, '9') data match {       case targetif (isNull(target)) => Right(min)       case targetif (isEmpty(target)) => Right(min)       case targetif (!isDigit(target)) => Right(min)       case allNineRegex(_*) => Right(max)       case targetif (targetMinPad.toLong <= min.toLong) => Right(min)       case targetif (targetMaxPad.toLong > max.toLong) => Right(min)       case targetif (!isDate(target, formatter)) => Right(min)       case target => Right(targetMinPad)    } }'
Detail: 'Can't OpenScope for symbol named: 'convTimeStamp(scala.String,scala.String,scala.String,DateTimeFormatter,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'convDateParts' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '168'
Statement: 'def convDateParts(data: String, min: String, max: String) = data match {    case targetif (isNull(target)) => Right(min)    case targetif (isEmpty(target)) => Right(min)    case targetif (!isDigit(target)) => Right(min)    case allNineRegex(_*) => Right(max)    case targetif (target.toInt < min.toInt) => Right(min)    case targetif (target.toInt > max.toInt) => Right(min)    case target => Right(target) }'
Detail: 'Can't OpenScope for symbol named: 'convDateParts(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'convTime' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '181'
Statement: 'def convTime(data: String, min: String, max: String) = data match {    case targetif (isNull(target)) => Right(min)    case targetif (isEmpty(target)) => Right(min)    case targetif (!isDigit(target)) => Right(min)    case allNineRegex(_*) => Right(max)    case targetif (target.toInt < min.toInt) => Right(min)    case targetif (target.toInt > max.toInt) => Right(min)    case targetif (!isDate(target.drop(2), FORMATTER_MMSS)) => Right(min)    case target => Right(target) }'
Detail: 'Can't OpenScope for symbol named: 'convTime(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'padForDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '192'
Statement: 'def padForDate(domain: String, data: String) = domain match {    case "年月日" => zeroPadLeft(data, FORMAT_YYYYMMDD.length)    case "年月日_SL" => throw new RuntimeException (s"${ ERR_MSG_INVALID_DOMAIN }:${ domain }")    case "年月" => zeroPadLeft(data, FORMAT_YYYYMM.length)    case "月日" => zeroPadLeft(data, FORMAT_MMDD.length)    case "年" => zeroPadLeft(data, FORMAT_YYYY.length)    case "月" => zeroPadLeft(data, FORMAT_MM.length)    case "日" => zeroPadLeft(data, FORMAT_DD.length)    case "年月日時分秒" => zeroPadLeft(data, FORMAT_YYYYMMDDHHMMSS.length)    case "年月日時分ミリ秒" => zeroPadLeft(data, FORMAT_YYYYMMDDHHMMSSMS.length)    case "年月日時" => zeroPadLeft(data, FORMAT_YYYYMMDDHH.length)    case "時分秒" => zeroPadLeft(data, FORMAT_HHMMSS.length)    case "時分秒_CO" => throw new RuntimeException (s"${ ERR_MSG_INVALID_DOMAIN }:${ domain }")    case "時分ミリ秒" => zeroPadLeft(data, FORMAT_HHMMSSMS.length)    case "時分" => zeroPadLeft(data, FORMAT_HHMM.length)    case "時" => zeroPadLeft(data, FORMAT_HH.length)    case "分" => zeroPadLeft(data, FORMAT_MI.length)    case "秒" => zeroPadLeft(data, FORMAT_SS.length)    case "時間" => zeroPadLeft(data, FORMAT_HHMMSS.length)    case _ => data }'
Detail: 'Can't OpenScope for symbol named: 'padForDate(scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'convPd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '214'
Statement: 'def convPd(domain: String, data: Array[Byte])(unpack: (Array[Byte] => String)) = { try       {exec(domain, unpack(data))}    catch {       case t:NumberFormatException => Left(ERR_MSG_INVALID_VALUE)       case t:Exception => throw t    } }'
Detail: 'Can't OpenScope for symbol named: 'convPd(scala.String,scala.Array[scala.Byte],Tuple2[lambda[Array[Byte],String]])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'convDatePd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '223'
Statement: 'def convDatePd(domain: String, data: Array[Byte]) = { try       { val unpacked = unpackForNum(data) exec(domain, padForDate(domain, unpacked))}    catch {       case t:NumberFormatException => exec(domain, "")       case t:Exception => throw t    } }'
Detail: 'Can't OpenScope for symbol named: 'convDatePd(scala.String,scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'convDigitOrDatePd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '233'
Statement: 'def convDigitOrDatePd(domain: String, data: Array[Byte]) = { domain match {       case "文字列" => convPd(domain, data)(unpackForStr)       case "識別子" => convPd(domain, data)(unpackForId)       case "数字" => convPd(domain, data)(unpackForNum)       case _ => convDatePd(domain, data)    } }'
Detail: 'Can't OpenScope for symbol named: 'convDigitOrDatePd(scala.String,scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'digitErrOrConvDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '242'
Statement: 'def digitErrOrConvDate(orgDomain: String, data: String) : Either[String, String] = orgDomain match {    case "数字" => Left(ERR_MSG_INVALID_VALUE)    case _ => exec(orgDomain, data) }'
Detail: 'Can't OpenScope for symbol named: 'digitErrOrConvDate(scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'convDataDiv' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '248'
Statement: 'def convDataDiv(data: String) = data match {    case REC_DIV_NUMBER_HEAD => Right(REC_DIV_ALPHABET_HEAD)    case REC_DIV_NUMBER_DATA => Right(REC_DIV_ALPHABET_DATA)    case REC_DIV_NUMBER_FOOT => Right(REC_DIV_ALPHABET_FOOT)    case _ => throw new RuntimeException (s"${ ERR_MSG_INVALID_DATA_DIV }:${ data }") }'
Detail: 'Can't OpenScope for symbol named: 'convDataDiv(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'convCommMthd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '255'
Statement: 'def convCommMthd(data: String, allSpace: String) = data match {    case targetif (isNull(target)) => Right("")    case targetif (isEmpty(target)) => Right("")    case _ => Right(data.trim().substring(0, 1)) }'
Detail: 'Can't OpenScope for symbol named: 'convCommMthd(scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'convSignedDigit' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '261'
Statement: 'def convSignedDigit(data: String) = { val default = "0"  val num = if (data.isEmpty)       {       default       } else       {       data       } convDigit(Try{BigDecimal(num).toString}.getOrElse(default)) }'
Detail: 'Can't OpenScope for symbol named: 'convSignedDigit(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'unpackForNum' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '267'
Statement: 'def unpackForNum(data: Array[Byte]) = { val str = data.foldLeft(""){(l, r) =>l + f"$r%02x"}  val decimal = BigInt(str.dropRight(1))  val isMinus = str.takeRight(1) == "d" (if (isMinus)       {       -decimal       } else       {       decimal       }).toString }'
Detail: 'Can't OpenScope for symbol named: 'unpackForNum(scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'unpackForStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '274'
Statement: 'def unpackForStr(data: Array[Byte]) = { val str = data.foldLeft(""){(l, r) =>l + f"$r%02x"}  val decimal = str.dropRight(1)  val isMinus = str.takeRight(1) == "d" (if (isMinus)       {       s"-$decimal"       } else       {       decimal       }) }'
Detail: 'Can't OpenScope for symbol named: 'unpackForStr(scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'unpackForId' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '281'
Statement: 'def unpackForId(data: Array[Byte]) = data.foldLeft(""){(l, r) =>l + f"$r%02x"}.dropRight(1)'
Detail: 'Can't OpenScope for symbol named: 'unpackForId(scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'zeroPadLeft' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '284'
Statement: 'def zeroPadLeft(target: String, fullLen: Int) = s"${ "0" * (fullLen - target.length()) }${ target }"'
Detail: 'Can't OpenScope for symbol named: 'zeroPadLeft(scala.String,scala.Int)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'isDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '286'
Statement: 'def isDate(target: String, format: DateTimeFormatter) = Try(format.withZoneUTC.parseDateTime(target)).map(_ =>true).getOrElse(false)'
Detail: 'Can't OpenScope for symbol named: 'isDate(scala.String,DateTimeFormatter)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'isEmpty' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '289'
Statement: 'def isEmpty(target: String) = target.trim.isEmpty'
Detail: 'Can't OpenScope for symbol named: 'isEmpty(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'isEmpty' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '291'
Statement: 'def isEmpty(target: Array[Byte]) = { target.foldLeft(true){(l, r) =>l && r == 0x20} }'
Detail: 'Can't OpenScope for symbol named: 'isEmpty(scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'isNull' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '295'
Statement: 'def isNull(target: Array[Byte]) = { target.foldLeft(true){(l, r) =>l && r == 0x00} }'
Detail: 'Can't OpenScope for symbol named: 'isNull(scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'isNull' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '299'
Statement: 'def isNull(target: String) : Boolean = { isNull(target.getBytes("MS932")) }'
Detail: 'Can't OpenScope for symbol named: 'isNull(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'isDigit' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '303'
Statement: 'def isDigit(target: String) = Try(target.toLong).map(_ =>true).getOrElse(false)'
Detail: 'Can't OpenScope for symbol named: 'isDigit(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'isPd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '305'
Statement: 'def isPd(domain: String) = domain.endsWith(PD_SUFFIX)'
Detail: 'Can't OpenScope for symbol named: 'isPd(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'isZd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '306'
Statement: 'def isZd(domain: String) = domain.endsWith(ZD_SUFFIX)'
Detail: 'Can't OpenScope for symbol named: 'isZd(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'convDigit' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '308'
Statement: 'def convDigit(data: String) = data match {    case targetif (isNull(target.getBytes("MS932"))) => Right(target)    case targetif (!isDigit(target.trim())) => Left(ERR_MSG_INVALID_VALUE)    case targetif (isEmpty(target)) => Left(ERR_MSG_INVALID_VALUE)    case target => Right(target) }'
Detail: 'Can't OpenScope for symbol named: 'convDigit(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'isValidSign' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '315'
Statement: 'def isValidSign(target: Array[Byte]) = { val sign = target.last & 0x0F sign == 0x0d || sign == 0x0c || sign == 0x0f }'
Detail: 'Can't OpenScope for symbol named: 'isValidSign(scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'isValidSignZd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '320'
Statement: 'def isValidSignZd(target: Array[Byte]) = { val sign = target.last & 0xF0 sign == 0xf0 || sign == 0xc0 || sign == 0xd0 }'
Detail: 'Can't OpenScope for symbol named: 'isValidSignZd(scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'addPeriod' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '327'
Statement: 'def addPeriod(target: Either[String, String], pos: Int) = { target.right.map( str =>str.take(pos) + "." + str.drop(pos)) }'
Detail: 'Can't OpenScope for symbol named: 'addPeriod(Either[String,String],scala.Int)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'convZd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '331'
Statement: 'def convZd(domain: String, data: Array[Byte]) = { try       { val unpacked = domain match {             case "文字列" => unzoneForStr(data)             case "識別子" => unzoneForId(data)             case _ => unzone(data)          } exec(domain, padForDate(domain, unpacked))}    catch {       case t:NumberFormatException => Left(ERR_MSG_INVALID_VALUE)       case t:Exception => throw t    } }'
Detail: 'Can't OpenScope for symbol named: 'convZd(scala.String,scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'unzone' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '345'
Statement: 'def unzone(data: Array[Byte]) = { val str = data.map( x =>f"$x%02x".drop(1)).mkString  val decimal = BigInt(str)  val isMinus = data.map( x =>f"$x%02x").mkString.reverse.apply(1) == 'd' (if (isMinus)       {       -decimal       } else       {       decimal       }).toString }'
Detail: 'Can't OpenScope for symbol named: 'unzone(scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'unzoneForStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '352'
Statement: 'def unzoneForStr(data: Array[Byte]) = { val decimal = data.map( x =>f"$x%02x".drop(1)).mkString  val isMinus = data.map( x =>f"$x%02x").mkString.reverse.apply(1) == 'd' if (isMinus)       {       s"-$decimal"       } else       {       decimal       } }'
Detail: 'Can't OpenScope for symbol named: 'unzoneForStr(scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'unzoneForId' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.fileConv' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '358'
Statement: 'def unzoneForId(data: Array[Byte]) = data.map( x =>f"$x%02x".drop(1)).mkString'
Detail: 'Can't OpenScope for symbol named: 'unzoneForId(scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/MultiPqToMapDf.scala'
Line number: '10'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'addColumnPrefix' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/TwoDfJoinToAny.scala'
Line number: '15'
Statement: 'def addColumnPrefix(name: String) = (df: DataFrame) =>{ df.schema.map( x =>df(x.name) as s"${ name }_${ x.name }") }'
Detail: 'Can't OpenScope for symbol named: 'addColumnPrefix(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'mergeWithPrefix' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/TwoDfJoinToAny.scala'
Line number: '29'
Statement: 'def mergeWithPrefix(left: DataFrame, right: DataFrame, name: String) = left("*") +: addColumnPrefix(name)(right)'
Detail: 'Can't OpenScope for symbol named: 'mergeWithPrefix(DataFrame,DataFrame,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/TwoDfJoinToAny.scala'
Line number: '32'
Statement: 'def preExec(left: DataFrame, right: DataFrame)(implicit inArgs: InputArgs) : DataFrame = { val joined = left.join(right, joinExprs(left, right), joinType) joined.select(select(left, right).toArray :_*) }'
Detail: 'Can't OpenScope for symbol named: 'preExec(DataFrame,DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'fileToStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.sql' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/sql/SqlGenerator.scala'
Line number: '35'
Statement: 'def fileToStr(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString'
Detail: 'Can't OpenScope for symbol named: 'fileToStr(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'generate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.sql' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/sql/SqlGenerator.scala'
Line number: '38'
Statement: 'def generate(appdef: SqlDef, sqlLogic: String) = { val mainTmpl = fileToStr("genTemplates/sqlMain.tmpl")  val mainRepList = Seq( ("%%appId%%", appdef.appInfo.id),  ("%%appDesc%%", appdef.appInfo.desc),  ("%%sql%%", sqlLogic)) mainRepList.foldLeft(mainTmpl){(l, r) =>l.replaceAllLiterally(r._1, r._2) } }'
Detail: 'Can't OpenScope for symbol named: 'generate(d2k.appdefdoc.gen.sql.SqlDef,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/FileToXxx.scala'
Line number: '9'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/FileToXxx.scala'
Line number: '13'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/FileToXxx.scala'
Line number: '17'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/FileToXxx.scala'
Line number: '21'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/FileToXxx.scala'
Line number: '25'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/FileToXxx.scala'
Line number: '29'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/AnyToDf.scala'
Line number: '9'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) = df'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'searchAndReplace' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.appdef' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/appdef/ApplicationDefGenerator.scala'
Line number: '19'
Statement: 'def searchAndReplace(target: Seq[String], searchElem: String, replaceElem: String) = { val idx = target.indexWhere(_.contains(searchElem)) target.updated(idx, replaceElem) }'
Detail: 'Can't OpenScope for symbol named: 'searchAndReplace(scala.Seq[scala.String],scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'fileToStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.appdef' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/appdef/ApplicationDefGenerator.scala'
Line number: '24'
Statement: 'def fileToStr(fileName: String) = Source.fromFile(fileName)'
Detail: 'Can't OpenScope for symbol named: 'fileToStr(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'resToStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.appdef' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/appdef/ApplicationDefGenerator.scala'
Line number: '26'
Statement: 'def resToStr(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString'
Detail: 'Can't OpenScope for symbol named: 'resToStr(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'createJsonPath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder.jsonbase' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '23'
Statement: 'def createJsonPath(basePath: String) = Path(basePath).parent / "concept_flow" / "json"'
Detail: 'Can't OpenScope for symbol named: 'createJsonPath(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'jsStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder.jsonbase' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '25'
Statement: 'def jsStr(jv: JValue)(name: String) = (jv \ name).values.toString'
Detail: 'Can't OpenScope for symbol named: 'jsStr(JValue,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'createLinkReadJson' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder.jsonbase' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '29'
Statement: 'def createLinkReadJson(jsonPath: Path) = { val linkReadPath = (jsonPath / "link_read.json").jfile Source.fromFile(linkReadPath, "MS932").getLines.map{ str => val value = jsStr(parse(str))_ LinkData(value("to_node"), value("from_node"), "input") } }'
Detail: 'Can't OpenScope for symbol named: 'createLinkReadJson(_Unresolved.Path)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'createLinkWriteJson' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder.jsonbase' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '37'
Statement: 'def createLinkWriteJson(jsonPath: Path) = { val linkWritePath = (jsonPath / "link_write.json").jfile Source.fromFile(linkWritePath, "MS932").getLines.map{ str => val value = jsStr(parse(str))_ LinkData(value("from_node"), value("to_node"), "output") } }'
Detail: 'Can't OpenScope for symbol named: 'createLinkWriteJson(_Unresolved.Path)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'createNodeAppMap' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder.jsonbase' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '45'
Statement: 'def createNodeAppMap(jsonPath: Path) = { val nodeAppPath = (jsonPath / "node_application.json").jfile Source.fromFile(nodeAppPath, "MS932").getLines.map{ str => val value = jsStr(parse(str))_ (value("physical_name"), NodeData(value("logical_name"), value("physical_name"), value("language"))) }.toMap }'
Detail: 'Can't OpenScope for symbol named: 'createNodeAppMap(_Unresolved.Path)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'createNodeResourceMap' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder.jsonbase' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '53'
Statement: 'def createNodeResourceMap(jsonPath: Path) = { val nodeResourcePath = (jsonPath / "node_io.json").jfile Source.fromFile(nodeResourcePath, "MS932").getLines.map{ str => val value = jsStr(parse(str))_ (value("key"), NodeData(value("logical_name"), value("physical_name"), value("data_type"))) }.toMap }'
Detail: 'Can't OpenScope for symbol named: 'createNodeResourceMap(_Unresolved.Path)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'appDefList' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder.jsonbase' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '61'
Statement: 'def appDefList(appBasePath: String) = { fcom.recList(new File (appBasePath)).filter(_.getName.contains("README.md")).filter( x =>fcom.appDefRegx.findFirstMatchIn(x.toString).isDefined).flatMap( x =>Try{    val appdef = AppDefParser(x.toString).get (appdef.appInfo.id, x.toString)    }.toOption.orElse{    println(s"  appDef parse error: ${ x.toString }");None    }).toList }'
Detail: 'Can't OpenScope for symbol named: 'appDefList(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'createIoData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder.jsonbase' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '70'
Statement: 'def createIoData(itemNamePath: Array[(String, String)], nodeResourceMap: Map[String, NodeData])(data: Option[List[LinkData]]) = { data.map{ _.map{ ld => val path = itemNamePath.find{ x =>ld.resourceId.contains(x._1) }.map(_._2).getOrElse("") IoData( nodeResourceMap.get(ld.resourceId).map(_.physical_name).getOrElse(""),  path,  nodeResourceMap.get(ld.resourceId).map(_.dataType).getOrElse(""),  nodeResourceMap.get(ld.resourceId).map(_.logical_name).getOrElse("")) } }.getOrElse(List.empty[IoData]) }'
Detail: 'Can't OpenScope for symbol named: 'createIoData(scala.Array[scala.Tuple2[scala.String,scala.String]],scala.Map[scala.String,d2k.appdefdoc.finder.jsonbase.NodeData],scala.Option[scala.List[d2k.appdefdoc.finder.jsonbase.LinkData]])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'createItemNamePathList' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder.jsonbase' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '86'
Statement: 'def createItemNamePathList(basePath: String) = { val itemBasePath = s"${ basePath }/apps/common"  val itemNames = fcom.recList(new File (itemBasePath)).filter(_.getName.contains(".md")) itemNames.map( x =>(Path(x).name.dropRight(3), Path(x).toString.replaceAllLiterally("\\", "/"))) }'
Detail: 'Can't OpenScope for symbol named: 'createItemNamePathList(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'removePhyphen' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder.jsonbase' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '92'
Statement: 'def removePhyphen(s: String) = s.replaceAllLiterally("-", "")'
Detail: 'Can't OpenScope for symbol named: 'removePhyphen(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'createJsonAppdef' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder.jsonbase' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '93'
Statement: 'def createJsonAppdef(basePath: String, itemNamePath: Array[(String, String)], nodeAppMap: Map[String, NodeData], nodeResourceMap: Map[String, NodeData], linkJson: List[LinkData]) = { val appDefResult = appDefList(s"${ basePath }/apps").toMap  val creIoData = createIoData(itemNamePath, nodeResourceMap)_ linkJson.toList.groupBy(_.appId).mapValues(_.groupBy(_.dataType)).map{    case (k, v) => AppDef(AppInfo(removePhyphen(k), nodeAppMap.get(k).map(_.logical_name).getOrElse(""), ""), None,  List.empty[ComponentDefInfo], creIoData(v.get("input")), creIoData(v.get("output")))    }.map( appdef =>(appDefResult.get(appdef.appInfo.id).getOrElse(""), appdef)) }'
Detail: 'Can't OpenScope for symbol named: 'createJsonAppdef(scala.String,scala.Array[scala.Tuple2[scala.String,scala.String]],scala.Map[scala.String,d2k.appdefdoc.finder.jsonbase.NodeData],scala.Map[scala.String,d2k.appdefdoc.finder.jsonbase.NodeData],scala.List[d2k.appdefdoc.finder.jsonbase.LinkData])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeFilePath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.mixIn' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/mixIn/OraLoaderHdfs.scala'
Line number: '11'
Statement: 'def writeFilePath(implicit inArgs: InputArgs) = sys.env("DB_LOADING_FILE_PATH")'
Detail: 'Can't OpenScope for symbol named: 'writeFilePath(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readParquet' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/PqCtl.scala'
Line number: '13'
Statement: 'def readParquet(appendPath: String, strictMode: Boolean = true, readPqEmptySchema: Seq[(String, String)] = Seq.empty[(String, String)]) = { val appendPaths = appendPath.split(",").map( path =>s"${ baseParquetFilePath }/${ path.trim }") if (strictMode)       {       context.read.parquet(appendPaths :_*)       } else       {       try             {context.read.parquet(appendPaths :_*)}          catch {             case t:org.apache.spark.sql.AnalysisException => {             if (t.getMessage.startsWith("Path does not exist"))                   {                   logger.warn(s"Not Found Read Parquet[${ appendPaths.mkString(",") }]")  val schema = MakeResource.makeSchema(readPqEmptySchema.map(_._1), readPqEmptySchema.map(_._2), readPqEmptySchema.map(_ =>"10")) context.createDataFrame(context.emptyDataFrame.rdd, schema)                   } else                   {                   throw t                   }             }          }       } }'
Detail: 'Can't OpenScope for symbol named: 'readParquet(scala.String,scala.Boolean,scala.Seq[scala.Tuple2[scala.String,scala.String]])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeParquet' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/PqCtl.scala'
Line number: '33'
Statement: 'def writeParquet(appendPath: String) = { df.write.mode("overwrite").parquet(s"${ baseParquetFilePath }/${ appendPath }") }'
Detail: 'Can't OpenScope for symbol named: 'writeParquet(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeParquetWithPartitionBy' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/PqCtl.scala'
Line number: '37'
Statement: 'def writeParquetWithPartitionBy(appendPath: String, partitionColumn: String*) = { try       {df.write.mode("overwrite").partitionBy(partitionColumn :_*).parquet(s"${ baseParquetFilePath }/${ appendPath }")}    catch {       case t:NullPointerException => if (df.count() > 0)          {          df.show();throw t          } else          {          println(s"${ baseParquetFilePath }/${ appendPath } IS NO RECORD")          }       case t:Throwable => throw t    } }'
Detail: 'Can't OpenScope for symbol named: 'writeParquetWithPartitionBy(scala.String,_Seq*[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readParquetAndWriteParquet' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/PqCtl.scala'
Line number: '50'
Statement: 'def readParquetAndWriteParquet(readParquetPath: String, writeParquetPath: String)(proc: DataFrame => DataFrame = df =>df) = proc(readParquet(readParquetPath)).writeParquet(writeParquetPath)'
Detail: 'Can't OpenScope for symbol named: 'readParquetAndWriteParquet(scala.String,scala.String,lambda[DataFrame,DataFrame])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readParquetAndWriteParquetWithPartitionBy' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/PqCtl.scala'
Line number: '53'
Statement: 'def readParquetAndWriteParquetWithPartitionBy(readParquetPath: String, writeParquetPath: String, partitionColumn: String*)(proc: DataFrame => DataFrame = df =>df) = proc(readParquet(readParquetPath)).writeParquetWithPartitionBy(writeParquetPath, partitionColumn :_*)'
Detail: 'Can't OpenScope for symbol named: 'readParquetAndWriteParquetWithPartitionBy(scala.String,scala.String,_Seq*[scala.String],lambda[DataFrame,DataFrame])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'dbToPq' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/PqCtl.scala'
Line number: '56'
Statement: 'def dbToPq(tableName: String, where: Array[String]) {    dbToPq(tableName, tableName, where, DbCtl.dbInfo1) }'
Detail: 'Can't OpenScope for symbol named: 'dbToPq(scala.String,scala.Array[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'dbToPq' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/PqCtl.scala'
Line number: '60'
Statement: 'def dbToPq(appendPath: String, tableName: String, where: Array[String], dbInfo: DbInfo) {    new DbCtl (dbInfo).readTable(tableName, where).writeParquet(appendPath) }'
Detail: 'Can't OpenScope for symbol named: 'dbToPq(scala.String,scala.String,scala.Array[scala.String],spark.common.DbInfo)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'dbToPq' of type 'Mobilize.Scala.AST.SclFunDefBlock' in 'spark.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/PqCtl.scala'
Line number: '64'
Statement: 'def dbToPq(appendPath: String, tableName: String, dbInfo: DbInfo) {    new DbCtl (dbInfo).readTable(tableName).writeParquet(appendPath) }'
Detail: 'Can't OpenScope for symbol named: 'dbToPq(scala.String,scala.String,spark.common.DbInfo)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/SparkAppTest.scala'
Line number: '18'
Statement: 'def exec(implicit inArgs: InputArgs) = { val df = Seq(Aaa("a", "b")).toDF comp1.run(df) }'
Detail: 'Can't OpenScope for symbol named: 'exec(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/SparkAppTest.scala'
Line number: '25'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) = df ~> f01'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/FileToAny.scala'
Line number: '10'
Statement: 'def preExec(in: Unit)(implicit inArgs: InputArgs) : DataFrame = readFile'
Detail: 'Can't OpenScope for symbol named: 'preExec(scala.Unit,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'fileToStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '16'
Statement: 'def fileToStr(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString'
Detail: 'Can't OpenScope for symbol named: 'fileToStr(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'mkTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '19'
Statement: 'def mkTable(data: String*) = data.mkString("| ", " | ", " |")'
Detail: 'Can't OpenScope for symbol named: 'mkTable(_Seq*[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'pathOutputString' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '21'
Statement: 'def pathOutputString(path: String) = path.replaceAllLiterally("\\", "/")'
Detail: 'Can't OpenScope for symbol named: 'pathOutputString(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'localPath2Url' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '23'
Statement: 'def localPath2Url(baseUrl: String, basePath: String, localPath: String) = s"${ baseUrl }/${ localPath.replaceAllLiterally("\\", "/").replaceAllLiterally(basePath.replaceAllLiterally("\\", "/"), "tree/master") }"'
Detail: 'Can't OpenScope for symbol named: 'localPath2Url(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'appDefList' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '35'
Statement: 'def appDefList(appBasePath: String) = { recList(new File (appBasePath)).filter(_.getName.contains("README.md")).filter( x =>appDefRegx.findFirstMatchIn(x.toString).isDefined).flatMap( x =>Try((x.toString, AppDefParser(x.toString).get)).toOption.orElse{    println(s"  appDef parse error: ${ x.toString }");None    }).toList }'
Detail: 'Can't OpenScope for symbol named: 'appDefList(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'createRrfData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '42'
Statement: 'def createRrfData(targetName: String, appDefList: Seq[(String, AppDef)]) = { appDefList.map{ x => val (path, appdef) = x  val in = appdef.inputList.map(_.id).contains(targetName)  val out = appdef.outputList.map(_.id).contains(targetName)  val containType = (in, out) match {       case (false, false) => "none"       case (true, false) => "in"       case (false, true) => "out"       case (true, true) => "io"    }  val ioData = appdef.inputList.filter(_.id == targetName).headOption.orElse(appdef.outputList.filter(_.id == targetName).headOption) RrfData(path, appdef.appInfo, ioData, containType) }.filter(_.containType != "none") }'
Detail: 'Can't OpenScope for symbol named: 'createRrfData(scala.String,Seq[Tuple2[String,AppDef]])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeRrfData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '58'
Statement: 'def writeRrfData(targetName: String, baseUrl: String, basePath: String, writePath: Directory, finishMessage: Option[String], rrfData: Seq[RrfData]) = { rrfData.headOption.map{ r =>writePath.createDirectory(true, false)  val targetObj = r.ioData.get  val targetObjTitle = if (targetObj.path.isEmpty)       {       val appInfo = s"${ rrfData.head.ioData.get.id }[${ targetObj.name }]" println(s"  appDef not found: ${ appInfo }") appInfo       } else       {       val targetObjPath = s"${ basePath }/apps/common/${ targetObj.path.split("/common/")(1) }"s"[${ rrfData.head.ioData.get.id }](${ targetObjPath })[${ targetObj.name }]"       }  val targetObjUml = s"""artifact "${ targetObj.id }\\n${ targetObj.name }" as ${ targetObj.id }_res"""  val appUml = rrfData.map{ d =>s"[${ d.appInfo.id }\\n${ d.appInfo.name }] as ${ d.appInfo.id }" }  val chainUml = rrfData.map{ d =>d.containType match {       case "in" => s"${ targetObj.id }_res --> ${ d.appInfo.id } :Input"       case "out" => s"${ d.appInfo.id } --> ${ targetObj.id }_res :Output"       case "io" => s"${ targetObj.id }_res --> ${ d.appInfo.id } :Input\\n${ d.appInfo.id } --> ${ targetObj.id }_res :Output"       case _ => ""    } }  val umls = targetObjUml :: (appUml ++ chainUml).toList  def dataToTable(rrf: RrfData) = if (rrf.path.isEmpty)       {       println(s"  appDef not found: ${ rrf.appInfo.id }[${ rrf.appInfo.name }]") s"| ${ rrf.appInfo.id } | ${ rrf.appInfo.name } |"       } else       {       s"| [${ rrf.appInfo.id }](${ rrf.path }) | ${ rrf.appInfo.name } |"       }  val outputTables = rrfData.filter(_.containType == "out").map(dataToTable)  val inputTables = rrfData.filter(_.containType == "in").map(dataToTable)  val tmpl = fileToStr("finderTemplates/rrResult.tmpl")  val writeFilePath = s"${ writePath.toString }/${ targetObj.id }.md"  val writer = new FileWriter (writeFilePath)  val conved = tmpl.replaceAllLiterally("%%SearchTarget%%", targetObjTitle).replaceAllLiterally("%%ResultPlantuml%%", umls.mkString("\n")).replaceAllLiterally("%%ResultOutput%%", outputTables.mkString("\n")).replaceAllLiterally("%%ResultInput%%", inputTables.mkString("\n")) writer.write(conved) writer.close  val csvTitle = Seq("Target Name", "App Id", "App Name", "Io Type", "Url").mkString("", " , ", "\n")  val csvData = rrfData.map{ rrf => val path = if (rrf.path.isEmpty)       {       ""       } else       {       localPath2Url(baseUrl, basePath, rrf.path)       } Seq(targetName, rrf.appInfo.id, rrf.appInfo.name, rrf.containType, path).mkString(" , ") }.mkString("\n")  val writeCsvFilePath = s"${ writePath.toString }/${ targetObj.id }.csv"  val csvWriter = new FileWriterWithEncoding (writeCsvFilePath, "MS932") csvWriter.write(csvTitle) csvWriter.write(csvData) csvWriter.write("\n") csvWriter.close finishMessage.foreach( mes =>println(s"${ mes } ${ pathOutputString(writeFilePath) }")) Some(targetObj, r) }.getOrElse({    finishMessage.foreach( mes =>println(s"${ mes } Not Found Application. target[${ targetName }]"));None    }) }'
Detail: 'Can't OpenScope for symbol named: 'writeRrfData(scala.String,scala.String,scala.String,Directory,scala.Option[scala.String],scala.Seq[d2k.appdefdoc.finder.RrfData])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'implementList' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '121'
Statement: 'def implementList(targetItemId: String, appdefList: List[(String, AppDef)]) = appdefList.flatMap{ case (appdefpath, appdef) => val compolist = appdef.componentList  val result = compolist.flatMap{ x => val path = appdefpath.dropRight(9) + x.mdName  val str = Source.fromFile(path).getLines.mkString if (str.contains(targetItemId))    Some(x.mdName) else    None } result.map{ subId => val subPath = s"${ appdefpath.dropRight(9) }/${ subId }"  val outputTable = mkTable(s"[${ appdef.appInfo.id }](${ appdefpath }) / [${ subId.dropRight(3) }](${ subPath })", appdef.appInfo.name) ((appdefpath, appdef), outputTable, subId) } }'
Detail: 'Can't OpenScope for symbol named: 'implementList(scala.String,List[Tuple2[String,AppDef]])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'createItemDefMap' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '140'
Statement: 'def createItemDefMap(targetItemId: String, itemNames: Array[File]) = { itemNames.flatMap{ path =>Try{    val itemdef = ItemDefParser(path.toString).get  val itemDetail = itemdef.details.find(_.id == targetItemId) itemDetail.map{ item =>(itemdef.id, RirfDetail(itemdef.id, itemdef.name, item.name, path.toString)) }    }.getOrElse{    println(s"  itemDef parse error: ${ path }");None    } }.toMap }'
Detail: 'Can't OpenScope for symbol named: 'createItemDefMap(scala.String,Array[File])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'recursiveSearch' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '153'
Statement: 'def recursiveSearch(appDefList: List[(String, AppDef)], itemDefMap: Map[String, RirfDetail], appFind: AppDef => Seq[IoData], resourceFind: AppDef => Seq[IoData], depth: Int = 0)(targetResourceId: String, targetAppId: Option[String] = None) : Seq[RirfData] = { val itemDefMapKeys = itemDefMap.keySet  val targetApp = appDefList.filter{    case (path, appdef) => appFind(appdef).exists(_.id == targetResourceId)    }  val result = targetApp.flatMap{ appdef => val resources = resourceFind(appdef._2).filter( x =>itemDefMapKeys.exists(_ == x.id))  val resResult = resources.foldLeft(Seq.empty[RirfData]){(l, r) =>if (depth > maxDepth)       {       l       } else       {       l ++ recursiveSearch(appDefList, itemDefMap, appFind, resourceFind, depth + 1)(r.id, Some(appdef._2.appInfo.id))       } } RirfData(itemDefMap.getOrElse(targetResourceId, RirfDetail(targetResourceId)), Some(RirfAppDetail(appdef._2, appdef._1)), targetAppId) +: resResult } if (result.isEmpty)       Seq(RirfData(itemDefMap.getOrElse(targetResourceId, RirfDetail(targetResourceId)), None, targetAppId)) else       result }'
Detail: 'Can't OpenScope for symbol named: 'recursiveSearch(List[Tuple2[String,AppDef]],scala.Map[scala.String,d2k.appdefdoc.finder.RirfDetail],lambda[AppDef,Seq[IoData]],lambda[AppDef,Seq[IoData]],scala.Int,scala.String,scala.Option[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'renameSearch' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '177'
Statement: 'def renameSearch(data: Map[String, String], targetId: String) : String = { val v = data.get(targetId) if (v.isEmpty)       targetId else       renameSearch((data - targetId), v.get) }'
Detail: 'Can't OpenScope for symbol named: 'renameSearch(scala.Map[scala.String,scala.String],scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'componentDetailData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '186'
Statement: 'def componentDetailData(app: (String, AppDef), targetItemId: String) = { app._2.componentList.scanLeft(RenameData("", targetItemId, targetItemId)){(l, r) => val path = app._1.dropRight(9) + r.mdName  val str = Source.fromFile(path).getLines.mkString("\n")  val regxResult = renameRegx.findAllMatchIn(str).map{ x =>(x.group(1), x.group(2))}.toMap RenameData(r.id, l.afterName, renameSearch(regxResult, l.afterName)) } }'
Detail: 'Can't OpenScope for symbol named: 'componentDetailData(Tuple2[String,AppDef],scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'recursiveSearchWithRename' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '195'
Statement: 'def recursiveSearchWithRename(appDefList: List[(String, AppDef)], itemDefMap: Map[String, RirfDetail], appFind: AppDef => Seq[IoData], resourceFind: AppDef => Seq[IoData], targetItemId: String, itemFileNames: Array[File], depth: Int = 0)(targetResourceId: String, targetAppId: Option[String] = None) : Seq[IrrfData] = { val targetApp = appDefList.filter{    case (path, appdef) => appFind(appdef).exists(_.id == targetResourceId)    }  val result = targetApp.flatMap{ appdef => val renamedItemIdList = componentDetailData(appdef, targetItemId)  val renameComponentList = renamedItemIdList.filter( x =>!x.componentId.isEmpty && x.beforeName != x.afterName)  val filteredItemDefMap = createItemDefMap(renamedItemIdList.last.afterName, itemFileNames)  val itemDefMapKeys = filteredItemDefMap.keySet  val resources = resourceFind(appdef._2).filter( x =>itemDefMapKeys.exists(_ == x.id))  val resResult = resources.foldLeft(Seq.empty[IrrfData]){(l, r) =>if (depth > 3)       {       l       } else       {       l ++ recursiveSearchWithRename(appDefList, itemDefMap, appFind, resourceFind, targetItemId, itemFileNames, depth + 1)(r.id, Some(appdef._2.appInfo.id))       } } IrrfData(RirfData(filteredItemDefMap.getOrElse(targetResourceId, RirfDetail(targetResourceId)), Some(RirfAppDetail(appdef._2, appdef._1)), targetAppId), renameComponentList) +: resResult } if (result.isEmpty)       Seq(IrrfData(RirfData(itemDefMap.getOrElse(targetResourceId, RirfDetail(targetResourceId)), None, targetAppId), Seq.empty[RenameData])) else       result }'
Detail: 'Can't OpenScope for symbol named: 'recursiveSearchWithRename(List[Tuple2[String,AppDef]],scala.Map[scala.String,d2k.appdefdoc.finder.RirfDetail],lambda[AppDef,Seq[IoData]],lambda[AppDef,Seq[IoData]],scala.String,Array[File],scala.Int,scala.String,scala.Option[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'createFlowRender' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '226'
Statement: 'def createFlowRender(result: Seq[RirfData], flowLinkFunc: RirfData => Seq[String] = d =>Seq(d.appDetail.map( x =>s"${ d.resDetail.id }_res --> ${ x.appDef.appInfo.id }"), d.parentAppId.map( x =>s"${ x } --> ${ d.resDetail.id }_res")).flatten) = { val flowResult = result.flatMap{ x =>Seq(Some(RirfFlow("res", x.resDetail.id, x.resDetail.name)), x.appDetail.map( d =>RirfFlow("app", d.appDef.appInfo.id, d.appDef.appInfo.name))).flatten }  val flowObjects = flowResult.map{ x =>x.kind match {       case "app" => s"[${ x.id }\\n${ x.name }] as ${ x.id }"       case "res" => s"""artifact "${ x.id }\\n${ x.name }" as ${ x.id }_res"""    } }  val flowLinks = result.flatMap{    case d:RirfData => flowLinkFunc(d)    case _ => Seq.empty[String]    }.distinct (flowObjects ++ flowLinks).mkString("\n") }'
Detail: 'Can't OpenScope for symbol named: 'createFlowRender(scala.Seq[d2k.appdefdoc.finder.RirfData],lambda[RirfData,Seq[String]])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'createFlowRenderWithRename' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '246'
Statement: 'def createFlowRenderWithRename(result: Seq[IrrfData], flowLinkFunc: RirfData => Seq[String] = d =>Seq(d.appDetail.map( x =>s"${ d.resDetail.id }_res --> ${ x.appDef.appInfo.id }"), d.parentAppId.map( x =>s"${ x } --> ${ d.resDetail.id }_res")).flatten) = { val flowResult = result.flatMap{ x => val beforeName = x.renameApps.headOption.map(_.beforeName).getOrElse("")  val afterName = x.renameApps.lastOption.map(_.afterName).getOrElse("") Seq( Some(IrrfFlow(RirfFlow("res", x.appData.resDetail.id, x.appData.resDetail.name), "", "")),  x.appData.appDetail.map( d =>IrrfFlow(RirfFlow("app", d.appDef.appInfo.id, d.appDef.appInfo.name), beforeName, afterName))).flatten }  val flowObjects = flowResult.map{ ir => val x = ir.rirfFlow x.kind match {       case "app"if (ir.beforeName == ir.afterName) => s"[${ x.id }\\n${ x.name }] as ${ x.id }"       case "app"if (ir.beforeName != ir.afterName) => s"[${ x.id }\\n${ x.name }] as ${ x.id }\nnote right of ${ x.id } : ${ ir.beforeName } -> ${ ir.afterName }"       case "res" => s"""artifact "${ x.id }\\n${ x.name }" as ${ x.id }_res"""    } }.distinct  val flowLinks = result.flatMap{    case d:IrrfData => flowLinkFunc(d.appData)    case _ => Seq.empty[String]    }.distinct (flowObjects ++ flowLinks).mkString("\n") }'
Detail: 'Can't OpenScope for symbol named: 'createFlowRenderWithRename(scala.Seq[d2k.appdefdoc.finder.IrrfData],lambda[RirfData,Seq[String]])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'createReferResult' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.finder' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '272'
Statement: 'def createReferResult(result: Seq[RirfData]) = { result.map{ x => val res = if (x.resDetail.path.isEmpty)       {       Seq(x.resDetail.id, x.resDetail.name)       } else       {       Seq(s"[${ x.resDetail.id }](${ x.resDetail.path })", x.resDetail.name)       }  val app = x.appDetail.map{ app =>if (app.path.isEmpty)       {       Seq(app.appDef.appInfo.id, app.appDef.appInfo.name)       } else       {       Seq(s"[${ app.appDef.appInfo.id }](${ app.path })", app.appDef.appInfo.name)       } }.getOrElse(Seq("-", "-")) (res ++ app).mkString("| ", " | ", " |") }.distinct.mkString("\n") }'
Detail: 'Can't OpenScope for symbol named: 'createReferResult(scala.Seq[d2k.appdefdoc.finder.RirfData])''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'fileToStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.tmpl' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/tmpl/TemplateCatalogGenerator.scala'
Line number: '13'
Statement: 'def fileToStr(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString'
Detail: 'Can't OpenScope for symbol named: 'fileToStr(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'makeTemplate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.tmpl' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/tmpl/TemplateCatalogGenerator.scala'
Line number: '52'
Statement: 'def makeTemplate(i: CatalogInfo, o: CatalogInfo) = { val templateName = s"${ i.name }To${ o.name }"  val fileName = s"_${ templateName }.md"  val repStr = i.data + o.data  val writer = new FileWriter ((writePath / fileName).toString) writer.write( base.replaceAll("%%templatePattern%%", templateName).replaceAll("%%insert%%", repStr)) writer.close }'
Detail: 'Can't OpenScope for symbol named: 'makeTemplate(d2k.appdefdoc.gen.tmpl.CatalogInfo,d2k.appdefdoc.gen.tmpl.CatalogInfo)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readParquet' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/SingleReadPq.scala'
Line number: '11'
Statement: 'def readParquet(implicit inArgs: InputArgs) = readParquetSingle(readPqName)'
Detail: 'Can't OpenScope for symbol named: 'readParquet(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'pre[IN]' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.app.test.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/D2kTest_.scala'
Line number: '97'
Statement: 'def pre [IN](component: OneInToOneOutForDf[IN, _])(setup: CTPre => IN)(check: String) = { "CT:" + testCase should {    "be success" when {       "pre" in {          ctpre.readMdTable(s"${ check }.md").checkDf(component.preExec(setup(ctpre)))          }       }    } }'
Detail: 'Can't OpenScope for symbol named: 'pre[IN](_Unresolved.OneInToOneOutForDf[IN,_],_Unresolved.lambda[CTPre,IN],scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'pre[IN]' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.app.test.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/D2kTest_.scala'
Line number: '118'
Statement: 'def pre [IN](component: TwoInToOneOutForDf[IN, IN, _])(setup: CTPre => (IN, IN))(check: String) = { "CT:" + testCase should {    "be success" when {       "pre" in {          val resultDfs = setup(ctpre) ctpre.readMdTable(check).checkDf(component.preExec(resultDfs._1, resultDfs._2))          }       }    } }'
Detail: 'Can't OpenScope for symbol named: 'pre[IN](TwoInToOneOutForDf[IN,IN,_],lambda[CTPre,Tuple2[IN,IN]],scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'postMdToDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.app.test.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/D2kTest_.scala'
Line number: '133'
Statement: 'def postMdToDf(name: String) = readMdTableBase("post")(name).toDf'
Detail: 'Can't OpenScope for symbol named: 'postMdToDf(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'post[OUT]' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.app.test.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/D2kTest_.scala'
Line number: '134'
Statement: 'def post [OUT](component: OneInToOneOutForDf[_, OUT])(setup: String)(check: CTPost[OUT] => Unit) = { "CT:" + testCase should {    "be success" when {       "post" in {          check(CTPost(component.postExec(postMdToDf(setup))))          }       }    } }'
Detail: 'Can't OpenScope for symbol named: 'post[OUT](OneInToOneOutForDf[_,OUT],scala.String,lambda[CTPost[OUT],Unit])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'post[OUT]' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.app.test.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/D2kTest_.scala'
Line number: '144'
Statement: 'def post [OUT](component: OneInToMapOutForDf[_, OUT])(setup: Map[String, String])(check: CTPost[OUT] => Unit) = { val mapDf = setup.mapValues{ name =>postMdToDf(name)} "CT:" + testCase should {    "be success" when {       "post" in {          check(CTPost(component.postExec(mapDf)))          }       }    } }'
Detail: 'Can't OpenScope for symbol named: 'post[OUT](OneInToMapOutForDf[_,OUT],scala.Map[scala.String,scala.String],lambda[CTPost[OUT],Unit])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'post[OUT]' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.app.test.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/D2kTest_.scala'
Line number: '155'
Statement: 'def post [OUT](component: TwoInToOneOutForDf[_, _, OUT])(setup: String)(check: CTPost[OUT] => Unit) = { "CT:" + testCase should {    "be success" when {       "post" in {          check(CTPost(component.postExec(postMdToDf(setup))))          }       }    } }'
Detail: 'Can't OpenScope for symbol named: 'post[OUT](TwoInToOneOutForDf[_,_,OUT],scala.String,lambda[CTPost[OUT],Unit])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.app.test.common' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/D2kTest_.scala'
Line number: '170'
Statement: 'def apply(target: Seq[Editors]) = { val mapTarget = target.map( t =>(t.colName, t)).toMap  val targetPath = (mdPath / testCase) if (!targetPath.isDirectory)       throw new FileNotFoundException (targetPath.toString)  val fileInfos = targetPath.walk.map{ path => val mdStr = Source.fromFile(path.toString).mkString  val splitted = mdStr.split("# expect") FileInfo(path, makeRes.MdInfo(splitted(0).split("# input")(1)), makeRes.MdInfo(splitted(1))) } s"FT:${ componentName }:${ testCase }" should {    "be success" when {       fileInfos.foreach{ fi => val targetColumn = Option(mapTarget(fi.name)).flatMap{          case e:Edit => Option(e.editor)          case _ => None          }.get  val inputDf = if (fi.inMdData.data.replaceAll("\n", "").trim.isEmpty)             {             Seq(DummyDf("")).toDF             } else             {             fi.inMdData.toDf             }  val result = inputDf.select(targetColumn as fi.name) s"${ fi.no }:${ fi.name }" in {          withClue(fi.no){             val outputPos = fi.inMdData.data.count(_ == '\n') fi.outMdData.checkDf(result, outputPos)             }          } }       }    } }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.Seq[spark.common.DfCtl.Editors])''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.executor.face' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/face/DomainConverter.scala'
Line number: '17'
Statement: 'def invoke(orgDf: DataFrame)(implicit inArgs: InputArgs) : DataFrame = targetColumns.foldLeft(orgDf.na.fill("", targetColumns.map(_._1).toSeq)){(df, t) => val (name, domain) = t  val convedColumn = domain match {    case "年月日" => MakeDate.date_yyyyMMdd(domainConvert(col(name), lit(domain))).cast("date")    case "年月日時分秒" => MakeDate.timestamp_yyyyMMddhhmmss(domainConvert(col(name), lit(domain))).cast("timestamp")    case "年月日時分ミリ秒" => MakeDate.timestamp_yyyyMMddhhmmssSSS(domainConvert(col(name), lit(domain))).cast("timestamp")    case _ => domainConvert(col(name), lit(domain)) } df.withColumn(name, convedColumn) }'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.common.df.template.base' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/TwoAnyToDf.scala'
Line number: '9'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) = df'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'fileToStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.src' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/src/SourceGenerator.scala'
Line number: '40'
Statement: 'def fileToStr(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString'
Detail: 'Can't OpenScope for symbol named: 'fileToStr(scala.String)''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'generateItemConf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in 'd2k.appdefdoc.gen.src' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/src/SourceGenerator.scala'
Line number: '134'
Statement: 'def generateItemConf(baseUrl: String, branch: String, appGroup: String, appId: String, writePath: Directory)(inputFiles: Seq[IoData]) = { val fileHeader = Seq("itemId", "itemName", "length", "cnvType", "extractTarget", "comment").mkString("\t")  val writeConfPath = Directory(s"${ writePath }/itemConf") writeConfPath.createDirectory(true, false) inputFiles.map{ iodata =>println(s"  Parsing ${ iodata.id }[${ iodata.name }](${ iodata.path })")  val filePath = iodata.path.split('/').takeRight(2).mkString("/")  val itemdef = ItemDefParser(baseUrl, branch, filePath).get  val itemDetails = itemdef.details.map{ d =>Seq(d.id, d.name, d.size, d.dataType, "false", "").mkString("\t") }  val outputList = fileHeader :: itemDetails  val writeFilePath = s"${ writeConfPath }/${ appGroup }_items_${ appId }_${ iodata.id }.conf"  val writer = new FileWriterWithEncoding (writeFilePath, "MS932") writer.write(outputList.mkString("\r\n")) writer.write("\r\n") writer.close println(s"  [generate itemConf] ${ writeFilePath.replaceAllLiterally("\\", "/") }") } }'
Detail: 'Can't OpenScope for symbol named: 'generateItemConf(scala.String,scala.String,scala.String,scala.String,Directory,Seq[IoData])''
[03/24/2023 05:42:22] Info: Step 7/9 - Pre-Conversion Assessment: COMPLETED
[03/24/2023 05:42:22] Debug: TaskParam ExecutionModeValue = Conversion
[03/24/2023 05:42:22] Debug: TaskParam codeModelWriter = Mobilize.Common.AssessmentModel.Writer.CodeModelWriter
[03/24/2023 05:42:22] Debug: TaskParam symbolTable = Mobilize.Scala.SymbolTable.SclSymbolTable
[03/24/2023 05:42:22] Debug: TaskParam inputPath = /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources
[03/24/2023 05:42:22] Debug: TaskParam outputPath = /tmp/Output-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Output/SparkSnowConvert
[03/24/2023 05:42:22] Debug: TaskParam processingProgressDescriptor = Mobilize.Common.Utils.Progress.SingleProgressDescriptor
[03/24/2023 05:42:22] Debug: TaskParam transformationVisitors = Artinsoft.Common.Tools.Transform.TransformationVisitors
[03/24/2023 05:42:22] Debug: TaskParam conversionStatusData = Mobilize.SparkCommon.Assessment.ConversionStatus.ConversionStatusData
[03/24/2023 05:42:22] Debug: TaskParam projectId = Sources
[03/24/2023 05:42:22] Debug: TaskParam TransformationsTask.Input = /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources/input.wsp
[03/24/2023 05:42:22] Debug: TaskParam workingSet = Artinsoft.Common.Store.RepositoryWorkingSet`2[System.String,Artinsoft.Common.Store.IItemContainer]
[03/24/2023 05:42:22] Debug: TaskParam TransformationsTask.ItemMedatada = Artinsoft.Common.Store.ItemMetadata
[03/24/2023 05:42:22] Debug: TaskParam TransformationsTask.Enabled = True
[03/24/2023 05:42:22] Debug: TaskParam Repository = Artinsoft.Common.Store.Repository
[03/24/2023 05:42:22] Debug: TaskParam RepositoryDirectory = /tmp/Output-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/.mobilize/CommonEF
[03/24/2023 05:42:22] Info: Step 8/9 - Conversion Execution: STARTED
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/MultiDbToMultiAny.scala'
Line number: '10'
Statement: 'def preExec(in: Unit)(implicit inArgs: InputArgs) : Map[String, DataFrame] = readDb'
Detail: 'Can't OpenScope for symbol named: 'preExec(scala.Unit,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Info: Transformed file MultiDbToMultiAny.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/DbOutputCommonFunctions.scala'
Line number: '10'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) = DbOutputCommonFunctions(df)'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/DbOutputCommonFunctions.scala'
Line number: '14'
Statement: 'def apply(df: DataFrame)(implicit inArgs: InputArgs) = PqCommonColumnRemover(RowErrorRemover(df)).withColumn("VC_DISPOYMD", lit(inArgs.runningDateYMD))'
Detail: 'Can't OpenScope for symbol named: 'apply(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Info: Transformed file FileConv.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/AnyToPq.scala'
Line number: '10'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) = writeParquet(df)'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Info: Transformed file AnyToPq.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readParquet' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/MultiReadPq.scala'
Line number: '11'
Statement: 'def readParquet(implicit inArgs: InputArgs) = readPqNames.map( pqName =>(pqName, readParquetSingle(pqName))).toMap'
Detail: 'Can't OpenScope for symbol named: 'readParquet(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Info: Transformed file MultiReadPq.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/JefConverter.scala'
Line number: '22'
Statement: 'def readData(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).getLines.map{ line => val kv = line.split('\t') (kv(0), kv(1)) }.toMap'
Detail: 'Can't OpenScope for symbol named: 'readData(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'addControlCode' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/JefConverter.scala'
Line number: '29'
Statement: 'def addControlCode(origin: Map[String, String]) = origin ++ Map(tab -> "\t", kanjiOut -> "", kanjiIn9 -> "", kanjiIn12 -> "")'
Detail: 'Can't OpenScope for symbol named: 'addControlCode(scala.Map[scala.String,scala.String])''
[03/24/2023 05:42:22] Info: Transformed file DbOutputCommonFunctions.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/ComponentFlowParser.scala'
Line number: '16'
Statement: 'def apply(target: String) = { parseAll(componentFlow, target) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'isJefHalf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/JefConverter.scala'
Line number: '38'
Statement: 'def isJefHalf(domain: String, charEnc: String) = charEnc == "JEF" && !domain.startsWith("全角文字列")'
Detail: 'Can't OpenScope for symbol named: 'isJefHalf(scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'isJefFull' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/JefConverter.scala'
Line number: '39'
Statement: 'def isJefFull(domain: String, charEnc: String) = charEnc == "JEF" && domain.startsWith("全角文字列")'
Detail: 'Can't OpenScope for symbol named: 'isJefFull(scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'convJefToUtfHalf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/JefConverter.scala'
Line number: '41'
Statement: 'def convJefToUtfHalf(data: Array[Byte]) : String = { val conved = data.map{ byte =>(byte, Try(jefToUtfHalfData(f"$byte%02X")))} if (!conved.map(_._1).forall(_ == 0x00))       printErrorHalf(conved) conved.map(_._2.getOrElse("*")).mkString }'
Detail: 'Can't OpenScope for symbol named: 'convJefToUtfHalf(scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Info: Transformed file ComponentFlowParser.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/Nothing_.scala'
Line number: '10'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) : DataFrame = df'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Info: Transformed file Nothing_.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'convJefToUtfFull' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/JefConverter.scala'
Line number: '52'
Statement: 'def convJefToUtfFull(data: Array[Byte]) : String = { data.grouped(2).map{ byteArr => val jefCode = byteArr.map( x =>f"$x%02X").mkString jefToUtfFullKddiData.get(jefCode).orElse{ jefToUtfFullData.get(jefCode) }.getOrElse{    println(s"!!!![JEF CONV ERROR:FULL]${ byteArr.map( x =>f"$x%02X").mkString }") "■"    } }.mkString }'
Detail: 'Can't OpenScope for symbol named: 'convJefToUtfFull(scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'convUtfToJefHalf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/JefConverter.scala'
Line number: '64'
Statement: 'def convUtfToJefHalf(data: String) : Array[Byte] = data.map( x =>Try(utfToJefHalfData(x)).getOrElse("5C")).map( x =>Integer.parseInt(x.toString, 16)).map(_.toByte).toArray'
Detail: 'Can't OpenScope for symbol named: 'convUtfToJefHalf(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'convUtfToJefFull' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/JefConverter.scala'
Line number: '67'
Statement: 'def convUtfToJefFull(data: String) : Array[Byte] = { data.flatMap( x =>Try(utfToJefFullData(x)).getOrElse("A2A3")).grouped(2).map( x =>Integer.parseInt(x.toString, 16)).map(_.toByte).toArray }'
Detail: 'Can't OpenScope for symbol named: 'convUtfToJefFull(scala.String)''
[03/24/2023 05:42:22] Info: Transformed file JefConverter.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'd2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/HiRDB_readTest.scala'
Line number: '29'
Statement: 'def d2s(dateMill: Long) = new DateTime (dateMill).toString("yyyy-MM-dd")'
Detail: 'Can't OpenScope for symbol named: 'd2s(scala.Long)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'd2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/HiRDB_readTest.scala'
Line number: '30'
Statement: 'def d2s(date: Date) = new DateTime (date).toString("yyyy-MM-dd")'
Detail: 'Can't OpenScope for symbol named: 'd2s(_Unresolved.Date)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'd2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/HiRDB_readTest.scala'
Line number: '31'
Statement: 'def d2s(date: Timestamp) = new DateTime (date).toString("yyyy-MM-dd hh:mm:ss")'
Detail: 'Can't OpenScope for symbol named: 'd2s(_Unresolved.Timestamp)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'parse' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/ConfParser.scala'
Line number: '23'
Statement: 'def parse(inFilePath: String) = parseAppConf(inFilePath)'
Detail: 'Can't OpenScope for symbol named: 'parse(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readConf[A]' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/ConfParser.scala'
Line number: '25'
Statement: 'def readConf [A](inFilePath: String)(proc: Array[String] => A) = { val fileEnc = "MS932"  val itemConfPath = s"itemConf/${ File(inFilePath).name }" Option(getClass.getClassLoader.getResourceAsStream(itemConfPath)).map( is =>Source.fromInputStream(is, fileEnc)).getOrElse{ Source.fromFile(inFilePath, fileEnc) }.getLines.drop(1).map( line =>proc(line.split('\t'))) }'
Detail: 'Can't OpenScope for symbol named: 'readConf[A](scala.String,lambda[Array[String],A])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'checkItem' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/ConfParser.scala'
Line number: '35'
Statement: 'def checkItem(items: Array[String])(abailables: Seq[String], targetIdx: Int, comment: String) = if (!abailables.contains(items(targetIdx)))    {    throw new IllegalArgumentException ( s"not available item:${ items(targetIdx) }(usage: ${ abailables.mkString(" or ") }) in $comment")    }'
Detail: 'Can't OpenScope for symbol named: 'checkItem(scala.Array[scala.String],scala.Seq[scala.String],scala.Int,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'parseAppConf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/ConfParser.scala'
Line number: '41'
Statement: 'def parseAppConf(inFilePath: String) = { val appConfs = readConf(inFilePath){ items =>appErrorCheck(items) if (items.size == 10)       {       AppConf(items(0), items(1), items(2), items(3), items(4) == "true", items(5) == "true", items(6) == "true", items(7), items(8), items(9))       } else       {       AppConf(items(0), items(1), items(2), items(3), items(4) == "true", items(5) == "true", items(6) == "true", items(7), items(8), items(9), items(10))       } }  val basePath = Path(inFilePath).toAbsolute.parent  val namePrefix = Path(inFilePath).name.split('_')(0) appConfs.map( appConf =>Conf(appConf, parseItemConf(basePath, namePrefix, appConf.AppId))) }'
Detail: 'Can't OpenScope for symbol named: 'parseAppConf(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'getAvailableBoolean' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/ConfParser.scala'
Line number: '55'
Statement: 'def getAvailableBoolean(items: Array[String]) = { if (items(3) == "fixed")       {       availableBoolean       } else       {       availableBooleanWithBlank       } }'
Detail: 'Can't OpenScope for symbol named: 'getAvailableBoolean(scala.Array[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'appErrorCheck' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/ConfParser.scala'
Line number: '59'
Statement: 'def appErrorCheck(items: Array[String]) = { val checker = checkItem(items)_  def comment(name: String) = s"AppConf[appId:${ items(0) } name:${ name }]" checker(availableFileFormat, 3, comment("fileFormat")) checker(getAvailableBoolean(items), 4, comment("newline")) checker(availableBoolean, 5, comment("header")) checker(availableBoolean, 6, comment("footer")) checker(availableStoreType, 7, comment("storeType")) }'
Detail: 'Can't OpenScope for symbol named: 'appErrorCheck(scala.Array[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'parseItemConf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/ConfParser.scala'
Line number: '69'
Statement: 'def parseItemConf(basePath: Directory, prefix: String, appId: String) = { val itemConfPath = basePath / s"${ prefix }_items_${ appId }.conf" readConf(itemConfPath.toString){ items =>itemErrorCheck(items, appId) if (items.size == 5)       {       ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true")       } else       {       ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true", items(5))       } } }'
Detail: 'Can't OpenScope for symbol named: 'parseItemConf(Directory,scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'itemErrorCheck' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/ConfParser.scala'
Line number: '81'
Statement: 'def itemErrorCheck(items: Array[String], appId: String) = { val checker = checkItem(items)_  def comment(name: String) = s"ItemConf[appId:${ appId } item:${ items(0) } name:${ name }]" checkItem(items)(availableBoolean, 4, comment("extractTarget")) }'
Detail: 'Can't OpenScope for symbol named: 'itemErrorCheck(scala.Array[scala.String],scala.String)''
[03/24/2023 05:42:22] Info: Transformed file ConfParser.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'toOptions' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '43'
Statement: 'def toOptions(tableName: String) = { val addFetchSize = fetchSize.map( fs =>baseMap + (JDBCOptions.JDBC_BATCH_FETCH_SIZE -> fs.toString)).getOrElse(baseMap) new JDBCOptions (addFetchSize + (JDBCOptions.JDBC_TABLE_NAME -> tableName)) }'
Detail: 'Can't OpenScope for symbol named: 'toOptions(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'toOptionsInWrite' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '48'
Statement: 'def toOptionsInWrite(tableName: String) = { val addFetchSize = fetchSize.map( fs =>baseMap + (JDBCOptions.JDBC_BATCH_FETCH_SIZE -> fs.toString)).getOrElse(baseMap) new JdbcOptionsInWrite (addFetchSize + (JDBCOptions.JDBC_TABLE_NAME -> tableName)) }'
Detail: 'Can't OpenScope for symbol named: 'toOptionsInWrite(scala.String)''
[03/24/2023 05:42:22] Info: Transformed file ReadPqTest.scala
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'makeWhere10' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '71'
Statement: 'def makeWhere10(columnName: String) = ((0 to 9).map( cnt =>f"substr(${ columnName },-1,1) = '$cnt'") :+ s" substr(${ columnName },-1,1) not in ('0','1','2','3','4','5','6','7','8','9') ").toArray'
Detail: 'Can't OpenScope for symbol named: 'makeWhere10(scala.String)''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'localImport' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '74'
Statement: 'def localImport(tableName: String, cnt: Int = 100) = { val orgTb = new DbCtl (DbCtl.dbInfo2).readTable(tableName)  val posgre = new DbCtl () import posgre.implicits._ context.createDataFrame(sc.makeRDD(orgTb.take(cnt)), orgTb.schema).writeTable(tableName, SaveMode.Overwrite) posgre.readTable(tableName).show }'
Detail: 'Can't OpenScope for symbol named: 'localImport(scala.String,scala.Int)''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'canHandle' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '91'
Statement: 'def canHandle(url: String) : Boolean = url.startsWith("jdbc:oracle") || url.contains("oracle")'
Detail: 'Can't OpenScope for symbol named: 'canHandle(scala.String)''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'getCatalystType' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '93'
Statement: 'def getCatalystType(sqlType: Int, typeName: String, size: Int, md: MetadataBuilder) : Option[DataType] = { if (sqlType == Types.NUMERIC && size == 0)       {       Option(DecimalType(DecimalType.MAX_PRECISION, 10))       } else       {       None       } }'
Detail: 'Can't OpenScope for symbol named: 'getCatalystType(scala.Int,scala.String,scala.Int,MetadataBuilder)''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'canHandle' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '110'
Statement: 'def canHandle(url: String) : Boolean = url.startsWith("jdbc:derby") || url.contains("derby")'
Detail: 'Can't OpenScope for symbol named: 'canHandle(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'getTableFullName' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '132'
Statement: 'def getTableFullName(tableName: String) = { val sql = "SELECT TABLE_OWNER FROM ALL_SYNONYMS WHERE TABLE_NAME = ? AND (OWNER ='PUBLIC' OR OWNER = ?) ORDER BY DECODE(OWNER,'PUBLIC',1,0)"  val rs = prepExecSql(tableName, sql){ prs =>prs.setString(1, tableName.toUpperCase()) prs.setString(2, dbInfo.user.toUpperCase()) } if (rs.next())       s"${ rs.getString(1) }.${ tableName }" else       tableName }'
Detail: 'Can't OpenScope for symbol named: 'getTableFullName(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'clearTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '141'
Statement: 'def clearTable(tableName: String) = { elapse("clearTable"){    val targetTable = if (tableName.contains("."))          tableName else          Try(getTableFullName(tableName)).getOrElse(tableName) Try(truncateTable(targetTable)).recover{       case e => errorLog(s"FAILED TO TRUNCATE ${ targetTable }", e);deleteTable(tableName)       }.get    } }'
Detail: 'Can't OpenScope for symbol named: 'clearTable(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'execSql' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '148'
Statement: 'def execSql(tableName: String, sql: String) = { println(tableName, sql) JdbcUtils.createConnectionFactory(dbInfo.toOptions(tableName))().prepareStatement(sql).executeUpdate }'
Detail: 'Can't OpenScope for symbol named: 'execSql(scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'prepExecSql' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '152'
Statement: 'def prepExecSql(tableName: String, sql: String)(prsFunc: PreparedStatement => Unit) = { val prep = JdbcUtils.createConnectionFactory(dbInfo.toOptions(tableName))().prepareStatement(sql) prsFunc(prep) prep.executeQuery }'
Detail: 'Can't OpenScope for symbol named: 'prepExecSql(scala.String,scala.String,lambda[PreparedStatement,Unit])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'truncateTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '157'
Statement: 'def truncateTable(tableName: String) = execSql(tableName, s"TRUNCATE TABLE $tableName")'
Detail: 'Can't OpenScope for symbol named: 'truncateTable(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'dropTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '158'
Statement: 'def dropTable(tableName: String) = JdbcUtils.dropTable(JdbcUtils.createConnectionFactory(dbInfo.toOptions)(), tableName, dbInfo.toOptions)'
Detail: 'Can't OpenScope for symbol named: 'dropTable(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'deleteTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '160'
Statement: 'def deleteTable(tableName: String) = execSql(tableName, s"DELETE FROM $tableName")'
Detail: 'Can't OpenScope for symbol named: 'deleteTable(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'columnTypes' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '161'
Statement: 'def columnTypes(conn: Connection, tableName: String) = { val result = conn.getMetaData.getColumns(null, null, tableName.toUpperCase, "%") new Iterator[(String, Int)] {    def hasNext = result.next  def next() = (result.getString("COLUMN_NAME"), result.getString("DATA_TYPE").toInt)    }.toMap }'
Detail: 'Can't OpenScope for symbol named: 'columnTypes(Connection,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'changeColToDfTypes' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '169'
Statement: 'def changeColToDfTypes(colType: Int, dfTypes: DataType) = dfTypes match {    case DateType => DATE    case TimestampType => TIMESTAMP    case IntegerType => INTEGER    case LongType => DOUBLE    case DecimalType() => DECIMAL    case _ => colType }'
Detail: 'Can't OpenScope for symbol named: 'changeColToDfTypes(scala.Int,DataType)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '178'
Statement: 'def readTable(tableName: String) = { context.read.jdbc(dbInfo.url, tableName, props) }'
Detail: 'Can't OpenScope for symbol named: 'readTable(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '182'
Statement: 'def readTable(tableName: String, where: Array[String]) = { context.read.jdbc(dbInfo.url, tableName, where, props) }'
Detail: 'Can't OpenScope for symbol named: 'readTable(scala.String,scala.Array[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '186'
Statement: 'def readTable(tableName: String, requiredColumns: Array[String], where: Array[String]) = { JdbcCtl.readTable(this, tableName, requiredColumns, where) }'
Detail: 'Can't OpenScope for symbol named: 'readTable(scala.String,scala.Array[scala.String],scala.Array[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'loanConnection' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '190'
Statement: 'def loanConnection(tableName: String, sqlStr: String, executeBatch: Boolean = true)(f: PreparedStatement => Unit) {    val jdbcConn = JdbcUtils.createConnectionFactory(dbInfo.toOptions(tableName))()  val prs = jdbcConn.prepareStatement(sqlStr) try       {f(prs) if (executeBatch)             prs.executeBatch}    finally       {       Some(prs).foreach(_.close) Some(jdbcConn).foreach(_.close)       } }'
Detail: 'Can't OpenScope for symbol named: 'loanConnection(scala.String,scala.String,scala.Boolean,lambda[PreparedStatement,Unit])''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'dateAddHyphen' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/HiRDB_readTest.scala'
Line number: '122'
Statement: 'def dateAddHyphen(str: String) = s"${ str.take(4) }-${ str.drop(4).take(2) }-${ str.drop(6) }"'
Detail: 'Can't OpenScope for symbol named: 'dateAddHyphen(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/HiRDB_readTest.scala'
Line number: '127'
Statement: 'def readDbWhere(inArgs: InputArgs) = Array(s""""DATE" = '${ dateAddHyphen(inArgs.runningDates(0)) }'""")'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/HiRDB_readTest.scala'
Line number: '138'
Statement: 'def readDbWhere(inArgs: InputArgs) = Array(s""""TMSTMP" = '${ dateAddHyphen(inArgs.runningDates(0)) } 00:00:00'""")'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'setValues' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '221'
Statement: 'def setValues(columnTypes: Map[String, Int], structType: StructType, cols: Array[String], prs: PreparedStatement)(row: Row) = { def setPreparedValue(preps: PreparedStatement, col: String, idx: Int) = {    structType(col).dataType match {          case _if row.isNullAt(row.fieldIndex(col)) => preps.setNull(idx + 1, changeColToDfTypes(columnTypes(col), structType(col).dataType))          case StringType => preps.setString(idx + 1, row.getAs[String](col))          case DateType => preps.setDate(idx + 1, row.getAs[Date](col))          case TimestampType => preps.setTimestamp(idx + 1, row.getAs[Timestamp](col))          case IntegerType => preps.setInt(idx + 1, row.getAs[Int](col))          case LongType => preps.setLong(idx + 1, row.getAs[Long](col))          case DecimalType() => preps.setBigDecimal(idx + 1, row.getAs[java.math.BigDecimal](col))       } preps    } cols.zipWithIndex.foldLeft(prs){    case (preps, (col, idx)) => setPreparedValue(preps, col, idx)    } prs.addBatch }'
Detail: 'Can't OpenScope for symbol named: 'setValues(scala.Map[scala.String,scala.Int],StructType,scala.Array[scala.String],PreparedStatement,Row)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'insertAccelerated' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '238'
Statement: 'def insertAccelerated(df: DataFrame, tableName: String, mode: SaveMode = SaveMode.Append, hint: String = "") = { val structType = df.schema  val fieldNames = structType.fieldNames  val columnsStr = fieldNames.mkString(",")  val bindStr = fieldNames.map(_ =>"?").mkString(",")  def nonDirectInsert = {    val insertStr = s"""       insert /*+ ${ hint } */ into ${ tableName }(${ columnsStr }) values(${ bindStr })       """ if (mode == SaveMode.Overwrite)          clearTable(tableName) df.foreachPartition(recordProcessor(tableName, insertStr, structType, fieldNames))    }  def directInsert = {    val insertStr = s"""       insert /*+ APPEND_VALUES ${ hint } */ into ${ tableName }(${ columnsStr }) values(${ bindStr })       """ if (mode == SaveMode.Overwrite)          clearTable(tableName) df.coalesce(1).foreachPartition(recordProcessor(tableName, insertStr, structType, fieldNames))    }  val dpi = sys.props.get(DbCtl.envName.DPI_MODE).map(DbCtl.checkTrueOrOn).getOrElse(dbInfo.isDirectPathInsertMode) if (dpi)       directInsert else       nonDirectInsert }'
Detail: 'Can't OpenScope for symbol named: 'insertAccelerated(DataFrame,scala.String,SaveMode,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'insertNotExists' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '267'
Statement: 'def insertNotExists(df: DataFrame, tableName: String, inKeys: Seq[String], mode: SaveMode = SaveMode.Append, hint: String = "") = { val keysStr = inKeys.mkString(",")  val structType = df.schema  val fieldNames = structType.fieldNames  val columnsStr = fieldNames.mkString(",")  val bindStr = fieldNames.map(_ =>"?").mkString(",")  val insertStr = s"""       insert /*+ ignore_row_on_dupkey_index(${ tableName }(${ keysStr })) ${ hint } */ into ${ tableName }(${ columnsStr }) values(${ bindStr })       """ if (mode == SaveMode.Overwrite)       clearTable(tableName) df.foreachPartition(recordProcessor(tableName, insertStr, structType, fieldNames)) }'
Detail: 'Can't OpenScope for symbol named: 'insertNotExists(DataFrame,scala.String,scala.Seq[scala.String],SaveMode,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'deleteRecords' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '280'
Statement: 'def deleteRecords(df: DataFrame, tableName: String, inKeys: Set[String], hint: String = "") = { val keysArray = inKeys.toArray  val keysStr = keysArray.map( k =>s"${ k.toLowerCase } = ?").mkString(" and ")  val deleteStr = s"""       delete /*+ ${ hint } */ from $tableName where $keysStr       """  val structType = df.schema df.foreachPartition(recordProcessor(tableName, deleteStr, structType, keysArray)) }'
Detail: 'Can't OpenScope for symbol named: 'deleteRecords(DataFrame,scala.String,scala.Set[scala.String],scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'dropArrayData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '294'
Statement: 'def dropArrayData(target: Seq[String], dropList: Seq[String]) = dropList.foldLeft(target){(l, r) =>l.filterNot(_ == r) }'
Detail: 'Can't OpenScope for symbol named: 'dropArrayData(scala.Seq[scala.String],scala.Seq[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'updateRecordsBase' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '298'
Statement: 'def updateRecordsBase(mode: DbCtl.UpdateMode)(df: DataFrame, tableName: String, inKeys: Set[String], ignoreColumnsForUpdate: Set[String] = Set.empty[String], hint: String = "") = { val keysArray = inKeys.toArray  val cols = df.columns  val colsWithoutKey = dropArrayData(df.columns, keysArray)  val colsWithoutKeyAndIgnore = dropArrayData(colsWithoutKey, ignoreColumnsForUpdate.toSeq)  val colsWithoutKeyStr = colsWithoutKeyAndIgnore.map( k =>s"${ k.toLowerCase } = ?").mkString(",")  val keysStr = keysArray.map( k =>s"${ k.toLowerCase } = ?").mkString(" and ")  val updateStr = s"""       update /*+ ${ hint } */ ${ tableName }  set $colsWithoutKeyStr where $keysStr       """  val usingStr = cols.map( x =>s"? $x").mkString(",")  val onStr = keysArray.map( x =>s"a.${ x } = b.${ x }").mkString(" and ")  val updateSetStr = colsWithoutKeyAndIgnore.map( x =>s"a.${ x } = b.${ x }").mkString(",")  val insertStrLeft = cols.map( x =>s"a.${ x }").mkString(",")  val insertStrRight = cols.map( x =>s"b.${ x }").mkString(",")  val mergeStr = s"""       merge /*+ ${ hint } */ into ${ tableName } a         using (select ${ usingStr } from dual) b         on (${ onStr })       when matched then         update set ${ updateSetStr }       when not matched then         insert (${ insertStrLeft }) values (${ insertStrRight })       """  val structType = df.schema mode match {       case DbCtl.UPSERT => df.foreachPartition(recordProcessor(tableName, mergeStr, structType, cols))       case DbCtl.UPDATE => df.foreachPartition(recordProcessor(tableName, updateStr, structType, colsWithoutKeyAndIgnore ++: keysArray))    } }'
Detail: 'Can't OpenScope for symbol named: 'updateRecordsBase(spark.common.DbCtl.UpdateMode,DataFrame,scala.String,scala.Set[scala.String],scala.Set[scala.String],scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '335'
Statement: 'def writeTable(tableName: String, mode: SaveMode = SaveMode.Append) = mode match {    case SaveMode.Overwrite => {    clearTable(tableName) JdbcUtils.saveTable(df, None, true, dbInfo.toOptionsInWrite(tableName))    }    case SaveMode.Append => {    JdbcUtils.saveTable(df, None, true, dbInfo.toOptionsInWrite(tableName))    }    case _ => df.write.mode(mode).jdbc(dbInfo.url, tableName, props) }'
Detail: 'Can't OpenScope for symbol named: 'writeTable(scala.String,SaveMode)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeTableStandard' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '349'
Statement: 'def writeTableStandard(tableName: String, mode: SaveMode = SaveMode.Append) = mode match {    case SaveMode.Overwrite => {    clearTable(tableName) df.write.mode(mode).jdbc(dbInfo.url, tableName, props)    }    case _ => df.write.mode(mode).jdbc(dbInfo.url, tableName, props) }'
Detail: 'Can't OpenScope for symbol named: 'writeTableStandard(scala.String,SaveMode)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'autoCreateTable' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '360'
Statement: 'def autoCreateTable(tableName: String) {    Try(df.limit(1).write.mode(SaveMode.Overwrite).jdbc(dbInfo.url, tableName, props)) clearTable(tableName) }'
Detail: 'Can't OpenScope for symbol named: 'autoCreateTable(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readParquetAndWriteDb' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DbCtl.scala'
Line number: '368'
Statement: 'def readParquetAndWriteDb(readParquetBasePath: String, readParquetPath: String, writeTableName: String, saveMode: SaveMode = SaveMode.Append)(proc: DataFrame => DataFrame = df =>df) = { proc(new PqCtl (readParquetBasePath).readParquet(readParquetPath)).writeTable(writeTableName, saveMode) }'
Detail: 'Can't OpenScope for symbol named: 'readParquetAndWriteDb(scala.String,scala.String,scala.String,SaveMode,lambda[DataFrame,DataFrame])''
[03/24/2023 05:42:22] Info: Transformed file HiRDB_readTest.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/MakeDf.scala'
Line number: '19'
Statement: 'def apply(confPath: String) = new Plane (confPath)'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'dc' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/MakeDf.scala'
Line number: '20'
Statement: 'def dc(confPath: String) = new DomainConverter (confPath)'
Detail: 'Can't OpenScope for symbol named: 'dc(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'makeInputDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/MakeDf.scala'
Line number: '22'
Statement: 'def makeInputDf(rows: Seq[Row], names: Seq[String], domains: Seq[String]) = { val rdd = SparkContexts.sc.makeRDD(rows)  val ziped = names.zip(domains)  val (nameList, domainList) = ziped.filter{    case (names, domain) => !(domain.startsWith(Converter.NOT_USE_PREFIX) || domain.startsWith(Converter.REC_DIV_PREFIX))    }.unzip SparkContexts.context.createDataFrame(rdd, Converter.makeSchema(nameList)) }'
Detail: 'Can't OpenScope for symbol named: 'makeInputDf(Seq[Row],scala.Seq[scala.String],scala.Seq[scala.String])''
[03/24/2023 05:42:22] Info: Transformed file ResourceItemRouteFinder.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'parseItemConf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/MakeDf.scala'
Line number: '29'
Statement: 'def parseItemConf(confPath: String) = { val conf = ConfParser.readConf(confPath){ items =>if (items.size == 5)       {       ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true")       } else       {       ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true", items(5))       } }.toSeq (conf, conf.map(_.itemId), conf.map(_.cnvType)) }'
Detail: 'Can't OpenScope for symbol named: 'parseItemConf(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/AnyToVal.scala'
Line number: '11'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) : T = outputValue(df.collect)'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Info: Transformed file AnyToVal.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readFile' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/ReadFile.scala'
Line number: '11'
Statement: 'def readFile(implicit inArgs: InputArgs) = { new FileConv (componentId, fileInputInfo, itemConfId).makeDf }'
Detail: 'Can't OpenScope for symbol named: 'readFile(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Info: Transformed file ReadFile.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readAppDefMd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/D2kParser.scala'
Line number: '6'
Statement: 'def readAppDefMd(baseUrl: String, branch: String, appGroup: String, appId: String, fileName: String) = { val appBaseUrl = s"${ baseUrl }/raw/${ branch }/apps/${ appGroup }/${ appId }"  val url = s"${ appBaseUrl }/${ fileName }" Source.fromURL(s"${ url }?private_token=${ sys.env("GITLAB_TOKEN") }").getLines.mkString("\n") }'
Detail: 'Can't OpenScope for symbol named: 'readAppDefMd(scala.String,scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readItemDefMd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/D2kParser.scala'
Line number: '12'
Statement: 'def readItemDefMd(baseUrl: String, branch: String, filePath: String) = { val itemsBaseUrl = s"${ baseUrl }/raw/${ branch }/apps/common/items"  val url = s"${ itemsBaseUrl }/${ filePath }" Source.fromURL(s"${ url }?private_token=${ sys.env("GITLAB_TOKEN") }").getLines.mkString("\n") }'
Detail: 'Can't OpenScope for symbol named: 'readItemDefMd(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readAppDefMd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/D2kParser.scala'
Line number: '18'
Statement: 'def readAppDefMd(basePath: String, appGroup: String, appId: String, fileName: String) = { val appBasePath = s"${ basePath }/apps/${ appGroup }/${ appId }"  val path = s"${ appBasePath }/${ fileName }" Source.fromFile(path).getLines.mkString("\n") }'
Detail: 'Can't OpenScope for symbol named: 'readAppDefMd(scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:22] Info: Transformed file DbCtl.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readAppDefMd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/D2kParser.scala'
Line number: '24'
Statement: 'def readAppDefMd(basePath: String) = { Source.fromFile(basePath).getLines.mkString("\n") }'
Detail: 'Can't OpenScope for symbol named: 'readAppDefMd(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readItemDefMd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/D2kParser.scala'
Line number: '28'
Statement: 'def readItemDefMd(basePath: String) = { Source.fromFile(basePath).getLines.mkString("\n") }'
Detail: 'Can't OpenScope for symbol named: 'readItemDefMd(scala.String)''
[03/24/2023 05:42:22] Info: Transformed file D2kParser.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'makeSliceLen' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '19'
Statement: 'def makeSliceLen(len: Seq[Int]) = len.foldLeft((0, List.empty[(Int, Int)])){(l, r) =>(l._1 + r, l._2 :+ (l._1, l._1 + r))}'
Detail: 'Can't OpenScope for symbol named: 'makeSliceLen(scala.Seq[scala.Int])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'cnvFromFixed' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '21'
Statement: 'def cnvFromFixed(names: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)])(inData: Array[Byte]) = { val dataAndDomainsAndNames = sliceLen.map{    case (start, end) => inData.slice(start, end)    }.zip(domains).zip(names)  val result = Converter.domainConvert(dataAndDomainsAndNames, charEnc) Row(result :_*) }'
Detail: 'Can't OpenScope for symbol named: 'cnvFromFixed(scala.Seq[scala.String],scala.Seq[scala.String],scala.List[scala.Tuple2[scala.Int,scala.Int]],scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'makeSchema' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/Converter.scala'
Line number: '25'
Statement: 'def makeSchema(inNames: Seq[String]) = { val names = inNames :+ SYSTEM_COLUMN_NAME.ROW_ERROR :+ SYSTEM_COLUMN_NAME.ROW_ERROR_MESSAGE StructType(names.map{    case name => StructField(name, StringType, true)    }) }'
Detail: 'Can't OpenScope for symbol named: 'makeSchema(scala.Seq[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'makeSchemaWithRecordError' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/Converter.scala'
Line number: '30'
Statement: 'def makeSchemaWithRecordError(inNames: Seq[String]) = { val names = inNames :+ SYSTEM_COLUMN_NAME.ROW_ERROR :+ SYSTEM_COLUMN_NAME.ROW_ERROR_MESSAGE :+ SYSTEM_COLUMN_NAME.RECORD_LENGTH_ERROR StructType(names.map{    case name => StructField(name, StringType, true)    }) }'
Detail: 'Can't OpenScope for symbol named: 'makeSchemaWithRecordError(scala.Seq[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'cnvFromFixedWithIndex' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '27'
Statement: 'def cnvFromFixedWithIndex(names: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)])(inData: (Array[Byte], Long)) = { val dataAndDomainsAndNames = sliceLen.map{    case (start, end)if start == -1 && end == -1 => inData._2.toString.getBytes    case (start, end) => inData._1.slice(start, end)    }.zip(domains).zip(names)  val result = Converter.domainConvert(dataAndDomainsAndNames, charEnc) Row(result :_*) }'
Detail: 'Can't OpenScope for symbol named: 'cnvFromFixedWithIndex(scala.Seq[scala.String],scala.Seq[scala.String],scala.List[scala.Tuple2[scala.Int,scala.Int]],Tuple2[Array[Byte],Long])''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'cnvFromFixedAddAllData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '36'
Statement: 'def cnvFromFixedAddAllData(names: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)])(inData: Array[Byte]) = { val dataAndDomainsAndNames = sliceLen.map{    case (start, end) => inData.slice(start, end)    }.zip(domains).zip(names)  val result = Seq(new String (inData, charEnc)) ++ Converter.domainConvert(dataAndDomainsAndNames, charEnc) Row(result :_*) }'
Detail: 'Can't OpenScope for symbol named: 'cnvFromFixedAddAllData(scala.Seq[scala.String],scala.Seq[scala.String],scala.List[scala.Tuple2[scala.Int,scala.Int]],scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'addLineBreak' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '42'
Statement: 'def addLineBreak(totalLen: Int, lineBreak: Boolean) = (lineBreak, newLineCode) match {    case (false, _) => totalLen    case (true, CR) => totalLen + 1    case (true, LF) => totalLen + 1    case (true, CRLF) => totalLen + 2 }'
Detail: 'Can't OpenScope for symbol named: 'addLineBreak(scala.Int,scala.Boolean)''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'addIndexColumn' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '49'
Statement: 'def addIndexColumn(names: Seq[String], nameList: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)]) = (names :+ SYSTEM_COLUMN_NAME.RECORD_INDEX,  nameList :+ SYSTEM_COLUMN_NAME.RECORD_INDEX,  domains :+ "文字列",  sliceLen :+ (-1, -1))'
Detail: 'Can't OpenScope for symbol named: 'addIndexColumn(scala.Seq[scala.String],scala.Seq[scala.String],scala.Seq[scala.String],scala.List[scala.Tuple2[scala.Int,scala.Int]])''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'arrToMap' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '55'
Statement: 'def arrToMap(targetNames: Seq[String], names: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)])(inData: Array[Byte]) : Map[String, String] = { val dataAndDomainsAndNames = sliceLen.map{    case (start, end) => inData.slice(start, end)    }.zip(domains).zip(names)  val targetData = targetNames.flatMap{ target =>dataAndDomainsAndNames.filter(_._2 == target) }  val converted = Converter.domainConvert(targetData, charEnc) targetData.map(_._2).zip(converted).toMap }'
Detail: 'Can't OpenScope for symbol named: 'arrToMap(scala.Seq[scala.String],scala.Seq[scala.String],scala.Seq[scala.String],scala.List[scala.Tuple2[scala.Int,scala.Int]],scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'addArr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '64'
Statement: 'def addArr(row: Row, arr: Array[Byte]) = { val rowValue = Row(row.toSeq :_*) withBinaryRecord.map(_ =>Row.merge(rowValue, Row(arr.clone))).getOrElse(rowValue) }'
Detail: 'Can't OpenScope for symbol named: 'addArr(Row,scala.Array[scala.Byte])''
[03/24/2023 05:42:22] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeFixedFile' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/MakeDf.scala'
Line number: '68'
Statement: 'def writeFixedFile(writePath: String, append: Boolean = false, header: Boolean = false, footer: Boolean = false, newLine: Boolean = true, lineSeparator: String = "\n") = { System.setProperty("line.separator", lineSeparator) FileCtl.writeToFile(writePath, append){ pw =>df.collect.map(_.mkString).foreach{ x =>if (header)       pw.println(" " * x.mkString.length) if (newLine)       pw.println(x.mkString) else       pw.print(x.mkString) if (footer)       pw.println(" " * x.mkString.length) } } }'
Detail: 'Can't OpenScope for symbol named: 'writeFixedFile(scala.String,scala.Boolean,scala.Boolean,scala.Boolean,scala.Boolean,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'makeInputDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '72'
Statement: 'def makeInputDf(len: Seq[Int], names: Seq[String], domains: Seq[String], filePath: String) = { val (totalLen_, sliceLen) = makeSliceLen(len)  val totalLen = addLineBreak(totalLen_, lineBreak)  val ziped = names.zip(domains)  val (nameList, domainList) = ziped.filter{    case (names, domain) => !(domain.startsWith(Converter.NOT_USE_PREFIX))    }.unzip  val rdd = Option(preFilter).map{ pf =>sc.binaryRecords(filePath, totalLen).flatMap{ arr =>if (pf._2(arrToMap(pf._1, names, domains, sliceLen)(arr)))       Some(arr) else       None } }.getOrElse(sc.binaryRecords(filePath, totalLen))  val df = if (withIndex)       {       if (charEnc == "JEF")             {             throw new IllegalArgumentException ("JEF CharEnc is not supportted")             }  val rddWithIdx = rdd.zipWithIndex  val (namesWithIdx, nameListWithIdx, domainsWithIdx, sliceLenWithIdx) = addIndexColumn(names, nameList, domains, sliceLen) context.createDataFrame(rddWithIdx.map{          case (arr, long) => addArr(cnvFromFixedWithIndex(namesWithIdx, domainsWithIdx, sliceLenWithIdx)(arr, long), arr)          }, addStructType(Converter.makeSchema(nameListWithIdx)))       } else       {       context.createDataFrame(rdd.map( arr =>addArr(cnvFromFixed(names, domains, sliceLen)(arr), arr)), addStructType(Converter.makeSchema(nameList)))       } Converter.removeHeaderAndFooter(df, hasHeader, hasFooter, names, domains) }'
Detail: 'Can't OpenScope for symbol named: 'makeInputDf(scala.Seq[scala.Int],scala.Seq[scala.String],scala.Seq[scala.String],scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'recordErrorCheckAndMakeInputDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/FixedConverter.scala'
Line number: '100'
Statement: 'def recordErrorCheckAndMakeInputDf(len: Seq[Int], names: Seq[String], domains: Seq[String], filePath: String) = { if (charEnc == "JEF")       {       throw new IllegalArgumentException ("JEF CharEnc is not supportted")       } if (!lineBreak)       {       throw new IllegalArgumentException ("must be lineBreak = true when recordLengthCheck = true")       }  val (totalLen_, sliceLen) = makeSliceLen(len)  val totalLen = addLineBreak(totalLen_, lineBreak)  val ziped = names.zip(domains)  val (nameList, domainList) = ziped.filter{    case (names, domain) => !(domain.startsWith(Converter.NOT_USE_PREFIX))    }.unzip  val splitByLineBreak = {    sc.binaryFiles(filePath).flatMap{       case (_, pds) => {       var buff = new Array[Byte] (1)  var resultArr = ArrayBuffer[Array[Byte]]()  var tmpArr = ArrayBuffer[Byte]()  val br = new BufferedInputStream (pds.open()) while (br.read(buff) != -1)             {             val byte = buff(0) if (byte == '\n' || byte == '\r')                   {                   resultArr += tmpArr.toArray tmpArr = ArrayBuffer[Byte]()                   } else                   {                   tmpArr += byte                   }             } Option(preFilter).map{ pf =>resultArr.flatMap{ arr =>if (pf._2(arrToMap(pf._1, names, domains, sliceLen)(arr)))             Some(arr) else             None } }.getOrElse(resultArr)       }       }    }  val df = if (withIndex)       {       val (namesWithIdx, nameListWithIdx, domainsWithIdx, sliceLenWithIdx) = addIndexColumn(names, nameList, domains, sliceLen)  val r = splitByLineBreak.zipWithIndex.map{          case (arr, idx) => val row = cnvFromFixedWithIndex(namesWithIdx, domainsWithIdx, sliceLenWithIdx)(arr, idx) addArr(Row(row.toSeq :+ (arr.size != totalLen_).toString :_*), arr)          } context.createDataFrame(r, addStructType(Converter.makeSchemaWithRecordError(nameListWithIdx)))       } else       {       val r = splitByLineBreak.map{ arr => val row = cnvFromFixed(names, domains, sliceLen)(arr) addArr(Row(row.toSeq :+ (arr.size != totalLen_).toString :_*), arr) } context.createDataFrame(r, addStructType(Converter.makeSchemaWithRecordError(nameList)))       } Converter.removeHeaderAndFooter(df, hasHeader, hasFooter, names, domains) }'
Detail: 'Can't OpenScope for symbol named: 'recordErrorCheckAndMakeInputDf(scala.Seq[scala.Int],scala.Seq[scala.String],scala.Seq[scala.String],scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'domainConvert' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/Converter.scala'
Line number: '52'
Statement: 'def domainConvert(dataAndDomainsAndNames: Seq[((Array[Byte], String), String)], charEnc: String) = { val (rowData, errMessage) = dataAndDomainsAndNames.foldLeft((Seq.empty[String], Seq.empty[ErrMessage])){(l, r) => val (convCols, errMessages) = l  val ((data, domain), name) = r if (domain == NOT_USE_PREFIX)       {       l       } else       {       DomainProcessor.execArrayByte(domain, data, charEnc) match {             case Right(d) => (convCols :+ d, errMessages)             case Left(m) => (convCols :+ null, errMessages :+ ErrMessage( name, domain, new String (data, if (charEnc == "JEF")                "ISO-8859-1" else                charEnc), m))          }       } } makeErrorMessage(errMessage, rowData) }'
Detail: 'Can't OpenScope for symbol named: 'domainConvert(Seq[Tuple3[Tuple2[Array[Byte],String],String]],scala.String)''
[03/24/2023 05:42:22] Info: Transformed file MakeDf.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'makeErrorMessage' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/Converter.scala'
Line number: '69'
Statement: 'def makeErrorMessage(errMessage: Seq[ErrMessage], rowData: Seq[String]) = if (errMessage.isEmpty)    {    rowData :+ "false" :+ ""    } else    {    rowData :+ "true" :+ errMessage.mkString("|")    }'
Detail: 'Can't OpenScope for symbol named: 'makeErrorMessage(scala.Seq[d2k.common.fileConv.ErrMessage],scala.Seq[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'removeHeaderAndFooter[A]' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/Converter.scala'
Line number: '76'
Statement: 'def removeHeaderAndFooter [A](data: Seq[A], hasHeader: Boolean, hasFooter: Boolean) = ((hasHeader, hasFooter) match {    case (true, true) => data.drop(1).dropRight(1)    case (true, false) => data.drop(1)    case (false, true) => data.dropRight(1)    case _ => data })'
Detail: 'Can't OpenScope for symbol named: 'removeHeaderAndFooter[A](Seq[A],scala.Boolean,scala.Boolean)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'removeHeaderAndFooter' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/Converter.scala'
Line number: '83'
Statement: 'def removeHeaderAndFooter(df: DataFrame, hasHeader: Boolean, hasFooter: Boolean, colNames: Seq[String], domainNames: Seq[String]) = { if (hasHeader || hasFooter)       {       val dataDivIdx = domainNames.indexWhere{ elem =>elem.startsWith(REC_DIV_PREFIX)}  val dataDivColName = colNames(dataDivIdx) df.filter(col(dataDivColName) === REC_DIV_EXTRACT).drop(dataDivColName)       } else       {       df       } }'
Detail: 'Can't OpenScope for symbol named: 'removeHeaderAndFooter(DataFrame,scala.Boolean,scala.Boolean,scala.Seq[scala.String],scala.Seq[scala.String])''
[03/24/2023 05:42:22] Info: Transformed file Converter.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeDb' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/WriteDb.scala'
Line number: '79'
Statement: 'def writeDb(dforg: DataFrame)(implicit inArgs: InputArgs) = { val df = convNa(dforg)  val tblName = inArgs.tableNameMapper.get(componentId).getOrElse(writeTableName)  val dbCtl = new DbCtl (writeDbInfo)  def modUpdateColumn(df: DataFrame) = df.withColumn("dt_d2kupddttm", lit(inArgs.sysSQLDate)).withColumn("id_d2kupdusr", lit(componentId))  def checkKeys = if (writeDbUpdateKeys.isEmpty)       throw new IllegalArgumentException ("writeDbUpdateKeys is empty") (writeDbMode, writeDbWithCommonColumn) match {          case (Insert, true) => {          dbCtl.insertAccelerated(DbCommonColumnAppender(df, componentId), tblName, writeDbSaveMode, writeDbHint)          }          case (Insert, false) => {          dbCtl.insertAccelerated(df, tblName, writeDbSaveMode, writeDbHint)          }          case (InsertAcc, true) => {          dbCtl.insertAccelerated(DbCommonColumnAppender(df, componentId), tblName, writeDbSaveMode, writeDbHint)          }          case (InsertAcc, false) => {          dbCtl.insertAccelerated(df, tblName, writeDbSaveMode, writeDbHint)          }          case (InsertNotExists(keys@_*), true) => {          dbCtl.insertNotExists(DbCommonColumnAppender(df, componentId), tblName, keys, writeDbSaveMode, writeDbHint)          }          case (InsertNotExists(keys@_*), false) => {          dbCtl.insertNotExists(df, tblName, keys, writeDbSaveMode, writeDbHint)          }          case (Update, true) => {          checkKeys dbCtl.updateRecords(modUpdateColumn(df), tblName, writeDbUpdateKeys, writeDbUpdateIgnoreColumns, writeDbHint)          }          case (Update, false) => {          checkKeys dbCtl.updateRecords(df, tblName, writeDbUpdateKeys, writeDbUpdateIgnoreColumns, writeDbHint)          }          case (Upsert, true) => {          checkKeys dbCtl.upsertRecords(DbCommonColumnAppender(df, componentId), tblName,  writeDbUpdateKeys, Set("dt_d2kmkdttm", "id_d2kmkusr", "nm_d2kupdtms", "fg_d2kdelflg") ++ writeDbUpdateIgnoreColumns, writeDbHint)          }          case (Upsert, false) => {          checkKeys dbCtl.upsertRecords(df, tblName, writeDbUpdateKeys, writeDbUpdateIgnoreColumns, writeDbHint)          }          case (DeleteLogical, true) => {          checkKeys  val deleteFlagName = "fg_d2kdelflg"  val deleteTarget = df.withColumn(deleteFlagName, lit("1")).select(deleteFlagName, writeDbUpdateKeys.toSeq :_*) dbCtl.updateRecords( modUpdateColumn(deleteTarget),  tblName, writeDbUpdateKeys, Set("dt_d2kmkdttm", "id_d2kmkusr", "nm_d2kupdtms"), writeDbHint)          }          case (DeleteLogical, false) => throw new IllegalArgumentException ("DeleteLogical and writeDbWithCommonColumn == false can not used be togather")          case (DeletePhysical, _) => {          checkKeys dbCtl.deleteRecords(df, tblName, writeDbUpdateKeys, writeDbHint)          }       } dforg.sqlContext.emptyDataFrame }'
Detail: 'Can't OpenScope for symbol named: 'writeDb(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Info: Transformed file FixedConverter.scala
[03/24/2023 05:42:22] Info: Transformed file ComponentFlowParserTest.scala
[03/24/2023 05:42:22] Info: Transformed file ResourceInfo.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/MultiDbToMapDf.scala'
Line number: '10'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Info: Transformed file MultiDbToMapDf.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'comm試算' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/component/sh/CommissionBaseChannelSelector.scala'
Line number: '11'
Statement: 'def comm試算(uniqueKeys: String*) = new trComm試算 {    override val groupingKeys = uniqueKeys }'
Detail: 'Can't OpenScope for symbol named: 'comm試算(_Seq*[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'comm実績_月次手数料' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/component/sh/CommissionBaseChannelSelector.scala'
Line number: '17'
Statement: 'def comm実績_月次手数料(uniqueKeys: String*) = new trComm実績_月次手数料 {    override val groupingKeys = uniqueKeys }'
Detail: 'Can't OpenScope for symbol named: 'comm実績_月次手数料(_Seq*[scala.String])''
[03/24/2023 05:42:22] Info: Transformed file WriteDb.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'comm実績_割賦充当' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/component/sh/CommissionBaseChannelSelector.scala'
Line number: '23'
Statement: 'def comm実績_割賦充当(uniqueKeys: String*) = new trComm実績_割賦充当 {    override val groupingKeys = uniqueKeys }'
Detail: 'Can't OpenScope for symbol named: 'comm実績_割賦充当(_Seq*[scala.String])''
[03/24/2023 05:42:22] Info: Transformed file ResourceRelationFinder.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'comm実績_直営店' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/component/sh/CommissionBaseChannelSelector.scala'
Line number: '29'
Statement: 'def comm実績_直営店(uniqueKeys: String*) = new trComm実績_直営店 {    override val groupingKeys = uniqueKeys }'
Detail: 'Can't OpenScope for symbol named: 'comm実績_直営店(_Seq*[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeFilePath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/WriteFile.scala'
Line number: '46'
Statement: 'def writeFilePath(implicit inArgs: InputArgs) : String = inArgs.baseOutputFilePath'
Detail: 'Can't OpenScope for symbol named: 'writeFilePath(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'comm毎月割一時金' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/component/sh/CommissionBaseChannelSelector.scala'
Line number: '35'
Statement: 'def comm毎月割一時金(uniqueKeys: String*) = new trComm毎月割一時金 {    override val groupingKeys = uniqueKeys }'
Detail: 'Can't OpenScope for symbol named: 'comm毎月割一時金(_Seq*[scala.String])''
[03/24/2023 05:42:22] Info: Transformed file CommissionBaseChannelSelector.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeFile' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/WriteFile.scala'
Line number: '79'
Statement: 'def writeFile(df: DataFrame)(implicit inArgs: InputArgs) = { val writeFilePathAndName = s"${ writeFilePath }/${ writeFileName }"  val vari = new VariableFile (writeFilePathAndName, writeFileVariableWrapDoubleQuote, writeFileVariableEscapeChar, writeCharEncoding, writeFilePartitionColumns, writeFilePartitionExtention)  val fixed = new FixedFile (writeFilePathAndName, writeFilePartitionColumns, writeFilePartitionExtention)  val writer = writeFileMode match {       case Csv => vari.writeSingle(",")       case Csv(wrapTargetCols@_*) => vari.writeSingleCsvWithDoubleQuote(wrapTargetCols.toSet)       case Tsv => vari.writeSingle("\t")       case Fixed(itemLengths@_*) => fixed.writeSingle_MS932(itemLengths)       case Fixed => new FixedFileWithConfFile (writeFilePathAndName).writeFile(writeFileFunc)       case partition.Csv => vari.writePartition(",")       case partition.Csv(wrapTargetCols@_*) => vari.writePartitionCsvWithDoubleQuote(wrapTargetCols.toSet)       case partition.Tsv => vari.writePartition("\t")       case partition.Fixed(itemLengths@_*) => fixed.writePartition_MS932(itemLengths)       case hdfs.Csv => vari.writeHdfs(",")       case hdfs.Tsv => vari.writeHdfs("\t")       case hdfs.Fixed(itemLengths@_*) => fixed.writeHdfs_MS932(itemLengths)       case sequence.Csv => vari.writeSequence(",")       case sequence.Csv(wrapTargetCols@_*) => vari.writeSequenceCsvWithDoubleQuote(wrapTargetCols.toSet)       case sequence.Tsv => vari.writeSequence("\t")       case sequence.Fixed(itemLengths@_*) => fixed.writeSequence_MS932(itemLengths)       case _ => throw new IllegalArgumentException (s"${ writeFileMode } is unusable")    } writer(df) df.sqlContext.emptyDataFrame }'
Detail: 'Can't OpenScope for symbol named: 'writeFile(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Info: Transformed file DictionaryGenerator.scala
[03/24/2023 05:42:22] Info: Transformed file WriteFile.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/org/apache/spark/sql/JdbcCtl.scala'
Line number: '21'
Statement: 'def readTable(dbCtl: DbCtl, tableName: String, requiredColumns: Array[String], where: Array[String] = Array("1 = 1"), whereFilter: Array[Filter] = Array.empty[Filter]) = { val partitions: Array[Partition] = where.zipWithIndex.map{    case (w, idx) => JDBCPartition(w, idx)    }  val structTypes = JDBCRDD.resolveTable(dbCtl.dbInfo.toOptions(tableName))  val reqStructTypes = pruneSchema(structTypes, requiredColumns)  val rdd = JDBCRDD.scanTable( SparkContexts.sc, structTypes, requiredColumns, whereFilter, partitions, dbCtl.dbInfo.toOptions(tableName)).map{ r => val values = internalRowToRow(r, reqStructTypes).zipWithIndex.map{    case (strType, idx) => strType match {       case _if r.isNullAt(idx) == true => null       case StringType => r.getString(idx)       case TimestampType => new Timestamp (r.getLong(idx) / 1000)       case x:DecimalType => r.getDecimal(idx, x.precision, x.scale).toBigDecimal    }    } Row(values :_*) } SparkContexts.context.createDataFrame(rdd, reqStructTypes) }'
Detail: 'Can't OpenScope for symbol named: 'readTable(DbCtl,scala.String,scala.Array[scala.String],scala.Array[scala.String],Array[Filter])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'pruneSchema' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/org/apache/spark/sql/JdbcCtl.scala'
Line number: '45'
Statement: 'def pruneSchema(schema: StructType, columns: Array[String]) = { val fieldMap = Map(schema.fields.map( x =>x.name -> x) :_*) new StructType (columns.map( c =>fieldMap(c))) }'
Detail: 'Can't OpenScope for symbol named: 'pruneSchema(org.apache.spark.sql.types.StructType,scala.Array[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'internalRowToRow' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/org/apache/spark/sql/JdbcCtl.scala'
Line number: '50'
Statement: 'def internalRowToRow(iRow: InternalRow, schema: StructType) = { schema.map(_.dataType) }'
Detail: 'Can't OpenScope for symbol named: 'internalRowToRow(InternalRow,org.apache.spark.sql.types.StructType)''
[03/24/2023 05:42:22] Info: Transformed file JdbcCtl.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'takek' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '32'
Statement: 'def takek(inStr: Column, len: Int) = udf_takek(inStr, lit(len))'
Detail: 'Can't OpenScope for symbol named: 'takek(Column,scala.Int)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/template/DfToFileTest.scala'
Line number: '40'
Statement: 'def exec(implicit inArgs: InputArgs) = targetComponent.run(df)(inArgs)'
Detail: 'Can't OpenScope for symbol named: 'exec(d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'rpad' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/template/DfToFileTest.scala'
Line number: '44'
Statement: 'def rpad(target: String, len: Int, pad: String = " ") = { val str = if (target == null)       {       ""       } else       {       target       }  val strSize = str.getBytes("MS932").size  val padSize = len - strSize s"${ str }${ pad * padSize }" }'
Detail: 'Can't OpenScope for symbol named: 'rpad(scala.String,scala.Int,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'rpadk' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '51'
Statement: 'def rpadk(str: Column, len: Int, pad: String) = udf_rpadk(str, lit(len), lit(pad))'
Detail: 'Can't OpenScope for symbol named: 'rpadk(Column,scala.Int,scala.String)''
[03/24/2023 05:42:22] Info: Transformed file ResourceRelationFinder.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/MultiPqToMultiAny.scala'
Line number: '10'
Statement: 'def preExec(in: Unit)(implicit inArgs: InputArgs) : Map[String, DataFrame] = readParquet'
Detail: 'Can't OpenScope for symbol named: 'preExec(scala.Unit,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Info: Transformed file MultiPqToMultiAny.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'makeBdField' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '39'
Statement: 'def makeBdField(len: String, name: String) = { val (p, s) = if (len.trim.isEmpty)       {       (10, 0)       } else       {       val ps = len.trim.split(',') if (ps.size == 2)             {             (ps(0).toInt, ps(1).toInt)             } else             {             (len.toInt, 0)             }       } StructField(name, DecimalType(p, s), true) }'
Detail: 'Can't OpenScope for symbol named: 'makeBdField(scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'itemConfToTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '53'
Statement: 'def itemConfToTable(confPath: String) = { val confs = parseItemConf(confPath)  val tableInfos = confs.map{ c => val data = DomainProcessor.exec(c.cnvType, "x" * c.length.toInt).right.get.mkString("\"", "", "\"")  val ml = Seq(toLength(c.itemName), toLength(c.itemId), toLength(c.length), toLength(c.cnvType), toLength(data)).max  val toMl = toMaxLength(ml)_ TableInfo(toMl(c.itemName), toMl(c.itemId), toMl(c.length), toMl(c.cnvType), toMl(data), ml) } tableInfoToTableStr(tableInfos) }'
Detail: 'Can't OpenScope for symbol named: 'itemConfToTable(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'itemConfMdToTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '64'
Statement: 'def itemConfMdToTable(url: String) = { val md = Source.fromURL(s"${ url }?private_token=${ sys.env("GITLAB_TOKEN") }").getLines.toList.drop(6)  val tableInfos = md.map{ line => val items = line.split('|').drop(1).map(_.trim)  val (id, name, domain, length) = (items(0), items(1), items(2), items(3))  val data = DomainProcessor.exec(domain, "x" * length.toInt).right.get.mkString("\"", "", "\"")  val ml = Seq(toLength(name), toLength(id), toLength(length), toLength(domain), toLength(data)).max  val toMl = toMaxLength(ml)_ TableInfo(toMl(name), toMl(id), toMl(length), toMl(domain), toMl(data), ml) } tableInfoToTableStr(tableInfos) }'
Detail: 'Can't OpenScope for symbol named: 'itemConfMdToTable(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'itemsMdToTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '77'
Statement: 'def itemsMdToTable(url: String) = { def tableInfos(md: BufferedSource) = md.getLines.toList.drop(7).filter(!_.isEmpty).map{ line => val items = line.split('|').drop(1).map(_.trim)  val (id, name, domain, length) = (items(0), items(1), items(2), items(3))  val data = (domain.toLowerCase match {       case "string" | "varchar2" | "char" => "x" * length.toInt       case "timestamp" | "日付時刻" => "00010101000000"       case "date" => "00010101"       case "bigdecimal" | "number" | "decimal" => "0"       case _ => DomainProcessor.exec(domain, "x" * length.toInt).right.get    }).mkString("\"", "", "\"")  val ml = Seq(toLength(name), toLength(id), toLength(length), toLength(domain), toLength(data)).max  val toMl = toMaxLength(ml)_ TableInfo(toMl(name), toMl(id), toMl(length), toMl(domain), toMl(data), ml) } Try{Source.fromURL(s"${ url }?private_token=${ sys.env("GITLAB_TOKEN") }")}.map( md =>tableInfoToTableStr(tableInfos(md))).toOption.orElse{    System.err.println(s"Specific is not found[$url]") None    } }'
Detail: 'Can't OpenScope for symbol named: 'itemsMdToTable(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclSimpleExpr'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclSimpleExpr'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclSimpleExpr'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclPath1Id'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'makeSchema' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '99'
Statement: 'def makeSchema(colNames: Seq[String], domains: Seq[String], colLengths: Seq[String]) = StructType(colNames.zip(colLengths).zip(domains.map(_.toLowerCase)).map{ case ((name, _), "string") => StructField(name, StringType, true) case ((name, _), "varchar2") => StructField(name, StringType, true) case ((name, _), "char") => StructField(name, StringType, true) case ((name, _), "date") => StructField(name, DateType, true) case ((name, _), "timestamp") => StructField(name, TimestampType, true) case ((name, _), "日付時刻") => StructField(name, TimestampType, true) case ((name, _), "int") => StructField(name, IntegerType, true) case ((name, _), "integer") => StructField(name, IntegerType, true) case ((name, len), "bigdecimal") => makeBdField(len, name) case ((name, len), "numeric") => makeBdField(len, name) case ((name, len), "decimal") => makeBdField(len, name) case ((name, len), "number") => makeBdField(len, name) case ((name, _), _) => StructField(name, StringType, true) })'
Detail: 'Can't OpenScope for symbol named: 'makeSchema(scala.Seq[scala.String],scala.Seq[scala.String],scala.Seq[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclPath1Id'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'toLength' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '116'
Statement: 'def toLength(str: String) = str.getBytes("MS932").length'
Detail: 'Can't OpenScope for symbol named: 'toLength(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'toMaxLength' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '117'
Statement: 'def toMaxLength(maxLen: Int)(str: String) = { val addLen = maxLen - str.getBytes("MS932").length str + (" " * addLen) }'
Detail: 'Can't OpenScope for symbol named: 'toMaxLength(scala.Int,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'tableColToList' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '121'
Statement: 'def tableColToList(cols: String, defaultValue: String = "") = cols.split('|').map{ col => val value = col.trim if (value.isEmpty)    defaultValue else    value }.toList'
Detail: 'Can't OpenScope for symbol named: 'tableColToList(scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailExprDotId'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'removeDq' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '126'
Statement: 'def removeDq(target: String) = "^\"(.*)\"$".r.findFirstMatchIn(target).map(_.group(1).replace("\"\"", "\"")).getOrElse(target)'
Detail: 'Can't OpenScope for symbol named: 'removeDq(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailExprDotId'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'toTableCol' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '128'
Statement: 'def toTableCol(strSeq: Seq[String]) = strSeq.mkString("| ", " | ", " |")'
Detail: 'Can't OpenScope for symbol named: 'toTableCol(scala.Seq[scala.String])''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:22] Info: Transformed file DfToFileTest.scala
[03/24/2023 05:42:22] Info: Transformed file DictionaryGeneratorTest.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/PqJoinToPq.scala'
Line number: '10'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = PqJoinToPq.this.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'parseItemConf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '139'
Statement: 'def parseItemConf(confPath: String) = { ConfParser.readConf(confPath){ items =>if (items.size == 5)       {       ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true")       } else       {       ItemConf(items(0), items(1), items(2), items(3), items(4).toLowerCase == "true", items(5))       } }.toSeq }'
Detail: 'Can't OpenScope for symbol named: 'parseItemConf(scala.String)''
[03/24/2023 05:42:22] Info: Transformed file PqJoinToPq.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'readMdTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '154'
Statement: 'def readMdTable(path: String) : MdInfo = { println(s"read:${ readMdPath }/${ path }") MdInfo(Source.fromFile(s"${ readMdPath }/${ path }").getLines.mkString("\n")) }'
Detail: 'Can't OpenScope for symbol named: 'readMdTable(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'toCsv' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '160'
Statement: 'def toCsv(writeName: String, wrapDoubleQuote: Boolean = false, hasHeader: Boolean = false, lineSeparator: String = "\n") = toVariable(",")(writeName, wrapDoubleQuote, hasHeader, lineSeparator)'
Detail: 'Can't OpenScope for symbol named: 'toCsv(scala.String,scala.Boolean,scala.Boolean,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'toTsv' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '164'
Statement: 'def toTsv(writeName: String, wrapDoubleQuote: Boolean = false, hasHeader: Boolean = false, lineSeparator: String = "\n") = toVariable("\t")(writeName, wrapDoubleQuote, hasHeader, lineSeparator)'
Detail: 'Can't OpenScope for symbol named: 'toTsv(scala.String,scala.Boolean,scala.Boolean,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'toVsv' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '168'
Statement: 'def toVsv(writeName: String, wrapDoubleQuote: Boolean = false, hasHeader: Boolean = false, lineSeparator: String = "\n") = toVariable("|")(writeName, wrapDoubleQuote, hasHeader, lineSeparator)'
Detail: 'Can't OpenScope for symbol named: 'toVsv(scala.String,scala.Boolean,scala.Boolean,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'toSsv' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '172'
Statement: 'def toSsv(writeName: String, wrapDoubleQuote: Boolean = false, hasHeader: Boolean = false, lineSeparator: String = "\n") = toVariable(" ")(writeName, wrapDoubleQuote, hasHeader, lineSeparator)'
Detail: 'Can't OpenScope for symbol named: 'toSsv(scala.String,scala.Boolean,scala.Boolean,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'dateCalc' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '96'
Statement: 'def dateCalc(inDt: DateTime, month: Int, day: Int) = { val calcDay = (dt: DateTime) =>if (day != 0)       dt.plusDays(day) else       dt  val calcMonth = (dt: DateTime) =>if (month != 0)       dt.plusMonths(month) else       dt Option(inDt).map((calcMonth andThen calcDay)(_)).getOrElse(inDt) }'
Detail: 'Can't OpenScope for symbol named: 'dateCalc(DateTime,scala.Int,scala.Int)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'toVariable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '176'
Statement: 'def toVariable(separator: String)(writeName: String, wrapDoubleQuote: Boolean, hasHeader: Boolean, lineSeparator: String) = { val allData = data.stripMargin.split("\n").drop(1)  val itemNames = tableColToList(allData(0))  val writeData = allData.drop(5).map(tableColToList(_).map( x   =>if (wrapDoubleQuote)       x else       removeDq(x)).mkString(separator)) System.setProperty("line.separator", lineSeparator) FileCtl.writeToFile(s"$outputPath/$writeName", false){ pw =>if (hasHeader)       pw.println(itemNames.mkString(separator)) writeData.foreach(pw.println) } }'
Detail: 'Can't OpenScope for symbol named: 'toVariable(scala.String,scala.String,scala.Boolean,scala.Boolean,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclBinaryIdentExpr'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeSingle_MS932' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFile.scala'
Line number: '33'
Statement: 'def writeSingle_MS932(itemLengths: Seq[Int]) = (df: DataFrame) =>{ if (writeFilePartitionColumns.isEmpty)       {       Directory(fileName).createDirectory(true, false) Files.deleteIfExists(FileSystems.getDefault.getPath(fileName)) FileCtl.writeToFile(fileName){ pw => val collected = df.collect elapse(s"fileWrite:${ fileName }"){ collected.foreach{ row =>pw.println(mkOutputStr(itemLengths)(row))} } }       } else       {       FileCtl.deleteDirectory(fileName) elapse(s"fileWrite:${ fileName }"){ FileCtl.loanPrintWriterCache{ cache =>df.collect.foldLeft(cache){(l, r) =>FileCtl.writeToFileWithPartitionColumns( fileName, partitionColumns = writeFilePartitionColumns, partitionExtention = writeFilePartitionExtention)( mkOutputStr(itemLengths))(l)(r) } } }       } }'
Detail: 'Can't OpenScope for symbol named: 'writeSingle_MS932(scala.Seq[scala.Int])''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclBinaryIdentExpr'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'toFixed' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '189'
Statement: 'def toFixed(writeName: String, hasHeader: Boolean = false, hasFooter: Boolean = false, lineBreak: Boolean = true, lineSeparator: String = "\n", charEnc: String = "MS932") = writeBinData(writeName, lineSeparator){(data: String, len: String, dataType: String) =>dataType match {    case xif x.endsWith("_PD") => pack(BigDecimal(data).toBigInt, BigDecimal(len).toInt)    case xif x.endsWith("_ZD") => zone(BigDecimal(data).toBigInt, BigDecimal(len).toInt)    case "数字" => s"%0${ len }d".format(data.toInt).getBytes(charEnc)    case "数字_SIGNED" => s"%+0${ len }d".format(data.toInt).getBytes(charEnc)    case "未使用" => (" " * BigDecimal(len).toInt).getBytes(charEnc)    case _ => data.padTo(len.toInt, ' ').getBytes(charEnc) } }'
Detail: 'Can't OpenScope for symbol named: 'toFixed(scala.String,scala.Boolean,scala.Boolean,scala.Boolean,scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclBinaryIdentExpr'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclBinaryIdentExpr'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclSimpleExpr'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'toJef' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '202'
Statement: 'def toJef(writeName: String, lineSeparator: String = "\n") = writeBinData(writeName, lineSeparator){(data: String, len: String, dataType: String) =>dataType match {    case xif x.endsWith("_PD") => pack(BigDecimal(data).toBigInt, BigDecimal(len).toInt)    case xif x.endsWith("_ZD") => zone(BigDecimal(data).toBigInt, BigDecimal(len).toInt)    case "全角文字列" => JefConvert(data).toJefFull    case "未使用" => JefConvert(" " * BigDecimal(len).toInt).toJefHalf    case _ => JefConvert(data).toJefHalf } }'
Detail: 'Can't OpenScope for symbol named: 'toJef(scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclSimpleExpr'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeHdfs_MS932' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFile.scala'
Line number: '57'
Statement: 'def writeHdfs_MS932(itemLengths: Seq[Int]) = (df: DataFrame) =>{ val partCheckeddDf = if (writeFilePartitionColumns.isEmpty)       {       val paddedDf = df.na.fill("") ~> paddingSpace(itemLengths)  val sch = StructType(Seq(StructField("str", StringType, true)))  val rows = paddedDf.rdd.map( r =>Row(r.toSeq.mkString(""))) SparkContexts.context.createDataFrame(rows, sch).write       } else       {       val targetSchemas = writeFilePartitionColumns.map{ n => val sc = df.schema(n) sc.copy(dataType = StringType) }  val sch = StructType(targetSchemas ++ Seq(StructField("value", StringType, true)))  val fieldNames = LinkedHashSet(df.schema.map(_.name) :_*)  val rows = df.rdd.map{ row => val keyValues = writeFilePartitionColumns.map( n =>row.get(row.fieldIndex(n)).toString)  val fixedValues = (fieldNames -- writeFilePartitionColumns).zip(itemLengths).foldLeft(ListBuffer.empty[String]){(l, r) =>l.append(paddingMS932(row.get(row.fieldIndex(r._1)), r._2)) l }.mkString("") Row((keyValues :+ fixedValues) :_*) } SparkContexts.context.createDataFrame(rows, sch).write.partitionBy(writeFilePartitionColumns :_*)       } partCheckeddDf.mode(SaveMode.Overwrite).text(fileName) }'
Detail: 'Can't OpenScope for symbol named: 'writeHdfs_MS932(scala.Seq[scala.Int])''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclSimpleExpr'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclPath1Id'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeBinData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '213'
Statement: 'def writeBinData(writeName: String, lineSeparator: String)(func: (String, String, String) => Array[Byte]) = { val allData = data.stripMargin.split("\n")  val itemNames = tableColToList(allData(1))  val itemTypes = tableColToList(allData(5))  val itemLengths = tableColToList(allData(4))  val outData = allData.drop(6).map{ tableColToList(_).zip(itemLengths).zip(itemTypes).map{    case ((data, len), types) => func(removeDq(data), len.split(',').head.trim, types)    }.foldLeft(Array[Byte]())(_ ++: _) } writeBytes(s"$outputPath/$writeName")(outData) }'
Detail: 'Can't OpenScope for symbol named: 'writeBinData(scala.String,scala.String,lambda[String,String,String,Array[Byte]])''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclPath1Id'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclSimpleExpr'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'zone' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '226'
Statement: 'def zone(target: BigInt, zonedByteLen: Int) = { val sign = if (target < 0)       {       'd'       } else       {       'f'       }  val targetStr = String.valueOf(target.abs)  val pad = "0" * (zonedByteLen - targetStr.length())  val targetBytes = (pad + targetStr).init.map{ char =>Integer.parseInt(s"f${ char }", 16).toByte} (targetBytes :+ Integer.parseInt(s"f${ sign }${ targetStr.last }", 16).toByte).toArray }'
Detail: 'Can't OpenScope for symbol named: 'zone(BigInt,scala.Int)''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclSimpleExpr'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclSimpleExpr'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'pack' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '234'
Statement: 'def pack(target: BigInt, packedByteLen: Int) = { val sign = if (target < 0)       {       'd'       } else       {       'f'       }  val targetStr = (String.valueOf(target.abs)) + sign  val pad = "0" * ((packedByteLen * 2) - targetStr.length())  val targetBytes = (pad + targetStr).grouped(2).map{ hexChar =>Integer.parseInt(hexChar, 16).toByte} targetBytes.toArray }'
Detail: 'Can't OpenScope for symbol named: 'pack(BigInt,scala.Int)''
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclPath1Id'.
[03/24/2023 05:42:22] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclPath1Id'.
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeBytes' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '242'
Statement: 'def writeBytes(path: String, lineBreak: Boolean = false, newLineCode: String = "\n")(contents: Array[Array[Byte]]) = { val out = new FileOutputStream (path) contents.foreach{ arr =>out.write(arr) if (lineBreak)       out.write(newLineCode.toCharArray.map(_.toByte)) } out.close }'
Detail: 'Can't OpenScope for symbol named: 'writeBytes(scala.String,scala.Boolean,scala.String,scala.Array[scala.Array[scala.Byte]])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'paddingSpace' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFile.scala'
Line number: '100'
Statement: 'def paddingSpace(targetLengths: Seq[Int]) = (df: DataFrame) =>{ val items = df.schema.map(_.name).zip(targetLengths)  val padSpace = items.map{    case (n, l) => (n, paddingMS932Udf(df(n), lit(l)))    } df ~> editColumnsAndSelect(padSpace.e) }'
Detail: 'Can't OpenScope for symbol named: 'paddingSpace(scala.Seq[scala.Int])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writePartition_MS932' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFile.scala'
Line number: '106'
Statement: 'def writePartition_MS932(itemLengths: Seq[Int]) = (df: DataFrame) =>{ import DfCtl.implicits._ if (writeFilePartitionColumns.isEmpty)       {       df.partitionWriteFile( fileName, true, partitionExtention = writeFilePartitionExtention)(mkOutputStr(itemLengths))       } else       {       FileCtl.deleteDirectory(fileName) df.partitionWriteToFileWithPartitionColumns( fileName, writeFilePartitionColumns, true, partitionExtention = writeFilePartitionExtention)(mkOutputStr(itemLengths))       } }'
Detail: 'Can't OpenScope for symbol named: 'writePartition_MS932(scala.Seq[scala.Int])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'writeSequence_MS932' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFile.scala'
Line number: '118'
Statement: 'def writeSequence_MS932(itemLengths: Seq[Int]) = (df: DataFrame) =>{ FileCtl.deleteDirectory(fileName) df.rdd.map( row =>(NullWritable.get, mkOutputBinary(itemLengths)(row))).saveAsSequenceFile(fileName, Some(classOf[org.apache.hadoop.io.compress.SnappyCodec])) }'
Detail: 'Can't OpenScope for symbol named: 'writeSequence_MS932(scala.Seq[scala.Int])''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'mkOutputStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFile.scala'
Line number: '124'
Statement: 'def mkOutputStr(itemLengths: Seq[Int])(row: Row) = itemLengths.zipWithIndex.foldLeft(new StringBuffer)((l, r)   =>l.append(new String (mkArrByte(row, r), charSet))).toString'
Detail: 'Can't OpenScope for symbol named: 'mkOutputStr(scala.Seq[scala.Int],Row)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'mkOutputBinary' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFile.scala'
Line number: '128'
Statement: 'def mkOutputBinary(itemLengths: Seq[Int])(row: Row) = itemLengths.zipWithIndex.foldLeft(Array.empty[Byte])((l, r) =>l ++ mkArrByte(row, r))'
Detail: 'Can't OpenScope for symbol named: 'mkOutputBinary(scala.Seq[scala.Int],Row)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'mkArrByte' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFile.scala'
Line number: '131'
Statement: 'def mkArrByte(row: Row, itemInfo: (Int, Int)) = { val (len, idx) = itemInfo Option(row.get(idx)).map{ x => val target = x.toString.getBytes(charSet) if (len <= target.size)       {       target.take(len)       } else       {       target ++ Array.fill(len - target.size)(pad)       } }.getOrElse(Array.fill(len)(pad)) }'
Detail: 'Can't OpenScope for symbol named: 'mkArrByte(Row,scala.Tuple2[scala.Int,scala.Int])''
[03/24/2023 05:42:22] Info: Transformed file FixedFile.scala
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/dic/GenerateDictionary.scala'
Line number: '13'
Statement: 'def apply(baseUrl: String) = new GenerateDictionary (baseUrl)'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'generate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/dic/GenerateDictionary.scala'
Line number: '20'
Statement: 'def generate(branch: String) = { val dbResult = generateDictionary(baseUrl, branch, "db")  val pqResult = generateDictionary(baseUrl, branch, "pq")  val result = (dbResult ++ pqResult).groupBy( d =>(d.id, d.dic)).map{    case (k, d) => d.head    }.toList  val outputPath = "data/dicGen"  val outputFile = s"${ outputPath }/d2k_appdef.txt" Directory(outputPath).createDirectory(true, false) FileCtl.writeToFile(outputFile, false, "MS932"){ w =>result.sortBy(_.id).foreach(w.println) } println(s"[Finish Dictionary Generate] ${ outputFile }") }'
Detail: 'Can't OpenScope for symbol named: 'generate(scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'generateDictionary' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/dic/GenerateDictionary.scala'
Line number: '34'
Statement: 'def generateDictionary(baseUrl: String, branch: String, category: String) = { val itemsUrl = s"${ baseUrl }/raw/${ branch }/apps/common/items/${ category }"  val url = s"${ itemsUrl }/README.md"  val dicConv = dicConvertPattern(baseUrl, "master")  val md = Source.fromURL(s"${ url }?private_token=${ sys.env("GITLAB_TOKEN") }").getLines.toList  val grpList = md.filter(!_.isEmpty).dropWhile( line =>!line.contains("## 業務グループ一覧")).drop(3).map(_.split('|')(1).trim.split('(')(1).dropRight(1))  val files = fileList(itemsUrl, grpList.head) (for {       g <- grpList  f <- fileList(itemsUrl, g)    } yield {    makeDic(dicConv, baseUrl, branch, category, g.split('/')(0), f)    }).flatten }'
Detail: 'Can't OpenScope for symbol named: 'generateDictionary(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'fileList' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/dic/GenerateDictionary.scala'
Line number: '52'
Statement: 'def fileList(baseUrl: String, grpReadme: String) = { val grpUrl = s"${ baseUrl }/${ grpReadme }"  val md = Source.fromURL(s"${ grpUrl }?private_token=${ sys.env("GITLAB_TOKEN") }").getLines.toList md.filter(!_.isEmpty).dropWhile( line =>!line.contains("## ")).drop(3).map(_.split('|')(1).trim.split('(')(1).dropRight(1)) }'
Detail: 'Can't OpenScope for symbol named: 'fileList(scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'makeDic' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/dic/GenerateDictionary.scala'
Line number: '59'
Statement: 'def makeDic(dicConv: Map[String, String], baseUrl: String, branch: String, category: String, grpName: String, fileName: String) = { val dicKeys = dicConv.keys.toList  val itemdef = ItemDefParser(baseUrl, branch, s"${ category }/${ grpName }/${ fileName }").get itemdef.details.map{ items => val target = dicConv.keys.filter( k =>itemdef.id.toLowerCase.startsWith(k)).head  val conved = itemdef.id.toLowerCase.replaceAllLiterally(target, dicConv(target)) DicData(conved, itemdef.name, s"${ items.id }[${ items.name }]") } }'
Detail: 'Can't OpenScope for symbol named: 'makeDic(scala.Map[scala.String,scala.String],scala.String,scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:22] Error: An error ocurred at 'OpenScope for node with name 'dicConvertPattern' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/dic/GenerateDictionary.scala'
Line number: '69'
Statement: 'def dicConvertPattern(baseUrl: String, branch: String) = { val dicConvUrl = s"${ baseUrl }/raw/${ branch }/guide/dicgen/dicPattern.md"  val md = Source.fromURL(s"${ dicConvUrl }?private_token=${ sys.env("GITLAB_TOKEN") }").getLines.toList md.filter(!_.isEmpty).dropWhile( line =>!line.contains("## 項目変換パターン")).drop(3).map{ x => val splitted = x.split('|') (splitted(1).trim -> splitted(2).trim) }.toMap }'
Detail: 'Can't OpenScope for symbol named: 'dicConvertPattern(scala.String,scala.String)''
[03/24/2023 05:42:22] Info: Transformed file GenerateDictionary.scala
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'toPq' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '280'
Statement: 'def toPq(name: String) : Unit = toPq(new PqCtl (outputPath), name)'
Detail: 'Can't OpenScope for symbol named: 'toPq(scala.String)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'toPq' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '282'
Statement: 'def toPq(pqCtl: PqCtl, name: String) = { import pqCtl.implicits._ toDf.writeParquet(name) }'
Detail: 'Can't OpenScope for symbol named: 'toPq(PqCtl,scala.String)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'toDb' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '287'
Statement: 'def toDb(tableName: String, dbInfo: DbInfo = DbConnectionInfo.bat1) : Unit = toDb(new DbCtl (dbInfo), tableName)'
Detail: 'Can't OpenScope for symbol named: 'toDb(scala.String,DbInfo)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'toDb' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '289'
Statement: 'def toDb(dbCtl: DbCtl, tableName: String) = { import dbCtl.implicits._ Try{dbCtl.dropTable(tableName)} toDf.writeTableStandard(tableName, SaveMode.Append) }'
Detail: 'Can't OpenScope for symbol named: 'toDb(DbCtl,scala.String)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'checkCsv' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '295'
Statement: 'def checkCsv(filePath: String, hasHeader: Boolean = false) = checkVariable(",")(filePath, hasHeader)'
Detail: 'Can't OpenScope for symbol named: 'checkCsv(scala.String,scala.Boolean)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'checkTsv' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '296'
Statement: 'def checkTsv(filePath: String, hasHeader: Boolean = false) = checkVariable("\t")(filePath, hasHeader)'
Detail: 'Can't OpenScope for symbol named: 'checkTsv(scala.String,scala.Boolean)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'checkVariable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '298'
Statement: 'def checkVariable(separator: String)(filePath: String, hasHeader: Boolean = false) = { val fileList = Source.fromFile(filePath).getLines.toList  val target = (if (hasHeader)       fileList.drop(1) else       fileList)  val expect = toDf.collect target.zip(expect.zipWithIndex).map{    case (t, (e, idx)) => val splitted = t.split(separator).map(removeDq)  val fieldNames = e.schema.fieldNames (0 until splitted.length).foreach{ pos =>withClue((s"LineNo:${ idx + 7 }", fieldNames(pos))){ splitted(pos).toString mustBe e(pos).toString } }    } }'
Detail: 'Can't OpenScope for symbol named: 'checkVariable(scala.String,scala.String,scala.Boolean)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'checkFixed' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '314'
Statement: 'def checkFixed(filePath: String, hasHeader: Boolean = false, hasFooter: Boolean = false) = { val fileList = Source.fromFile(filePath).getLines.toList  val headerChecked = if (hasHeader)       fileList.drop(1) else       fileList  val target = (hasHeader, hasFooter) match {       case (true, true) => fileList.drop(1).dropRight(1)       case (true, false) => fileList.drop(1)       case (false, true) => fileList.dropRight(1)       case (false, false) => fileList    }  val dataTable = data.stripMargin.split("\n").drop(1)  val colLengths = tableColToList(dataTable(3)).map(_.toInt)  val expect = toDf.collect target.zip(expect.zipWithIndex).map{    case (t, (e, idx)) => val splitted = colLengths.foldLeft((t.getBytes("MS932"), Seq.empty[String])){(l, r) =>(l._1.drop(r), l._2 :+ new String (l._1.take(r), "MS932")) }._2  val fieldNames = e.schema.fieldNames (0 until splitted.length).foreach{ pos =>withClue((s"LineNo:${ idx + 7 }", fieldNames(pos))){ splitted(pos).toString mustBe e(pos).toString } }    } }'
Detail: 'Can't OpenScope for symbol named: 'checkFixed(scala.String,scala.Boolean,scala.Boolean)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'sortDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '342'
Statement: 'def sortDf(target: DataFrame, expect: DataFrame, sortKeys: Seq[String]) = { target.show  val result = if (!sortKeys.isEmpty)       {       (target.sort(sortKeys.head, sortKeys.tail :_*), expect.sort(sortKeys.head, sortKeys.tail :_*))       } else       {       (target, expect)       }  val expectCollect = result._2.collect result._1.show(expectCollect.size, false) testingRows(result._1.collect, expect.rdd.zipWithIndex.sortBy( x =>sortKeys.map( k =>x._1.getAs[String](k)).mkString).collect) }'
Detail: 'Can't OpenScope for symbol named: 'sortDf(DataFrame,DataFrame,scala.Seq[scala.String])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'checkPq' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '352'
Statement: 'def checkPq(name: String) : Unit = checkPq(new PqCtl (outputPath), name, Seq.empty[String])'
Detail: 'Can't OpenScope for symbol named: 'checkPq(scala.String)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'checkPq' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '354'
Statement: 'def checkPq(name: String, sortKeys: Seq[String]) : Unit = checkPq(new PqCtl (outputPath), name, sortKeys)'
Detail: 'Can't OpenScope for symbol named: 'checkPq(scala.String,scala.Seq[scala.String])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'checkPq' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '356'
Statement: 'def checkPq(pqCtl: PqCtl, name: String, sortKeys: Seq[String] = Seq.empty[String]) = sortDf(pqCtl.readParquet(name), toDf, sortKeys)'
Detail: 'Can't OpenScope for symbol named: 'checkPq(PqCtl,scala.String,scala.Seq[scala.String])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'checkDb' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '358'
Statement: 'def checkDb(tableName: String, sortKeys: Seq[String] = Seq.empty[String], dbInfo: DbInfo = DbConnectionInfo.bat1) : Unit = checkDb(new DbCtl (dbInfo), tableName, sortKeys)'
Detail: 'Can't OpenScope for symbol named: 'checkDb(scala.String,scala.Seq[scala.String],DbInfo)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'checkDb' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '361'
Statement: 'def checkDb(dbCtl: DbCtl, tableName: String, sortKeys: Seq[String]) = sortDf(dbCtl.readTable(tableName), toDf, sortKeys)'
Detail: 'Can't OpenScope for symbol named: 'checkDb(DbCtl,scala.String,scala.Seq[scala.String])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'checkDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '363'
Statement: 'def checkDf(df: DataFrame, lineOffset: Int = 0) = testingRows(df.collect, toDf.rdd.zipWithIndex.collect, lineOffset)'
Detail: 'Can't OpenScope for symbol named: 'checkDf(DataFrame,scala.Int)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'testingRows' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeResource.scala'
Line number: '365'
Statement: 'def testingRows(target: Array[Row], expect: Array[(Row, Long)], lineOffset: Int = 0) = { val fieldNames = expect.headOption.map(_._1.schema.fieldNames).getOrElse(Array.empty[String])  val expectTypes = expect.headOption.map(_._1.schema.fields.map(_.dataType.toString)).getOrElse(Array.empty[String])  val targetTypes = target.headOption.map(_.schema.fields.map( x =>(x.name, x.dataType.toString)).toMap).getOrElse(Map.empty[String, String])  val systemItems = Seq("DT_D2KMKDTTM", "ID_D2KMKUSR", "DT_D2KUPDDTTM", "ID_D2KUPDUSR", "NM_D2KUPDTMS", "FG_D2KDELFLG") if (target.isEmpty)       logger.warn("Target Data is Empty") target.zip(expect).map{    case (t, (e, idx)) => (0 until e.length).foreach{ pos => val name = fieldNames(pos) if (!systemItems.contains(name))       {       withClue((s"LineNo:${ idx + 7 + lineOffset }", name)){ expectTypes(pos) match {             case "DateType" => t.getAs[Any](name).toString.replaceAll("-", "").take(8) mustBe e.getAs[Any](name).toString.replaceAll("-", "").take(8)             case "TimestampType" => t.getAs[Timestamp](name).toString.replaceAll("[-:\\s\\.]", "").take(14) mustBe e.getAs[Timestamp](name).toString.replaceAll("[-:\\s\\.]", "").take(14)             case "IntegerType" => val targetData = if (targetTypes(name).startsWith("Integer"))                {                t.getAs[Integer](name)                } else                {                t.getAs[java.math.BigDecimal](name)                } Option(targetData).map(_.toString).getOrElse("null") mustBe Option(e.getAs[Integer](name)).map(_.toString).getOrElse("null")             case typif typ.startsWith("DecimalType") => {             val targetData = if (targetTypes(name).startsWith("Integer"))                   {                   t.getAs[Integer](name)                   } else                   {                   t.getAs[java.math.BigDecimal](name)                   } Option(targetData).map(_.toString).getOrElse("null") mustBe Option(e.getAs[java.math.BigDecimal](name)).map(_.toString).getOrElse("null")             }             case _ => {             val targetVal = t.getAs[String](name) (if (targetVal == null)                   "" else                   targetVal) mustBe e.getAs[String](name)             }          } }       } }    } }'
Detail: 'Can't OpenScope for symbol named: 'testingRows(Array[Row],Array[Tuple2[Row,Long]],scala.Int)''
[03/24/2023 05:42:23] Info: Transformed file DfUnionToDfTest.scala
[03/24/2023 05:42:23] Info: Transformed file MakeResource.scala
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'calcAndDiff' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '157'
Statement: 'def calcAndDiff(base: String, beginDaysDiff: Int, endDaysDiff: Int = 0)(targets: Column*) : Column = { targets.foldLeft(lit(false)){    case (result, target) => {    result || calcAndDiff(target, lit(base), lit(beginDaysDiff), lit(endDaysDiff))    }    } }'
Detail: 'Can't OpenScope for symbol named: 'calcAndDiff(scala.String,scala.Int,scala.Int,_Seq*[Column])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'mkFilePath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/file/output/FixedFileTest.scala'
Line number: '32'
Statement: 'def mkFilePath(fileName: String) = s"${ outputDir }/${ fileName }"'
Detail: 'Can't OpenScope for symbol named: 'mkFilePath(scala.String)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'getDateRangeStatus' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '236'
Statement: 'def getDateRangeStatus(beginDate: String, endDate: String, runningDateYMD: String) = { runningDateYMD match {       case targetif target < nullToMinDate(beginDate) => STATUS_BEFORE       case targetif target >= nullToMaxDate(endDate) => STATUS_END       case _ => STATUS_ON    } }'
Detail: 'Can't OpenScope for symbol named: 'getDateRangeStatus(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'isBlankDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '244'
Statement: 'def isBlankDate(target: String) = { target match {       case null => true       case "" => true       case "00010101" => true       case _ => false    } }'
Detail: 'Can't OpenScope for symbol named: 'isBlankDate(scala.String)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'removeHyphen' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '253'
Statement: 'def removeHyphen(target: String) = { target match {       case null => null       case "" => ""       case _ => target.replaceAll("-", "")    } }'
Detail: 'Can't OpenScope for symbol named: 'removeHyphen(scala.String)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'blankToMaxDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '261'
Statement: 'def blankToMaxDate(target: String) = if (isBlankDate(target))    "99991231" else    target'
Detail: 'Can't OpenScope for symbol named: 'blankToMaxDate(scala.String)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'nullToMinDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '262'
Statement: 'def nullToMinDate(target: String) = if (target == null)    "00010101" else    target'
Detail: 'Can't OpenScope for symbol named: 'nullToMinDate(scala.String)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'nullToMaxDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '263'
Statement: 'def nullToMaxDate(target: String) = if (target == null)    "99991231" else    target'
Detail: 'Can't OpenScope for symbol named: 'nullToMaxDate(scala.String)''
[03/24/2023 05:42:23] Info: Transformed file WritePqTest.scala
[03/24/2023 05:42:23] Info: Transformed file MakeResourceTest.scala
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/PqToXxx.scala'
Line number: '9'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/PqToXxx.scala'
Line number: '13'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/PqToXxx.scala'
Line number: '17'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/PqToXxx.scala'
Line number: '21'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/PqToXxx.scala'
Line number: '25'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Info: Transformed file PqToXxx.scala
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'writePqPath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/WritePq.scala'
Line number: '11'
Statement: 'def writePqPath(implicit inArgs: InputArgs) : String = inArgs.baseOutputFilePath'
Detail: 'Can't OpenScope for symbol named: 'writePqPath(d2k.common.InputArgs)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'strToDt' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Udfs.scala'
Line number: '308'
Statement: 'def strToDt(dateStr: String) = LocalDate.parse(dateStr, DateTimeFormatter.ofPattern("yyyyMMdd"))'
Detail: 'Can't OpenScope for symbol named: 'strToDt(scala.String)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'writeParquet' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/WritePq.scala'
Line number: '15'
Statement: 'def writeParquet(df: DataFrame)(implicit inArgs: InputArgs) = { val pqCtl = new PqCtl (writePqPath) import pqCtl.implicits._ if (writePqPartitionColumns.isEmpty)       {       df.writeParquet(writePqName)       } else       {       df.writeParquetWithPartitionBy(writePqName, writePqPartitionColumns :_*)       } df.sqlContext.emptyDataFrame }'
Detail: 'Can't OpenScope for symbol named: 'writeParquet(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Info: Transformed file WritePq.scala
[03/24/2023 05:42:23] Info: Transformed file FileConvPartition1Test.scala
[03/24/2023 05:42:23] Info: Transformed file Udfs.scala
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/OneInToOneOutForDf.scala'
Line number: '10'
Statement: 'preExec(in: IN)(implicit inArgs: InputArgs): DataFrame'
Detail: 'Can't OpenScope for symbol named: 'preExec(d2k.common.df.flow.OneInToOneOutForDf[IN,OUT].IN,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/OneInToOneOutForDf.scala'
Line number: '12'
Statement: 'exec(df: DataFrame)(implicit inArgs: InputArgs): DataFrame'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/OneInToOneOutForDf.scala'
Line number: '14'
Statement: 'postExec(df: DataFrame)(implicit inArgs: InputArgs): OUT'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'run' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/OneInToOneOutForDf.scala'
Line number: '16'
Statement: 'def run(in: IN)(implicit inArgs: InputArgs) : OUT = { val input = try       {preExec(in)}    catch {       case t:Throwable => platformError(t);throw t    } if (inArgs.isDebug)       {       println(s"${ inArgs.applicationId }[input]") input.show(false)       }  val output = try       {exec(input)}    catch {       case t:Throwable => appError(t);throw t    } if (inArgs.isDebug)       {       println(s"${ inArgs.applicationId }[output]") output.show(false)       } try       {postExec(output)}    catch {       case t:Throwable => platformError(t);throw t    } }'
Detail: 'Can't OpenScope for symbol named: 'run(d2k.common.df.flow.OneInToOneOutForDf[IN,OUT].IN,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'debug' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/OneInToOneOutForDf.scala'
Line number: '46'
Statement: 'def debug(in: IN)(implicit inArgs: InputArgs) : OUT = run(in)(inArgs.copy(isDebug = true))'
Detail: 'Can't OpenScope for symbol named: 'debug(d2k.common.df.flow.OneInToOneOutForDf[IN,OUT].IN,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Info: Transformed file OneInToOneOutForDf.scala
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/ConvNa.scala'
Line number: '11'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) : DataFrame = ConvNaTs(ConvNaDate(df, dateColumns), tsColumns)'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/ConvNa.scala'
Line number: '17'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) : DataFrame = ConvNaDate(df, dateColumns)'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/ConvNa.scala'
Line number: '22'
Statement: 'def apply(df: DataFrame, dateColumnNames: Seq[String])(implicit inArgs: InputArgs) = df.na.fill(dateInit, dateColumnNames).na.replace(dateColumnNames, Map("" -> dateInit))'
Detail: 'Can't OpenScope for symbol named: 'apply(DataFrame,scala.Seq[scala.String],d2k.common.InputArgs)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/ConvNa.scala'
Line number: '28'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) : DataFrame = ConvNaTs(df, tsColumns)'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/ConvNa.scala'
Line number: '33'
Statement: 'def apply(df: DataFrame, tsColumnNames: Seq[String])(implicit inArgs: InputArgs) = df.na.fill(tsInit, tsColumnNames).na.replace(tsColumnNames, Map("" -> tsInit))'
Detail: 'Can't OpenScope for symbol named: 'apply(DataFrame,scala.Seq[scala.String],d2k.common.InputArgs)''
[03/24/2023 05:42:23] Info: Transformed file ConvNa.scala
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/DfJoinVariableToAny.scala'
Line number: '22'
Statement: 'def preExec(left: DataFrame)(implicit inArgs: InputArgs) : DataFrame = { val orgDf = left.columns.foldLeft(left)((df, name) =>df.withColumnRenamed(name, s"$prefixName#$name")) joins.foldLeft(orgDf){(odf, vj) => val (joinDf, uniqId) = vj.inputInfo match {       case x:PqInputInfoBase => (new PqCtl (x.inputDir(componentId)).readParquet(x.pqName), x.pqName)       case x:FileInputInfoBase => {       val fileDf = new FileConv (componentId, x, x.envName, true).makeDf  val droppedRowErr = if (x.dropRowError)             fileDf.drop("ROW_ERR").drop("ROW_ERR_MESSAGE") else             fileDf (droppedRowErr, x.itemConfId)       }    }  val joinedPrefixName = if (vj.prefixName.isEmpty)       uniqId else       vj.prefixName  val addNameDf = joinDf.columns.foldLeft(joinDf){(df, name) =>df.withColumnRenamed(name, s"${ joinedPrefixName }#${ name }") }  val joinedDf = odf.join(addNameDf, vj.joinExprs, "left_outer") vj.dropCols.foldLeft(joinedDf)((l, r) =>l.drop(r)) } }'
Detail: 'Can't OpenScope for symbol named: 'preExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Info: Transformed file DfJoinVariableToAny.scala
[03/24/2023 05:42:23] Info: Transformed file ItemReferenceFinder.scala
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'elapse' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/Logging.scala'
Line number: '13'
Statement: 'def elapse(message: String)(func: =>Unit) = { logger.info(s" Start[${ message }]")  val startTime = System.currentTimeMillis func  val endTime = System.currentTimeMillis  val elapse = BigDecimal(endTime - startTime) / 1000 logger.info(f"finish[${ message }] elapse:${ elapse }%,.3fs") }'
Detail: 'Can't OpenScope for symbol named: 'elapse(scala.String,=>Unit)''
[03/24/2023 05:42:23] Info: Transformed file Logging.scala
[03/24/2023 05:42:23] Info: Transformed file DateConverter.scala
[03/24/2023 05:42:23] Info: Transformed file FileToDf_UTF8Test.scala
[03/24/2023 05:42:23] Info: Transformed file PqCtlTest.scala
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'toCols' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '13'
Statement: 'trait Editors {    val colName: String     def toCols(colNames: Set[String], cols: Seq[Column]): (Set[String], Seq[Column]) }'
Detail: 'Can't OpenScope for symbol named: 'toCols(scala.Set[scala.String],Seq[Column])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'toCols' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '18'
Statement: 'def toCols(colNames: Set[String], cols: Seq[Column]) = (colNames - colName, cols :+ (editor as colName))'
Detail: 'Can't OpenScope for symbol named: 'toCols(scala.Set[scala.String],Seq[Column])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'toCols' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '24'
Statement: 'def toCols(colNames: Set[String], cols: Seq[Column]) = Option(editor).map( e =>(colNames - colNameFrom, cols :+ (editor as colNameTo))).getOrElse((colNames - colNameFrom, cols :+ (col(colNameFrom) as colNameTo)))'
Detail: 'Can't OpenScope for symbol named: 'toCols(scala.Set[scala.String],Seq[Column])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'toCols' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '31'
Statement: 'def toCols(colNames: Set[String], cols: Seq[Column]) = (colNames - colName, cols)'
Detail: 'Can't OpenScope for symbol named: 'toCols(scala.Set[scala.String],Seq[Column])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'toCols' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '37'
Statement: 'def toCols(colNames: Set[String], cols: Seq[Column]) = (colNames - colName, cols :+ (col(inColName).cast(castType)))'
Detail: 'Can't OpenScope for symbol named: 'toCols(scala.Set[scala.String],Seq[Column])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'toCols' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '43'
Statement: 'def toCols(colNames: Set[String], cols: Seq[Column]) = { val regxMatchNames = colNames.flatMap(inRegex.r.findFirstIn) (colNames -- regxMatchNames, cols ++ regxMatchNames.map( name =>col(name).cast(castType))) }'
Detail: 'Can't OpenScope for symbol named: 'toCols(scala.Set[scala.String],Seq[Column])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'toCols' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '51'
Statement: 'def toCols(colNames: Set[String], cols: Seq[Column]) = (colNames - colName, cols :+ func(col(inColName)))'
Detail: 'Can't OpenScope for symbol named: 'toCols(scala.Set[scala.String],Seq[Column])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'toCols' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '57'
Statement: 'def toCols(colNames: Set[String], cols: Seq[Column]) = { val regxMatchNames = colNames.flatMap(inRegex.r.findFirstIn) (colNames -- regxMatchNames, cols ++ regxMatchNames.map( name =>func(col(name)))) }'
Detail: 'Can't OpenScope for symbol named: 'toCols(scala.Set[scala.String],Seq[Column])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'editColumns' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '63'
Statement: 'def editColumns(editors: Seq[Editors]) = (df: DataFrame) =>{ val (colNames, cols) = editors.foldLeft((df.schema.fieldNames.toSet, Seq.empty[Column])){    case ((colNames, cols), target) => target.toCols(colNames, cols)    }  val schemaNames = colNames.map( d =>col(d)) df.select((schemaNames.toSeq ++ cols) :_*) }'
Detail: 'Can't OpenScope for symbol named: 'editColumns(scala.Seq[spark.common.DfCtl.Editors])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'editColumnsAndSelect' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '71'
Statement: 'def editColumnsAndSelect(editors: Seq[Editors]) = (df: DataFrame) =>{ val (colNames, cols) = editors.foldLeft((df.schema.fieldNames.toSet, Seq.empty[Column])){    case ((colNames, cols), target) => target.toCols(colNames, cols)    } df.select(cols :_*) }'
Detail: 'Can't OpenScope for symbol named: 'editColumnsAndSelect(scala.Seq[spark.common.DfCtl.Editors])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'applyAll' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '78'
Statement: 'def applyAll(colNames: Seq[String], applyCode: Column => Column) = (df: DataFrame) =>{ val schemaNames = colNames.map( d =>applyCode(col(d))) df.select(schemaNames :_*) }'
Detail: 'Can't OpenScope for symbol named: 'applyAll(scala.Seq[scala.String],lambda[Column,Column])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'selectMaxValue' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '83'
Statement: 'def selectMaxValue(targetCols: Seq[String], orderCols: Seq[Column]) = (df: DataFrame) =>{ val win = Window.partitionBy(targetCols.map(col) :_*).orderBy(orderCols :_*) df.withColumn("rank", row_number.over(win)).filter("rank = 1").drop("rank") }'
Detail: 'Can't OpenScope for symbol named: 'selectMaxValue(scala.Seq[scala.String],Seq[Column])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'groupingAgg' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '88'
Statement: 'def groupingAgg(groupingColumns: Seq[String], aggColumns: Seq[Column]) = (df: DataFrame) =>{ df.groupBy(groupingColumns.map(col) :_*).agg(aggColumns.head, aggColumns.tail :_*) }'
Detail: 'Can't OpenScope for symbol named: 'groupingAgg(scala.Seq[scala.String],Seq[Column])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'groupingSum' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '92'
Statement: 'def groupingSum(groupingColumns: Seq[String], sumColumns: Seq[String]) = (df: DataFrame) =>{ val cols = sumColumns.map( col =>sum(col) as col) groupingAgg(groupingColumns, cols)(df) }'
Detail: 'Can't OpenScope for symbol named: 'groupingSum(scala.Seq[scala.String],scala.Seq[scala.String])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'addColumnPrefix' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '97'
Statement: 'def addColumnPrefix(name: String) = (df: DataFrame) =>{ val cols = df.schema.map( x =>df(x.name) as s"${ name }_${ x.name }") df.select(cols :_*) }'
Detail: 'Can't OpenScope for symbol named: 'addColumnPrefix(scala.String)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'dropColumnPrefix' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '102'
Statement: 'def dropColumnPrefix(name: String) = (df: DataFrame) =>{ val cols = df.schema.map(_.name).filter(!_.startsWith(s"${ name }_")).map(col) df.select(cols :_*) }'
Detail: 'Can't OpenScope for symbol named: 'dropColumnPrefix(scala.String)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'pickMaxValueRow' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '112'
Statement: 'def pickMaxValueRow(pks: String*)(maxValueTarget: String*) = df.sort((pks.map( x =>col(x)) ++ maxValueTarget.map( x =>col(x).desc)) :_*).dropDuplicates(pks)'
Detail: 'Can't OpenScope for symbol named: 'pickMaxValueRow(_Seq*[scala.String],_Seq*[scala.String])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'partitionWriteFile' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '115'
Statement: 'def partitionWriteFile(filePath: String, overwrite: Boolean = true, charEnc: String = "MS932", partitionExtention: String = "")(func: Row => String) {    if (overwrite)       FileCtl.deleteDirectory(filePath) FileCtl.createDirectory(filePath) df.rdd.mapPartitionsWithIndex{(idx, iterRow) => val fullPath = FileCtl.addExtention(s"${ filePath }/${ idx }", partitionExtention) FileCtl.writeToFile(fullPath, true, charEnc){ pw =>elapse(s"fileWrite:${ fullPath }"){ iterRow.foreach( row =>pw.println(func(row))) } } Seq.empty[Row].toIterator }.foreach(_ =>()) }'
Detail: 'Can't OpenScope for symbol named: 'partitionWriteFile(scala.String,scala.Boolean,scala.String,scala.String,lambda[Row,String])''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'partitionWriteToFileWithPartitionColumns' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/DfCtl.scala'
Line number: '132'
Statement: 'def partitionWriteToFileWithPartitionColumns(filePath: String, partitionColumns: Seq[String], overwrite: Boolean = true, charEnc: String = "MS932", partitionExtention: String = "")(func: Row => String) {    if (overwrite)       FileCtl.deleteDirectory(filePath) FileCtl.createDirectory(filePath) df.rdd.mapPartitionsWithIndex{(idx, iterRow) => val fullPath = FileCtl.addExtention(s"${ filePath }/${ idx }", partitionExtention) elapse(s"fileWrite:${ fullPath }"){ FileCtl.loanPrintWriterCache{ cache =>iterRow.foldLeft(cache){(l, r) =>FileCtl.writeToFileWithPartitionColumns( filePath, idx, charEnc, partitionColumns, partitionExtention)(func)(l)(r) } } } Seq.empty[Row].toIterator }.foreach(_ =>()) }'
Detail: 'Can't OpenScope for symbol named: 'partitionWriteToFileWithPartitionColumns(scala.String,scala.Seq[scala.String],scala.Boolean,scala.String,scala.String,lambda[Row,String])''
[03/24/2023 05:42:23] Info: Transformed file NothingTest.scala
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/AnyToPq_Db.scala'
Line number: '14'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) = { writeParquet(df)  val pqCtl = new PqCtl (writePqPath) writeDb(pqCtl.readParquet(writePqName)) }'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Info: Transformed file AnyToPq_Db.scala
[03/24/2023 05:42:23] Info: Transformed file DfCtl.scala
[03/24/2023 05:42:23] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:23] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:23] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:23] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'lastUpdateTime' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/InputArgs.scala'
Line number: '48'
Statement: 'def lastUpdateTime(tableName: String, readDbInfo: DbInfo = DbConnectionInfo.bat1) = { val options = new JDBCOptions (Map( JDBCOptions.JDBC_URL -> readDbInfo.url,  JDBCOptions.JDBC_TABLE_NAME -> tableName,  "user" -> readDbInfo.user,  "password" -> readDbInfo.password,  "charSet" -> readDbInfo.charSet))  val ps = JdbcUtils.createConnectionFactory(options)().prepareStatement("select DT_FROMUPDYMDTM, DT_TOUPDYMDTM from MOP012 where ID_TBLID = ?") ps.setString(1, tableName)  val rs = ps.executeQuery  val result = try       {Iterator.continually((rs.next, rs)).takeWhile(_._1).map{          case (_, rec) => LastUpdateTime(rec.getTimestamp("DT_FROMUPDYMDTM"), rec.getTimestamp("DT_TOUPDYMDTM"))          }.toSeq}    finally       {       rs.close       } result.headOption.getOrElse(throw new IllegalArgumentException (s"tableName is not defined[$tableName]")) }'
Detail: 'Can't OpenScope for symbol named: 'lastUpdateTime(scala.String,DbInfo)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/InputArgs.scala'
Line number: '74'
Statement: 'def apply(d2kBasePath: String, productionMode: String, confPath: String, dataPath: String, projectId: String, processId: String, applicationId: String, runningDateFileFullPath: String) = { val confBasePath = s"$d2kBasePath/$productionMode/$confPath"  val fileConvInputFile = s"$confBasePath/import/${ projectId }_app.conf"  val fileConvOutputFile = s"$confBasePath/export/${ projectId }_app.conf"  val baseInputFilePath = s"$d2kBasePath/$productionMode/$dataPath/output"  val baseOutputFilePath = s"$d2kBasePath/$productionMode/$dataPath/output"  val baseErrProofFilePath = s"$d2kBasePath/$productionMode/$dataPath/error"  val runningDates = Source.fromFile(runningDateFileFullPath).getLines.toList(1).split(" ")  val dateFormat = DateTimeFormat.forPattern("yyyyMMdd")  val runningSQLDate = new Date (dateFormat.withZoneUTC.parseDateTime(runningDates(0)).getMillis) new InputArgs (d2kBasePath, productionMode, confPath, dataPath,  projectId, processId, applicationId,  runningDateFileFullPath, confBasePath, fileConvInputFile, fileConvOutputFile,  baseInputFilePath, baseOutputFilePath, baseErrProofFilePath,  runningDates, runningSQLDate) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/InputArgs.scala'
Line number: '95'
Statement: 'def apply(projectId: String, processId: String, applicationId: String, runningDateFileFullPath: String) : InputArgs = { apply("/D2Khome", "HN", "APL/conf/spark", "sparkWK/Parquet", projectId, processId, applicationId, runningDateFileFullPath) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:23] Info: Transformed file InputArgs.scala
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'execUt' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/MarkdownTester.scala'
Line number: '14'
Statement: 'def execUt(componentInstanceName: String)(targets: (DataFrame => DataFrame)*) = { val classNames = targets.head.getClass.getName.split('$')  val appName = classNames.head.split('.').last  val makeRes = MakeResource("test/dev/data/output", s"${ appName }Test/ut/${ componentInstanceName }") s"be success ${ componentInstanceName }" when {    targets.foreach{ func => val funcName = func.getClass.getName.split('$').dropRight(1).takeRight(1).head funcName in {       val df = makeRes.readMdTable(s"${ funcName }_data.md").toDf if (showData)             println(s"[Input Data:${ componentInstanceName }:${ funcName }]");df.show(false)  val expect = makeRes.readMdTable(s"${ funcName }_expect.md") if (showData)             println(s"[Expect Data:${ componentInstanceName }:${ funcName }]");expect.toDf.show(false)  val result = func(df) if (showData)             println(s"[Result Data:${ componentInstanceName }:${ funcName }]");result.show(false) withClue("Record Size Check"){ result.count mustBe expect.toDf.count } expect.checkDf(result)       } }    } }'
Detail: 'Can't OpenScope for symbol named: 'execUt(scala.String,_Seq*[Tuple2[lambda[DataFrame,DataFrame]]])''
[03/24/2023 05:42:23] Info: Transformed file MarkdownTester.scala
[03/24/2023 05:42:23] Info: Transformed file appErrorDetect.scala
[03/24/2023 05:42:23] Info: Transformed file ConvNaTest.scala
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/OneInToOneOut.scala'
Line number: '6'
Statement: 'preExec(in: IN)(implicit inArgs: InputArgs): PREOUT'
Detail: 'Can't OpenScope for symbol named: 'preExec(d2k.common.df.flow.base.OneInToOneOut[IN,PREOUT,MID,POSTIN,OUT].IN,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/OneInToOneOut.scala'
Line number: '8'
Statement: 'exec(df: MID)(implicit inArgs: InputArgs): MID'
Detail: 'Can't OpenScope for symbol named: 'exec(d2k.common.df.flow.base.OneInToOneOut[IN,PREOUT,MID,POSTIN,OUT].MID,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/OneInToOneOut.scala'
Line number: '10'
Statement: 'postExec(df: POSTIN)(implicit inArgs: InputArgs): OUT'
Detail: 'Can't OpenScope for symbol named: 'postExec(d2k.common.df.flow.base.OneInToOneOut[IN,PREOUT,MID,POSTIN,OUT].POSTIN,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'run' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/OneInToOneOut.scala'
Line number: '12'
Statement: 'run(in: IN)(implicit inArgs: InputArgs): OUT'
Detail: 'Can't OpenScope for symbol named: 'run(d2k.common.df.flow.base.OneInToOneOut[IN,PREOUT,MID,POSTIN,OUT].IN,d2k.common.InputArgs)''
[03/24/2023 05:42:23] Info: Transformed file OneInToOneOut.scala
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/ItemDefParser.scala'
Line number: '9'
Statement: 'def apply(s: String) = { val splitted = s.split('|') new TableItem5 (splitted(1), splitted(2), splitted(3), splitted(4), splitted(5)) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/ItemDefParser.scala'
Line number: '17'
Statement: 'def apply(baseUrl: String, branch: String, filePath: String) = { val parsed = parseAll(itemDef, readItemDefMd(baseUrl, branch, filePath)) println(parsed) parsed }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/ItemDefParser.scala'
Line number: '23'
Statement: 'def apply(basePath: String) = { parseAll(itemDef, readItemDefMd(basePath)) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:23] Info: Transformed file ItemDefParser.scala
[03/24/2023 05:42:23] Info: Transformed file ConfParserTest.scala
[03/24/2023 05:42:23] Error: An error ocurred at 'OpenScope for node with name 'writeFilePath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/mixIn/OraLoader.scala'
Line number: '11'
Statement: 'def writeFilePath(implicit inArgs: InputArgs) = sys.env("DB_LOADING_FILE_PATH")'
Detail: 'Can't OpenScope for symbol named: 'writeFilePath(d2k.common.InputArgs)''
[03/24/2023 05:42:23] Info: Transformed file OraLoader.scala
[03/24/2023 05:42:23] Info: Transformed file FixedFileTest.scala
[03/24/2023 05:42:23] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:23] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:23] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:23] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:23] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:23] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:23] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:23] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:23] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:23] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'checkData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/spark/common/DfCtlTest.scala'
Line number: '367'
Statement: 'def checkData(path: String, target: String) = { val basePath = Path(path)  val files = basePath.jfile.listFiles  val recs = files.flatMap( name =>Source.fromFile(name).getLines) recs.contains(target) }'
Detail: 'Can't OpenScope for symbol named: 'checkData(scala.String,scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'dateTest' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/fileConv/DomainProcessorTest.scala'
Line number: '728'
Statement: 'def dateTest(domain_name: String, empty: String, max: String, max_over: String, min: String, min_under: String, not_digit: String, invalid: String, INVALID_DATE_MSG: String = null) {    val max_exp = max.replaceAll("[-:/]", "")  val min_exp = min.replaceAll("[-:/]", "")  val actual_empty = execRight(domain_name, empty) actual_empty mustBe min_exp  val nullString = new String (Array[Byte](0x00))  val actual_null = execRight(domain_name, empty.replaceAll(" ", nullString)) actual_null mustBe min_exp  val actual_max = execRight(domain_name, max) actual_max mustBe max_exp  val actual_min = execRight(domain_name, min) actual_min mustBe min_exp if (min_under != null)       {       val actual_min_under = execRight(domain_name, min_under) actual_min_under mustBe min_exp       } }'
Detail: 'Can't OpenScope for symbol named: 'dateTest(scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'timestampTest' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/fileConv/DomainProcessorTest.scala'
Line number: '762'
Statement: 'def timestampTest(domain_name: String, empty: String, max: String, max_over: String, min: String, min_under: String, not_digit: String, invalid: String, max_exp: String, min_exp: String) {    val actual_empty = execRight(domain_name, empty) actual_empty mustBe min_exp  val nullString = new String (Array[Byte](0x00))  val actual_null = execRight(domain_name, empty.replaceAll(" ", nullString)) actual_null mustBe min_exp  val actual_max = execRight(domain_name, max) actual_max mustBe max_exp  val actual_min = execRight(domain_name, min) actual_min mustBe min_exp if (min_under != null)       {       val actual_min_under = execRight(domain_name, min_under) actual_min_under mustBe min_exp       } }'
Detail: 'Can't OpenScope for symbol named: 'timestampTest(scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'execLeft' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/fileConv/DomainProcessorTest.scala'
Line number: '784'
Statement: 'def execLeft(domain_name: String, data: String) = checkLeft(DomainProcessor.exec(domain_name, data))'
Detail: 'Can't OpenScope for symbol named: 'execLeft(scala.String,scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'execLeft2' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/fileConv/DomainProcessorTest.scala'
Line number: '787'
Statement: 'def execLeft2(domain_name: String, data: Array[Int], charEnc: String = "MS932") = checkLeft(DomainProcessor.execArrayByte(domain_name, data.map(_.toByte), charEnc))'
Detail: 'Can't OpenScope for symbol named: 'execLeft2(scala.String,scala.Array[scala.Int],scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'execRight' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/fileConv/DomainProcessorTest.scala'
Line number: '797'
Statement: 'def execRight(domain_name: String, data: String) = checkRight(DomainProcessor.exec(domain_name, data))'
Detail: 'Can't OpenScope for symbol named: 'execRight(scala.String,scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'execRight2' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/fileConv/DomainProcessorTest.scala'
Line number: '800'
Statement: 'def execRight2(domain_name: String, data: Array[Int], charEnc: String = "MS932") = checkRight(DomainProcessor.execArrayByte(domain_name, data.map(_.toByte), charEnc))'
Detail: 'Can't OpenScope for symbol named: 'execRight2(scala.String,scala.Array[scala.Int],scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'extentionCheck' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/spark/common/DfCtlTest.scala'
Line number: '408'
Statement: 'def extentionCheck(path: String, extention: String) = Path(path).jfile.listFiles.map(_.toString.endsWith(extention)).forall(_ == true)'
Detail: 'Can't OpenScope for symbol named: 'extentionCheck(scala.String,scala.String)''
[03/24/2023 05:42:24] Info: Transformed file DomainProcessorTest.scala
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfUnionToDf.scala'
Line number: '10'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:24] Info: Transformed file DfUnionToDf.scala
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'makeDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/TextConverter.scala'
Line number: '19'
Statement: 'def makeDf(options: Map[String, String])(names: Seq[String], domains: Seq[String], path: Set[String]) = { val rdd = SparkContexts.context.read.options(options).csv(path.toSeq :_*).rdd.map{ row => val dataAndDomainsAndNames = row.toSeq.map( s =>Option(s).map(_.toString).getOrElse("")).zip(domains).zip(names) Row(Converter.domainConvert(dataAndDomainsAndNames) :_*) }  val ziped = names.zip(domains)  val (nameList, domainList) = ziped.filter{    case (names, domain) => !(domain.startsWith(Converter.NOT_USE_PREFIX) || domain.startsWith(Converter.REC_DIV_PREFIX))    }.unzip context.createDataFrame(rdd, Converter.makeSchema(nameList)) }'
Detail: 'Can't OpenScope for symbol named: 'makeDf(scala.Map[scala.String,scala.String],scala.Seq[scala.String],scala.Seq[scala.String],scala.Set[scala.String])''
[03/24/2023 05:42:24] Info: Transformed file TextConverter.scala
[03/24/2023 05:42:24] Info: Transformed file DfCtlTest.scala
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfJoinPqToDf.scala'
Line number: '10'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:24] Info: Transformed file DfJoinPqToDf.scala
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'writeToFile' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '14'
Statement: 'def writeToFile(fileName: String, append: Boolean = false, charEnc: String = "MS932")(func: PrintWriter => Unit) {    val outFile = new PrintWriter (new OutputStreamWriter (new FileOutputStream (fileName, append), charEnc)) func(outFile) outFile.close() }'
Detail: 'Can't OpenScope for symbol named: 'writeToFile(scala.String,scala.Boolean,scala.String,lambda[PrintWriter,Unit])''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'addExtention' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '27'
Statement: 'def addExtention(path: String, ext: String) = if (ext.isEmpty)    path else    s"${ path }.${ ext }"'
Detail: 'Can't OpenScope for symbol named: 'addExtention(scala.String,scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'writeToFileWithPartitionColumns' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '29'
Statement: 'def writeToFileWithPartitionColumns(fileName: String, partitionIndex: Int = 0, charEnc: String = "MS932", partitionColumns: Seq[String] = Seq.empty[String], partitionExtention: String = "")(func: Row => String)(pwCache: Map[String, PrintWriter])(row: Row) = { val outPath = partitionColumns.map{ col =>s"${ col }=${ row.getAs[String](col) }" }.mkString(s"${ fileName }/", "/", "") FileCtl.createDirectory(outPath)  val outFile = pwCache.getOrElse(outPath,  new PrintWriter (new OutputStreamWriter (new FileOutputStream ( addExtention(s"${ outPath }/${ partitionIndex }", partitionExtention), true), charEnc))) outFile.println(func(row)) if (pwCache.isDefinedAt(outPath))       pwCache else       pwCache.updated(outPath, outFile) }'
Detail: 'Can't OpenScope for symbol named: 'writeToFileWithPartitionColumns(scala.String,scala.Int,scala.String,scala.Seq[scala.String],scala.String,lambda[Row,String],Map[String,PrintWriter],Row)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'loadEnv' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '46'
Statement: 'def loadEnv(filePath: String) : Properties = { val env = new Properties () env.load(new FileInputStream (filePath)) env }'
Detail: 'Can't OpenScope for symbol named: 'loadEnv(scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'exists' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '52'
Statement: 'def exists(filePath: String) : Boolean = { val conf = SparkContexts.sc.hadoopConfiguration  val fs = FileSystem.get(conf) Option(fs.globStatus(new Path (filePath))).map(!_.isEmpty).getOrElse(false) }'
Detail: 'Can't OpenScope for symbol named: 'exists(scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'createDirectory' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '58'
Statement: 'def createDirectory(fullPath: String) {    Directory(fullPath).createDirectory(true, false) }'
Detail: 'Can't OpenScope for symbol named: 'createDirectory(scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'createDirectory' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '62'
Statement: 'def createDirectory(dirPath: String, filePath: String) {    (Directory(dirPath) / filePath).createDirectory(true, false) }'
Detail: 'Can't OpenScope for symbol named: 'createDirectory(scala.String,scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'deleteDirectory' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '66'
Statement: 'def deleteDirectory(fullPath: String) {    Directory(fullPath).deleteRecursively }'
Detail: 'Can't OpenScope for symbol named: 'deleteDirectory(scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'deleteDirectory' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/FileCtl.scala'
Line number: '70'
Statement: 'def deleteDirectory(dirPath: String, filePath: String) {    (Directory(dirPath) / filePath).deleteRecursively }'
Detail: 'Can't OpenScope for symbol named: 'deleteDirectory(scala.String,scala.String)''
[03/24/2023 05:42:24] Info: Transformed file FileCtl.scala
[03/24/2023 05:42:24] Info: Transformed file JdbcCtlTest.scala
[03/24/2023 05:42:24] Info: Transformed file PostCodeNormalizerTest.scala
[03/24/2023 05:42:24] Info: Transformed file FileConvTest.scala
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'toMarkdown' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/ExcelConverter.scala'
Line number: '9'
Statement: 'def toMarkdown(excelPath: String, sheet: String, mdFile: String) = { val data = to2DArray(excelPath, sheet)  val colSize = data(0).filter(!_.isEmpty).size  val x: Seq[List[String]] = Seq( data(0).toList,  data(0).map(_ =>"----").toList,  data(0).toList,  data(1).map{    case xif x.endsWith("_ZD") => "ZD"    case xif x.endsWith("_PD") => "PD"    case "全角文字列" => "全角文字列"    case _ => "半角文字列"    }.toList.take(colSize),  data(2).toList.take(colSize))  val x2 = x ++ data.toList.drop(3).map(_.toList)  val x3 = x2.map( a =>a.take(colSize).mkString("|", "|", "|")) System.setProperty("line.separator", "\n") FileCtl.writeToFile(mdFile, charEnc = "UTF-8"){ pw =>pw.println(s"# ${ sheet }") x3.foreach(pw.println) } }'
Detail: 'Can't OpenScope for symbol named: 'toMarkdown(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'to2DArray' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/ExcelConverter.scala'
Line number: '33'
Statement: 'def to2DArray(path: String, sheetName: String) = { val sheet = getTargetSheet(path, sheetName)  val rowCnt = sheet.getLastRowNum() + 1 (0 to rowCnt).flatMap{ i =>Option(sheet.getRow(i)).map{ row =>(0 to row.getLastCellNum).flatMap{ cellCnt =>Option(row.getCell(cellCnt)).map( cell =>getStrVal(cell)) } } } }'
Detail: 'Can't OpenScope for symbol named: 'to2DArray(scala.String,scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'getTargetSheet' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/ExcelConverter.scala'
Line number: '60'
Statement: 'def getTargetSheet(path: String, sheetName: String) = { val input = new File (path)  val book = WorkbookFactory.create(input) book.getSheet(sheetName) }'
Detail: 'Can't OpenScope for symbol named: 'getTargetSheet(scala.String,scala.String)''
[03/24/2023 05:42:24] Info: Transformed file ExcelConverter.scala
[03/24/2023 05:42:24] Info: Transformed file TestArgs.scala
[03/24/2023 05:42:24] Info: Transformed file VariableFileTest.scala
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/AnyToDb.scala'
Line number: '10'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) = writeDb(df)'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:24] Info: Transformed file AnyToDb.scala
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DbToXxx.scala'
Line number: '9'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DbToXxx.scala'
Line number: '13'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DbToXxx.scala'
Line number: '17'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DbToXxx.scala'
Line number: '21'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DbToXxx.scala'
Line number: '25'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:24] Info: Transformed file DbToXxx.scala
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/CommonServices.scala'
Line number: '9'
Statement: 'def apply(df: DataFrame, componentId: String)(implicit inArgs: InputArgs) = { val addCommonItems = df.withColumn("DT_D2KMKDTTM", lit(inArgs.sysSQLDate)).withColumn("ID_D2KMKUSR", lit(componentId)).withColumn("DT_D2KUPDDTTM", lit(inArgs.sysSQLDate)).withColumn("ID_D2KUPDUSR", lit(componentId)).withColumn("NM_D2KUPDTMS", lit("0")).withColumn("FG_D2KDELFLG", lit("0"))  val comonColumnNames = Array("DT_D2KMKDTTM", "ID_D2KMKUSR", "DT_D2KUPDDTTM", "ID_D2KUPDUSR", "NM_D2KUPDTMS", "FG_D2KDELFLG")  val otherColumns = addCommonItems.columns  val dropCommonColumns = comonColumnNames.foldLeft(otherColumns){(l, r) =>l.filter(_ != r)}  val moveToFrontColumns = comonColumnNames ++ dropCommonColumns addCommonItems.select(moveToFrontColumns.head, moveToFrontColumns.drop(1).toSeq :_*) }'
Detail: 'Can't OpenScope for symbol named: 'apply(DataFrame,scala.String,d2k.common.InputArgs)''
[03/24/2023 05:42:24] Info: Transformed file CommonServices.scala
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'inputDir' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/InputInfo.scala'
Line number: '53'
Statement: 'def inputDir(componentId: String) : String = Try{sys.env(s"PQ_INPUT_PATH_${ componentId }")}. getOrElse(Try{sys.env(s"PQ_INPUT_PATH_${ envName }")}.getOrElse(sys.env(s"PQ_INPUT_PATH_${ ENV_NAME_DEFAULT }")))'
Detail: 'Can't OpenScope for symbol named: 'inputDir(scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'inputDir' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/InputInfo.scala'
Line number: '64'
Statement: 'def inputDir(componentId: String) : String = Try{sys.env(s"FILE_INPUT_PATH_${ componentId }")}. getOrElse(Try{sys.env(s"FILE_INPUT_PATH_${ envName }")}.getOrElse(sys.env(s"FILE_INPUT_PATH_${ ENV_NAME_DEFAULT }")))'
Detail: 'Can't OpenScope for symbol named: 'inputDir(scala.String)''
[03/24/2023 05:42:24] Info: Transformed file InputInfo.scala
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'd2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/spark/common/DbCtlTest.scala'
Line number: '23'
Statement: 'def d2s(dateMill: Long) = new DateTime (dateMill).toString("yyyy-MM-dd")'
Detail: 'Can't OpenScope for symbol named: 'd2s(scala.Long)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'date2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/spark/common/DbCtlTest.scala'
Line number: '24'
Statement: 'def date2s(date: Date) = new DateTime (date).toString("yyyy-MM-dd")'
Detail: 'Can't OpenScope for symbol named: 'date2s(_Unresolved.Date)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'd2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/spark/common/DbCtlTest.scala'
Line number: '25'
Statement: 'def d2s(date: Timestamp) = new DateTime (date).toString("yyyy-MM-dd")'
Detail: 'Can't OpenScope for symbol named: 'd2s(_Unresolved.Timestamp)''
[03/24/2023 05:42:24] Info: Transformed file PostCodeConverterTest.scala
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'searchResource' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/GeneratingApplicationRouteFinder.scala'
Line number: '27'
Statement: 'def searchResource(resourceName: String) = appDefList.filter{ case (_, appdef) => val existCheck = appdef.outputList.filter(_.id == resourceName) !existCheck.isEmpty }'
Detail: 'Can't OpenScope for symbol named: 'searchResource(scala.String)''
[03/24/2023 05:42:24] Info: Transformed file GeneratingApplicationRouteFinder.scala
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'readDb' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/MultiReadDb.scala'
Line number: '11'
Statement: 'def readDb(implicit inArgs: InputArgs) = readTableNames.map( tblnm =>(tblnm, readDbSingle(tblnm))).toMap'
Detail: 'Can't OpenScope for symbol named: 'readDb(d2k.common.InputArgs)''
[03/24/2023 05:42:24] Info: Transformed file MultiReadDb.scala
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/PqCommonColumnRemover.scala'
Line number: '10'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) : DataFrame = PqCommonColumnRemover(df)'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/PqCommonColumnRemover.scala'
Line number: '14'
Statement: 'def apply(df: DataFrame)(implicit inArgs: InputArgs) = df.drop(Converter.SYSTEM_COLUMN_NAME.ROW_ERROR).drop(Converter.SYSTEM_COLUMN_NAME.ROW_ERROR_MESSAGE)'
Detail: 'Can't OpenScope for symbol named: 'apply(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:24] Info: Transformed file PqCommonColumnRemover.scala
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'timestamp_yyyymmdd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/InputArgsTest.scala'
Line number: '63'
Statement: 'def timestamp_yyyymmdd(yyyy: Int, mm: Int, dd: Int) = Timestamp.valueOf(LocalDateTime.of(yyyy, mm, dd, 0, 0, 0))'
Detail: 'Can't OpenScope for symbol named: 'timestamp_yyyymmdd(scala.Int,scala.Int,scala.Int)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/InputArgsTest.scala'
Line number: '140'
Statement: 'def exec(implicit inArgs: InputArgs) = { dbTodb.run(Unit) }'
Detail: 'Can't OpenScope for symbol named: 'exec(d2k.common.InputArgs)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/InputArgsTest.scala'
Line number: '147'
Statement: 'def readDbWhere(inArgs: InputArgs) = { val ut = inArgs.lastUpdateTime(writeTableName) Array(s"TMSTMP >= '${ ut.from }' and TMSTMP <= '${ ut.to }'") }'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/InputArgsTest.scala'
Line number: '169'
Statement: 'def exec(implicit inArgs: InputArgs) = { dbTodb.run(Unit) }'
Detail: 'Can't OpenScope for symbol named: 'exec(d2k.common.InputArgs)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/InputArgsTest.scala'
Line number: '176'
Statement: 'def readDbWhere(inArgs: InputArgs) = Array(s"TMSTMP >= '${ inArgs.lastUpdateTime("xxx") }'")'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:24] Info: Transformed file InputArgsTest.scala
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'readData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/KanaConverter.scala'
Line number: '8'
Statement: 'def readData(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(s"kanaConv/$fileName"), charEnc).getLines'
Detail: 'Can't OpenScope for symbol named: 'readData(scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/KanaConverter.scala'
Line number: '18'
Statement: 'def apply(inStr: String) = { if (inStr != null)       {       inStr.map( c =>cnvMap.getOrElse(c.toString, zenkakuCnv(c.toString))).mkString       } else       {       inStr       } }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:24] Error: An error ocurred at 'OpenScope for node with name 'select' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/KanaConverter.scala'
Line number: '19'
Statement: 'def select(inStr: String) = { if (inStr != null)       {       inStr.map( c =>cnvMapSelect.getOrElse(c.toString, zenkakuCnv(c.toString))).mkString       } else       {       inStr       } }'
Detail: 'Can't OpenScope for symbol named: 'select(scala.String)''
[03/24/2023 05:42:24] Info: Transformed file KanaConverter.scala
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/TwoInToOneOutForDf.scala'
Line number: '10'
Statement: 'preExec(in1: IN1, in2: IN2)(implicit inArgs: InputArgs): DataFrame'
Detail: 'Can't OpenScope for symbol named: 'preExec(d2k.common.df.flow.TwoInToOneOutForDf[IN1,IN2,OUT].IN1,d2k.common.df.flow.TwoInToOneOutForDf[IN1,IN2,OUT].IN2,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/TwoInToOneOutForDf.scala'
Line number: '12'
Statement: 'exec(df: DataFrame)(implicit inArgs: InputArgs): DataFrame'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/TwoInToOneOutForDf.scala'
Line number: '14'
Statement: 'postExec(df: DataFrame)(implicit inArgs: InputArgs): OUT'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'run' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/TwoInToOneOutForDf.scala'
Line number: '16'
Statement: 'def run(in1: IN1, in2: IN2)(implicit inArgs: InputArgs) : OUT = { val input = try       {preExec(in1, in2)}    catch {       case t:Throwable => platformError(t);throw t    } if (inArgs.isDebug)       {       println(s"${ inArgs.applicationId }[input]") input.show(false)       }  val output = try       {exec(input)}    catch {       case t:Throwable => appError(t);throw t    } if (inArgs.isDebug)       {       println(s"${ inArgs.applicationId }[output]") output.show(false)       } try       {postExec(output)}    catch {       case t:Throwable => platformError(t);throw t    } }'
Detail: 'Can't OpenScope for symbol named: 'run(d2k.common.df.flow.TwoInToOneOutForDf[IN1,IN2,OUT].IN1,d2k.common.df.flow.TwoInToOneOutForDf[IN1,IN2,OUT].IN2,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'debug' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/TwoInToOneOutForDf.scala'
Line number: '46'
Statement: 'def debug(in1: IN1, in2: IN2)(implicit inArgs: InputArgs) : OUT = run(in1, in2)(inArgs.copy(isDebug = true))'
Detail: 'Can't OpenScope for symbol named: 'debug(d2k.common.df.flow.TwoInToOneOutForDf[IN1,IN2,OUT].IN1,d2k.common.df.flow.TwoInToOneOutForDf[IN1,IN2,OUT].IN2,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Info: Transformed file TwoInToOneOutForDf.scala
[03/24/2023 05:42:25] Info: Transformed file FileToXxxTest.scala
[03/24/2023 05:42:25] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:25] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:25] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:25] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:25] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:25] Info: Transformed file ResourceRunningEnvs.scala
[03/24/2023 05:42:25] Info: Transformed file DbCtlWithHintTest.scala
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/sh/CommissionBaseChannelSelectorTmpl.scala'
Line number: '33'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = if (groupingKeys.contains(UniqueKey))    {    (df ~> c03_DfToDf.run, broadcast(c01_DbToDf(info).run(Unit))) ~> c02_DfJoinToDf.run ~> c04_DfToDf.run    } else    {    (df, broadcast(c01_DbToDf(info).run(Unit))) ~> c02_DfJoinToDf.run    }'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'c01_DbToDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/sh/CommissionBaseChannelSelectorTmpl.scala'
Line number: '40'
Statement: 'def c01_DbToDf(info: CommissionBaseChannelSelectorInfo) = new DbToDf with Executor {    val componentId = "MAA300"     override val columns = Array("DV_DISCRDIV", "CD_CHNLCD", "DV_OUTOBJDIV", "DV_TRICALCOBJDIV")     override val readDbWhere = Array(s"DV_DISPODIV = '${ info.DV_DISPODIV }'")     def invoke(df: DataFrame)(implicit inArgs: InputArgs) = df ~> f01     def f01(implicit inArgs: InputArgs) = (_ : DataFrame).na.fill(" ") }'
Detail: 'Can't OpenScope for symbol named: 'c01_DbToDf(d2k.common.df.template.sh.CommissionBaseChannelSelectorInfo)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/sh/CommissionBaseChannelSelectorTmpl.scala'
Line number: '61'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) = df ~> f01 ~> f02 ~> f03 ~> f04'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/sh/CommissionBaseChannelSelectorTmpl.scala'
Line number: '78'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) = df ~> f01'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/sh/CommissionBaseChannelSelectorTmpl.scala'
Line number: '91'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) = df ~> f01'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Info: Transformed file CommissionBaseChannelSelectorTmpl.scala
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/OneInToMapOut.scala'
Line number: '7'
Statement: 'preExec(in: IN)(implicit inArgs: InputArgs): Map[String, MID]'
Detail: 'Can't OpenScope for symbol named: 'preExec(d2k.common.df.flow.base.OneInToMapOut[IN,MID,OUT].IN,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/OneInToMapOut.scala'
Line number: '9'
Statement: 'exec(df: MID)(implicit inArgs: InputArgs): MID'
Detail: 'Can't OpenScope for symbol named: 'exec(d2k.common.df.flow.base.OneInToMapOut[IN,MID,OUT].MID,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/OneInToMapOut.scala'
Line number: '11'
Statement: 'postExec(df: Map[String, MID])(implicit inArgs: InputArgs): OUT'
Detail: 'Can't OpenScope for symbol named: 'postExec(scala.Map[scala.String,d2k.common.df.flow.base.OneInToMapOut[IN,MID,OUT].MID],d2k.common.InputArgs)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'run' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/OneInToMapOut.scala'
Line number: '13'
Statement: 'run(in: IN)(implicit inArgs: InputArgs): OUT'
Detail: 'Can't OpenScope for symbol named: 'run(d2k.common.df.flow.base.OneInToMapOut[IN,MID,OUT].IN,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Info: Transformed file OneInToMapOut.scala
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'readDb' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/SingleReadDb.scala'
Line number: '11'
Statement: 'def readDb(implicit inArgs: InputArgs) = readDbSingle(readTableName)'
Detail: 'Can't OpenScope for symbol named: 'readDb(d2k.common.InputArgs)''
[03/24/2023 05:42:25] Info: Transformed file SingleReadDb.scala
[03/24/2023 05:42:25] Info: Transformed file SourceGeneratorTest.scala
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/sql/SqlDefParser.scala'
Line number: '12'
Statement: 'def apply(baseUrl: String, branch: String, appGroup: String, appId: String) = { val parsed = parseAll(sqlDef, readAppDefMd(baseUrl, branch, appGroup, appId, "README.md")) println(parsed) parsed }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/sql/SqlDefParser.scala'
Line number: '18'
Statement: 'def apply(baseUrl: String, appGroup: String, appId: String) = { val parsed = parseAll(sqlDef, readAppDefMd(baseUrl, appGroup, appId, "README.md")) println(parsed) parsed }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:25] Info: Transformed file SqlDefParser.scala
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/RowErrorRemover.scala'
Line number: '9'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) : DataFrame = RowErrorRemover(df)'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/RowErrorRemover.scala'
Line number: '13'
Statement: 'def apply(df: DataFrame)(implicit inArgs: InputArgs) = df.filter(col("ROW_ERR") === lit("false"))'
Detail: 'Can't OpenScope for symbol named: 'apply(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Info: Transformed file RowErrorRemover.scala
[03/24/2023 05:42:25] Info: Transformed file DateConverterTest.scala
[03/24/2023 05:42:25] Info: Transformed file SparkContexts.scala
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'confs' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFileWithConfFile.scala'
Line number: '12'
Statement: 'def confs(confPath: String) = { Source.fromFile(confPath).getLines.map{ line => val data = line.split("\t") (data(0), data(1)) }.toMap }'
Detail: 'Can't OpenScope for symbol named: 'confs(scala.String)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'rpad' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/file/output/FixedFileWithConfFile.scala'
Line number: '40'
Statement: 'def rpad(target: String, len: Int, pad: String = " ") = { val str = if (target == null)       {       ""       } else       {       target       }  val strSize = str.getBytes("MS932").size  val padSize = len - strSize s"${ str }${ pad * padSize }" }'
Detail: 'Can't OpenScope for symbol named: 'rpad(scala.String,scala.Int,scala.String)''
[03/24/2023 05:42:25] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclSimpleExpr'.
[03/24/2023 05:42:25] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclSimpleExpr'.
[03/24/2023 05:42:25] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclSimpleExpr'.
[03/24/2023 05:42:25] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclPath1Id'.
[03/24/2023 05:42:25] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclPath1Id'.
[03/24/2023 05:42:25] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:25] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:25] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:25] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailCallFunction'.
[03/24/2023 05:42:25] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailExprDotId'.
[03/24/2023 05:42:25] Error: An error ocurred when checking IsApplicable for node of type 'Mobilize.Scala.AST.SclTailExprDotId'.
[03/24/2023 05:42:25] Info: Transformed file FixedFileWithConfFile.scala
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/DfToAny.scala'
Line number: '9'
Statement: 'def preExec(in: DataFrame)(implicit inArgs: InputArgs) : DataFrame = in'
Detail: 'Can't OpenScope for symbol named: 'preExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Info: Transformed file DfToAny.scala
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/AnyToFile.scala'
Line number: '10'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) = writeFile(df)'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Info: Transformed file AnyToFile.scala
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfJoinVariableToDf.scala'
Line number: '10'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Info: Transformed file DfJoinVariableToDf.scala
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/DbToAny.scala'
Line number: '10'
Statement: 'def preExec(in: Unit)(implicit inArgs: InputArgs) : DataFrame = readDb'
Detail: 'Can't OpenScope for symbol named: 'preExec(scala.Unit,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Info: Transformed file DbToAny.scala
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'timestamp_yyyyMMdd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeDate.scala'
Line number: '8'
Statement: 'def timestamp_yyyyMMdd(str: String) = Timestamp.valueOf(LocalDateTime.of(str.take(4).toInt, str.drop(4).take(2).toInt, str.drop(6).take(2).toInt, 0, 0, 0))'
Detail: 'Can't OpenScope for symbol named: 'timestamp_yyyyMMdd(scala.String)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'timestamp_yyyyMMddhhmmss' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeDate.scala'
Line number: '11'
Statement: 'def timestamp_yyyyMMddhhmmss(str: String) = Timestamp.valueOf(LocalDateTime.of( str.take(4).toInt, str.drop(4).take(2).toInt, str.drop(6).take(2).toInt,  str.drop(8).take(2).toInt, str.drop(10).take(2).toInt, str.drop(12).take(2).toInt))'
Detail: 'Can't OpenScope for symbol named: 'timestamp_yyyyMMddhhmmss(scala.String)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'timestamp_yyyyMMddhhmmssSSS' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeDate.scala'
Line number: '16'
Statement: 'def timestamp_yyyyMMddhhmmssSSS(str: String) = Timestamp.valueOf(LocalDateTime.of( str.take(4).toInt, str.drop(4).take(2).toInt, str.drop(6).take(2).toInt,  str.drop(8).take(2).toInt, str.drop(10).take(2).toInt, str.drop(12).take(2).toInt, str.drop(14).toInt * 1000000))'
Detail: 'Can't OpenScope for symbol named: 'timestamp_yyyyMMddhhmmssSSS(scala.String)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'date_yyyyMMdd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeDate.scala'
Line number: '21'
Statement: 'def date_yyyyMMdd(str: String) = new Date (timestamp_yyyyMMdd(str).getTime)'
Detail: 'Can't OpenScope for symbol named: 'date_yyyyMMdd(scala.String)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'timestamp_yyyymmdd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MakeDate.scala'
Line number: '37'
Statement: 'def timestamp_yyyymmdd(yyyy: Int, mm: Int, dd: Int) = Timestamp.valueOf(LocalDateTime.of(yyyy, mm, dd, 0, 0, 0))'
Detail: 'Can't OpenScope for symbol named: 'timestamp_yyyymmdd(scala.Int,scala.Int,scala.Int)''
[03/24/2023 05:42:25] Info: Transformed file MakeDate.scala
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/ReadDbTest.scala'
Line number: '77'
Statement: 'def readDbWhere(inArgs: InputArgs) = Array("NUM5 = '2000'")'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/ReadDbTest.scala'
Line number: '117'
Statement: 'def readDbWhere(inArgs: InputArgs) = Array("NUM5 = '2000'")'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/ReadDbTest.scala'
Line number: '210'
Statement: 'def readDbWhere(inArgs: InputArgs) = Array("NUM5 = '2000'")'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/ReadDbTest.scala'
Line number: '254'
Statement: 'def readDbWhere(inArgs: InputArgs) = Array("NUM5 = '2000'")'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:25] Info: Transformed file ReadDbTest.scala
[03/24/2023 05:42:25] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:25] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:25] Info: Transformed file GeneratingApplicationRouteFinder.scala
[03/24/2023 05:42:25] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/TwoAnyToPq.scala'
Line number: '10'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) = writeParquet(df)'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:25] Info: Transformed file TwoAnyToPq.scala
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'testDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/spark/common/DbCtlTest.scala'
Line number: '546'
Statement: 'def testDate(day: Int) = DateTime.parse(s"2016-1-${ day }").getMillis'
Detail: 'Can't OpenScope for symbol named: 'testDate(scala.Int)''
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'day' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/spark/common/DbCtlTest.scala'
Line number: '566'
Statement: 'def day(date: Timestamp) = new DateTime (date.getTime).getDayOfMonth'
Detail: 'Can't OpenScope for symbol named: 'day(_Unresolved.Timestamp)''
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'execAssertEquals' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '255'
Statement: 'def execAssertEquals(input: String, expected: String, targetUdf: UserDefinedFunction) {    val df = context.createDataFrame(Seq(Test(input)))  val result = df.withColumn("result", targetUdf(df("str"))).collect result(0).getAs[String]("result") mustBe expected }'
Detail: 'Can't OpenScope for symbol named: 'execAssertEquals(scala.String,scala.String,UserDefinedFunction)''
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'execGetStatus' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '447'
Statement: 'def execGetStatus(df: DataFrame, expected: String) = assertEquals(df.withColumn("result", getStatus(df("DT_BEGIN"), df("DT_END"), lit(MANG_DT_STR_TODAY))), expected)'
Detail: 'Can't OpenScope for symbol named: 'execGetStatus(DataFrame,scala.String)''
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'execGetStatusWithDeleteDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '451'
Statement: 'def execGetStatusWithDeleteDate(df: DataFrame, expected: String) = assertEquals(df.withColumn("result", getStatusWithDeleteDate(df("DT_BEGIN"), df("DT_END"), df("DT_DELETE"), lit(MANG_DT_STR_TODAY))), expected)'
Detail: 'Can't OpenScope for symbol named: 'execGetStatusWithDeleteDate(DataFrame,scala.String)''
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'execGetStatusWithBlankReplace' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '455'
Statement: 'def execGetStatusWithBlankReplace(df: DataFrame, expected: String) = assertEquals(df.withColumn("result", getStatusWithBlankReplace(df("DT_BEGIN"), df("DT_END"), lit(MANG_DT_STR_TODAY))), expected)'
Detail: 'Can't OpenScope for symbol named: 'execGetStatusWithBlankReplace(DataFrame,scala.String)''
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'execGetStatusWithBlankReplaceAndDeleteDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '459'
Statement: 'def execGetStatusWithBlankReplaceAndDeleteDate(df: DataFrame, expected: String) = assertEquals(df.withColumn("result", getStatusWithBlankReplaceAndDeleteDate(df("DT_BEGIN"), df("DT_END"), df("DT_DELETE"), lit(MANG_DT_STR_TODAY))), expected)'
Detail: 'Can't OpenScope for symbol named: 'execGetStatusWithBlankReplaceAndDeleteDate(DataFrame,scala.String)''
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'assertEquals' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '462'
Statement: 'def assertEquals(df: DataFrame, expected: String) {    val actual = df.collect()(0).getAs[String]("result") actual mustBe expected }'
Detail: 'Can't OpenScope for symbol named: 'assertEquals(DataFrame,scala.String)''
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'makeStringDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '466'
Statement: 'def makeStringDf(DT_BEGIN: String, DT_END: String, DT_DELETE: String) = context.createDataFrame(Seq(DateRangeTestStringType(DT_BEGIN, DT_END, DT_DELETE)))'
Detail: 'Can't OpenScope for symbol named: 'makeStringDf(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'makeDateDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '469'
Statement: 'def makeDateDf(DT_BEGIN: java.sql.Date, DT_END: java.sql.Date, DT_DELETE: java.sql.Date) = context.createDataFrame(Seq(DateRangeTestDateType(DT_BEGIN, DT_END, DT_DELETE)))'
Detail: 'Can't OpenScope for symbol named: 'makeDateDf(_Unresolved.Date,_Unresolved.Date,_Unresolved.Date)''
[03/24/2023 05:42:26] Info: Transformed file DfJoinVariableToDfTest.scala
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/TwoDfUnionToAny.scala'
Line number: '9'
Statement: 'def preExec(left: DataFrame, right: DataFrame)(implicit inArgs: InputArgs) : DataFrame = left.union(right)'
Detail: 'Can't OpenScope for symbol named: 'preExec(DataFrame,DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:26] Info: Transformed file TwoDfUnionToAny.scala
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/Executor.scala'
Line number: '7'
Statement: 'invoke(df: DataFrame)(implicit inArgs: InputArgs): DataFrame'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:26] Info: Transformed file Executor.scala
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/PqToAny.scala'
Line number: '10'
Statement: 'def preExec(in: Unit)(implicit inArgs: InputArgs) : DataFrame = readParquet'
Detail: 'Can't OpenScope for symbol named: 'preExec(scala.Unit,d2k.common.InputArgs)''
[03/24/2023 05:42:26] Info: Transformed file PqToAny.scala
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfJoinToDf.scala'
Line number: '10'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:26] Info: Transformed file DfJoinToDf.scala
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/SparkApp.scala'
Line number: '18'
Statement: 'exec(implicit inArgs: InputArgs): DataFrame'
Detail: 'Can't OpenScope for symbol named: 'exec(d2k.common.InputArgs)''
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'runner' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/SparkApp.scala'
Line number: '20'
Statement: 'def runner(args: Array[String], isDebug: Boolean = false) {    println(s"${ new DateTime toString (DATE_FORMAT) } INFO START")  val inputArgs = if (args.length == 8)       {       InputArgs(args(0), args(1), args(2), args(3), args(4), args(5), args(6), args(7))       } else       {       InputArgs(args(0), args(1), args(2), args(3))       } if (isDebug)       {       println(inputArgs)       } try       exec(inputArgs.copy(isDebug = isDebug)).sparkSession.stop    catch {       case e:Throwable => println(s"${ new DateTime toString (DATE_FORMAT) } ERROR ${ e.toString() }");throw e    } println(s"${ new DateTime toString (DATE_FORMAT) } INFO FINISHED") }'
Detail: 'Can't OpenScope for symbol named: 'runner(scala.Array[scala.String],scala.Boolean)''
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'main' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/SparkApp.scala'
Line number: '34'
Statement: 'def main(args: Array[String]) {    runner(args) }'
Detail: 'Can't OpenScope for symbol named: 'main(scala.Array[scala.String])''
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'debug' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/SparkApp.scala'
Line number: '38'
Statement: 'def debug(args: Array[String]) {    runner(args, true) }'
Detail: 'Can't OpenScope for symbol named: 'debug(scala.Array[scala.String])''
[03/24/2023 05:42:26] Info: Transformed file SparkApp.scala
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'errorLog' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/Logging.scala'
Line number: '8'
Statement: 'def errorLog(message: String, t: Throwable) = logger.error(message, t)'
Detail: 'Can't OpenScope for symbol named: 'errorLog(scala.String,Throwable)''
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'elapse' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/Logging.scala'
Line number: '12'
Statement: 'def elapse(message: String)(func: =>Unit) = { logger.info(s" Start[${ message }]")  val startTime = System.currentTimeMillis func  val endTime = System.currentTimeMillis  val elapse = BigDecimal(endTime - startTime) / 1000 logger.info(f"finish[${ message }] elapse:${ elapse }%,.3fs") }'
Detail: 'Can't OpenScope for symbol named: 'elapse(scala.String,=>Unit)''
[03/24/2023 05:42:26] Info: Transformed file Logging.scala
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:26] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprUnderscore' is not supported
[03/24/2023 05:42:26] Info: Transformed file BinaryRecordTest.scala
[03/24/2023 05:42:26] Info: Transformed file DbConnectionInfoTest.scala
[03/24/2023 05:42:26] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/TwoPqJoinToAny.scala'
Line number: '16'
Statement: 'def preExec(in1: Unit, in2: Unit)(implicit inArgs: InputArgs) : DataFrame = { val left = readParquetSingle(leftPqName)  val right = readParquetSingle(rightPqName)  val joined = left.join(right, joinExprs(left, right), joinType) joined.select(select(left, right).toArray :_*) }'
Detail: 'Can't OpenScope for symbol named: 'preExec(scala.Unit,scala.Unit,d2k.common.InputArgs)''
[03/24/2023 05:42:26] Info: Transformed file TwoPqJoinToAny.scala
[03/24/2023 05:42:27] Info: Transformed file FileCtlTest.scala
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/src/FlowLogicGenerator.scala'
Line number: '26'
Statement: 'def apply(target: Seq[(String, String)]) = { val flowMap = target.foldLeft(Map.empty[String, Seq[String]]){(l, r) => val (a, b) = r l.updated(b, l.get(b).getOrElse(Seq.empty[String]) :+ a) }  def conv(s: String) : Tree = {    val flowId = flowMap.get(s).getOrElse(Seq.empty[String]) flowId.size match {          case 0 => Top(s)          case 1 => Node(s, conv(flowId.head))          case 2 => Join(s, conv(flowId(0)), conv(flowId(1)))       }    } flowMap("CfEnd").map( e =>conv(e).toFlow).mkString("\n\n") }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.Seq[scala.Tuple2[scala.String,scala.String]])''
[03/24/2023 05:42:27] Info: Transformed file FlowLogicGenerator.scala
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/TwoAnyToDb.scala'
Line number: '10'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) = writeDb(df)'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:27] Info: Transformed file TwoAnyToDb.scala
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'mkDbInfo' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/ReadDb.scala'
Line number: '9'
Statement: 'def mkDbInfo(envLabel: String) = DbInfo(sys.env(s"DB_URL_$envLabel"), sys.env(s"DB_USER_$envLabel"), sys.env(s"DB_PASSWORD_$envLabel"))'
Detail: 'Can't OpenScope for symbol named: 'mkDbInfo(scala.String)''
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'readDbWhere' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/ReadDb.scala'
Line number: '31'
Statement: 'def readDbWhere(inArgs: InputArgs) : Array[String] = Array.empty[String]'
Detail: 'Can't OpenScope for symbol named: 'readDbWhere(d2k.common.InputArgs)''
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'readDbSingle' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/ReadDb.scala'
Line number: '38'
Statement: 'def readDbSingle(tableName: String)(implicit inArgs: InputArgs) = { val tblName = inArgs.tableNameMapper.get(componentId).getOrElse(tableName)  val dbCtl = new DbCtl (readDbInfo)  val readDbWhereWithArgs = readDbWhere(inArgs) (readDbWhere.isEmpty, readDbWhereWithArgs.isEmpty) match {       case (true, true) => selectReadTable(dbCtl, tblName)       case (false, true) => selectReadTable(dbCtl, tblName, readDbWhere)       case (true, false) => selectReadTable(dbCtl, tblName, readDbWhereWithArgs)       case (false, false) => throw new IllegalArgumentException ("Can not defined both readDbWhere and readDbWhere(inArgs)")    } }'
Detail: 'Can't OpenScope for symbol named: 'readDbSingle(scala.String,d2k.common.InputArgs)''
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'selectReadTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/ReadDb.scala'
Line number: '50'
Statement: 'def selectReadTable(dbCtl: DbCtl, tableName: String, readDbWhere: Array[String] = Array.empty[String]) = { (columns.isEmpty, readDbWhere.isEmpty) match {       case (true, true) => dbCtl.readTable(tableName)       case (true, false) => dbCtl.readTable(tableName, readDbWhere)       case (false, true) => dbCtl.readTable(tableName, columns, Array("1 = 1"))       case (false, false) => dbCtl.readTable(tableName, columns, readDbWhere)    } }'
Detail: 'Can't OpenScope for symbol named: 'selectReadTable(DbCtl,scala.String,scala.Array[scala.String])''
[03/24/2023 05:42:27] Info: Transformed file ReadDb.scala
[03/24/2023 05:42:27] Info: Transformed file ResourceDates.scala
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'colData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/spark/common/LargeInsertTest.scala'
Line number: '30'
Statement: 'def colData(rec: Int) = (1 to 10).map( cnt =>s"${ cnt }_${ rec }").toSeq'
Detail: 'Can't OpenScope for symbol named: 'colData(scala.Int)''
[03/24/2023 05:42:27] Info: Transformed file LargeInsertTest.scala
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/AppDefParser.scala'
Line number: '12'
Statement: 'def apply(baseUrl: String, branch: String, appGroup: String, appId: String) = { val parsed = parseAll(appDef, readAppDefMd(baseUrl, branch, appGroup, appId, "README.md")) println(parsed) parsed }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/AppDefParser.scala'
Line number: '18'
Statement: 'def apply(baseUrl: String, appGroup: String, appId: String) = { val parsed = parseAll(appDef, readAppDefMd(baseUrl, appGroup, appId, "README.md")) println(parsed) parsed }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/parser/AppDefParser.scala'
Line number: '24'
Statement: 'def apply(path: String) = { parseAll(appDef, readAppDefMd(path)) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:27] Info: Transformed file AppDefParser.scala
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'testCutLimitStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '764'
Statement: 'def testCutLimitStr(input: String, cutLen: Int, exp: String) = { import SparkContexts.context.implicits._  val df = SparkContexts.sc.makeRDD( Seq(Test(input))).toDF  val result = df.withColumn("result", Udfs.cutLimitStr(col("str"), lit(cutLen))).collect result(0).getAs[String]("result") mustBe exp }'
Detail: 'Can't OpenScope for symbol named: 'testCutLimitStr(scala.String,scala.Int,scala.String)''
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'testCalcSchoolAge' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '783'
Statement: 'def testCalcSchoolAge(birthY: Int, birthM: Int, birthD: Int, runningYMD: String, exp: Int) = { import SparkContexts.context.implicits._  val cal = Calendar.getInstance cal.set(Calendar.YEAR, birthY) cal.set(Calendar.MONTH, birthM) cal.set(Calendar.DATE, birthD)  val df = SparkContexts.sc.makeRDD( Seq(DateTest(runningYMD, null, new java.sql.Timestamp (cal.getTime.getTime)))).toDF  val result = df.withColumn("result", Udfs.calcSchoolAge(df("sqlTimestamp"), df("str"))).collect result(0).getAs[Integer]("result") mustBe exp }'
Detail: 'Can't OpenScope for symbol named: 'testCalcSchoolAge(scala.Int,scala.Int,scala.Int,scala.String,scala.Int)''
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'testCalcAge' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/UdfsTest.scala'
Line number: '824'
Statement: 'def testCalcAge(birthYMD: String, runningYMD: String, exp: Int) = { var simpleDateFormat = new SimpleDateFormat ("yyyyMMdd")  var date = simpleDateFormat.parse(birthYMD)  val df = SparkContexts.sc.makeRDD( Seq(DateTest(runningYMD, null, new java.sql.Timestamp (date.getTime)))).toDF  val result = df.withColumn("result", Udfs.calcAge(df("sqlTimestamp"), df("str"))).collect result(0).getAs[Integer]("result") mustBe exp }'
Detail: 'Can't OpenScope for symbol named: 'testCalcAge(scala.String,scala.String,scala.Int)''
[03/24/2023 05:42:27] Info: Transformed file UdfsTest.scala
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'getMD5Str' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MD5Utils.scala'
Line number: '8'
Statement: 'def getMD5Str(targetStr: String) : String = { val md5 = MessageDigest.getInstance("MD5")  val md5Data = md5.digest(targetStr.getBytes(charEnc)) md5Data.foldLeft(""){(l, r) => val i: Int = r.asInstanceOf[Int]  val result = if (i < 0)       {       i + 256       } else       {       i       } if (result < 16)       {       s"${ l }0${ Integer.toHexString(result) }"       } else       {       s"${ l }${ Integer.toHexString(result) }"       } } }'
Detail: 'Can't OpenScope for symbol named: 'getMD5Str(scala.String)''
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'getMD5Str' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MD5Utils.scala'
Line number: '26'
Statement: 'def getMD5Str(targetStr: String, md5WordStr: String) : String = getMD5Str(targetStr.trim + getMD5Str(md5WordStr))'
Detail: 'Can't OpenScope for symbol named: 'getMD5Str(scala.String,scala.String)''
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'getMD5Base64Str' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/MD5Utils.scala'
Line number: '29'
Statement: 'def getMD5Base64Str(targetStr: String, md5WordStr: String, flag: Boolean = true) : String = { val bytes = MessageDigest.getInstance("MD5").digest((targetStr.trim + getMD5Str(md5WordStr)).getBytes(charEnc))  val base64 = new String (Base64.encodeBase64(bytes)) if (flag && base64.takeRight(2) == "==")       {       base64.dropRight(2)       } else       {       base64       } }'
Detail: 'Can't OpenScope for symbol named: 'getMD5Base64Str(scala.String,scala.String,scala.Boolean)''
[03/24/2023 05:42:27] Info: Transformed file MD5Utils.scala
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'check' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/component/sh/CommissionBaseChannelSelectorTest.scala'
Line number: '26'
Statement: 'def check(key: String, DV_OUTOBJDIV: String, DV_TRICALCOBJDIV: String) = { (df: DataFrame) =>df.filter($"key" === key).collect.foreach{ row =>row.getAs[String]("DV_OUTOBJDIV") mustBe DV_OUTOBJDIV row.getAs[String]("DV_TRICALCOBJDIV") mustBe DV_TRICALCOBJDIV } df }'
Detail: 'Can't OpenScope for symbol named: 'check(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:27] Info: Transformed file DfJoinPqToDfTest.scala
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'checkPostCode' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/PostCodeNormalizer.scala'
Line number: '11'
Statement: 'def checkPostCode(sizeList: Seq[Int])(postCode: String) = Option(postCode).flatMap{ p =>if (p.forall(_.isDigit) && sizeList.contains(p.size))    Some(p) else    None }.getOrElse("")'
Detail: 'Can't OpenScope for symbol named: 'checkPostCode(scala.Seq[scala.Int],scala.String)''
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/PostCodeNormalizer.scala'
Line number: '18'
Statement: 'def apply(postCode: String) : String = Option(postCode).map{ pCode => val splitted = pCode.split("-") splitted.size match {    case EXIST_HYPHEN => val p = parent(splitted(PARENT_POSITION))  val c = child(splitted(CHILE_POSITION)) if (c.isEmpty)       p else       s"${ p }-${ c }"    case NO_HYPHEN => val target = splitted.head  val parentSize = PARENT_CORRECT_SIZE.head parent(target.take(parentSize)) + child(target.drop(parentSize)) } }.getOrElse("")'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'single' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/PostCodeNormalizer.scala'
Line number: '32'
Statement: 'def single(postCode: String) : String = apply(postCode)'
Detail: 'Can't OpenScope for symbol named: 'single(scala.String)''
[03/24/2023 05:42:27] Info: Transformed file PostCodeNormalizer.scala
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'd2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteDbTest.scala'
Line number: '25'
Statement: 'def d2s(dateMill: Long) = new DateTime (dateMill).toString("yyyy-MM-dd")'
Detail: 'Can't OpenScope for symbol named: 'd2s(scala.Long)''
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'd2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteDbTest.scala'
Line number: '26'
Statement: 'def d2s(date: Date) = new DateTime (date).toString("yyyy-MM-dd")'
Detail: 'Can't OpenScope for symbol named: 'd2s(_Unresolved.Date)''
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'd2s' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteDbTest.scala'
Line number: '27'
Statement: 'def d2s(date: Timestamp) = new DateTime (date).toString("yyyy-MM-dd hh:mm:ss")'
Detail: 'Can't OpenScope for symbol named: 'd2s(_Unresolved.Timestamp)''
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Info: Transformed file CommissionBaseChannelSelectorTest.scala
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'splitIdAndName' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/rc/RoughConceptGenerator.scala'
Line number: '16'
Statement: 'def splitIdAndName(s: String) = { val splitted = s.split('[') (splitted(0).trim, splitted(1).trim.dropRight(1)) }'
Detail: 'Can't OpenScope for symbol named: 'splitIdAndName(scala.String)''
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name '+=' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/rc/RoughConceptGenerator.scala'
Line number: '79'
Statement: 'def +=(rcd: RCData) = this.copy( if (rcd.obj.isEmpty)    objs else    rcd.obj :: objs,  if (rcd.frame.isEmpty)    frames else    rcd.frame :: frames,  if (rcd.link.isEmpty)    links else    rcd.link :: links)'
Detail: 'Can't OpenScope for symbol named: '+=(d2k.appdefdoc.gen.rc.RCData)''
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'fileToStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/rc/RoughConceptGenerator.scala'
Line number: '113'
Statement: 'def fileToStr(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString'
Detail: 'Can't OpenScope for symbol named: 'fileToStr(scala.String)''
[03/24/2023 05:42:27] Error: An error ocurred at 'OpenScope for node with name 'objToMd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/rc/RoughConceptGenerator.scala'
Line number: '151'
Statement: 'def objToMd(rcds: RCDataStore) = { s"""${ rcds.objs.mkString("\n") }        frame ${ appdef.appInfo.id } { ${ rcds.frames.mkString("\n") } }  ${ (rcds.links ++ links).mkString("\n") }  !include ps.puml""" }'
Detail: 'Can't OpenScope for symbol named: 'objToMd(d2k.appdefdoc.gen.rc.RCDataStore)''
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:27] Info: Transformed file RoughConceptGenerator.scala
[03/24/2023 05:42:28] Info: Transformed file TestToolsTest.scala
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Info: Transformed file DomainProcessorJefTest.scala
[03/24/2023 05:42:28] Info: Transformed file DbCtlTest.scala
[03/24/2023 05:42:28] Info: Transformed file DfUnionToDfTest.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/BinaryRecordConverter.scala'
Line number: '16'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) : DataFrame = BinaryRecordConverter(binaryRecordName, itemConfId, charEnc)(df)'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/BinaryRecordConverter.scala'
Line number: '20'
Statement: 'def apply(binaryRecordName: String, itemConfId: String, charEnc: String)(df: DataFrame)(implicit inArgs: InputArgs) : DataFrame = { val itemConfs = ConfParser.parseItemConf(Path(inArgs.fileConvInputFile).toAbsolute.parent, inArgs.projectId, itemConfId).toList  val len = itemConfs.map(_.length.toInt)  val names = itemConfs.map(_.itemId)  val domains = itemConfs.map(_.cnvType)  def makeSliceLen(len: Seq[Int]) = len.foldLeft((0, List.empty[(Int, Int)])){(l, r) =>(l._1 + r, l._2 :+ (l._1, l._1 + r))}  val (totalLen_, sliceLen) = makeSliceLen(len)  val ziped = names.zip(domains)  val (nameList, domainList) = ziped.filter{    case (names, domain) => !(domain.startsWith(Converter.NOT_USE_PREFIX))    }.unzip  def cnvFromFixed(names: Seq[String], domains: Seq[String], sliceLen: List[(Int, Int)])(inData: Array[Byte]) = {    val dataAndDomainsAndNames = sliceLen.map{       case (start, end) => inData.slice(start, end)       }.zip(domains).zip(names)  val result = Converter.domainConvert(dataAndDomainsAndNames, charEnc) Row.fromSeq(result)    }  val droppedDf = df.drop("ROW_ERR").drop("ROW_ERR_MESSAGE")  val rdd = droppedDf.rdd.map{ orgRow => val row = cnvFromFixed(names, domains, sliceLen)(orgRow.getAs[Array[Byte]](binaryRecordName)) Row.merge(orgRow, row) }  val schema = Converter.makeSchema(nameList).foldLeft(droppedDf.schema){(l, r) =>l.add(r.name, r.dataType)} SparkContexts.context.createDataFrame(rdd, schema).drop(binaryRecordName) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String,DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Info: Transformed file BinaryRecordConverter.scala
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Info: Transformed file JefConverterTest.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'toSchema' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/ReadPq.scala'
Line number: '9'
Statement: 'def toSchema(names: Seq[String]) = names.map{ name =>name.split("_").toList.headOption.map{ case "DT" => "date" case "NM" | "AM" => "decimal" case _ => "string" } }'
Detail: 'Can't OpenScope for symbol named: 'toSchema(scala.Seq[scala.String])''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'readPqPath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/ReadPq.scala'
Line number: '19'
Statement: 'def readPqPath(implicit inArgs: InputArgs) : String = inArgs.baseInputFilePath'
Detail: 'Can't OpenScope for symbol named: 'readPqPath(d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'readParquetSingle' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/ReadPq.scala'
Line number: '32'
Statement: 'def readParquetSingle(pqName: String)(implicit inArgs: InputArgs) = { val pqCtl = new PqCtl (readPqPath) pqCtl.readParquet(pqName, readPqStrictCheckMode, readPqEmptySchema) }'
Detail: 'Can't OpenScope for symbol named: 'readParquetSingle(scala.String,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Info: Transformed file ReadPq.scala
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Info: Transformed file ItemRenameRouteFinder.scala
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprTypeArgs' is not supported
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'readPqPath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteFileTest.scala'
Line number: '286'
Statement: 'def readPqPath(implicit inArgs: InputArgs) : String = s"test/dev/data/mypath"'
Detail: 'Can't OpenScope for symbol named: 'readPqPath(d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'writePqPath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteFileTest.scala'
Line number: '287'
Statement: 'def writePqPath(implicit inArgs: InputArgs) : String = s"test/dev/data/mypath"'
Detail: 'Can't OpenScope for symbol named: 'writePqPath(d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'writeFilePath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteFileTest.scala'
Line number: '288'
Statement: 'def writeFilePath(implicit inArgs: InputArgs) : String = s"test/dev/data/mypath2"'
Detail: 'Can't OpenScope for symbol named: 'writeFilePath(d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'readPqPath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteFileTest.scala'
Line number: '306'
Statement: 'def readPqPath(implicit inArgs: InputArgs) : String = s"test/dev/data/mypath"'
Detail: 'Can't OpenScope for symbol named: 'readPqPath(d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'writePqPath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteFileTest.scala'
Line number: '307'
Statement: 'def writePqPath(implicit inArgs: InputArgs) : String = s"test/dev/data/mypath"'
Detail: 'Can't OpenScope for symbol named: 'writePqPath(d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'writeFilePath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/df/WriteFileTest.scala'
Line number: '308'
Statement: 'def writeFilePath(implicit inArgs: InputArgs) : String = s"test/dev/data/mypath"'
Detail: 'Can't OpenScope for symbol named: 'writeFilePath(d2k.common.InputArgs)''
[03/24/2023 05:42:28] Info: Transformed file FileToPq_DbTest.scala
[03/24/2023 05:42:28] Info: Transformed file WriteFileTest.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'write' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/test/GenerateTestCase.scala'
Line number: '11'
Statement: 'def write(outputBasePath: String = s"data/testGen") = { writeTestCase(outputBasePath) writeInputMd(outputBasePath) writeOutputMd(outputBasePath) }'
Detail: 'Can't OpenScope for symbol named: 'write(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'writeTestCase' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/test/GenerateTestCase.scala'
Line number: '16'
Statement: 'def writeTestCase(outputBasePath: String) = { val writePath = Path(s"${ outputBasePath }/${ groupId }") writePath.createDirectory(failIfExists = false)  val outPath = writePath / s"${ appId }Test.scala" outPath.toFile.writeAll(testCase) }'
Detail: 'Can't OpenScope for symbol named: 'writeTestCase(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'writeInputMd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/test/GenerateTestCase.scala'
Line number: '23'
Statement: 'def writeInputMd(outputBasePath: String) = { val writePath = Path(s"${ outputBasePath }/markdown/${ appId }/AT") writePath.createDirectory(failIfExists = false) inputMdData.foreach{    case (ioMd, tableData) => val outPath = writePath / s"${ ioMd.id }.md"  val outputData = s"# ${ ioMd.name }\n${ tableData }" outPath.toFile.writeAll(tableData)    } }'
Detail: 'Can't OpenScope for symbol named: 'writeInputMd(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'writeOutputMd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/test/GenerateTestCase.scala'
Line number: '34'
Statement: 'def writeOutputMd(outputBasePath: String) = { val writePath = Path(s"${ outputBasePath }/markdown/${ appId }/AT") writePath.createDirectory(failIfExists = false) outputMdData.foreach{    case (ioMd, tableData) => val outPath = writePath / s"${ ioMd.id }.md"  val outputData = s"# ${ ioMd.name }\n${ tableData }" outPath.toFile.writeAll(tableData)    } }'
Detail: 'Can't OpenScope for symbol named: 'writeOutputMd(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/test/GenerateTestCase.scala'
Line number: '47'
Statement: 'def apply(baseUrl: String) = new GenerateTestCase (baseUrl)'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'generate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/test/GenerateTestCase.scala'
Line number: '51'
Statement: 'def generate(branch: String, appGroup: String, appId: String) = { val appBaseUrl = s"${ baseUrl }/raw/${ branch }/apps/${ appGroup }/${ appId }"  val itemsBaseUrl = s"${ baseUrl }/raw/${ branch }/apps/common/items"  val url = s"${ appBaseUrl }/README.md"  val md = Source.fromURL(s"${ url }?private_token=${ sys.env("GITLAB_TOKEN") }").getLines.toList  val ioList = md.filter(!_.isEmpty).dropWhile( line =>!line.contains("## 03. 入出力データ一覧"))  val inputList = ioList.drop(2).takeWhile(!_.contains("### 03.02. 出力")).drop(2)  val outputList = ioList.dropWhile(!_.contains("### 03.02. 出力")).drop(1)  def strToIoMdInfo(str: String) = {    val ioInfoRegx = "\\|\\s*\\[(.*)]\\((.*)\\)\\s*\\|(.*)\\|(.*)\\|".r ioInfoRegx.findFirstMatchIn(str).map( g =>IoMdInfo(g.group(3).trim, g.group(1).trim, g.group(4).trim, g.group(2).trim))    }  val ioTypeToCnvMethodName = (ioType: String, appId: String) =>ioType.toLowerCase match {       case "pq" => s"""toPq("${ appId }")"""       case "db" => s"""toDb("${ appId }")"""       case "jef" => s"""toJef("${ appId }")"""       case "file(fixed)" => "toFixed(\"writePath\")"       case "file(csv)" => "toCsv(\"writePath\")"       case "file(tsv)" => "toTsv(\"writePath\")"    }  val ioTypeToCheckMethodName = (ioType: String, appId: String) =>ioType.toLowerCase match {       case "pq" => s"""checkPq("${ appId }.pq")"""       case "db" => s"""checkDb("${ appId }")"""       case "file(fixed)" => "checkFixed(\"writePath\")"       case "file(csv)" => "checkCsv(\"writePath\")"       case "file(tsv)" => "checkTsv(\"writePath\")"    }  val tableTemplate = "        //%%inputDataName%%\n        %%inputData%%.%%inputConvMethod%%"  def imiToTemplate(imi: IoMdInfo, ioTypeCnv: (String, String) => String) = {    val itemName = imi.path.split("/").takeRight(2).mkString("/") tableTemplate.replaceAllLiterally("%%inputDataName%%", imi.name).replaceAllLiterally("%%inputData%%", s"""x.readMdTable("${ imi.id }.md")""").replaceAllLiterally("%%inputConvMethod%%", ioTypeCnv(imi.ioType, imi.id))    }  def imiToMdData(imi: IoMdInfo) = {    println(s"read:${ imi.id }:${ imi.name }")  val itemName = imi.path.split("/").takeRight(3).mkString("/") s"# ${ imi.name }\n${ MakeResource.itemsMdToTable(s"${ itemsBaseUrl }/${ itemName }").getOrElse("") }\n"    }  val template = Option(getClass.getClassLoader.getResourceAsStream("genTemplates/testcaseAt.tmpl")).map( is =>Source.fromInputStream(is)).get.mkString  val inputInfos = inputList.flatMap(strToIoMdInfo).map( d =>imiToTemplate(d, ioTypeToCnvMethodName))  val inputMdData = inputList.flatMap(strToIoMdInfo).map( d =>(d, imiToMdData(d)))  val outputInfos = outputList.flatMap(strToIoMdInfo).map( d =>imiToTemplate(d, ioTypeToCheckMethodName))  val outputMdData = outputList.flatMap(strToIoMdInfo).map( d =>(d, imiToMdData(d)))  val testCaseStr = template.replaceAllLiterally("%%APP_NAME%%", appId).replaceAllLiterally("%%PROJECT_ID%%", appGroup).replaceAllLiterally("%%READ_DATA%%", inputInfos.mkString("\n\n")).replaceAllLiterally("%%CHECK_DATA%%", outputInfos.mkString("\n\n")) OutputData(appGroup, appId, testCaseStr, inputMdData, outputMdData) }'
Detail: 'Can't OpenScope for symbol named: 'generate(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:28] Info: Transformed file GenerateTestCase.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/DfJoinMultiPqToAny.scala'
Line number: '19'
Statement: 'def preExec(left: DataFrame)(implicit inArgs: InputArgs) : DataFrame = { val inFilePath = inputFilePath.getOrElse(inArgs.baseInputFilePath)  val orgDf = left.columns.foldLeft(left)((df, name) =>df.withColumnRenamed(name, s"$prefixName#$name")) joinPqInfoList.foldLeft(orgDf){(odf, pqInfo) => val pqDf = new PqCtl (inFilePath).readParquet(pqInfo.name)  val pname = if (pqInfo.prefixName.isEmpty)       pqInfo.name else       pqInfo.prefixName  val addNameDf = pqDf.columns.foldLeft(pqDf){(df, name) =>df.withColumnRenamed(name, s"$pname#$name") }  val joinedDf = odf.join(addNameDf, pqInfo.joinExprs, pqInfo.joinType) pqInfo.dropCols.foldLeft(joinedDf)((l, r) =>l.drop(r)) } }'
Detail: 'Can't OpenScope for symbol named: 'preExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Info: Transformed file DfJoinMultiPqToAny.scala
[03/24/2023 05:42:28] Info: Transformed file FileConvPartition2Test.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/component/cmn/PostCodeConverter.scala'
Line number: '25'
Statement: 'def apply()(implicit inArgs: InputArgs) = new PostCodeConverter'
Detail: 'Can't OpenScope for symbol named: 'apply(d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'localGovernmentCode' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/component/cmn/PostCodeConverter.scala'
Line number: '32'
Statement: 'def localGovernmentCode(postCodeName1: String, postCodeName2: String = "")(outName1: String, outName2: String = "") = cnvLocalGovernmentCode(postCodeName1, postCodeName2)(outName1, outName2)'
Detail: 'Can't OpenScope for symbol named: 'localGovernmentCode(scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'cnvLocalGovernmentCode' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/component/cmn/PostCodeConverter.scala'
Line number: '35'
Statement: 'def cnvLocalGovernmentCode(postCodeName1: String, postCodeName2: String)(outName1: String, outName2: String) = (df: DataFrame) =>{ val convertUdf = udf{(inPostCode: String) => val postCode = Option(inPostCode).map(_.replaceAllLiterally("-", "").trim).getOrElse("")  def code3 = postMap.get(postCode)  def code5 = postMap.get(postCode).orElse{ postMap.get(postCode.take(3)) }  def code7 = postMap.get(postCode).orElse{ postMap.get(postCode.take(3) + "0000") }.orElse{ postMap.get(postCode.take(3)) } (postCode.size match {       case 3 => code3       case 5 => code5       case 7 => code7       case _ => None    }).getOrElse(("", "")) }  val postCodeCol = if (postCodeName2.isEmpty)       col(postCodeName1) else       concat(trim(col(postCodeName1)), col(postCodeName2))  val df2 = df ~> editColumns(Seq(("_POSTCODES_", convertUdf(postCodeCol)).e))  val outCol = if (outName2.isEmpty)       Seq((outName1, $"_POSTCODES_._1").e) else       Seq((outName1, $"_POSTCODES_._1").e, (outName2, $"_POSTCODES_._2").e) (df2 ~> editColumns(outCol)).drop("_POSTCODES_").na.fill("", Seq(outName1, outName2)).na.replace(outName1, Map("" -> "99")).na.replace(outName2, Map("" -> "999")) }'
Detail: 'Can't OpenScope for symbol named: 'cnvLocalGovernmentCode(scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:28] Info: Transformed file PostCodeConverter.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/TwoInToOneOut.scala'
Line number: '6'
Statement: 'preExec(in1: IN1, in2: IN2)(implicit inArgs: InputArgs): PREOUT'
Detail: 'Can't OpenScope for symbol named: 'preExec(d2k.common.df.flow.base.TwoInToOneOut[IN1,IN2,PREOUT,MID,POSTIN,OUT].IN1,d2k.common.df.flow.base.TwoInToOneOut[IN1,IN2,PREOUT,MID,POSTIN,OUT].IN2,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/TwoInToOneOut.scala'
Line number: '8'
Statement: 'exec(df: MID)(implicit inArgs: InputArgs): MID'
Detail: 'Can't OpenScope for symbol named: 'exec(d2k.common.df.flow.base.TwoInToOneOut[IN1,IN2,PREOUT,MID,POSTIN,OUT].MID,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/TwoInToOneOut.scala'
Line number: '10'
Statement: 'postExec(df: POSTIN)(implicit inArgs: InputArgs): OUT'
Detail: 'Can't OpenScope for symbol named: 'postExec(d2k.common.df.flow.base.TwoInToOneOut[IN1,IN2,PREOUT,MID,POSTIN,OUT].POSTIN,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'run' of type 'Mobilize.Scala.AST.SclFunDcl' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/flow/base/TwoInToOneOut.scala'
Line number: '12'
Statement: 'run(in1: IN1, in2: IN2)(implicit inArgs: InputArgs): OUT'
Detail: 'Can't OpenScope for symbol named: 'run(d2k.common.df.flow.base.TwoInToOneOut[IN1,IN2,PREOUT,MID,POSTIN,OUT].IN1,d2k.common.df.flow.base.TwoInToOneOut[IN1,IN2,PREOUT,MID,POSTIN,OUT].IN2,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Info: Transformed file TwoInToOneOut.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/sql/SqlLogicParser.scala'
Line number: '9'
Statement: 'def apply(baseUrl: String, branch: String, appGroup: String, appId: String) = { (parser_ andThen replaceComment_)(readAppDefMd(baseUrl, branch, appGroup, appId, "README.md")) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String,scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/sql/SqlLogicParser.scala'
Line number: '13'
Statement: 'def apply(baseUrl: String, appGroup: String, appId: String) = { (parser_ andThen replaceComment_)(readAppDefMd(baseUrl, appGroup, appId, "README.md")) }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'parser' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/sql/SqlLogicParser.scala'
Line number: '17'
Statement: 'def parser(s: String) = s.split("```sql")(1).split("```")(0)'
Detail: 'Can't OpenScope for symbol named: 'parser(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'replaceComment' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/sql/SqlLogicParser.scala'
Line number: '23'
Statement: 'def replaceComment(inStr: String) = { Seq( regxChar.findAllMatchIn(inStr).map( x =>(x.toString, x.group(1))),  regxItem.findAllMatchIn(inStr).map( x =>(x.toString, x.group(1))),  regxDecimal.findAllMatchIn(inStr).map( x =>(x.toString, x.group(1))),  regxParent.findAllMatchIn(inStr).map( x =>(x.toString, s"""${ x.group(1) }."""))).reduce(_ ++ _).toList.sortBy( x =>x._1.size).reverse.foldLeft(inStr){(l, r) =>l.replaceAllLiterally(r._1, r._2)} }'
Detail: 'Can't OpenScope for symbol named: 'replaceComment(scala.String)''
[03/24/2023 05:42:28] Info: Transformed file SqlLogicParser.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/MultiAnyToMapDf.scala'
Line number: '10'
Statement: 'def postExec(df: Map[String, DataFrame])(implicit inArgs: InputArgs) : Map[String, DataFrame] = df'
Detail: 'Can't OpenScope for symbol named: 'postExec(Map[String,DataFrame],d2k.common.InputArgs)''
[03/24/2023 05:42:28] Info: Transformed file MultiAnyToMapDf.scala
[03/24/2023 05:42:28] Info: Transformed file KanaConverterTest.scala
[03/24/2023 05:42:28] Info: Transformed file ResourceItemRouteFinder.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '59'
Statement: 'def exec(domain: String, data: String) : Either[String, String] = dp{ domain match {    case "年月日" => convDate(data, "00010101", "99991231", FORMATTER_YYYYMMDD)    case "年月日_SL" => convDate(data.replaceAll("/", ""), "00010101", "99991231", FORMATTER_YYYYMMDD)    case "年月日_HY" => convDate(data.replaceAll("-", ""), "00010101", "99991231", FORMATTER_YYYYMMDD)    case "年月" => convDate(data, "000101", "999912", FORMATTER_YYYYMM)    case "年月_SL" => convDate(data.replaceAll("/", ""), "000101", "999912", FORMATTER_YYYYMM)    case "月日" => convDate(data, "0101", "1231", FORMATTER_MMDD)    case "年" => convDateParts(data, "0001", "9999")    case "月" => convDateParts(data, "01", "12")    case "日" => convDateParts(data, "01", "31")    case "年月日時分秒" => convDate(data, "00010101000000", "99991231235959", FORMATTER_YYYYMMDDHHMMSS)    case "年月日時分秒_HC" => convDate(data.replaceAll("[- :]", ""), "00010101000000", "99991231235959", FORMATTER_YYYYMMDDHHMMSS)    case "年月日時分秒_SC" => convDate(data.replaceAll("[/ :]", ""), "00010101000000", "99991231235959", FORMATTER_YYYYMMDDHHMMSS)    case "年月日時分秒_CO" => convDate(data.replaceAll("[ :]", ""), "00010101000000", "99991231235959", FORMATTER_YYYYMMDDHHMMSS)    case "年月日時分ミリ秒" => convTimeStamp(data, "00010101000000000", "99991231235959999", FORMATTER_YYYYMMDDHHMMSSMS, FORMAT_YYYYMMDDHHMMSSMS)    case "年月日時分ミリ秒/ミリ秒小数点付加" => addPeriod(convTimeStamp(data, "00010101000000000", "99991231235959999", FORMATTER_YYYYMMDDHHMMSSMS, FORMAT_YYYYMMDDHHMMSSMS), 14)    case "年月日時" => convDate(data, "0001010100", "9999123123", FORMATTER_YYYYMMDDHH)    case "年月日時分" => convDate(data, "000101010000", "999912312359", FORMATTER_YYYYMMDDHHMM)    case "年月日時分_SC" => convDate(data.replaceAll("[/ :]", ""), "000101010000", "999912312359", FORMATTER_YYYYMMDDHHMM)    case "時分秒" => convDate(data, "000000", "235959", FORMATTER_HHMMSS)    case "時分秒_CO" => convDate(data.replaceAll(":", ""), "000000", "235959", FORMATTER_HHMMSS)    case "時分ミリ秒" => convTimeStamp(data, "000000000", "235959999", FORMATTER_HHMMSSMS, FORMAT_HHMMSSMS)    case "時分ミリ秒/ミリ秒小数点付加" => addPeriod(convTimeStamp(data, "000000000", "235959999", FORMATTER_HHMMSSMS, FORMAT_HHMMSSMS), 6)    case "時分" => convDate(data, "0000", "2359", FORMATTER_HHMM)    case "時" => convDateParts(data, "00", "23")    case "分" => convDateParts(data, "00", "59")    case "秒" => convDateParts(data, "00", "59")    case "時間" => convTime(data, "000000", "995959")    case "数字" => convDigit(data)    case "数字_SIGNED" => convSignedDigit(data)    case "Byte配列" => Right(data)    case "文字列" => Right(data.trim)    case "文字列_trim_無し" => Right(data)    case "文字列_trim_半角" => Right(data.trim)    case "文字列_trim_全角" => Right(trimFull(data))    case "文字列_trim_全半角" => Right(trimFullAndHalf(data))    case "全角文字列" => Right(trimFull(data))    case "全角文字列_trim_無し" => Right(data)    case "全角文字列_trim_全角" => Right(trimFull(data))    case "レコード区分_NUMBER" => convDataDiv(data)    case "レコード区分_ALPHABET" => Right(data)    case "通信方式" => convCommMthd(data, "       ")    case "識別子" => Right(data.trim)    case _ => throw new RuntimeException (s"${ ERR_MSG_INVALID_DOMAIN }:${ domain }") } }'
Detail: 'Can't OpenScope for symbol named: 'exec(scala.String,scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'trimFull' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '107'
Statement: 'def trimFull(data: String) = data.dropWhile(_ == '　').reverse.dropWhile(_ == '　').reverse'
Detail: 'Can't OpenScope for symbol named: 'trimFull(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'trimFullAndHalf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '108'
Statement: 'def trimFullAndHalf(data: String) = data.dropWhile( s =>s == '　' || s == ' ').reverse.dropWhile( s =>s == '　' || s == ' ').reverse'
Detail: 'Can't OpenScope for symbol named: 'trimFullAndHalf(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'execArrayByte' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '109'
Statement: 'def execArrayByte(domain: String, data: Array[Byte], charEnc: String) : Either[String, String] = data match {    case targetif domain == "Byte配列" => Right(new String (target, "ISO-8859-1"))    case targetif (isPd(domain) || isZd(domain)) && (isNull(target)) => exec(domain.dropRight(3), new String (data, charEnc))    case targetif (isPd(domain) || isZd(domain)) && (isEmpty(target)) => digitErrOrConvDate(domain.dropRight(3), new String (data, charEnc))    case targetif isPd(domain) && !isValidSign(target) => digitErrOrConvDate(domain.dropRight(3), new String (data, charEnc))    case targetif isPd(domain) => convDigitOrDatePd(domain.dropRight(3), target)    case targetif isZd(domain) && (!isValidSignZd(target)) => digitErrOrConvDate(domain.dropRight(3), new String (data, charEnc))    case targetif isZd(domain) => convZd(domain.dropRight(3), target)    case targetif JefConverter.isJefHalf(domain, charEnc) => exec(domain, JefConverter.convJefToUtfHalf(data))    case targetif JefConverter.isJefFull(domain, charEnc) => exec(domain, JefConverter.convJefToUtfFull(data))    case _ => exec(domain, new String (data, charEnc)) }'
Detail: 'Can't OpenScope for symbol named: 'execArrayByte(scala.String,scala.Array[scala.Byte],scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'convDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '137'
Statement: 'def convDate(data: String, min: String, max: String, format: DateTimeFormatter) = data match {    case targetif (isNull(target)) => Right(min)    case targetif (isEmpty(target)) => Right(min)    case targetif (!isDigit(target)) => Right(min)    case allNineRegex(_*) => Right(max)    case targetif (target.toLong < min.toLong) => Right(min)    case targetif (target.toLong > max.toLong) => Right(min)    case targetif (!isDate(target, format)) => Right(min)    case target => Right(target) }'
Detail: 'Can't OpenScope for symbol named: 'convDate(scala.String,scala.String,scala.String,DateTimeFormatter)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'convTimeStamp' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '150'
Statement: 'def convTimeStamp(data: String, min: String, max: String, formatter: DateTimeFormatter, format: String) = { val targetMinPad = data.padTo(format.size, '0')  val targetMaxPad = data.padTo(format.size, '9') data match {       case targetif (isNull(target)) => Right(min)       case targetif (isEmpty(target)) => Right(min)       case targetif (!isDigit(target)) => Right(min)       case allNineRegex(_*) => Right(max)       case targetif (targetMinPad.toLong <= min.toLong) => Right(min)       case targetif (targetMaxPad.toLong > max.toLong) => Right(min)       case targetif (!isDate(target, formatter)) => Right(min)       case target => Right(targetMinPad)    } }'
Detail: 'Can't OpenScope for symbol named: 'convTimeStamp(scala.String,scala.String,scala.String,DateTimeFormatter,scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'convDateParts' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '168'
Statement: 'def convDateParts(data: String, min: String, max: String) = data match {    case targetif (isNull(target)) => Right(min)    case targetif (isEmpty(target)) => Right(min)    case targetif (!isDigit(target)) => Right(min)    case allNineRegex(_*) => Right(max)    case targetif (target.toInt < min.toInt) => Right(min)    case targetif (target.toInt > max.toInt) => Right(min)    case target => Right(target) }'
Detail: 'Can't OpenScope for symbol named: 'convDateParts(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'convTime' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '181'
Statement: 'def convTime(data: String, min: String, max: String) = data match {    case targetif (isNull(target)) => Right(min)    case targetif (isEmpty(target)) => Right(min)    case targetif (!isDigit(target)) => Right(min)    case allNineRegex(_*) => Right(max)    case targetif (target.toInt < min.toInt) => Right(min)    case targetif (target.toInt > max.toInt) => Right(min)    case targetif (!isDate(target.drop(2), FORMATTER_MMSS)) => Right(min)    case target => Right(target) }'
Detail: 'Can't OpenScope for symbol named: 'convTime(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'padForDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '192'
Statement: 'def padForDate(domain: String, data: String) = domain match {    case "年月日" => zeroPadLeft(data, FORMAT_YYYYMMDD.length)    case "年月日_SL" => throw new RuntimeException (s"${ ERR_MSG_INVALID_DOMAIN }:${ domain }")    case "年月" => zeroPadLeft(data, FORMAT_YYYYMM.length)    case "月日" => zeroPadLeft(data, FORMAT_MMDD.length)    case "年" => zeroPadLeft(data, FORMAT_YYYY.length)    case "月" => zeroPadLeft(data, FORMAT_MM.length)    case "日" => zeroPadLeft(data, FORMAT_DD.length)    case "年月日時分秒" => zeroPadLeft(data, FORMAT_YYYYMMDDHHMMSS.length)    case "年月日時分ミリ秒" => zeroPadLeft(data, FORMAT_YYYYMMDDHHMMSSMS.length)    case "年月日時" => zeroPadLeft(data, FORMAT_YYYYMMDDHH.length)    case "時分秒" => zeroPadLeft(data, FORMAT_HHMMSS.length)    case "時分秒_CO" => throw new RuntimeException (s"${ ERR_MSG_INVALID_DOMAIN }:${ domain }")    case "時分ミリ秒" => zeroPadLeft(data, FORMAT_HHMMSSMS.length)    case "時分" => zeroPadLeft(data, FORMAT_HHMM.length)    case "時" => zeroPadLeft(data, FORMAT_HH.length)    case "分" => zeroPadLeft(data, FORMAT_MI.length)    case "秒" => zeroPadLeft(data, FORMAT_SS.length)    case "時間" => zeroPadLeft(data, FORMAT_HHMMSS.length)    case _ => data }'
Detail: 'Can't OpenScope for symbol named: 'padForDate(scala.String,scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'convPd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '214'
Statement: 'def convPd(domain: String, data: Array[Byte])(unpack: (Array[Byte] => String)) = { try       {exec(domain, unpack(data))}    catch {       case t:NumberFormatException => Left(ERR_MSG_INVALID_VALUE)       case t:Exception => throw t    } }'
Detail: 'Can't OpenScope for symbol named: 'convPd(scala.String,scala.Array[scala.Byte],Tuple2[lambda[Array[Byte],String]])''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'convDatePd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '223'
Statement: 'def convDatePd(domain: String, data: Array[Byte]) = { try       { val unpacked = unpackForNum(data) exec(domain, padForDate(domain, unpacked))}    catch {       case t:NumberFormatException => exec(domain, "")       case t:Exception => throw t    } }'
Detail: 'Can't OpenScope for symbol named: 'convDatePd(scala.String,scala.Array[scala.Byte])''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'convDigitOrDatePd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '233'
Statement: 'def convDigitOrDatePd(domain: String, data: Array[Byte]) = { domain match {       case "文字列" => convPd(domain, data)(unpackForStr)       case "識別子" => convPd(domain, data)(unpackForId)       case "数字" => convPd(domain, data)(unpackForNum)       case _ => convDatePd(domain, data)    } }'
Detail: 'Can't OpenScope for symbol named: 'convDigitOrDatePd(scala.String,scala.Array[scala.Byte])''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'digitErrOrConvDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '242'
Statement: 'def digitErrOrConvDate(orgDomain: String, data: String) : Either[String, String] = orgDomain match {    case "数字" => Left(ERR_MSG_INVALID_VALUE)    case _ => exec(orgDomain, data) }'
Detail: 'Can't OpenScope for symbol named: 'digitErrOrConvDate(scala.String,scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'convDataDiv' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '248'
Statement: 'def convDataDiv(data: String) = data match {    case REC_DIV_NUMBER_HEAD => Right(REC_DIV_ALPHABET_HEAD)    case REC_DIV_NUMBER_DATA => Right(REC_DIV_ALPHABET_DATA)    case REC_DIV_NUMBER_FOOT => Right(REC_DIV_ALPHABET_FOOT)    case _ => throw new RuntimeException (s"${ ERR_MSG_INVALID_DATA_DIV }:${ data }") }'
Detail: 'Can't OpenScope for symbol named: 'convDataDiv(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'convCommMthd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '255'
Statement: 'def convCommMthd(data: String, allSpace: String) = data match {    case targetif (isNull(target)) => Right("")    case targetif (isEmpty(target)) => Right("")    case _ => Right(data.trim().substring(0, 1)) }'
Detail: 'Can't OpenScope for symbol named: 'convCommMthd(scala.String,scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'convSignedDigit' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '261'
Statement: 'def convSignedDigit(data: String) = { val default = "0"  val num = if (data.isEmpty)       {       default       } else       {       data       } convDigit(Try{BigDecimal(num).toString}.getOrElse(default)) }'
Detail: 'Can't OpenScope for symbol named: 'convSignedDigit(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'unpackForNum' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '267'
Statement: 'def unpackForNum(data: Array[Byte]) = { val str = data.foldLeft(""){(l, r) =>l + f"$r%02x"}  val decimal = BigInt(str.dropRight(1))  val isMinus = str.takeRight(1) == "d" (if (isMinus)       {       -decimal       } else       {       decimal       }).toString }'
Detail: 'Can't OpenScope for symbol named: 'unpackForNum(scala.Array[scala.Byte])''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'unpackForStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '274'
Statement: 'def unpackForStr(data: Array[Byte]) = { val str = data.foldLeft(""){(l, r) =>l + f"$r%02x"}  val decimal = str.dropRight(1)  val isMinus = str.takeRight(1) == "d" (if (isMinus)       {       s"-$decimal"       } else       {       decimal       }) }'
Detail: 'Can't OpenScope for symbol named: 'unpackForStr(scala.Array[scala.Byte])''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'unpackForId' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '281'
Statement: 'def unpackForId(data: Array[Byte]) = data.foldLeft(""){(l, r) =>l + f"$r%02x"}.dropRight(1)'
Detail: 'Can't OpenScope for symbol named: 'unpackForId(scala.Array[scala.Byte])''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'zeroPadLeft' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '284'
Statement: 'def zeroPadLeft(target: String, fullLen: Int) = s"${ "0" * (fullLen - target.length()) }${ target }"'
Detail: 'Can't OpenScope for symbol named: 'zeroPadLeft(scala.String,scala.Int)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'isDate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '286'
Statement: 'def isDate(target: String, format: DateTimeFormatter) = Try(format.withZoneUTC.parseDateTime(target)).map(_ =>true).getOrElse(false)'
Detail: 'Can't OpenScope for symbol named: 'isDate(scala.String,DateTimeFormatter)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'isEmpty' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '289'
Statement: 'def isEmpty(target: String) = target.trim.isEmpty'
Detail: 'Can't OpenScope for symbol named: 'isEmpty(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'isEmpty' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '291'
Statement: 'def isEmpty(target: Array[Byte]) = { target.foldLeft(true){(l, r) =>l && r == 0x20} }'
Detail: 'Can't OpenScope for symbol named: 'isEmpty(scala.Array[scala.Byte])''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'isNull' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '295'
Statement: 'def isNull(target: Array[Byte]) = { target.foldLeft(true){(l, r) =>l && r == 0x00} }'
Detail: 'Can't OpenScope for symbol named: 'isNull(scala.Array[scala.Byte])''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'isNull' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '299'
Statement: 'def isNull(target: String) : Boolean = { isNull(target.getBytes("MS932")) }'
Detail: 'Can't OpenScope for symbol named: 'isNull(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'isDigit' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '303'
Statement: 'def isDigit(target: String) = Try(target.toLong).map(_ =>true).getOrElse(false)'
Detail: 'Can't OpenScope for symbol named: 'isDigit(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'isPd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '305'
Statement: 'def isPd(domain: String) = domain.endsWith(PD_SUFFIX)'
Detail: 'Can't OpenScope for symbol named: 'isPd(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'isZd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '306'
Statement: 'def isZd(domain: String) = domain.endsWith(ZD_SUFFIX)'
Detail: 'Can't OpenScope for symbol named: 'isZd(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'convDigit' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '308'
Statement: 'def convDigit(data: String) = data match {    case targetif (isNull(target.getBytes("MS932"))) => Right(target)    case targetif (!isDigit(target.trim())) => Left(ERR_MSG_INVALID_VALUE)    case targetif (isEmpty(target)) => Left(ERR_MSG_INVALID_VALUE)    case target => Right(target) }'
Detail: 'Can't OpenScope for symbol named: 'convDigit(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'isValidSign' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '315'
Statement: 'def isValidSign(target: Array[Byte]) = { val sign = target.last & 0x0F sign == 0x0d || sign == 0x0c || sign == 0x0f }'
Detail: 'Can't OpenScope for symbol named: 'isValidSign(scala.Array[scala.Byte])''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'isValidSignZd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '320'
Statement: 'def isValidSignZd(target: Array[Byte]) = { val sign = target.last & 0xF0 sign == 0xf0 || sign == 0xc0 || sign == 0xd0 }'
Detail: 'Can't OpenScope for symbol named: 'isValidSignZd(scala.Array[scala.Byte])''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'addPeriod' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '327'
Statement: 'def addPeriod(target: Either[String, String], pos: Int) = { target.right.map( str =>str.take(pos) + "." + str.drop(pos)) }'
Detail: 'Can't OpenScope for symbol named: 'addPeriod(Either[String,String],scala.Int)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'convZd' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '331'
Statement: 'def convZd(domain: String, data: Array[Byte]) = { try       { val unpacked = domain match {             case "文字列" => unzoneForStr(data)             case "識別子" => unzoneForId(data)             case _ => unzone(data)          } exec(domain, padForDate(domain, unpacked))}    catch {       case t:NumberFormatException => Left(ERR_MSG_INVALID_VALUE)       case t:Exception => throw t    } }'
Detail: 'Can't OpenScope for symbol named: 'convZd(scala.String,scala.Array[scala.Byte])''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'unzone' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '345'
Statement: 'def unzone(data: Array[Byte]) = { val str = data.map( x =>f"$x%02x".drop(1)).mkString  val decimal = BigInt(str)  val isMinus = data.map( x =>f"$x%02x").mkString.reverse.apply(1) == 'd' (if (isMinus)       {       -decimal       } else       {       decimal       }).toString }'
Detail: 'Can't OpenScope for symbol named: 'unzone(scala.Array[scala.Byte])''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'unzoneForStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '352'
Statement: 'def unzoneForStr(data: Array[Byte]) = { val decimal = data.map( x =>f"$x%02x".drop(1)).mkString  val isMinus = data.map( x =>f"$x%02x").mkString.reverse.apply(1) == 'd' if (isMinus)       {       s"-$decimal"       } else       {       decimal       } }'
Detail: 'Can't OpenScope for symbol named: 'unzoneForStr(scala.Array[scala.Byte])''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'unzoneForId' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/fileConv/DomainProcessor.scala'
Line number: '358'
Statement: 'def unzoneForId(data: Array[Byte]) = data.map( x =>f"$x%02x".drop(1)).mkString'
Detail: 'Can't OpenScope for symbol named: 'unzoneForId(scala.Array[scala.Byte])''
[03/24/2023 05:42:28] Info: Transformed file DomainProcessor.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfToXxx.scala'
Line number: '9'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfToXxx.scala'
Line number: '13'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfToXxx.scala'
Line number: '17'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfToXxx.scala'
Line number: '21'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/DfToXxx.scala'
Line number: '25'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Info: Transformed file DfToXxx.scala
[03/24/2023 05:42:28] Info: Transformed file TestGenerator.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/MultiPqToMapDf.scala'
Line number: '10'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Info: Transformed file MultiPqToMapDf.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'addColumnPrefix' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/TwoDfJoinToAny.scala'
Line number: '15'
Statement: 'def addColumnPrefix(name: String) = (df: DataFrame) =>{ df.schema.map( x =>df(x.name) as s"${ name }_${ x.name }") }'
Detail: 'Can't OpenScope for symbol named: 'addColumnPrefix(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'mergeWithPrefix' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/TwoDfJoinToAny.scala'
Line number: '29'
Statement: 'def mergeWithPrefix(left: DataFrame, right: DataFrame, name: String) = left("*") +: addColumnPrefix(name)(right)'
Detail: 'Can't OpenScope for symbol named: 'mergeWithPrefix(DataFrame,DataFrame,scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/TwoDfJoinToAny.scala'
Line number: '32'
Statement: 'def preExec(left: DataFrame, right: DataFrame)(implicit inArgs: InputArgs) : DataFrame = { val joined = left.join(right, joinExprs(left, right), joinType) joined.select(select(left, right).toArray :_*) }'
Detail: 'Can't OpenScope for symbol named: 'preExec(DataFrame,DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Info: Transformed file TwoDfJoinToAny.scala
[03/24/2023 05:42:28] Info: Transformed file ItemRenameRouteFinder.scala
[03/24/2023 05:42:28] Info: Transformed file TestArgs.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'fileToStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/sql/SqlGenerator.scala'
Line number: '35'
Statement: 'def fileToStr(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString'
Detail: 'Can't OpenScope for symbol named: 'fileToStr(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'generate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/sql/SqlGenerator.scala'
Line number: '38'
Statement: 'def generate(appdef: SqlDef, sqlLogic: String) = { val mainTmpl = fileToStr("genTemplates/sqlMain.tmpl")  val mainRepList = Seq( ("%%appId%%", appdef.appInfo.id),  ("%%appDesc%%", appdef.appInfo.desc),  ("%%sql%%", sqlLogic)) mainRepList.foldLeft(mainTmpl){(l, r) =>l.replaceAllLiterally(r._1, r._2) } }'
Detail: 'Can't OpenScope for symbol named: 'generate(d2k.appdefdoc.gen.sql.SqlDef,scala.String)''
[03/24/2023 05:42:28] Info: Transformed file OraLoaderHdfsTest.scala
[03/24/2023 05:42:28] Info: Transformed file SqlGenerator.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/FileToXxx.scala'
Line number: '9'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/FileToXxx.scala'
Line number: '13'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/FileToXxx.scala'
Line number: '17'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/FileToXxx.scala'
Line number: '21'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/FileToXxx.scala'
Line number: '25'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/FileToXxx.scala'
Line number: '29'
Statement: 'def exec(df: DataFrame)(implicit inArgs: InputArgs) = self.invoke(df)'
Detail: 'Can't OpenScope for symbol named: 'exec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Info: Transformed file FileToXxx.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/AnyToDf.scala'
Line number: '9'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) = df'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:28] Info: Transformed file AnyToDf.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'searchAndReplace' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/appdef/ApplicationDefGenerator.scala'
Line number: '19'
Statement: 'def searchAndReplace(target: Seq[String], searchElem: String, replaceElem: String) = { val idx = target.indexWhere(_.contains(searchElem)) target.updated(idx, replaceElem) }'
Detail: 'Can't OpenScope for symbol named: 'searchAndReplace(scala.Seq[scala.String],scala.String,scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'fileToStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/appdef/ApplicationDefGenerator.scala'
Line number: '24'
Statement: 'def fileToStr(fileName: String) = Source.fromFile(fileName)'
Detail: 'Can't OpenScope for symbol named: 'fileToStr(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'resToStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/appdef/ApplicationDefGenerator.scala'
Line number: '26'
Statement: 'def resToStr(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString'
Detail: 'Can't OpenScope for symbol named: 'resToStr(scala.String)''
[03/24/2023 05:42:28] Info: Transformed file PqCommonColumnRemoverTest.scala
[03/24/2023 05:42:28] Info: Transformed file ApplicationDefGenerator.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'createJsonPath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '23'
Statement: 'def createJsonPath(basePath: String) = Path(basePath).parent / "concept_flow" / "json"'
Detail: 'Can't OpenScope for symbol named: 'createJsonPath(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'jsStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '25'
Statement: 'def jsStr(jv: JValue)(name: String) = (jv \ name).values.toString'
Detail: 'Can't OpenScope for symbol named: 'jsStr(JValue,scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'createLinkReadJson' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '29'
Statement: 'def createLinkReadJson(jsonPath: Path) = { val linkReadPath = (jsonPath / "link_read.json").jfile Source.fromFile(linkReadPath, "MS932").getLines.map{ str => val value = jsStr(parse(str))_ LinkData(value("to_node"), value("from_node"), "input") } }'
Detail: 'Can't OpenScope for symbol named: 'createLinkReadJson(_Unresolved.Path)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'createLinkWriteJson' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '37'
Statement: 'def createLinkWriteJson(jsonPath: Path) = { val linkWritePath = (jsonPath / "link_write.json").jfile Source.fromFile(linkWritePath, "MS932").getLines.map{ str => val value = jsStr(parse(str))_ LinkData(value("from_node"), value("to_node"), "output") } }'
Detail: 'Can't OpenScope for symbol named: 'createLinkWriteJson(_Unresolved.Path)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'createNodeAppMap' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '45'
Statement: 'def createNodeAppMap(jsonPath: Path) = { val nodeAppPath = (jsonPath / "node_application.json").jfile Source.fromFile(nodeAppPath, "MS932").getLines.map{ str => val value = jsStr(parse(str))_ (value("physical_name"), NodeData(value("logical_name"), value("physical_name"), value("language"))) }.toMap }'
Detail: 'Can't OpenScope for symbol named: 'createNodeAppMap(_Unresolved.Path)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'createNodeResourceMap' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '53'
Statement: 'def createNodeResourceMap(jsonPath: Path) = { val nodeResourcePath = (jsonPath / "node_io.json").jfile Source.fromFile(nodeResourcePath, "MS932").getLines.map{ str => val value = jsStr(parse(str))_ (value("key"), NodeData(value("logical_name"), value("physical_name"), value("data_type"))) }.toMap }'
Detail: 'Can't OpenScope for symbol named: 'createNodeResourceMap(_Unresolved.Path)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'appDefList' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '61'
Statement: 'def appDefList(appBasePath: String) = { fcom.recList(new File (appBasePath)).filter(_.getName.contains("README.md")).filter( x =>fcom.appDefRegx.findFirstMatchIn(x.toString).isDefined).flatMap( x =>Try{    val appdef = AppDefParser(x.toString).get (appdef.appInfo.id, x.toString)    }.toOption.orElse{    println(s"  appDef parse error: ${ x.toString }");None    }).toList }'
Detail: 'Can't OpenScope for symbol named: 'appDefList(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'createIoData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '70'
Statement: 'def createIoData(itemNamePath: Array[(String, String)], nodeResourceMap: Map[String, NodeData])(data: Option[List[LinkData]]) = { data.map{ _.map{ ld => val path = itemNamePath.find{ x =>ld.resourceId.contains(x._1) }.map(_._2).getOrElse("") IoData( nodeResourceMap.get(ld.resourceId).map(_.physical_name).getOrElse(""),  path,  nodeResourceMap.get(ld.resourceId).map(_.dataType).getOrElse(""),  nodeResourceMap.get(ld.resourceId).map(_.logical_name).getOrElse("")) } }.getOrElse(List.empty[IoData]) }'
Detail: 'Can't OpenScope for symbol named: 'createIoData(scala.Array[scala.Tuple2[scala.String,scala.String]],scala.Map[scala.String,d2k.appdefdoc.finder.jsonbase.NodeData],scala.Option[scala.List[d2k.appdefdoc.finder.jsonbase.LinkData]])''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'createItemNamePathList' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '86'
Statement: 'def createItemNamePathList(basePath: String) = { val itemBasePath = s"${ basePath }/apps/common"  val itemNames = fcom.recList(new File (itemBasePath)).filter(_.getName.contains(".md")) itemNames.map( x =>(Path(x).name.dropRight(3), Path(x).toString.replaceAllLiterally("\\", "/"))) }'
Detail: 'Can't OpenScope for symbol named: 'createItemNamePathList(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'removePhyphen' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '92'
Statement: 'def removePhyphen(s: String) = s.replaceAllLiterally("-", "")'
Detail: 'Can't OpenScope for symbol named: 'removePhyphen(scala.String)''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'createJsonAppdef' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/jsonbase/Commons.scala'
Line number: '93'
Statement: 'def createJsonAppdef(basePath: String, itemNamePath: Array[(String, String)], nodeAppMap: Map[String, NodeData], nodeResourceMap: Map[String, NodeData], linkJson: List[LinkData]) = { val appDefResult = appDefList(s"${ basePath }/apps").toMap  val creIoData = createIoData(itemNamePath, nodeResourceMap)_ linkJson.toList.groupBy(_.appId).mapValues(_.groupBy(_.dataType)).map{    case (k, v) => AppDef(AppInfo(removePhyphen(k), nodeAppMap.get(k).map(_.logical_name).getOrElse(""), ""), None,  List.empty[ComponentDefInfo], creIoData(v.get("input")), creIoData(v.get("output")))    }.map( appdef =>(appDefResult.get(appdef.appInfo.id).getOrElse(""), appdef)) }'
Detail: 'Can't OpenScope for symbol named: 'createJsonAppdef(scala.String,scala.Array[scala.Tuple2[scala.String,scala.String]],scala.Map[scala.String,d2k.appdefdoc.finder.jsonbase.NodeData],scala.Map[scala.String,d2k.appdefdoc.finder.jsonbase.NodeData],scala.List[d2k.appdefdoc.finder.jsonbase.LinkData])''
[03/24/2023 05:42:28] Info: Transformed file Commons.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'writeFilePath' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/mixIn/OraLoaderHdfs.scala'
Line number: '11'
Statement: 'def writeFilePath(implicit inArgs: InputArgs) = sys.env("DB_LOADING_FILE_PATH")'
Detail: 'Can't OpenScope for symbol named: 'writeFilePath(d2k.common.InputArgs)''
[03/24/2023 05:42:28] Info: Transformed file OraLoaderHdfs.scala
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'readParquet' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/PqCtl.scala'
Line number: '13'
Statement: 'def readParquet(appendPath: String, strictMode: Boolean = true, readPqEmptySchema: Seq[(String, String)] = Seq.empty[(String, String)]) = { val appendPaths = appendPath.split(",").map( path =>s"${ baseParquetFilePath }/${ path.trim }") if (strictMode)       {       context.read.parquet(appendPaths :_*)       } else       {       try             {context.read.parquet(appendPaths :_*)}          catch {             case t:org.apache.spark.sql.AnalysisException => {             if (t.getMessage.startsWith("Path does not exist"))                   {                   logger.warn(s"Not Found Read Parquet[${ appendPaths.mkString(",") }]")  val schema = MakeResource.makeSchema(readPqEmptySchema.map(_._1), readPqEmptySchema.map(_._2), readPqEmptySchema.map(_ =>"10")) context.createDataFrame(context.emptyDataFrame.rdd, schema)                   } else                   {                   throw t                   }             }          }       } }'
Detail: 'Can't OpenScope for symbol named: 'readParquet(scala.String,scala.Boolean,scala.Seq[scala.Tuple2[scala.String,scala.String]])''
[03/24/2023 05:42:28] Error: An error ocurred at 'OpenScope for node with name 'writeParquet' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/PqCtl.scala'
Line number: '33'
Statement: 'def writeParquet(appendPath: String) = { df.write.mode("overwrite").parquet(s"${ baseParquetFilePath }/${ appendPath }") }'
Detail: 'Can't OpenScope for symbol named: 'writeParquet(scala.String)''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'writeParquetWithPartitionBy' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/PqCtl.scala'
Line number: '37'
Statement: 'def writeParquetWithPartitionBy(appendPath: String, partitionColumn: String*) = { try       {df.write.mode("overwrite").partitionBy(partitionColumn :_*).parquet(s"${ baseParquetFilePath }/${ appendPath }")}    catch {       case t:NullPointerException => if (df.count() > 0)          {          df.show();throw t          } else          {          println(s"${ baseParquetFilePath }/${ appendPath } IS NO RECORD")          }       case t:Throwable => throw t    } }'
Detail: 'Can't OpenScope for symbol named: 'writeParquetWithPartitionBy(scala.String,_Seq*[scala.String])''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'readParquetAndWriteParquet' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/PqCtl.scala'
Line number: '50'
Statement: 'def readParquetAndWriteParquet(readParquetPath: String, writeParquetPath: String)(proc: DataFrame => DataFrame = df =>df) = proc(readParquet(readParquetPath)).writeParquet(writeParquetPath)'
Detail: 'Can't OpenScope for symbol named: 'readParquetAndWriteParquet(scala.String,scala.String,lambda[DataFrame,DataFrame])''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'readParquetAndWriteParquetWithPartitionBy' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/PqCtl.scala'
Line number: '53'
Statement: 'def readParquetAndWriteParquetWithPartitionBy(readParquetPath: String, writeParquetPath: String, partitionColumn: String*)(proc: DataFrame => DataFrame = df =>df) = proc(readParquet(readParquetPath)).writeParquetWithPartitionBy(writeParquetPath, partitionColumn :_*)'
Detail: 'Can't OpenScope for symbol named: 'readParquetAndWriteParquetWithPartitionBy(scala.String,scala.String,_Seq*[scala.String],lambda[DataFrame,DataFrame])''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'dbToPq' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/PqCtl.scala'
Line number: '56'
Statement: 'def dbToPq(tableName: String, where: Array[String]) {    dbToPq(tableName, tableName, where, DbCtl.dbInfo1) }'
Detail: 'Can't OpenScope for symbol named: 'dbToPq(scala.String,scala.Array[scala.String])''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'dbToPq' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/PqCtl.scala'
Line number: '60'
Statement: 'def dbToPq(appendPath: String, tableName: String, where: Array[String], dbInfo: DbInfo) {    new DbCtl (dbInfo).readTable(tableName, where).writeParquet(appendPath) }'
Detail: 'Can't OpenScope for symbol named: 'dbToPq(scala.String,scala.String,scala.Array[scala.String],spark.common.DbInfo)''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'dbToPq' of type 'Mobilize.Scala.AST.SclFunDefBlock' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/spark/common/PqCtl.scala'
Line number: '64'
Statement: 'def dbToPq(appendPath: String, tableName: String, dbInfo: DbInfo) {    new DbCtl (dbInfo).readTable(tableName).writeParquet(appendPath) }'
Detail: 'Can't OpenScope for symbol named: 'dbToPq(scala.String,scala.String,spark.common.DbInfo)''
[03/24/2023 05:42:29] Info: Transformed file PqCtl.scala
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'exec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/SparkAppTest.scala'
Line number: '18'
Statement: 'def exec(implicit inArgs: InputArgs) = { val df = Seq(Aaa("a", "b")).toDF comp1.run(df) }'
Detail: 'Can't OpenScope for symbol named: 'exec(d2k.common.InputArgs)''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/test/scala/d2k/common/SparkAppTest.scala'
Line number: '25'
Statement: 'def invoke(df: DataFrame)(implicit inArgs: InputArgs) = df ~> f01'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:29] Info: Transformed file SparkAppTest.scala
[03/24/2023 05:42:29] Info: Transformed file HiRDBTest.scala
[03/24/2023 05:42:29] Info: Transformed file DbInfoTest.scala
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'preExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/FileToAny.scala'
Line number: '10'
Statement: 'def preExec(in: Unit)(implicit inArgs: InputArgs) : DataFrame = readFile'
Detail: 'Can't OpenScope for symbol named: 'preExec(scala.Unit,d2k.common.InputArgs)''
[03/24/2023 05:42:29] Info: Transformed file FileToAny.scala
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'fileToStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '16'
Statement: 'def fileToStr(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString'
Detail: 'Can't OpenScope for symbol named: 'fileToStr(scala.String)''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'mkTable' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '19'
Statement: 'def mkTable(data: String*) = data.mkString("| ", " | ", " |")'
Detail: 'Can't OpenScope for symbol named: 'mkTable(_Seq*[scala.String])''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'pathOutputString' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '21'
Statement: 'def pathOutputString(path: String) = path.replaceAllLiterally("\\", "/")'
Detail: 'Can't OpenScope for symbol named: 'pathOutputString(scala.String)''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'localPath2Url' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '23'
Statement: 'def localPath2Url(baseUrl: String, basePath: String, localPath: String) = s"${ baseUrl }/${ localPath.replaceAllLiterally("\\", "/").replaceAllLiterally(basePath.replaceAllLiterally("\\", "/"), "tree/master") }"'
Detail: 'Can't OpenScope for symbol named: 'localPath2Url(scala.String,scala.String,scala.String)''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'appDefList' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '35'
Statement: 'def appDefList(appBasePath: String) = { recList(new File (appBasePath)).filter(_.getName.contains("README.md")).filter( x =>appDefRegx.findFirstMatchIn(x.toString).isDefined).flatMap( x =>Try((x.toString, AppDefParser(x.toString).get)).toOption.orElse{    println(s"  appDef parse error: ${ x.toString }");None    }).toList }'
Detail: 'Can't OpenScope for symbol named: 'appDefList(scala.String)''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'createRrfData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '42'
Statement: 'def createRrfData(targetName: String, appDefList: Seq[(String, AppDef)]) = { appDefList.map{ x => val (path, appdef) = x  val in = appdef.inputList.map(_.id).contains(targetName)  val out = appdef.outputList.map(_.id).contains(targetName)  val containType = (in, out) match {       case (false, false) => "none"       case (true, false) => "in"       case (false, true) => "out"       case (true, true) => "io"    }  val ioData = appdef.inputList.filter(_.id == targetName).headOption.orElse(appdef.outputList.filter(_.id == targetName).headOption) RrfData(path, appdef.appInfo, ioData, containType) }.filter(_.containType != "none") }'
Detail: 'Can't OpenScope for symbol named: 'createRrfData(scala.String,Seq[Tuple2[String,AppDef]])''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'writeRrfData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '58'
Statement: 'def writeRrfData(targetName: String, baseUrl: String, basePath: String, writePath: Directory, finishMessage: Option[String], rrfData: Seq[RrfData]) = { rrfData.headOption.map{ r =>writePath.createDirectory(true, false)  val targetObj = r.ioData.get  val targetObjTitle = if (targetObj.path.isEmpty)       {       val appInfo = s"${ rrfData.head.ioData.get.id }[${ targetObj.name }]" println(s"  appDef not found: ${ appInfo }") appInfo       } else       {       val targetObjPath = s"${ basePath }/apps/common/${ targetObj.path.split("/common/")(1) }"s"[${ rrfData.head.ioData.get.id }](${ targetObjPath })[${ targetObj.name }]"       }  val targetObjUml = s"""artifact "${ targetObj.id }\\n${ targetObj.name }" as ${ targetObj.id }_res"""  val appUml = rrfData.map{ d =>s"[${ d.appInfo.id }\\n${ d.appInfo.name }] as ${ d.appInfo.id }" }  val chainUml = rrfData.map{ d =>d.containType match {       case "in" => s"${ targetObj.id }_res --> ${ d.appInfo.id } :Input"       case "out" => s"${ d.appInfo.id } --> ${ targetObj.id }_res :Output"       case "io" => s"${ targetObj.id }_res --> ${ d.appInfo.id } :Input\\n${ d.appInfo.id } --> ${ targetObj.id }_res :Output"       case _ => ""    } }  val umls = targetObjUml :: (appUml ++ chainUml).toList  def dataToTable(rrf: RrfData) = if (rrf.path.isEmpty)       {       println(s"  appDef not found: ${ rrf.appInfo.id }[${ rrf.appInfo.name }]") s"| ${ rrf.appInfo.id } | ${ rrf.appInfo.name } |"       } else       {       s"| [${ rrf.appInfo.id }](${ rrf.path }) | ${ rrf.appInfo.name } |"       }  val outputTables = rrfData.filter(_.containType == "out").map(dataToTable)  val inputTables = rrfData.filter(_.containType == "in").map(dataToTable)  val tmpl = fileToStr("finderTemplates/rrResult.tmpl")  val writeFilePath = s"${ writePath.toString }/${ targetObj.id }.md"  val writer = new FileWriter (writeFilePath)  val conved = tmpl.replaceAllLiterally("%%SearchTarget%%", targetObjTitle).replaceAllLiterally("%%ResultPlantuml%%", umls.mkString("\n")).replaceAllLiterally("%%ResultOutput%%", outputTables.mkString("\n")).replaceAllLiterally("%%ResultInput%%", inputTables.mkString("\n")) writer.write(conved) writer.close  val csvTitle = Seq("Target Name", "App Id", "App Name", "Io Type", "Url").mkString("", " , ", "\n")  val csvData = rrfData.map{ rrf => val path = if (rrf.path.isEmpty)       {       ""       } else       {       localPath2Url(baseUrl, basePath, rrf.path)       } Seq(targetName, rrf.appInfo.id, rrf.appInfo.name, rrf.containType, path).mkString(" , ") }.mkString("\n")  val writeCsvFilePath = s"${ writePath.toString }/${ targetObj.id }.csv"  val csvWriter = new FileWriterWithEncoding (writeCsvFilePath, "MS932") csvWriter.write(csvTitle) csvWriter.write(csvData) csvWriter.write("\n") csvWriter.close finishMessage.foreach( mes =>println(s"${ mes } ${ pathOutputString(writeFilePath) }")) Some(targetObj, r) }.getOrElse({    finishMessage.foreach( mes =>println(s"${ mes } Not Found Application. target[${ targetName }]"));None    }) }'
Detail: 'Can't OpenScope for symbol named: 'writeRrfData(scala.String,scala.String,scala.String,Directory,scala.Option[scala.String],scala.Seq[d2k.appdefdoc.finder.RrfData])''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'implementList' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '121'
Statement: 'def implementList(targetItemId: String, appdefList: List[(String, AppDef)]) = appdefList.flatMap{ case (appdefpath, appdef) => val compolist = appdef.componentList  val result = compolist.flatMap{ x => val path = appdefpath.dropRight(9) + x.mdName  val str = Source.fromFile(path).getLines.mkString if (str.contains(targetItemId))    Some(x.mdName) else    None } result.map{ subId => val subPath = s"${ appdefpath.dropRight(9) }/${ subId }"  val outputTable = mkTable(s"[${ appdef.appInfo.id }](${ appdefpath }) / [${ subId.dropRight(3) }](${ subPath })", appdef.appInfo.name) ((appdefpath, appdef), outputTable, subId) } }'
Detail: 'Can't OpenScope for symbol named: 'implementList(scala.String,List[Tuple2[String,AppDef]])''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'createItemDefMap' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '140'
Statement: 'def createItemDefMap(targetItemId: String, itemNames: Array[File]) = { itemNames.flatMap{ path =>Try{    val itemdef = ItemDefParser(path.toString).get  val itemDetail = itemdef.details.find(_.id == targetItemId) itemDetail.map{ item =>(itemdef.id, RirfDetail(itemdef.id, itemdef.name, item.name, path.toString)) }    }.getOrElse{    println(s"  itemDef parse error: ${ path }");None    } }.toMap }'
Detail: 'Can't OpenScope for symbol named: 'createItemDefMap(scala.String,Array[File])''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'recursiveSearch' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '153'
Statement: 'def recursiveSearch(appDefList: List[(String, AppDef)], itemDefMap: Map[String, RirfDetail], appFind: AppDef => Seq[IoData], resourceFind: AppDef => Seq[IoData], depth: Int = 0)(targetResourceId: String, targetAppId: Option[String] = None) : Seq[RirfData] = { val itemDefMapKeys = itemDefMap.keySet  val targetApp = appDefList.filter{    case (path, appdef) => appFind(appdef).exists(_.id == targetResourceId)    }  val result = targetApp.flatMap{ appdef => val resources = resourceFind(appdef._2).filter( x =>itemDefMapKeys.exists(_ == x.id))  val resResult = resources.foldLeft(Seq.empty[RirfData]){(l, r) =>if (depth > maxDepth)       {       l       } else       {       l ++ recursiveSearch(appDefList, itemDefMap, appFind, resourceFind, depth + 1)(r.id, Some(appdef._2.appInfo.id))       } } RirfData(itemDefMap.getOrElse(targetResourceId, RirfDetail(targetResourceId)), Some(RirfAppDetail(appdef._2, appdef._1)), targetAppId) +: resResult } if (result.isEmpty)       Seq(RirfData(itemDefMap.getOrElse(targetResourceId, RirfDetail(targetResourceId)), None, targetAppId)) else       result }'
Detail: 'Can't OpenScope for symbol named: 'recursiveSearch(List[Tuple2[String,AppDef]],scala.Map[scala.String,d2k.appdefdoc.finder.RirfDetail],lambda[AppDef,Seq[IoData]],lambda[AppDef,Seq[IoData]],scala.Int,scala.String,scala.Option[scala.String])''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'renameSearch' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '177'
Statement: 'def renameSearch(data: Map[String, String], targetId: String) : String = { val v = data.get(targetId) if (v.isEmpty)       targetId else       renameSearch((data - targetId), v.get) }'
Detail: 'Can't OpenScope for symbol named: 'renameSearch(scala.Map[scala.String,scala.String],scala.String)''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'componentDetailData' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '186'
Statement: 'def componentDetailData(app: (String, AppDef), targetItemId: String) = { app._2.componentList.scanLeft(RenameData("", targetItemId, targetItemId)){(l, r) => val path = app._1.dropRight(9) + r.mdName  val str = Source.fromFile(path).getLines.mkString("\n")  val regxResult = renameRegx.findAllMatchIn(str).map{ x =>(x.group(1), x.group(2))}.toMap RenameData(r.id, l.afterName, renameSearch(regxResult, l.afterName)) } }'
Detail: 'Can't OpenScope for symbol named: 'componentDetailData(Tuple2[String,AppDef],scala.String)''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'recursiveSearchWithRename' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '195'
Statement: 'def recursiveSearchWithRename(appDefList: List[(String, AppDef)], itemDefMap: Map[String, RirfDetail], appFind: AppDef => Seq[IoData], resourceFind: AppDef => Seq[IoData], targetItemId: String, itemFileNames: Array[File], depth: Int = 0)(targetResourceId: String, targetAppId: Option[String] = None) : Seq[IrrfData] = { val targetApp = appDefList.filter{    case (path, appdef) => appFind(appdef).exists(_.id == targetResourceId)    }  val result = targetApp.flatMap{ appdef => val renamedItemIdList = componentDetailData(appdef, targetItemId)  val renameComponentList = renamedItemIdList.filter( x =>!x.componentId.isEmpty && x.beforeName != x.afterName)  val filteredItemDefMap = createItemDefMap(renamedItemIdList.last.afterName, itemFileNames)  val itemDefMapKeys = filteredItemDefMap.keySet  val resources = resourceFind(appdef._2).filter( x =>itemDefMapKeys.exists(_ == x.id))  val resResult = resources.foldLeft(Seq.empty[IrrfData]){(l, r) =>if (depth > 3)       {       l       } else       {       l ++ recursiveSearchWithRename(appDefList, itemDefMap, appFind, resourceFind, targetItemId, itemFileNames, depth + 1)(r.id, Some(appdef._2.appInfo.id))       } } IrrfData(RirfData(filteredItemDefMap.getOrElse(targetResourceId, RirfDetail(targetResourceId)), Some(RirfAppDetail(appdef._2, appdef._1)), targetAppId), renameComponentList) +: resResult } if (result.isEmpty)       Seq(IrrfData(RirfData(itemDefMap.getOrElse(targetResourceId, RirfDetail(targetResourceId)), None, targetAppId), Seq.empty[RenameData])) else       result }'
Detail: 'Can't OpenScope for symbol named: 'recursiveSearchWithRename(List[Tuple2[String,AppDef]],scala.Map[scala.String,d2k.appdefdoc.finder.RirfDetail],lambda[AppDef,Seq[IoData]],lambda[AppDef,Seq[IoData]],scala.String,Array[File],scala.Int,scala.String,scala.Option[scala.String])''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'createFlowRender' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '226'
Statement: 'def createFlowRender(result: Seq[RirfData], flowLinkFunc: RirfData => Seq[String] = d =>Seq(d.appDetail.map( x =>s"${ d.resDetail.id }_res --> ${ x.appDef.appInfo.id }"), d.parentAppId.map( x =>s"${ x } --> ${ d.resDetail.id }_res")).flatten) = { val flowResult = result.flatMap{ x =>Seq(Some(RirfFlow("res", x.resDetail.id, x.resDetail.name)), x.appDetail.map( d =>RirfFlow("app", d.appDef.appInfo.id, d.appDef.appInfo.name))).flatten }  val flowObjects = flowResult.map{ x =>x.kind match {       case "app" => s"[${ x.id }\\n${ x.name }] as ${ x.id }"       case "res" => s"""artifact "${ x.id }\\n${ x.name }" as ${ x.id }_res"""    } }  val flowLinks = result.flatMap{    case d:RirfData => flowLinkFunc(d)    case _ => Seq.empty[String]    }.distinct (flowObjects ++ flowLinks).mkString("\n") }'
Detail: 'Can't OpenScope for symbol named: 'createFlowRender(scala.Seq[d2k.appdefdoc.finder.RirfData],lambda[RirfData,Seq[String]])''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'createFlowRenderWithRename' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '246'
Statement: 'def createFlowRenderWithRename(result: Seq[IrrfData], flowLinkFunc: RirfData => Seq[String] = d =>Seq(d.appDetail.map( x =>s"${ d.resDetail.id }_res --> ${ x.appDef.appInfo.id }"), d.parentAppId.map( x =>s"${ x } --> ${ d.resDetail.id }_res")).flatten) = { val flowResult = result.flatMap{ x => val beforeName = x.renameApps.headOption.map(_.beforeName).getOrElse("")  val afterName = x.renameApps.lastOption.map(_.afterName).getOrElse("") Seq( Some(IrrfFlow(RirfFlow("res", x.appData.resDetail.id, x.appData.resDetail.name), "", "")),  x.appData.appDetail.map( d =>IrrfFlow(RirfFlow("app", d.appDef.appInfo.id, d.appDef.appInfo.name), beforeName, afterName))).flatten }  val flowObjects = flowResult.map{ ir => val x = ir.rirfFlow x.kind match {       case "app"if (ir.beforeName == ir.afterName) => s"[${ x.id }\\n${ x.name }] as ${ x.id }"       case "app"if (ir.beforeName != ir.afterName) => s"[${ x.id }\\n${ x.name }] as ${ x.id }\nnote right of ${ x.id } : ${ ir.beforeName } -> ${ ir.afterName }"       case "res" => s"""artifact "${ x.id }\\n${ x.name }" as ${ x.id }_res"""    } }.distinct  val flowLinks = result.flatMap{    case d:IrrfData => flowLinkFunc(d.appData)    case _ => Seq.empty[String]    }.distinct (flowObjects ++ flowLinks).mkString("\n") }'
Detail: 'Can't OpenScope for symbol named: 'createFlowRenderWithRename(scala.Seq[d2k.appdefdoc.finder.IrrfData],lambda[RirfData,Seq[String]])''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'createReferResult' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/finder/Commons.scala'
Line number: '272'
Statement: 'def createReferResult(result: Seq[RirfData]) = { result.map{ x => val res = if (x.resDetail.path.isEmpty)       {       Seq(x.resDetail.id, x.resDetail.name)       } else       {       Seq(s"[${ x.resDetail.id }](${ x.resDetail.path })", x.resDetail.name)       }  val app = x.appDetail.map{ app =>if (app.path.isEmpty)       {       Seq(app.appDef.appInfo.id, app.appDef.appInfo.name)       } else       {       Seq(s"[${ app.appDef.appInfo.id }](${ app.path })", app.appDef.appInfo.name)       } }.getOrElse(Seq("-", "-")) (res ++ app).mkString("| ", " | ", " |") }.distinct.mkString("\n") }'
Detail: 'Can't OpenScope for symbol named: 'createReferResult(scala.Seq[d2k.appdefdoc.finder.RirfData])''
[03/24/2023 05:42:29] Info: Transformed file Commons.scala
[03/24/2023 05:42:29] Info: Transformed file ItemReferenceFinder.scala
[03/24/2023 05:42:29] Info: Transformed file GenerateTestCaseTest.scala
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'fileToStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/tmpl/TemplateCatalogGenerator.scala'
Line number: '13'
Statement: 'def fileToStr(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString'
Detail: 'Can't OpenScope for symbol named: 'fileToStr(scala.String)''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'makeTemplate' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/tmpl/TemplateCatalogGenerator.scala'
Line number: '52'
Statement: 'def makeTemplate(i: CatalogInfo, o: CatalogInfo) = { val templateName = s"${ i.name }To${ o.name }"  val fileName = s"_${ templateName }.md"  val repStr = i.data + o.data  val writer = new FileWriter ((writePath / fileName).toString) writer.write( base.replaceAll("%%templatePattern%%", templateName).replaceAll("%%insert%%", repStr)) writer.close }'
Detail: 'Can't OpenScope for symbol named: 'makeTemplate(d2k.appdefdoc.gen.tmpl.CatalogInfo,d2k.appdefdoc.gen.tmpl.CatalogInfo)''
[03/24/2023 05:42:29] Info: Transformed file TemplateCatalogGenerator.scala
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'readParquet' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/SingleReadPq.scala'
Line number: '11'
Statement: 'def readParquet(implicit inArgs: InputArgs) = readParquetSingle(readPqName)'
Detail: 'Can't OpenScope for symbol named: 'readParquet(d2k.common.InputArgs)''
[03/24/2023 05:42:29] Info: Transformed file SingleReadPq.scala
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'pre[IN]' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/D2kTest_.scala'
Line number: '97'
Statement: 'def pre [IN](component: OneInToOneOutForDf[IN, _])(setup: CTPre => IN)(check: String) = { "CT:" + testCase should {    "be success" when {       "pre" in {          ctpre.readMdTable(s"${ check }.md").checkDf(component.preExec(setup(ctpre)))          }       }    } }'
Detail: 'Can't OpenScope for symbol named: 'pre[IN](_Unresolved.OneInToOneOutForDf[IN,_],_Unresolved.lambda[CTPre,IN],scala.String)''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'pre[IN]' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/D2kTest_.scala'
Line number: '118'
Statement: 'def pre [IN](component: TwoInToOneOutForDf[IN, IN, _])(setup: CTPre => (IN, IN))(check: String) = { "CT:" + testCase should {    "be success" when {       "pre" in {          val resultDfs = setup(ctpre) ctpre.readMdTable(check).checkDf(component.preExec(resultDfs._1, resultDfs._2))          }       }    } }'
Detail: 'Can't OpenScope for symbol named: 'pre[IN](TwoInToOneOutForDf[IN,IN,_],lambda[CTPre,Tuple2[IN,IN]],scala.String)''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'postMdToDf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/D2kTest_.scala'
Line number: '133'
Statement: 'def postMdToDf(name: String) = readMdTableBase("post")(name).toDf'
Detail: 'Can't OpenScope for symbol named: 'postMdToDf(scala.String)''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'post[OUT]' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/D2kTest_.scala'
Line number: '134'
Statement: 'def post [OUT](component: OneInToOneOutForDf[_, OUT])(setup: String)(check: CTPost[OUT] => Unit) = { "CT:" + testCase should {    "be success" when {       "post" in {          check(CTPost(component.postExec(postMdToDf(setup))))          }       }    } }'
Detail: 'Can't OpenScope for symbol named: 'post[OUT](OneInToOneOutForDf[_,OUT],scala.String,lambda[CTPost[OUT],Unit])''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'post[OUT]' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/D2kTest_.scala'
Line number: '144'
Statement: 'def post [OUT](component: OneInToMapOutForDf[_, OUT])(setup: Map[String, String])(check: CTPost[OUT] => Unit) = { val mapDf = setup.mapValues{ name =>postMdToDf(name)} "CT:" + testCase should {    "be success" when {       "post" in {          check(CTPost(component.postExec(mapDf)))          }       }    } }'
Detail: 'Can't OpenScope for symbol named: 'post[OUT](OneInToMapOutForDf[_,OUT],scala.Map[scala.String,scala.String],lambda[CTPost[OUT],Unit])''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'post[OUT]' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/D2kTest_.scala'
Line number: '155'
Statement: 'def post [OUT](component: TwoInToOneOutForDf[_, _, OUT])(setup: String)(check: CTPost[OUT] => Unit) = { "CT:" + testCase should {    "be success" when {       "post" in {          check(CTPost(component.postExec(postMdToDf(setup))))          }       }    } }'
Detail: 'Can't OpenScope for symbol named: 'post[OUT](TwoInToOneOutForDf[_,_,OUT],scala.String,lambda[CTPost[OUT],Unit])''
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'apply' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/app/test/common/D2kTest_.scala'
Line number: '170'
Statement: 'def apply(target: Seq[Editors]) = { val mapTarget = target.map( t =>(t.colName, t)).toMap  val targetPath = (mdPath / testCase) if (!targetPath.isDirectory)       throw new FileNotFoundException (targetPath.toString)  val fileInfos = targetPath.walk.map{ path => val mdStr = Source.fromFile(path.toString).mkString  val splitted = mdStr.split("# expect") FileInfo(path, makeRes.MdInfo(splitted(0).split("# input")(1)), makeRes.MdInfo(splitted(1))) } s"FT:${ componentName }:${ testCase }" should {    "be success" when {       fileInfos.foreach{ fi => val targetColumn = Option(mapTarget(fi.name)).flatMap{          case e:Edit => Option(e.editor)          case _ => None          }.get  val inputDf = if (fi.inMdData.data.replaceAll("\n", "").trim.isEmpty)             {             Seq(DummyDf("")).toDF             } else             {             fi.inMdData.toDf             }  val result = inputDf.select(targetColumn as fi.name) s"${ fi.no }:${ fi.name }" in {          withClue(fi.no){             val outputPos = fi.inMdData.data.count(_ == '\n') fi.outMdData.checkDf(result, outputPos)             }          } }       }    } }'
Detail: 'Can't OpenScope for symbol named: 'apply(scala.Seq[spark.common.DfCtl.Editors])''
[03/24/2023 05:42:29] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:29] Error: Expression of type 'Mobilize.Scala.AST.SclTailExprArgumentExprs' is not supported
[03/24/2023 05:42:29] Info: Transformed file WriteFilePartitionTest.scala
[03/24/2023 05:42:29] Info: Transformed file D2kTest_.scala
[03/24/2023 05:42:29] Info: Transformed file OraLoaderTest.scala
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'invoke' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/executor/face/DomainConverter.scala'
Line number: '17'
Statement: 'def invoke(orgDf: DataFrame)(implicit inArgs: InputArgs) : DataFrame = targetColumns.foldLeft(orgDf.na.fill("", targetColumns.map(_._1).toSeq)){(df, t) => val (name, domain) = t  val convedColumn = domain match {    case "年月日" => MakeDate.date_yyyyMMdd(domainConvert(col(name), lit(domain))).cast("date")    case "年月日時分秒" => MakeDate.timestamp_yyyyMMddhhmmss(domainConvert(col(name), lit(domain))).cast("timestamp")    case "年月日時分ミリ秒" => MakeDate.timestamp_yyyyMMddhhmmssSSS(domainConvert(col(name), lit(domain))).cast("timestamp")    case _ => domainConvert(col(name), lit(domain)) } df.withColumn(name, convedColumn) }'
Detail: 'Can't OpenScope for symbol named: 'invoke(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:29] Info: Transformed file DomainConverter.scala
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'postExec' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/common/df/template/base/TwoAnyToDf.scala'
Line number: '9'
Statement: 'def postExec(df: DataFrame)(implicit inArgs: InputArgs) = df'
Detail: 'Can't OpenScope for symbol named: 'postExec(DataFrame,d2k.common.InputArgs)''
[03/24/2023 05:42:29] Info: Transformed file TwoAnyToDf.scala
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'fileToStr' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/src/SourceGenerator.scala'
Line number: '40'
Statement: 'def fileToStr(fileName: String) = Source.fromInputStream(getClass.getClassLoader.getResourceAsStream(fileName)).mkString'
Detail: 'Can't OpenScope for symbol named: 'fileToStr(scala.String)''
[03/24/2023 05:42:29] Info: Transformed file DomainConvereterTest.scala
[03/24/2023 05:42:29] Error: An error ocurred at 'OpenScope for node with name 'generateItemConf' of type 'Mobilize.Scala.AST.SclFunDefExpr' in '' package.
The node and its children will be skipped.
Location: 'src/main/scala/d2k/appdefdoc/gen/src/SourceGenerator.scala'
Line number: '134'
Statement: 'def generateItemConf(baseUrl: String, branch: String, appGroup: String, appId: String, writePath: Directory)(inputFiles: Seq[IoData]) = { val fileHeader = Seq("itemId", "itemName", "length", "cnvType", "extractTarget", "comment").mkString("\t")  val writeConfPath = Directory(s"${ writePath }/itemConf") writeConfPath.createDirectory(true, false) inputFiles.map{ iodata =>println(s"  Parsing ${ iodata.id }[${ iodata.name }](${ iodata.path })")  val filePath = iodata.path.split('/').takeRight(2).mkString("/")  val itemdef = ItemDefParser(baseUrl, branch, filePath).get  val itemDetails = itemdef.details.map{ d =>Seq(d.id, d.name, d.size, d.dataType, "false", "").mkString("\t") }  val outputList = fileHeader :: itemDetails  val writeFilePath = s"${ writeConfPath }/${ appGroup }_items_${ appId }_${ iodata.id }.conf"  val writer = new FileWriterWithEncoding (writeFilePath, "MS932") writer.write(outputList.mkString("\r\n")) writer.write("\r\n") writer.close println(s"  [generate itemConf] ${ writeFilePath.replaceAllLiterally("\\", "/") }") } }'
Detail: 'Can't OpenScope for symbol named: 'generateItemConf(scala.String,scala.String,scala.String,scala.String,Directory,Seq[IoData])''
[03/24/2023 05:42:29] Info: Transformed file SourceGenerator.scala
[03/24/2023 05:42:29] Info: Transformed file WriteDbTest.scala
[03/24/2023 05:42:29] Info: Step 8/9 - Conversion Execution: COMPLETED
[03/24/2023 05:42:29] Debug: TaskParam issuesReader = Mobilize.SparkCommon.Reports.IssuesReader
[03/24/2023 05:42:29] Debug: TaskParam genericScannerReader = Mobilize.SparkSnow.Assessment.GenericScannerReader
[03/24/2023 05:42:29] Debug: TaskParam initialTime = 03/24/2023 05:41:40
[03/24/2023 05:42:29] Debug: TaskParam reportPath = /tmp/Output-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Reports/SparkSnowConvert
[03/24/2023 05:42:29] Debug: TaskParam sparkUsagesInventoryStorage = Mobilize.SparkSnow.Assessment.SparkUsagesInventoryStorage
[03/24/2023 05:42:29] Debug: TaskParam importUsagesInventoryStorage = Mobilize.SparkCommon.Assessment.ImportUsagesInventoryStorage
[03/24/2023 05:42:29] Debug: TaskParam sqlExtractionReportStorage = Mobilize.SparkSnow.Assessment.SqlExtractionReportStorage
[03/24/2023 05:42:29] Debug: TaskParam writeResultsProgressDescriptor = Mobilize.Common.Utils.Progress.SingleProgressDescriptor
[03/24/2023 05:42:29] Debug: TaskParam inputPath = /tmp/Repo-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Sources
[03/24/2023 05:42:29] Debug: TaskParam outputPath = /tmp/Output-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/Output/SparkSnowConvert
[03/24/2023 05:42:29] Debug: TaskParam sparkSnowExecutionResult = Mobilize.SparkSnow.Configuration.Models.SparkSnowExecutionResult
[03/24/2023 05:42:29] Debug: TaskParam assessmentReportSummaryInformation = Mobilize.SparkCommon.Reports.AssessmentReportSummaryInformation
[03/24/2023 05:42:29] Debug: TaskParam summaryReportInformation = Mobilize.SparkCommon.Reports.SummaryReportInformation
[03/24/2023 05:42:29] Debug: TaskParam assessmentManager = Mobilize.SparkCommon.Reports.AssessmentManager
[03/24/2023 05:42:29] Debug: TaskParam usagesCalculator = Mobilize.SparkCommon.Reports.SparkUsagesCalculator
[03/24/2023 05:42:29] Debug: TaskParam importsCalculator = Mobilize.SparkCommon.Reports.ImportUsagesCalculator
[03/24/2023 05:42:29] Debug: TaskParam snowConvertCoreVersion = 1.01.039
[03/24/2023 05:42:29] Debug: TaskParam sessionId = 589b8506-ce73-40ee-9bc0-d938649a9163
[03/24/2023 05:42:29] Debug: TaskParam ConversionEndTask.Enabled = True
[03/24/2023 05:42:29] Debug: TaskParam Repository = Artinsoft.Common.Store.Repository
[03/24/2023 05:42:29] Debug: TaskParam RepositoryDirectory = /tmp/Output-77c8fe1a-649e-45e2-9dc0-c8386908d0f3/.mobilize/CommonEF
[03/24/2023 05:42:29] Info: Step 9/9 - Cleanup: STARTED
[03/24/2023 05:42:29] Info: EFProcess 638152333022131823 ended. Excecution time: 00:00:47.6279135
[03/24/2023 05:42:29] Info: Reports generation: STARTED
[03/24/2023 05:42:29] Info: Starting the assessment docx report generation
[03/24/2023 05:42:30] Info: Writing the assessment docx report
[03/24/2023 05:42:30] Info: Finished the assessment docx report generation
[03/24/2023 05:42:30] Info: Starting the summary assessment docx report generation
[03/24/2023 05:42:30] Info: Writing the summary assessment docx report
[03/24/2023 05:42:30] Info: Finished the summary assessment docx report generation
[03/24/2023 05:42:30] Info: Starting the summary HTML report generation
[03/24/2023 05:42:30] Info: Writing the summary HTML report
[03/24/2023 05:42:30] Info: Finished the summary HTML report generation
[03/24/2023 05:42:30] Info: Starting the detailed HTML report generation
[03/24/2023 05:42:30] Info: Writing the detailed HTML report
[03/24/2023 05:42:30] Info: Finished the detailed HTML report generation
[03/24/2023 05:42:30] Info: Reports generation: COMPLETED
